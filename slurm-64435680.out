SLURM_JOB_ID: 64435680
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Thu May  1 11:40:00 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:40:16,265][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:40:16,265][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:40:16,265][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:40:16,265][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:40:16,269][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 11:40:16,270][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:40:18,264][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:40:20,510][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:40:20,511][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,649][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,678][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,773][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 11:40:20,780][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,781][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 11:40:20,781][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,820][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,831][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 11:40:20,832][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,832][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 11:40:20,833][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,848][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,869][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,879][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 11:40:20,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,880][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 11:40:20,881][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,882][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 11:40:20,882][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,883][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 11:40:20,883][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,884][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 11:40:20,884][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:40:20,885][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:40:20,885][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:40:24,798][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:40:24,803][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:40:24,804][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:40:24,804][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:40:24,804][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 11:40:24,805][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:40:24,805][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.8145Epoch 1/10: [                              ] 2/63 batches, loss: 0.8446Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7718Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7524Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7532Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7585Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7449Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7424Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7300Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7407Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7360Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7349Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7268Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7327Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7320Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7330Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7372Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7427Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7398Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7346Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7308Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7302Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7258Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7253Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7248Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7245Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7265Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7232Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7250Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7245Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7214Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7210Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7208Epoch 1/10: [================              ] 34/63 batches, loss: 0.7221Epoch 1/10: [================              ] 35/63 batches, loss: 0.7219Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7234Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7241Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7213Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7219Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7217Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7207Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7211Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7201Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7200Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7206Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7215Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7212Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7190Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7185Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7166Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7153Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7172Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7159Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7157Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7160Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7148Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7164Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7137Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7126Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7117Epoch 1/10: [==============================] 63/63 batches, loss: 0.7132
[2025-05-01 11:40:33,805][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7132
[2025-05-01 11:40:33,982][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7450, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961, 'precision': 0.6451612903225806, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7625Epoch 2/10: [                              ] 2/63 batches, loss: 0.7733Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7537Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7262Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7238Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7200Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7277Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7272Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7212Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7156Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7228Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7157Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7044Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6984Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6949Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6984Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6938Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6917Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6919Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6906Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6866Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6902Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6886Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6851Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6877Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6925Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6933Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6917Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6880Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6829Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6802Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6788Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6767Epoch 2/10: [================              ] 34/63 batches, loss: 0.6758Epoch 2/10: [================              ] 35/63 batches, loss: 0.6757Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6719Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6684Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6657Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6629Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6609Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6594Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6601Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6576Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6539Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6510Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6495Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6459Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6408Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6394Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6372Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6370Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6357Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6345Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6338Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6323Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6305Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6284Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6270Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6262Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6235Epoch 2/10: [==============================] 63/63 batches, loss: 0.6238
[2025-05-01 11:40:40,671][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6238
[2025-05-01 11:40:40,850][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5717, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6418Epoch 3/10: [                              ] 2/63 batches, loss: 0.6040Epoch 3/10: [=                             ] 3/63 batches, loss: 0.5731Epoch 3/10: [=                             ] 4/63 batches, loss: 0.5429Epoch 3/10: [==                            ] 5/63 batches, loss: 0.5245Epoch 3/10: [==                            ] 6/63 batches, loss: 0.5065Epoch 3/10: [===                           ] 7/63 batches, loss: 0.5174Epoch 3/10: [===                           ] 8/63 batches, loss: 0.5236Epoch 3/10: [====                          ] 9/63 batches, loss: 0.5277Epoch 3/10: [====                          ] 10/63 batches, loss: 0.5290Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.5317Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.5225Epoch 3/10: [======                        ] 13/63 batches, loss: 0.5197Epoch 3/10: [======                        ] 14/63 batches, loss: 0.5229Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.5190Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.5170Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5166Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5137Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5125Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5101Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5132Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5148Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5146Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5160Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5191Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5195Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5167Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5141Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5139Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5144Epoch 3/10: [================              ] 34/63 batches, loss: 0.5156Epoch 3/10: [================              ] 35/63 batches, loss: 0.5174Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5164Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5162Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5159Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5138Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5137Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5140Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5139Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5125Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5146Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5133Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5137Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5130Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5138Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5137Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5130Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5115Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5114Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5122Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5120Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5136Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5131Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5125Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5123Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5126Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5121Epoch 3/10: [============================= ] 61/63 batches, loss: 0.5128Epoch 3/10: [============================= ] 62/63 batches, loss: 0.5115Epoch 3/10: [==============================] 63/63 batches, loss: 0.5144
[2025-05-01 11:40:47,593][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5144
[2025-05-01 11:40:47,784][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5252, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.4093Epoch 4/10: [                              ] 2/63 batches, loss: 0.4450Epoch 4/10: [=                             ] 3/63 batches, loss: 0.4568Epoch 4/10: [=                             ] 4/63 batches, loss: 0.4627Epoch 4/10: [==                            ] 5/63 batches, loss: 0.4666Epoch 4/10: [==                            ] 6/63 batches, loss: 0.4730Epoch 4/10: [===                           ] 7/63 batches, loss: 0.4810Epoch 4/10: [===                           ] 8/63 batches, loss: 0.4779Epoch 4/10: [====                          ] 9/63 batches, loss: 0.4808Epoch 4/10: [====                          ] 10/63 batches, loss: 0.4856Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.4808Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.4828Epoch 4/10: [======                        ] 13/63 batches, loss: 0.4863Epoch 4/10: [======                        ] 14/63 batches, loss: 0.4876Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.4887Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.4881Epoch 4/10: [========                      ] 17/63 batches, loss: 0.4878Epoch 4/10: [========                      ] 18/63 batches, loss: 0.4860Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.4882Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.4878Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.4897Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.4936Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.4967Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.4989Epoch 4/10: [============                  ] 26/63 batches, loss: 0.4973Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4967Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.5004Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4981Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4991Epoch 4/10: [==============                ] 31/63 batches, loss: 0.5000Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4994Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4989Epoch 4/10: [================              ] 34/63 batches, loss: 0.5006Epoch 4/10: [================              ] 35/63 batches, loss: 0.5007Epoch 4/10: [=================             ] 36/63 batches, loss: 0.5008Epoch 4/10: [=================             ] 37/63 batches, loss: 0.5028Epoch 4/10: [==================            ] 38/63 batches, loss: 0.5029Epoch 4/10: [==================            ] 39/63 batches, loss: 0.5041Epoch 4/10: [===================           ] 40/63 batches, loss: 0.5036Epoch 4/10: [===================           ] 41/63 batches, loss: 0.5053Epoch 4/10: [====================          ] 42/63 batches, loss: 0.5047Epoch 4/10: [====================          ] 43/63 batches, loss: 0.5059Epoch 4/10: [====================          ] 44/63 batches, loss: 0.5064Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.5074Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.5089Epoch 4/10: [======================        ] 47/63 batches, loss: 0.5073Epoch 4/10: [======================        ] 48/63 batches, loss: 0.5052Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.5047Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.5047Epoch 4/10: [========================      ] 51/63 batches, loss: 0.5056Epoch 4/10: [========================      ] 52/63 batches, loss: 0.5042Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.5047Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.5051Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.5047Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.5051Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.5063Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.5063Epoch 4/10: [============================  ] 59/63 batches, loss: 0.5067Epoch 4/10: [============================  ] 60/63 batches, loss: 0.5066Epoch 4/10: [============================= ] 61/63 batches, loss: 0.5058Epoch 4/10: [============================= ] 62/63 batches, loss: 0.5058Epoch 4/10: [==============================] 63/63 batches, loss: 0.5067
[2025-05-01 11:40:54,489][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5067
[2025-05-01 11:40:54,680][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5243, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6225Epoch 5/10: [                              ] 2/63 batches, loss: 0.6108Epoch 5/10: [=                             ] 3/63 batches, loss: 0.5751Epoch 5/10: [=                             ] 4/63 batches, loss: 0.5577Epoch 5/10: [==                            ] 5/63 batches, loss: 0.5426Epoch 5/10: [==                            ] 6/63 batches, loss: 0.5322Epoch 5/10: [===                           ] 7/63 batches, loss: 0.5180Epoch 5/10: [===                           ] 8/63 batches, loss: 0.5192Epoch 5/10: [====                          ] 9/63 batches, loss: 0.5176Epoch 5/10: [====                          ] 10/63 batches, loss: 0.5186Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.5152Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.5143Epoch 5/10: [======                        ] 13/63 batches, loss: 0.5135Epoch 5/10: [======                        ] 14/63 batches, loss: 0.5128Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.5072Epoch 5/10: [========                      ] 17/63 batches, loss: 0.5112Epoch 5/10: [========                      ] 18/63 batches, loss: 0.5095Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.5079Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.5018Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.5019Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.4998Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.5021Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.5022Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.5013Epoch 5/10: [============                  ] 26/63 batches, loss: 0.5023Epoch 5/10: [============                  ] 27/63 batches, loss: 0.5024Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.5016Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.5041Epoch 5/10: [==============                ] 30/63 batches, loss: 0.5065Epoch 5/10: [==============                ] 31/63 batches, loss: 0.5087Epoch 5/10: [===============               ] 32/63 batches, loss: 0.5093Epoch 5/10: [===============               ] 33/63 batches, loss: 0.5084Epoch 5/10: [================              ] 34/63 batches, loss: 0.5082Epoch 5/10: [================              ] 35/63 batches, loss: 0.5095Epoch 5/10: [=================             ] 36/63 batches, loss: 0.5060Epoch 5/10: [=================             ] 37/63 batches, loss: 0.5053Epoch 5/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 5/10: [==================            ] 39/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 40/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 41/63 batches, loss: 0.5057Epoch 5/10: [====================          ] 42/63 batches, loss: 0.5046Epoch 5/10: [====================          ] 43/63 batches, loss: 0.5023Epoch 5/10: [====================          ] 44/63 batches, loss: 0.5040Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.5035Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.5040Epoch 5/10: [======================        ] 47/63 batches, loss: 0.5045Epoch 5/10: [======================        ] 48/63 batches, loss: 0.5055Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.5050Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.5040Epoch 5/10: [========================      ] 51/63 batches, loss: 0.5049Epoch 5/10: [========================      ] 52/63 batches, loss: 0.5063Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.5073Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.5068Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.5063Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.5062Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.5074Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.5070Epoch 5/10: [============================  ] 59/63 batches, loss: 0.5065Epoch 5/10: [============================  ] 60/63 batches, loss: 0.5061Epoch 5/10: [============================= ] 61/63 batches, loss: 0.5053Epoch 5/10: [============================= ] 62/63 batches, loss: 0.5053Epoch 5/10: [==============================] 63/63 batches, loss: 0.5042
[2025-05-01 11:41:01,369][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5042
[2025-05-01 11:41:01,565][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5520, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 11:41:01,565][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.5647Epoch 6/10: [                              ] 2/63 batches, loss: 0.5578Epoch 6/10: [=                             ] 3/63 batches, loss: 0.5329Epoch 6/10: [=                             ] 4/63 batches, loss: 0.5315Epoch 6/10: [==                            ] 5/63 batches, loss: 0.5164Epoch 6/10: [==                            ] 6/63 batches, loss: 0.5104Epoch 6/10: [===                           ] 7/63 batches, loss: 0.5196Epoch 6/10: [===                           ] 8/63 batches, loss: 0.5116Epoch 6/10: [====                          ] 9/63 batches, loss: 0.5071Epoch 6/10: [====                          ] 10/63 batches, loss: 0.5068Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.5003Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.5026Epoch 6/10: [======                        ] 13/63 batches, loss: 0.5045Epoch 6/10: [======                        ] 14/63 batches, loss: 0.5044Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.5087Epoch 6/10: [========                      ] 17/63 batches, loss: 0.5070Epoch 6/10: [========                      ] 18/63 batches, loss: 0.5082Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.5117Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.5137Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.5155Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.5139Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.5120Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.5079Epoch 6/10: [============                  ] 26/63 batches, loss: 0.5059Epoch 6/10: [============                  ] 27/63 batches, loss: 0.5076Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.5074Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.5089Epoch 6/10: [==============                ] 30/63 batches, loss: 0.5056Epoch 6/10: [==============                ] 31/63 batches, loss: 0.5040Epoch 6/10: [===============               ] 32/63 batches, loss: 0.5062Epoch 6/10: [===============               ] 33/63 batches, loss: 0.5061Epoch 6/10: [================              ] 34/63 batches, loss: 0.5089Epoch 6/10: [================              ] 35/63 batches, loss: 0.5094Epoch 6/10: [=================             ] 36/63 batches, loss: 0.5099Epoch 6/10: [=================             ] 37/63 batches, loss: 0.5097Epoch 6/10: [==================            ] 38/63 batches, loss: 0.5102Epoch 6/10: [==================            ] 39/63 batches, loss: 0.5088Epoch 6/10: [===================           ] 40/63 batches, loss: 0.5093Epoch 6/10: [===================           ] 41/63 batches, loss: 0.5097Epoch 6/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 6/10: [====================          ] 43/63 batches, loss: 0.5095Epoch 6/10: [====================          ] 44/63 batches, loss: 0.5099Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.5102Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.5101Epoch 6/10: [======================        ] 47/63 batches, loss: 0.5115Epoch 6/10: [======================        ] 48/63 batches, loss: 0.5118Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.5107Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.5105Epoch 6/10: [========================      ] 51/63 batches, loss: 0.5099Epoch 6/10: [========================      ] 52/63 batches, loss: 0.5084Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.5074Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.5069Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.5064Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.5077Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.5080Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.5059Epoch 6/10: [============================  ] 59/63 batches, loss: 0.5055Epoch 6/10: [============================  ] 60/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 61/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 62/63 batches, loss: 0.5054Epoch 6/10: [==============================] 63/63 batches, loss: 0.5064
[2025-05-01 11:41:07,928][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5064
[2025-05-01 11:41:08,121][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5391, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561, 'precision': 0.9523809523809523, 'recall': 1.0}
[2025-05-01 11:41:08,122][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.4559Epoch 7/10: [                              ] 2/63 batches, loss: 0.5157Epoch 7/10: [=                             ] 3/63 batches, loss: 0.5037Epoch 7/10: [=                             ] 4/63 batches, loss: 0.5037Epoch 7/10: [==                            ] 5/63 batches, loss: 0.4941Epoch 7/10: [==                            ] 6/63 batches, loss: 0.4918Epoch 7/10: [===                           ] 7/63 batches, loss: 0.4907Epoch 7/10: [===                           ] 8/63 batches, loss: 0.4923Epoch 7/10: [====                          ] 9/63 batches, loss: 0.4883Epoch 7/10: [====                          ] 10/63 batches, loss: 0.4947Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.4976Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.5061Epoch 7/10: [======                        ] 13/63 batches, loss: 0.5095Epoch 7/10: [======                        ] 14/63 batches, loss: 0.5041Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.5025Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.4996Epoch 7/10: [========                      ] 17/63 batches, loss: 0.5013Epoch 7/10: [========                      ] 18/63 batches, loss: 0.4987Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.4965Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.4968Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.4960Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.4974Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.4967Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.4990Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 26/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 27/63 batches, loss: 0.5012Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.5013Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.5030Epoch 7/10: [==============                ] 30/63 batches, loss: 0.5046Epoch 7/10: [==============                ] 31/63 batches, loss: 0.5061Epoch 7/10: [===============               ] 32/63 batches, loss: 0.5069Epoch 7/10: [===============               ] 33/63 batches, loss: 0.5090Epoch 7/10: [================              ] 34/63 batches, loss: 0.5074Epoch 7/10: [================              ] 35/63 batches, loss: 0.5060Epoch 7/10: [=================             ] 36/63 batches, loss: 0.5073Epoch 7/10: [=================             ] 37/63 batches, loss: 0.5065Epoch 7/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 7/10: [==================            ] 39/63 batches, loss: 0.5045Epoch 7/10: [===================           ] 40/63 batches, loss: 0.5057Epoch 7/10: [===================           ] 41/63 batches, loss: 0.5051Epoch 7/10: [====================          ] 42/63 batches, loss: 0.5067Epoch 7/10: [====================          ] 43/63 batches, loss: 0.5072Epoch 7/10: [====================          ] 44/63 batches, loss: 0.5077Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.5092Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.5080Epoch 7/10: [======================        ] 47/63 batches, loss: 0.5089Epoch 7/10: [======================        ] 48/63 batches, loss: 0.5083Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.5077Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 7/10: [========================      ] 51/63 batches, loss: 0.5066Epoch 7/10: [========================      ] 52/63 batches, loss: 0.5066Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.5061Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.5047Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.5051Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.5042Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.5046Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.5050Epoch 7/10: [============================  ] 59/63 batches, loss: 0.5046Epoch 7/10: [============================  ] 60/63 batches, loss: 0.5042Epoch 7/10: [============================= ] 61/63 batches, loss: 0.5046Epoch 7/10: [============================= ] 62/63 batches, loss: 0.5038Epoch 7/10: [==============================] 63/63 batches, loss: 0.5048
[2025-05-01 11:41:14,488][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5048
[2025-05-01 11:41:14,674][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5218, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.5040Epoch 8/10: [                              ] 2/63 batches, loss: 0.5392Epoch 8/10: [=                             ] 3/63 batches, loss: 0.5116Epoch 8/10: [=                             ] 4/63 batches, loss: 0.5333Epoch 8/10: [==                            ] 5/63 batches, loss: 0.5131Epoch 8/10: [==                            ] 6/63 batches, loss: 0.5114Epoch 8/10: [===                           ] 7/63 batches, loss: 0.5137Epoch 8/10: [===                           ] 8/63 batches, loss: 0.5065Epoch 8/10: [====                          ] 9/63 batches, loss: 0.5009Epoch 8/10: [====                          ] 10/63 batches, loss: 0.4988Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.4970Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.4996Epoch 8/10: [======                        ] 13/63 batches, loss: 0.4926Epoch 8/10: [======                        ] 14/63 batches, loss: 0.4917Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.4941Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.4888Epoch 8/10: [========                      ] 17/63 batches, loss: 0.4883Epoch 8/10: [========                      ] 18/63 batches, loss: 0.4931Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.4936Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.4977Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.4979Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.4960Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.4947Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.4969Epoch 8/10: [============                  ] 26/63 batches, loss: 0.4954Epoch 8/10: [============                  ] 27/63 batches, loss: 0.4948Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.4952Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 30/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 31/63 batches, loss: 0.4973Epoch 8/10: [===============               ] 32/63 batches, loss: 0.4960Epoch 8/10: [===============               ] 33/63 batches, loss: 0.4941Epoch 8/10: [================              ] 34/63 batches, loss: 0.4951Epoch 8/10: [================              ] 35/63 batches, loss: 0.4953Epoch 8/10: [=================             ] 36/63 batches, loss: 0.4949Epoch 8/10: [=================             ] 37/63 batches, loss: 0.4964Epoch 8/10: [==================            ] 38/63 batches, loss: 0.4972Epoch 8/10: [==================            ] 39/63 batches, loss: 0.4992Epoch 8/10: [===================           ] 40/63 batches, loss: 0.5011Epoch 8/10: [===================           ] 41/63 batches, loss: 0.5012Epoch 8/10: [====================          ] 42/63 batches, loss: 0.5013Epoch 8/10: [====================          ] 43/63 batches, loss: 0.5019Epoch 8/10: [====================          ] 44/63 batches, loss: 0.5030Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.5036Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.5025Epoch 8/10: [======================        ] 47/63 batches, loss: 0.5036Epoch 8/10: [======================        ] 48/63 batches, loss: 0.5036Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.5022Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.5013Epoch 8/10: [========================      ] 51/63 batches, loss: 0.5024Epoch 8/10: [========================      ] 52/63 batches, loss: 0.5019Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.5015Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.5011Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.5016Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.5008Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.5009Epoch 8/10: [============================  ] 59/63 batches, loss: 0.5013Epoch 8/10: [============================  ] 60/63 batches, loss: 0.5017Epoch 8/10: [============================= ] 61/63 batches, loss: 0.5037Epoch 8/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 8/10: [==============================] 63/63 batches, loss: 0.5071
[2025-05-01 11:41:21,444][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5071
[2025-05-01 11:41:21,644][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5239, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:21,645][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.4561Epoch 9/10: [                              ] 2/63 batches, loss: 0.5154Epoch 9/10: [=                             ] 3/63 batches, loss: 0.5036Epoch 9/10: [=                             ] 4/63 batches, loss: 0.5214Epoch 9/10: [==                            ] 5/63 batches, loss: 0.5225Epoch 9/10: [==                            ] 6/63 batches, loss: 0.5272Epoch 9/10: [===                           ] 7/63 batches, loss: 0.5102Epoch 9/10: [===                           ] 8/63 batches, loss: 0.5005Epoch 9/10: [====                          ] 9/63 batches, loss: 0.5061Epoch 9/10: [====                          ] 10/63 batches, loss: 0.5034Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.5014Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.5075Epoch 9/10: [======                        ] 13/63 batches, loss: 0.5090Epoch 9/10: [======                        ] 14/63 batches, loss: 0.5069Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.5082Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.5079Epoch 9/10: [========                      ] 17/63 batches, loss: 0.5077Epoch 9/10: [========                      ] 18/63 batches, loss: 0.5062Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.5073Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.5083Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.5092Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.5100Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.5107Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.5075Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.5054Epoch 9/10: [============                  ] 26/63 batches, loss: 0.5044Epoch 9/10: [============                  ] 27/63 batches, loss: 0.5026Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.5035Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.5018Epoch 9/10: [==============                ] 30/63 batches, loss: 0.5019Epoch 9/10: [==============                ] 31/63 batches, loss: 0.5019Epoch 9/10: [===============               ] 32/63 batches, loss: 0.5042Epoch 9/10: [===============               ] 33/63 batches, loss: 0.5049Epoch 9/10: [================              ] 34/63 batches, loss: 0.5070Epoch 9/10: [================              ] 35/63 batches, loss: 0.5055Epoch 9/10: [=================             ] 36/63 batches, loss: 0.5081Epoch 9/10: [=================             ] 37/63 batches, loss: 0.5079Epoch 9/10: [==================            ] 38/63 batches, loss: 0.5091Epoch 9/10: [==================            ] 39/63 batches, loss: 0.5077Epoch 9/10: [===================           ] 40/63 batches, loss: 0.5094Epoch 9/10: [===================           ] 41/63 batches, loss: 0.5104Epoch 9/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 9/10: [====================          ] 43/63 batches, loss: 0.5084Epoch 9/10: [====================          ] 44/63 batches, loss: 0.5094Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.5098Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.5096Epoch 9/10: [======================        ] 47/63 batches, loss: 0.5085Epoch 9/10: [======================        ] 48/63 batches, loss: 0.5079Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.5078Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 51/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 52/63 batches, loss: 0.5067Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.5062Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.5061Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.5052Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.5052Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.5060Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.5052Epoch 9/10: [============================  ] 59/63 batches, loss: 0.5048Epoch 9/10: [============================  ] 60/63 batches, loss: 0.5047Epoch 9/10: [============================= ] 61/63 batches, loss: 0.5043Epoch 9/10: [============================= ] 62/63 batches, loss: 0.5036Epoch 9/10: [==============================] 63/63 batches, loss: 0.5046
[2025-05-01 11:41:27,997][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5046
[2025-05-01 11:41:28,189][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5224, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:28,189][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.5508Epoch 10/10: [                              ] 2/63 batches, loss: 0.5033Epoch 10/10: [=                             ] 3/63 batches, loss: 0.5191Epoch 10/10: [=                             ] 4/63 batches, loss: 0.5211Epoch 10/10: [==                            ] 5/63 batches, loss: 0.5081Epoch 10/10: [==                            ] 6/63 batches, loss: 0.5073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.5000Epoch 10/10: [===                           ] 8/63 batches, loss: 0.5123Epoch 10/10: [====                          ] 9/63 batches, loss: 0.5140Epoch 10/10: [====                          ] 10/63 batches, loss: 0.5153Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.5056Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.4995Epoch 10/10: [======                        ] 13/63 batches, loss: 0.5017Epoch 10/10: [======                        ] 14/63 batches, loss: 0.4968Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.5035Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.5006Epoch 10/10: [========                      ] 17/63 batches, loss: 0.5035Epoch 10/10: [========                      ] 18/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.5035Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.5058Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.5057Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.5077Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.5085Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.5092Epoch 10/10: [============                  ] 26/63 batches, loss: 0.5090Epoch 10/10: [============                  ] 27/63 batches, loss: 0.5097Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.5078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.5061Epoch 10/10: [==============                ] 30/63 batches, loss: 0.5044Epoch 10/10: [==============                ] 31/63 batches, loss: 0.5051Epoch 10/10: [===============               ] 32/63 batches, loss: 0.5036Epoch 10/10: [===============               ] 33/63 batches, loss: 0.5043Epoch 10/10: [================              ] 34/63 batches, loss: 0.5029Epoch 10/10: [================              ] 35/63 batches, loss: 0.5022Epoch 10/10: [=================             ] 36/63 batches, loss: 0.5023Epoch 10/10: [=================             ] 37/63 batches, loss: 0.5017Epoch 10/10: [==================            ] 38/63 batches, loss: 0.5023Epoch 10/10: [==================            ] 39/63 batches, loss: 0.4993Epoch 10/10: [===================           ] 40/63 batches, loss: 0.4988Epoch 10/10: [===================           ] 41/63 batches, loss: 0.4984Epoch 10/10: [====================          ] 42/63 batches, loss: 0.4996Epoch 10/10: [====================          ] 43/63 batches, loss: 0.4997Epoch 10/10: [====================          ] 44/63 batches, loss: 0.4998Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.5004Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 47/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 48/63 batches, loss: 0.5016Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.5018Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.5027Epoch 10/10: [========================      ] 51/63 batches, loss: 0.5023Epoch 10/10: [========================      ] 52/63 batches, loss: 0.5005Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.5001Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.5006Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.5020Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.5037Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 59/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 60/63 batches, loss: 0.5037Epoch 10/10: [============================= ] 61/63 batches, loss: 0.5033Epoch 10/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 10/10: [==============================] 63/63 batches, loss: 0.5030
[2025-05-01 11:41:34,543][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5030
[2025-05-01 11:41:34,739][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.5531, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 11:41:34,739][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Training completed in 67.94 seconds
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.922077922077922, 'f1': 0.875, 'precision': 0.8076923076923077, 'recall': 0.9545454545454546}
[2025-05-01 11:41:38,869][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
[2025-05-01 11:41:38,874][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▁▅███
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▅▅▄▄▅▅▅
wandb:            train_loss █▅▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆██▇▇███▇
wandb:                val_f1 ▁▆██▆▇███▆
wandb:              val_loss █▃▁▁▂▂▁▁▁▂
wandb:         val_precision ▁▅██▆▇███▆
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.5218
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 10
wandb:                 epoch 10
wandb:   final_test_accuracy 0.92208
wandb:         final_test_f1 0.875
wandb:  final_test_precision 0.80769
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 1
wandb:        final_train_f1 1
wandb: final_train_precision 1
wandb:    final_train_recall 1
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50304
wandb:            train_time 67.94414
wandb:          val_accuracy 0.95455
wandb:                val_f1 0.95238
wandb:              val_loss 0.55315
wandb:         val_precision 0.90909
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114016-07wlukwc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114016-07wlukwc/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:41:50,537][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:41:50,537][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:41:50,537][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:41:50,538][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:41:50,542][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 11:41:50,542][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:41:52,060][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:41:54,310][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:41:54,311][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,388][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,482][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 11:41:54,489][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,490][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 11:41:54,491][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,507][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,534][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,547][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,564][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,592][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,605][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 11:41:54,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,606][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 11:41:54,607][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,608][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,608][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,609][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,609][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,610][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,610][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:41:54,611][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:41:54,611][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:41:54,611][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:41:58,426][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:41:58,432][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:41:58,433][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:41:58,433][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:41:58,433][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 11:41:58,434][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:41:58,434][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.2303Epoch 1/10: [                              ] 2/63 batches, loss: 0.2078Epoch 1/10: [=                             ] 3/63 batches, loss: 0.2220Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2301Epoch 1/10: [==                            ] 5/63 batches, loss: 0.2037Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1926Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1881Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1857Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1875Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1871Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1827Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1822Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1838Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1789Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1721Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1689Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1630Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1590Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1582Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1561Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1553Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1519Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1510Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1471Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1437Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1411Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1407Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1398Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1378Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1350Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1325Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1296Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1272Epoch 1/10: [================              ] 34/63 batches, loss: 0.1249Epoch 1/10: [================              ] 35/63 batches, loss: 0.1232Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1213Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1195Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1187Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1166Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1147Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1134Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1116Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1112Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1099Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1088Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1069Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1060Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1051Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1037Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1024Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1008Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0993Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0983Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0969Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0962Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0953Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0942Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0937Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0928Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0916Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0906Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0896Epoch 1/10: [==============================] 63/63 batches, loss: 0.0889
[2025-05-01 11:42:07,289][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0889
[2025-05-01 11:42:07,458][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0799, Metrics: {'mse': 0.08098221570253372, 'rmse': 0.2845737438741208, 'r2': -0.248205304145813}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0437Epoch 2/10: [                              ] 2/63 batches, loss: 0.0360Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0361Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0326Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0318Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0303Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0295Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0285Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0291Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0275Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0278Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0271Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0285Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0289Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0293Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0286Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0287Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0277Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0273Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0269Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0263Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0260Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0256Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0252Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0252Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0251Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0251Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0250Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0250Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0247Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0247Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0250Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0250Epoch 2/10: [================              ] 34/63 batches, loss: 0.0253Epoch 2/10: [================              ] 35/63 batches, loss: 0.0251Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0250Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0246Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0241Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0241Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0238Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0238Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0242Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0239Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0239Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0237Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0236Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0233Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0232Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0231Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0230Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0228Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0222Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0221Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0216Epoch 2/10: [==============================] 63/63 batches, loss: 0.0215
[2025-05-01 11:42:14,213][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0215
[2025-05-01 11:42:14,393][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0622, Metrics: {'mse': 0.06337393075227737, 'rmse': 0.2517417938131795, 'r2': 0.0231969952583313}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0210Epoch 3/10: [                              ] 2/63 batches, loss: 0.0156Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0158Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0144Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0153Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0155Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0146Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0145Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0144Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0142Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0141Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0138Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0157Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0157Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0154Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0158Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0156Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0158Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0159Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0162Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0157Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0156Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0157Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0157Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0161Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0161Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0160Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0160Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0159Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0160Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0169Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0170Epoch 3/10: [================              ] 34/63 batches, loss: 0.0167Epoch 3/10: [================              ] 35/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0164Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0163Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0163Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0161Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0160Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0161Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0162Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0165Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0166Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0167Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0166Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0164Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0163Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0161Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0162Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0164Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0163Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0162Epoch 3/10: [==============================] 63/63 batches, loss: 0.0161
[2025-05-01 11:42:21,172][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0161
[2025-05-01 11:42:21,359][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0502, Metrics: {'mse': 0.05127166584134102, 'rmse': 0.22643247523564508, 'r2': 0.20973312854766846}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0150Epoch 4/10: [                              ] 2/63 batches, loss: 0.0187Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0129Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0135Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0136Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0134Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0132Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0162Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0168Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0178Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0179Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0173Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0177Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0182Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0182Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0178Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0172Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0169Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0165Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0164Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0162Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0167Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0169Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0165Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0163Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0165Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0164Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0162Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0160Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0158Epoch 4/10: [================              ] 34/63 batches, loss: 0.0158Epoch 4/10: [================              ] 35/63 batches, loss: 0.0158Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0157Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0156Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0154Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0151Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0150Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0151Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0151Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0150Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0151Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0150Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0151Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0152Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0152Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0154Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0154Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0152Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0150Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0151Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0153Epoch 4/10: [==============================] 63/63 batches, loss: 0.0153
[2025-05-01 11:42:28,067][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0153
[2025-05-01 11:42:28,249][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0382, Metrics: {'mse': 0.03820837289094925, 'rmse': 0.19546962140176474, 'r2': 0.4110819101333618}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0113Epoch 5/10: [                              ] 2/63 batches, loss: 0.0082Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0114Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0153Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0170Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0162Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0168Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0166Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0175Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0167Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0165Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0162Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0150Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0146Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0145Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0141Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0139Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0138Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0136Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0138Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0135Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0134Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0136Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0134Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0132Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0136Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0135Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0135Epoch 5/10: [================              ] 34/63 batches, loss: 0.0135Epoch 5/10: [================              ] 35/63 batches, loss: 0.0134Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0133Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0132Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0131Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0130Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0132Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0132Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0131Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0134Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0133Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0131Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0131Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0130Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0131Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0130Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0130Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0131Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0132Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0133Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0132Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0134Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0134Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0133Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0135Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0136Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0135Epoch 5/10: [==============================] 63/63 batches, loss: 0.0134
[2025-05-01 11:42:34,974][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0134
[2025-05-01 11:42:35,165][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0359, Metrics: {'mse': 0.03661568462848663, 'rmse': 0.19135225273951345, 'r2': 0.43563055992126465}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0175Epoch 6/10: [                              ] 2/63 batches, loss: 0.0172Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0135Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0130Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0130Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0124Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0123Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0127Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0122Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0117Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0131Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0130Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0134Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0139Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0137Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0132Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0130Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0125Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0124Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0124Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0121Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0122Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0120Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0119Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0117Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0115Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0112Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0111Epoch 6/10: [================              ] 34/63 batches, loss: 0.0111Epoch 6/10: [================              ] 35/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0108Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0107Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0108Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0109Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0109Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0108Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0107Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0106Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0105Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0105Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0106Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0106Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0105Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0106Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0106Epoch 6/10: [==============================] 63/63 batches, loss: 0.0104
[2025-05-01 11:42:41,905][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0104
[2025-05-01 11:42:42,100][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0371, Metrics: {'mse': 0.03692213073372841, 'rmse': 0.19215132248758635, 'r2': 0.4309071898460388}
[2025-05-01 11:42:42,100][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0070Epoch 7/10: [                              ] 2/63 batches, loss: 0.0090Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0095Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0104Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0096Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0110Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0111Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0107Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0100Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0102Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0096Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0095Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0091Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0096Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0095Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0091Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0094Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0093Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0092Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0091Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0089Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0088Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0085Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0089Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0087Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0088Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0088Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0087Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0087Epoch 7/10: [================              ] 34/63 batches, loss: 0.0087Epoch 7/10: [================              ] 35/63 batches, loss: 0.0087Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0086Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0087Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0086Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0086Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0085Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0084Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0083Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0082Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0083Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0084Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0085Epoch 7/10: [==============================] 63/63 batches, loss: 0.0084
[2025-05-01 11:42:48,462][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0084
[2025-05-01 11:42:48,650][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0329, Metrics: {'mse': 0.03260713443160057, 'rmse': 0.18057445675288786, 'r2': 0.4974156618118286}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0032Epoch 8/10: [                              ] 2/63 batches, loss: 0.0052Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0054Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0054Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0060Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0059Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0061Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0068Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0073Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0075Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0081Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0087Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0090Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0090Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0087Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0083Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0082Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0083Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0081Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0083Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0087Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0090Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0091Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0094Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0100Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0098Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0097Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0096Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0096Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0097Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0097Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0096Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0098Epoch 8/10: [================              ] 34/63 batches, loss: 0.0096Epoch 8/10: [================              ] 35/63 batches, loss: 0.0095Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0096Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0096Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0097Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0098Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0097Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0099Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0101Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0102Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0102Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0101Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0101Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0103Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0104Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0105Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0105Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0104Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0104Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0103Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0103Epoch 8/10: [==============================] 63/63 batches, loss: 0.0106
[2025-05-01 11:42:55,447][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0106
[2025-05-01 11:42:55,641][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0266, Metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.0077Epoch 9/10: [                              ] 2/63 batches, loss: 0.0091Epoch 9/10: [=                             ] 3/63 batches, loss: 0.0088Epoch 9/10: [=                             ] 4/63 batches, loss: 0.0091Epoch 9/10: [==                            ] 5/63 batches, loss: 0.0097Epoch 9/10: [==                            ] 6/63 batches, loss: 0.0097Epoch 9/10: [===                           ] 7/63 batches, loss: 0.0090Epoch 9/10: [===                           ] 8/63 batches, loss: 0.0091Epoch 9/10: [====                          ] 9/63 batches, loss: 0.0086Epoch 9/10: [====                          ] 10/63 batches, loss: 0.0085Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.0082Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.0082Epoch 9/10: [======                        ] 13/63 batches, loss: 0.0080Epoch 9/10: [======                        ] 14/63 batches, loss: 0.0078Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.0080Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.0077Epoch 9/10: [========                      ] 17/63 batches, loss: 0.0076Epoch 9/10: [========                      ] 18/63 batches, loss: 0.0077Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.0075Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.0076Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.0077Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.0078Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 26/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 27/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.0081Epoch 9/10: [==============                ] 30/63 batches, loss: 0.0080Epoch 9/10: [==============                ] 31/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 32/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 33/63 batches, loss: 0.0079Epoch 9/10: [================              ] 34/63 batches, loss: 0.0079Epoch 9/10: [================              ] 35/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 36/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 37/63 batches, loss: 0.0081Epoch 9/10: [==================            ] 38/63 batches, loss: 0.0082Epoch 9/10: [==================            ] 39/63 batches, loss: 0.0082Epoch 9/10: [===================           ] 40/63 batches, loss: 0.0081Epoch 9/10: [===================           ] 41/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 42/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 43/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 44/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 47/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 48/63 batches, loss: 0.0080Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.0081Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 51/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 52/63 batches, loss: 0.0081Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.0082Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 59/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 60/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 61/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 62/63 batches, loss: 0.0080Epoch 9/10: [==============================] 63/63 batches, loss: 0.0079
[2025-05-01 11:43:02,377][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0079
[2025-05-01 11:43:02,576][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0328, Metrics: {'mse': 0.03263333812355995, 'rmse': 0.18064699865638498, 'r2': 0.4970117211341858}
[2025-05-01 11:43:02,577][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.0058Epoch 10/10: [                              ] 2/63 batches, loss: 0.0090Epoch 10/10: [=                             ] 3/63 batches, loss: 0.0076Epoch 10/10: [=                             ] 4/63 batches, loss: 0.0071Epoch 10/10: [==                            ] 5/63 batches, loss: 0.0074Epoch 10/10: [==                            ] 6/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 8/63 batches, loss: 0.0069Epoch 10/10: [====                          ] 9/63 batches, loss: 0.0071Epoch 10/10: [====                          ] 10/63 batches, loss: 0.0071Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.0072Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.0071Epoch 10/10: [======                        ] 13/63 batches, loss: 0.0072Epoch 10/10: [======                        ] 14/63 batches, loss: 0.0072Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.0070Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 17/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 18/63 batches, loss: 0.0070Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.0072Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.0071Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.0073Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.0075Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.0075Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.0076Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.0077Epoch 10/10: [============                  ] 26/63 batches, loss: 0.0079Epoch 10/10: [============                  ] 27/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 30/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 31/63 batches, loss: 0.0076Epoch 10/10: [===============               ] 32/63 batches, loss: 0.0078Epoch 10/10: [===============               ] 33/63 batches, loss: 0.0080Epoch 10/10: [================              ] 34/63 batches, loss: 0.0079Epoch 10/10: [================              ] 35/63 batches, loss: 0.0078Epoch 10/10: [=================             ] 36/63 batches, loss: 0.0077Epoch 10/10: [=================             ] 37/63 batches, loss: 0.0078Epoch 10/10: [==================            ] 38/63 batches, loss: 0.0077Epoch 10/10: [==================            ] 39/63 batches, loss: 0.0078Epoch 10/10: [===================           ] 40/63 batches, loss: 0.0079Epoch 10/10: [===================           ] 41/63 batches, loss: 0.0078Epoch 10/10: [====================          ] 42/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 43/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 44/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 47/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 48/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.0075Epoch 10/10: [========================      ] 51/63 batches, loss: 0.0074Epoch 10/10: [========================      ] 52/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.0074Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.0073Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 59/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 60/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 61/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 62/63 batches, loss: 0.0073Epoch 10/10: [==============================] 63/63 batches, loss: 0.0072
[2025-05-01 11:43:08,935][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0072
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0373, Metrics: {'mse': 0.0370272658765316, 'rmse': 0.1924247018356313, 'r2': 0.4292866587638855}
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Training completed in 69.08 seconds
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007992316037416458, 'rmse': 0.089399754123915, 'r2': 0.7396416068077087}
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07233811914920807, 'rmse': 0.26895746717503133, 'r2': -0.24707698822021484}
[2025-05-01 11:43:13,231][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/ar/model.pt
[2025-05-01 11:43:13,236][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁
wandb:     best_val_mse █▆▄▂▂▂▁
wandb:      best_val_r2 ▁▃▅▇▇▇█
wandb:    best_val_rmse █▆▅▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▆▇▆▇▇▇
wandb:       train_loss █▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▂▁▂▂
wandb:          val_mse █▆▄▂▂▂▂▁▂▂
wandb:           val_r2 ▁▃▅▇▇▇▇█▇▇
wandb:         val_rmse █▆▅▃▃▃▂▁▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02665
wandb:     best_val_mse 0.02684
wandb:      best_val_r2 0.58628
wandb:    best_val_rmse 0.16383
wandb:            epoch 10
wandb:   final_test_mse 0.07234
wandb:    final_test_r2 -0.24708
wandb:  final_test_rmse 0.26896
wandb:  final_train_mse 0.00799
wandb:   final_train_r2 0.73964
wandb: final_train_rmse 0.0894
wandb:    final_val_mse 0.02684
wandb:     final_val_r2 0.58628
wandb:   final_val_rmse 0.16383
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00717
wandb:       train_time 69.07752
wandb:         val_loss 0.03734
wandb:          val_mse 0.03703
wandb:           val_r2 0.42929
wandb:         val_rmse 0.19242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114150-fr4hh7tz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114150-fr4hh7tz/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:43:24,183][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:43:24,183][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:43:24,183][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:43:24,183][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:43:24,187][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-05-01 11:43:24,188][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:43:25,852][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:43:28,380][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:43:28,380][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,428][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,456][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,576][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 11:43:28,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,585][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 11:43:28,586][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,601][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,629][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,641][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 11:43:28,642][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,642][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 11:43:28,643][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,660][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,689][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,701][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 11:43:28,703][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,703][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 11:43:28,704][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,705][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-05-01 11:43:28,705][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,706][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:43:28,706][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,707][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:43:28,707][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:43:28,707][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:43:28,708][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:43:32,576][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:43:32,582][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:43:32,582][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:43:32,583][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:43:32,583][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 11:43:32,583][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:43:32,584][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7244Epoch 1/10: [                              ] 2/75 batches, loss: 0.7709Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7345Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7469Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7655Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7285Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7406Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7416Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7494Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7309Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7394Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7360Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7374Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7416Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7414Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7419Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7397Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7407Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7387Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7353Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7348Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7330Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7302Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7263Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7272Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7254Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7283Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7299Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7300Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7288Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7264Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7268Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7256Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7222Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7218Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7207Epoch 1/10: [================              ] 40/75 batches, loss: 0.7197Epoch 1/10: [================              ] 41/75 batches, loss: 0.7194Epoch 1/10: [================              ] 42/75 batches, loss: 0.7208Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7211Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7192Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7178Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7169Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7182Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7161Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7159Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7168Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7172Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7176Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7172Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7161Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7151Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7146Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7146Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7156Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7145Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7131Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7118Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7114Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7120Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7115Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7095Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7084Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7064Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7060Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7046Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7022Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7006Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6999Epoch 1/10: [==============================] 75/75 batches, loss: 0.6998
[2025-05-01 11:43:42,515][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6998
[2025-05-01 11:43:42,756][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6192, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5898Epoch 2/10: [                              ] 2/75 batches, loss: 0.6357Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6189Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6576Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6427Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6484Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6562Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6519Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6442Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6339Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6304Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6263Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6254Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6207Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6138Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6117Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6112Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6107Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6047Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6032Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5990Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5964Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5949Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5953Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5908Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5898Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5912Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5897Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5896Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5882Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5892Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5884Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5876Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5861Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5834Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5833Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5826Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5795Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5760Epoch 2/10: [================              ] 40/75 batches, loss: 0.5735Epoch 2/10: [================              ] 41/75 batches, loss: 0.5713Epoch 2/10: [================              ] 42/75 batches, loss: 0.5692Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5681Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5665Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5643Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5630Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5609Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5606Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5595Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5605Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5604Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5594Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5589Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5581Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5563Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5555Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5559Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5552Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5545Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5542Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5535Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5514Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5510Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5503Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5514Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5500Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5486Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5481Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5491Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5492Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5482Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5477Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5471Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5462Epoch 2/10: [==============================] 75/75 batches, loss: 0.5457
[2025-05-01 11:43:50,751][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5457
[2025-05-01 11:43:50,990][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5355, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5078Epoch 3/10: [                              ] 2/75 batches, loss: 0.4961Epoch 3/10: [=                             ] 3/75 batches, loss: 0.4835Epoch 3/10: [=                             ] 4/75 batches, loss: 0.4891Epoch 3/10: [==                            ] 5/75 batches, loss: 0.4831Epoch 3/10: [==                            ] 6/75 batches, loss: 0.4988Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5000Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5039Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5068Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5067Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5066Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5084Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5119Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5078Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5015Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5042Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5031Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5021Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.4991Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.4994Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.4987Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.4990Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5036Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5053Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5045Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5066Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5094Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5072Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5071Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5050Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5056Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5057Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5074Epoch 3/10: [================              ] 40/75 batches, loss: 0.5068Epoch 3/10: [================              ] 41/75 batches, loss: 0.5067Epoch 3/10: [================              ] 42/75 batches, loss: 0.5067Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5061Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5066Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5056Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5061Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5055Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5065Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5074Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5078Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5055Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5064Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5055Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5057Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5065Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5061Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5061Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5061Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5064Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5060Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5067Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5063Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5060Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5063Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5064Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5071Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5077Epoch 3/10: [==============================] 75/75 batches, loss: 0.5083
[2025-05-01 11:43:58,973][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5083
[2025-05-01 11:43:59,221][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5405, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:43:59,221][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5045Epoch 4/10: [                              ] 2/75 batches, loss: 0.5169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5049Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5106Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5143Epoch 4/10: [==                            ] 6/75 batches, loss: 0.4930Epoch 4/10: [==                            ] 7/75 batches, loss: 0.4981Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5019Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5022Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5048Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5039Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4982Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4951Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4940Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4900Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4864Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4884Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4906Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4926Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4957Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4972Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4965Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4989Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5027Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.4980Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.4992Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5011Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5012Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5013Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5022Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5045Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5052Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5066Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5072Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5079Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5078Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5096Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5095Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5087Epoch 4/10: [================              ] 40/75 batches, loss: 0.5056Epoch 4/10: [================              ] 41/75 batches, loss: 0.5056Epoch 4/10: [================              ] 42/75 batches, loss: 0.5055Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5060Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5060Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5034Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5030Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5025Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5026Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5018Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5002Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5015Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5012Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5020Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5024Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5030Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5027Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5045Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5038Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5048Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5051Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5058Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5054Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5061Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5060Epoch 4/10: [==============================] 75/75 batches, loss: 0.5073
[2025-05-01 11:44:06,786][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5073
[2025-05-01 11:44:07,034][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5446, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:44:07,034][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4084Epoch 5/10: [                              ] 2/75 batches, loss: 0.4206Epoch 5/10: [=                             ] 3/75 batches, loss: 0.4087Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4324Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4277Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4523Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4698Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4800Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4826Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4864Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4898Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4836Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4816Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4847Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4873Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4925Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4958Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4974Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4930Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4935Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4907Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4934Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4958Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4923Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4960Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4945Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4932Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4927Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4915Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4949Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4937Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4947Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4950Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4946Epoch 5/10: [==============                ] 36/75 batches, loss: 0.4948Epoch 5/10: [==============                ] 37/75 batches, loss: 0.4963Epoch 5/10: [===============               ] 38/75 batches, loss: 0.4972Epoch 5/10: [===============               ] 39/75 batches, loss: 0.4985Epoch 5/10: [================              ] 40/75 batches, loss: 0.5004Epoch 5/10: [================              ] 41/75 batches, loss: 0.5017Epoch 5/10: [================              ] 42/75 batches, loss: 0.5017Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5018Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5029Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5019Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5033Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5038Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5033Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5023Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5009Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5005Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5015Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5011Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5027Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5036Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5032Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5036Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5048Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5040Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5048Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5059Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5066Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5073Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5065Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5065Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5054Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5060Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5063Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5049Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5058Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5064Epoch 5/10: [==============================] 75/75 batches, loss: 0.5057
[2025-05-01 11:44:14,591][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5057
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5448, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 11:44:14,835][src.training.lm_trainer][INFO] - Training completed in 40.57 seconds
[2025-05-01 11:44:14,835][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:44:17,739][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488, 'precision': 0.9933333333333333, 'recall': 1.0}
[2025-05-01 11:44:17,740][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
[2025-05-01 11:44:17,740][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9363636363636364, 'f1': 0.9369369369369369, 'precision': 0.9285714285714286, 'recall': 0.9454545454545454}
[2025-05-01 11:44:19,392][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
[2025-05-01 11:44:19,397][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy █▁
wandb:           best_val_f1 █▁
wandb:         best_val_loss █▁
wandb:    best_val_precision █▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃
wandb:            train_loss █▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy █▄▁▁▁
wandb:                val_f1 █▄▁▁▁
wandb:              val_loss █▁▁▂▂
wandb:         val_precision █▄▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.94444
wandb:           best_val_f1 0.94737
wandb:         best_val_loss 0.53551
wandb:    best_val_precision 0.9
wandb:       best_val_recall 1
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.93636
wandb:         final_test_f1 0.93694
wandb:  final_test_precision 0.92857
wandb:     final_test_recall 0.94545
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99666
wandb: final_train_precision 0.99333
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.94444
wandb:          final_val_f1 0.94737
wandb:   final_val_precision 0.9
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50574
wandb:            train_time 40.57217
wandb:          val_accuracy 0.93056
wandb:                val_f1 0.93506
wandb:              val_loss 0.54483
wandb:         val_precision 0.87805
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114324-v27te4ak
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114324-v27te4ak/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:44:30,992][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/en
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:44:30,992][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:44:30,992][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:44:30,992][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:44:30,996][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-05-01 11:44:30,997][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:44:32,367][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:44:34,572][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:44:34,572][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,733][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 11:44:34,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,742][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 11:44:34,743][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,759][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,800][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 11:44:34,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,802][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 11:44:34,802][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,848][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,860][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 11:44:34,861][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,862][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,864][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,864][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,866][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:44:34,866][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:44:34,867][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:44:38,700][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:44:38,706][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 11:44:38,707][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:44:38,707][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.2066Epoch 1/10: [                              ] 2/75 batches, loss: 0.1540Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1667Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1615Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1548Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1549Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1526Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1525Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1538Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1519Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1514Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1493Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1487Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1461Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1438Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1426Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1370Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1344Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1307Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1295Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1290Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1307Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1324Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1332Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1346Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1334Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1322Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1296Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1286Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1269Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1253Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1240Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1229Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1238Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1226Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1207Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1200Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1187Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1176Epoch 1/10: [================              ] 40/75 batches, loss: 0.1170Epoch 1/10: [================              ] 41/75 batches, loss: 0.1176Epoch 1/10: [================              ] 42/75 batches, loss: 0.1187Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1177Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1176Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1167Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1151Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1138Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1125Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1113Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1096Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1083Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1075Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1064Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.1054Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1045Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1032Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1028Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1014Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.1008Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0998Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0991Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0980Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0970Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0963Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0956Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0948Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0939Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0930Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0918Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0910Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0904Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0895Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0890Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0881Epoch 1/10: [==============================] 75/75 batches, loss: 0.0873
[2025-05-01 11:44:48,736][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0873
[2025-05-01 11:44:48,957][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0408, Metrics: {'mse': 0.04292728006839752, 'rmse': 0.2071889960118479, 'r2': -0.025711774826049805}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0591Epoch 2/10: [                              ] 2/75 batches, loss: 0.0610Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0468Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0415Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0435Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0436Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0406Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0380Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0350Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0334Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0317Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0312Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0310Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0307Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0304Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0310Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0303Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0304Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0308Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0299Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0294Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0303Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0305Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0305Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0309Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0308Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0304Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0320Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0323Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0330Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0327Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0321Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0316Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0313Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0309Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0313Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0309Epoch 2/10: [================              ] 40/75 batches, loss: 0.0305Epoch 2/10: [================              ] 41/75 batches, loss: 0.0307Epoch 2/10: [================              ] 42/75 batches, loss: 0.0308Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0304Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0300Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0298Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0296Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0300Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0302Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0306Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0305Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0308Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0311Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0307Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0305Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0303Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0302Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0300Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0297Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0298Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0296Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0294Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0291Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0291Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0289Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0288Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0290Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0289Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0286Epoch 2/10: [==============================] 75/75 batches, loss: 0.0283
[2025-05-01 11:44:56,904][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0283
[2025-05-01 11:44:57,132][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0278, Metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0207Epoch 3/10: [                              ] 2/75 batches, loss: 0.0363Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0314Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0312Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0305Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0275Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0283Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0270Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0298Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0279Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0285Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0297Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0301Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0299Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0297Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0313Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0308Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0298Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0297Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0287Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0283Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0285Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0280Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0276Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0275Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0273Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0266Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0262Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0258Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0257Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0256Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0257Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0255Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0253Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0253Epoch 3/10: [================              ] 40/75 batches, loss: 0.0248Epoch 3/10: [================              ] 41/75 batches, loss: 0.0248Epoch 3/10: [================              ] 42/75 batches, loss: 0.0249Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0248Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0250Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0249Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0250Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0251Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0248Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0249Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0250Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0249Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0247Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0249Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0251Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0250Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0250Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0252Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0251Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0249Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0249Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0248Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0246Epoch 3/10: [==============================] 75/75 batches, loss: 0.0247
[2025-05-01 11:45:05,108][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0247
[2025-05-01 11:45:05,616][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0504, Metrics: {'mse': 0.04933957755565643, 'rmse': 0.2221251394049229, 'r2': -0.17892837524414062}
[2025-05-01 11:45:05,617][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0155Epoch 4/10: [                              ] 2/75 batches, loss: 0.0169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0168Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0193Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0183Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0175Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0171Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0169Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0175Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0178Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0177Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0173Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0175Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0179Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0176Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0175Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0175Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0181Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0178Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0178Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0176Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0174Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0172Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0178Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0176Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0175Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0173Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0174Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0172Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0172Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0171Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0170Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0172Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0171Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0175Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0175Epoch 4/10: [================              ] 40/75 batches, loss: 0.0175Epoch 4/10: [================              ] 41/75 batches, loss: 0.0176Epoch 4/10: [================              ] 42/75 batches, loss: 0.0175Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0174Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0174Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0173Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0171Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0173Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0174Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0176Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0174Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0178Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0177Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0178Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0178Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0179Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0181Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0182Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0181Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0179Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0179Epoch 4/10: [==============================] 75/75 batches, loss: 0.0178
[2025-05-01 11:45:13,568][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0178
[2025-05-01 11:45:13,953][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0353, Metrics: {'mse': 0.03462854400277138, 'rmse': 0.18608746331435488, 'r2': 0.17257964611053467}
[2025-05-01 11:45:13,954][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0090Epoch 5/10: [                              ] 2/75 batches, loss: 0.0128Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0166Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0164Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0165Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0166Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0165Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0183Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0186Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0185Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0180Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0175Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0172Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0167Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0159Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0167Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0162Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0160Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0164Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0165Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0162Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0160Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0166Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0163Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0166Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0163Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0165Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0162Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0169Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0175Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0173Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0173Epoch 5/10: [================              ] 40/75 batches, loss: 0.0173Epoch 5/10: [================              ] 41/75 batches, loss: 0.0178Epoch 5/10: [================              ] 42/75 batches, loss: 0.0179Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0181Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0182Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0184Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0183Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0185Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0184Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0184Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0190Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0191Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0192Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0191Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0191Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0190Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0188Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0187Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0186Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0184Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0184Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0183Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0184Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0183Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0182Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0183Epoch 5/10: [==============================] 75/75 batches, loss: 0.0183
[2025-05-01 11:45:21,521][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0183
[2025-05-01 11:45:21,783][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0332, Metrics: {'mse': 0.03250707685947418, 'rmse': 0.1802971903815314, 'r2': 0.22327035665512085}
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Training completed in 41.52 seconds
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02293914183974266, 'rmse': 0.15145673256657383, 'r2': 0.14497649669647217}
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.022624490782618523, 'rmse': 0.15041439685953775, 'r2': 0.4129396677017212}
[2025-05-01 11:45:26,297][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/en/model.pt
[2025-05-01 11:45:26,303][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▆▁▅
wandb:       train_loss █▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▁█▃▃
wandb:          val_mse ▆▁█▃▃
wandb:           val_r2 ▃█▁▆▆
wandb:         val_rmse ▆▁█▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02783
wandb:     best_val_mse 0.02712
wandb:      best_val_r2 0.35211
wandb:    best_val_rmse 0.16467
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.02262
wandb:    final_test_r2 0.41294
wandb:  final_test_rmse 0.15041
wandb:  final_train_mse 0.02294
wandb:   final_train_r2 0.14498
wandb: final_train_rmse 0.15146
wandb:    final_val_mse 0.02712
wandb:     final_val_r2 0.35211
wandb:   final_val_rmse 0.16467
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01835
wandb:       train_time 41.5229
wandb:         val_loss 0.03321
wandb:          val_mse 0.03251
wandb:           val_r2 0.22327
wandb:         val_rmse 0.1803
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114431-aylozcfq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114431-aylozcfq/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:45:38,669][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:45:38,670][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:45:38,670][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:45:38,670][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:45:38,675][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-01 11:45:38,675][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:45:40,096][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:45:42,504][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:45:42,505][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,564][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,685][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 11:45:42,694][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,695][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 11:45:42,696][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,719][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,749][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,762][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 11:45:42,764][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,764][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 11:45:42,765][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,786][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,812][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,824][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 11:45:42,826][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,826][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 11:45:42,827][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,830][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:45:42,830][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:45:42,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:45:42,831][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:45:42,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:45:47,018][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:45:47,023][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:45:47,024][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:45:47,024][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:45:47,024][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 11:45:47,025][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:45:47,025][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7251Epoch 1/10: [                              ] 2/75 batches, loss: 0.7237Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7232Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7154Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6918Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6868Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6918Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6915Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6982Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7075Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7120Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6995Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7033Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7046Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7039Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7053Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7037Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7076Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7071Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7057Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7142Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7131Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7165Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7166Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7206Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7235Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7237Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7236Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7235Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7244Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7224Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7214Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7231Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7249Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7237Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7219Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7227Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7220Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7210Epoch 1/10: [================              ] 40/75 batches, loss: 0.7223Epoch 1/10: [================              ] 41/75 batches, loss: 0.7245Epoch 1/10: [================              ] 42/75 batches, loss: 0.7255Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7263Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7245Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7288Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7291Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7306Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7283Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7270Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7248Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7230Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7232Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7218Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7223Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7216Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7220Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7204Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7205Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7194Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7199Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7211Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7209Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7210Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7202Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7205Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 74/75 batches, loss: 0.7203Epoch 1/10: [==============================] 75/75 batches, loss: 0.7186
[2025-05-01 11:45:57,421][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7186
[2025-05-01 11:45:57,626][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7320, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.7242Epoch 2/10: [                              ] 2/75 batches, loss: 0.7205Epoch 2/10: [=                             ] 3/75 batches, loss: 0.7064Epoch 2/10: [=                             ] 4/75 batches, loss: 0.7046Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6933Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6944Epoch 2/10: [==                            ] 7/75 batches, loss: 0.7067Epoch 2/10: [===                           ] 8/75 batches, loss: 0.7179Epoch 2/10: [===                           ] 9/75 batches, loss: 0.7155Epoch 2/10: [====                          ] 10/75 batches, loss: 0.7152Epoch 2/10: [====                          ] 11/75 batches, loss: 0.7094Epoch 2/10: [====                          ] 12/75 batches, loss: 0.7034Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.7042Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6998Epoch 2/10: [======                        ] 15/75 batches, loss: 0.7004Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6975Epoch 2/10: [======                        ] 17/75 batches, loss: 0.7006Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.7019Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.7028Epoch 2/10: [========                      ] 20/75 batches, loss: 0.7052Epoch 2/10: [========                      ] 21/75 batches, loss: 0.7100Epoch 2/10: [========                      ] 22/75 batches, loss: 0.7144Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.7172Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.7195Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.7193Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.7207Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.7200Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.7176Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.7156Epoch 2/10: [============                  ] 30/75 batches, loss: 0.7145Epoch 2/10: [============                  ] 31/75 batches, loss: 0.7127Epoch 2/10: [============                  ] 32/75 batches, loss: 0.7154Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.7116Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.7072Epoch 2/10: [==============                ] 35/75 batches, loss: 0.7090Epoch 2/10: [==============                ] 36/75 batches, loss: 0.7053Epoch 2/10: [==============                ] 37/75 batches, loss: 0.7053Epoch 2/10: [===============               ] 38/75 batches, loss: 0.7035Epoch 2/10: [===============               ] 39/75 batches, loss: 0.7028Epoch 2/10: [================              ] 40/75 batches, loss: 0.7020Epoch 2/10: [================              ] 41/75 batches, loss: 0.7016Epoch 2/10: [================              ] 42/75 batches, loss: 0.6999Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6986Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6967Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6976Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6984Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6991Epoch 2/10: [===================           ] 48/75 batches, loss: 0.7012Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6995Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6988Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6982Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6985Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6979Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6983Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6974Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6972Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6975Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6968Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6970Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6967Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6946Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6940Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6936Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6914Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6917Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6892Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6876Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6872Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6871Epoch 2/10: [==============================] 75/75 batches, loss: 0.6854
[2025-05-01 11:46:05,612][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6854
[2025-05-01 11:46:05,832][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6544, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315, 'precision': 1.0, 'recall': 0.9}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6111Epoch 3/10: [                              ] 2/75 batches, loss: 0.6070Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5885Epoch 3/10: [=                             ] 4/75 batches, loss: 0.6017Epoch 3/10: [==                            ] 5/75 batches, loss: 0.6115Epoch 3/10: [==                            ] 6/75 batches, loss: 0.6209Epoch 3/10: [==                            ] 7/75 batches, loss: 0.6121Epoch 3/10: [===                           ] 8/75 batches, loss: 0.6182Epoch 3/10: [===                           ] 9/75 batches, loss: 0.6020Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5966Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5901Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5919Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5863Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5812Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5789Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5707Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5708Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5712Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5739Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5699Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5725Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5714Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5681Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5681Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5671Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5655Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5620Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5629Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5591Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5578Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5541Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5548Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5513Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5509Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5497Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5499Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5494Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5479Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5482Epoch 3/10: [================              ] 40/75 batches, loss: 0.5496Epoch 3/10: [================              ] 41/75 batches, loss: 0.5474Epoch 3/10: [================              ] 42/75 batches, loss: 0.5487Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5488Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5489Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5486Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5483Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5464Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5466Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5454Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5441Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5439Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5438Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5440Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5425Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5415Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5408Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5403Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5389Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5380Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5363Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5358Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5361Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5359Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5358Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5344Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5336Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5337Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5336Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5334Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5327Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5323Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5315Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5312Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5315Epoch 3/10: [==============================] 75/75 batches, loss: 0.5327
[2025-05-01 11:46:13,829][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5327
[2025-05-01 11:46:14,062][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5255, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5542Epoch 4/10: [                              ] 2/75 batches, loss: 0.5660Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5297Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5116Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5151Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5095Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5043Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5046Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5074Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5072Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5004Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5091Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5170Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5162Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5127Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5150Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5106Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5091Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5108Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5127Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5150Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5155Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5180Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5184Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5135Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5178Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5182Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5183Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5185Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5189Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5204Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5192Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5175Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5172Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5182Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5189Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5204Epoch 4/10: [================              ] 40/75 batches, loss: 0.5206Epoch 4/10: [================              ] 41/75 batches, loss: 0.5199Epoch 4/10: [================              ] 42/75 batches, loss: 0.5179Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5194Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5192Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5208Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5200Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5206Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5194Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5182Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5188Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5208Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5196Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5185Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5191Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5180Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5174Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5160Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5154Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5156Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5147Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5149Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5144Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5157Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5151Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5160Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5159Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5150Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5145Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5144Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5142Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5141Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5146Epoch 4/10: [==============================] 75/75 batches, loss: 0.5140
[2025-05-01 11:46:22,037][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5140
[2025-05-01 11:46:22,278][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5248, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.5751Epoch 5/10: [                              ] 2/75 batches, loss: 0.5396Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5120Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4922Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4898Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4762Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4667Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4654Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4617Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4659Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4628Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4643Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4619Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4648Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4690Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4771Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4801Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4828Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4839Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4822Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4858Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4856Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4895Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4941Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4945Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4922Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4944Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4964Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4975Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4985Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4986Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4973Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4961Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4970Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4972Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5007Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5027Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5034Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5034Epoch 5/10: [================              ] 40/75 batches, loss: 0.5046Epoch 5/10: [================              ] 41/75 batches, loss: 0.5046Epoch 5/10: [================              ] 42/75 batches, loss: 0.5079Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5067Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5092Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5086Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5090Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5079Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5098Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5087Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5086Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5094Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5102Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5100Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5091Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5103Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5097Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5096Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5111Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5118Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5128Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5127Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5122Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5105Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5104Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5096Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5102Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5108Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5107Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5116Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5105Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5104Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5100Epoch 5/10: [==============================] 75/75 batches, loss: 0.5088
[2025-05-01 11:46:30,237][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5088
[2025-05-01 11:46:30,478][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5249, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:30,479][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.5510Epoch 6/10: [                              ] 2/75 batches, loss: 0.4920Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5119Epoch 6/10: [=                             ] 4/75 batches, loss: 0.4921Epoch 6/10: [==                            ] 5/75 batches, loss: 0.4991Epoch 6/10: [==                            ] 6/75 batches, loss: 0.4840Epoch 6/10: [==                            ] 7/75 batches, loss: 0.4969Epoch 6/10: [===                           ] 8/75 batches, loss: 0.4978Epoch 6/10: [===                           ] 9/75 batches, loss: 0.5090Epoch 6/10: [====                          ] 10/75 batches, loss: 0.5132Epoch 6/10: [====                          ] 11/75 batches, loss: 0.5102Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5155Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5146Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5172Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5115Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5110Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5106Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5116Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5124Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5108Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5104Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5069Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5058Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5047Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5075Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5064Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5037Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5071Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5078Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5085Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5068Epoch 6/10: [============                  ] 32/75 batches, loss: 0.5059Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.5051Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.5079Epoch 6/10: [==============                ] 35/75 batches, loss: 0.5091Epoch 6/10: [==============                ] 36/75 batches, loss: 0.5109Epoch 6/10: [==============                ] 37/75 batches, loss: 0.5101Epoch 6/10: [===============               ] 38/75 batches, loss: 0.5087Epoch 6/10: [===============               ] 39/75 batches, loss: 0.5086Epoch 6/10: [================              ] 40/75 batches, loss: 0.5091Epoch 6/10: [================              ] 41/75 batches, loss: 0.5090Epoch 6/10: [================              ] 42/75 batches, loss: 0.5077Epoch 6/10: [=================             ] 43/75 batches, loss: 0.5071Epoch 6/10: [=================             ] 44/75 batches, loss: 0.5059Epoch 6/10: [==================            ] 45/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 46/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 47/75 batches, loss: 0.5032Epoch 6/10: [===================           ] 48/75 batches, loss: 0.5037Epoch 6/10: [===================           ] 49/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 50/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 51/75 batches, loss: 0.5047Epoch 6/10: [====================          ] 52/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 55/75 batches, loss: 0.5037Epoch 6/10: [======================        ] 56/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 57/75 batches, loss: 0.5041Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.5049Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5061Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5073Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5064Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5064Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5063Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5052Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5055Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5064Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5074Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5077Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5089Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5085Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5078Epoch 6/10: [==============================] 75/75 batches, loss: 0.5071
[2025-05-01 11:46:38,077][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5071
[2025-05-01 11:46:38,296][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5256, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:38,297][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.4801Epoch 7/10: [                              ] 2/75 batches, loss: 0.4917Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5196Epoch 7/10: [=                             ] 4/75 batches, loss: 0.4919Epoch 7/10: [==                            ] 5/75 batches, loss: 0.4895Epoch 7/10: [==                            ] 6/75 batches, loss: 0.4959Epoch 7/10: [==                            ] 7/75 batches, loss: 0.4902Epoch 7/10: [===                           ] 8/75 batches, loss: 0.4859Epoch 7/10: [===                           ] 9/75 batches, loss: 0.4852Epoch 7/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 7/10: [====                          ] 11/75 batches, loss: 0.4886Epoch 7/10: [====                          ] 12/75 batches, loss: 0.4858Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.4909Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.4969Epoch 7/10: [======                        ] 15/75 batches, loss: 0.4989Epoch 7/10: [======                        ] 16/75 batches, loss: 0.4992Epoch 7/10: [======                        ] 17/75 batches, loss: 0.5009Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.5089Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.5050Epoch 7/10: [========                      ] 20/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 21/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 22/75 batches, loss: 0.5026Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.5047Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5037Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5065Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5082Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5072Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.5096Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.5102Epoch 7/10: [============                  ] 30/75 batches, loss: 0.5084Epoch 7/10: [============                  ] 31/75 batches, loss: 0.5052Epoch 7/10: [============                  ] 32/75 batches, loss: 0.5044Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.5037Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.5058Epoch 7/10: [==============                ] 35/75 batches, loss: 0.5057Epoch 7/10: [==============                ] 36/75 batches, loss: 0.5069Epoch 7/10: [==============                ] 37/75 batches, loss: 0.5068Epoch 7/10: [===============               ] 38/75 batches, loss: 0.5074Epoch 7/10: [===============               ] 39/75 batches, loss: 0.5079Epoch 7/10: [================              ] 40/75 batches, loss: 0.5101Epoch 7/10: [================              ] 41/75 batches, loss: 0.5100Epoch 7/10: [================              ] 42/75 batches, loss: 0.5093Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5086Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5074Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5089Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5093Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5096Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5100Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5099Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5102Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5092Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5100Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5101Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5082Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5090Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5080Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5080Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5083Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5066Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5069Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5073Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5068Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5052Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5052Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5059Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5062Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5055Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5074Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5067Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5070Epoch 7/10: [==============================] 75/75 batches, loss: 0.5062
[2025-05-01 11:46:45,886][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5062
[2025-05-01 11:46:46,114][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5262, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Training completed in 57.06 seconds
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:46:48,974][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9874476987447699, 'f1': 0.9872988992379339, 'precision': 1.0, 'recall': 0.9749163879598662}
[2025-05-01 11:46:48,975][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:48,975][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9074074074074074, 'precision': 0.9245283018867925, 'recall': 0.8909090909090909}
[2025-05-01 11:46:50,649][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/fi/model.pt
[2025-05-01 11:46:50,654][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁███
wandb:           best_val_f1 ▁███
wandb:         best_val_loss █▅▁▁
wandb:    best_val_precision ▁███
wandb:       best_val_recall ▁███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▄▄▄▄
wandb:            train_loss █▇▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁██████
wandb:                val_f1 ▁██████
wandb:              val_loss █▅▁▁▁▁▁
wandb:         val_precision ▁██████
wandb:            val_recall ▁██████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.96825
wandb:           best_val_f1 0.96552
wandb:         best_val_loss 0.52477
wandb:    best_val_precision 1
wandb:       best_val_recall 0.93333
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.90909
wandb:         final_test_f1 0.90741
wandb:  final_test_precision 0.92453
wandb:     final_test_recall 0.89091
wandb:  final_train_accuracy 0.98745
wandb:        final_train_f1 0.9873
wandb: final_train_precision 1
wandb:    final_train_recall 0.97492
wandb:    final_val_accuracy 0.96825
wandb:          final_val_f1 0.96552
wandb:   final_val_precision 1
wandb:      final_val_recall 0.93333
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50621
wandb:            train_time 57.06461
wandb:          val_accuracy 0.96825
wandb:                val_f1 0.96552
wandb:              val_loss 0.52622
wandb:         val_precision 1
wandb:            val_recall 0.93333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114538-y3q4fxeo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114538-y3q4fxeo/logs
Experiment finetune_question_type_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/results.json
Running experiment: finetune_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:47:02,909][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi
experiment_name: finetune_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:47:02,909][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:47:02,910][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:47:02,910][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:47:02,914][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-01 11:47:02,914][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:47:04,441][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:47:06,683][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:47:06,683][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:06,742][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,768][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,919][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 11:47:06,928][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:06,929][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 11:47:06,930][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:06,950][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,992][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 11:47:06,993][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:06,993][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 11:47:06,994][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:07,015][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:07,045][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:07,059][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 11:47:07,061][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:07,061][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,063][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,063][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,065][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:47:07,065][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:47:07,065][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:47:11,073][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:47:11,079][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:47:11,080][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:47:11,080][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:47:11,080][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 11:47:11,081][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:47:11,081][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1450Epoch 1/10: [                              ] 2/75 batches, loss: 0.1576Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1505Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1437Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1353Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1373Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1349Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1362Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1400Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1364Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1313Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1318Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1303Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1276Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1253Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1244Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1274Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1266Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1242Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1213Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1182Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1176Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1157Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1135Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1126Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1126Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1108Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1082Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1064Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1054Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1033Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1019Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1006Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0997Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0985Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0969Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0959Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0951Epoch 1/10: [================              ] 40/75 batches, loss: 0.0934Epoch 1/10: [================              ] 41/75 batches, loss: 0.0925Epoch 1/10: [================              ] 42/75 batches, loss: 0.0910Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0903Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0899Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0885Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0881Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0873Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0862Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0856Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0846Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0836Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0828Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0822Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0810Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0803Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0793Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0785Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0778Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0769Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0762Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0755Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0745Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0736Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0728Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0722Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0713Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0707Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0700Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0693Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0684Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0679Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0673Epoch 1/10: [==============================] 75/75 batches, loss: 0.0666
[2025-05-01 11:47:21,275][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0666
[2025-05-01 11:47:21,483][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0897, Metrics: {'mse': 0.08961991965770721, 'rmse': 0.2993658625456604, 'r2': -0.3669794797897339}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0185Epoch 2/10: [                              ] 2/75 batches, loss: 0.0216Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0224Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0247Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0270Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0252Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0243Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0235Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0239Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0246Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0251Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0246Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0246Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0241Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0252Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0249Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0241Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0248Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0242Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0240Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0237Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0232Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0233Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0229Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0226Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0224Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0225Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0221Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0218Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0215Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0219Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0219Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0217Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0215Epoch 2/10: [================              ] 40/75 batches, loss: 0.0215Epoch 2/10: [================              ] 41/75 batches, loss: 0.0216Epoch 2/10: [================              ] 42/75 batches, loss: 0.0214Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0213Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0213Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0212Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0211Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0213Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0212Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0211Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0212Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0212Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0214Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0213Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0212Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0211Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0211Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0210Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0212Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0211Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0212Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0211Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0213Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0215Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0216Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0218Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0217Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0216Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0215Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0213Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0213Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0213Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0214Epoch 2/10: [==============================] 75/75 batches, loss: 0.0215
[2025-05-01 11:47:29,466][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0215
[2025-05-01 11:47:29,688][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0582, Metrics: {'mse': 0.05815792828798294, 'rmse': 0.24115954944389603, 'r2': 0.11291265487670898}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0139Epoch 3/10: [                              ] 2/75 batches, loss: 0.0152Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0157Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0206Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0213Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0199Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0197Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0194Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0190Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0187Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0181Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0180Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0186Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0185Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0191Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0201Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0204Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0211Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0216Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0212Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0210Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0208Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0208Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0205Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0203Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0203Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0203Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0202Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0200Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0197Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0197Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0198Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0195Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0194Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0195Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0195Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0194Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0192Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0190Epoch 3/10: [================              ] 40/75 batches, loss: 0.0189Epoch 3/10: [================              ] 41/75 batches, loss: 0.0192Epoch 3/10: [================              ] 42/75 batches, loss: 0.0190Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0189Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0189Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0188Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0187Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0185Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0185Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0184Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0186Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0185Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0185Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0184Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0183Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0185Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0184Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0182Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0183Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0183Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0184Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0183Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0182Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0181Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0183Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0182Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0182Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0184Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0182Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0181Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0182Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0182Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0182Epoch 3/10: [==============================] 75/75 batches, loss: 0.0181
[2025-05-01 11:47:37,703][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0181
[2025-05-01 11:47:37,941][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0381, Metrics: {'mse': 0.03821783885359764, 'rmse': 0.19549383328790104, 'r2': 0.41706037521362305}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0304Epoch 4/10: [                              ] 2/75 batches, loss: 0.0258Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0251Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0206Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0201Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0202Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0213Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0205Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0197Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0184Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0181Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0177Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0186Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0182Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0183Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0182Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0181Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0189Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0184Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0185Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0182Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0180Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0180Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0178Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0176Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0177Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0177Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0175Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0180Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0179Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0176Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0173Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0171Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0169Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0166Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0171Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0170Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0169Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0168Epoch 4/10: [================              ] 40/75 batches, loss: 0.0165Epoch 4/10: [================              ] 41/75 batches, loss: 0.0164Epoch 4/10: [================              ] 42/75 batches, loss: 0.0164Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0163Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0161Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0163Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0165Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0165Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0167Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0165Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0164Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0165Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0163Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0163Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0163Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0163Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0162Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0162Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0163Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0162Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0160Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0159Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0159Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0161Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0160Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0158Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0157Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0157Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0158Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0158Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0159Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0160Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0159Epoch 4/10: [==============================] 75/75 batches, loss: 0.0158
[2025-05-01 11:47:45,886][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0158
[2025-05-01 11:47:46,120][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0404, Metrics: {'mse': 0.04041058570146561, 'rmse': 0.20102384361429768, 'r2': 0.3836142420768738}
[2025-05-01 11:47:46,121][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0189Epoch 5/10: [                              ] 2/75 batches, loss: 0.0134Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0116Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0109Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0097Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0093Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0107Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0109Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0107Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0105Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0111Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0111Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0111Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0110Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0107Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0107Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0106Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0105Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0116Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0114Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0118Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0116Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0116Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0117Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0117Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0115Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0112Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0114Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0113Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0114Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0115Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0113Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0112Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0112Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0111Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0111Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0112Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0110Epoch 5/10: [================              ] 40/75 batches, loss: 0.0111Epoch 5/10: [================              ] 41/75 batches, loss: 0.0111Epoch 5/10: [================              ] 42/75 batches, loss: 0.0112Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0113Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0112Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0112Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0113Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0113Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0111Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0111Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0117Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0117Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0117Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0116Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0115Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0115Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0114Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0114Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0114Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0113Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0113Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0114Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0113Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0113Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0112Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0112Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0111Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0111Epoch 5/10: [==============================] 75/75 batches, loss: 0.0110
[2025-05-01 11:47:53,717][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0110
[2025-05-01 11:47:53,949][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0284, Metrics: {'mse': 0.028346620500087738, 'rmse': 0.1683645464463577, 'r2': 0.5676268339157104}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0123Epoch 6/10: [                              ] 2/75 batches, loss: 0.0126Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0146Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0127Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0115Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0119Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0118Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0117Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0120Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0115Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0114Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0114Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0117Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0114Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0113Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0112Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0109Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0116Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0118Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0115Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0113Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0113Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0113Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0111Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0113Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0112Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0110Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0112Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0112Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0111Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0114Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0113Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0112Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0112Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0111Epoch 6/10: [================              ] 40/75 batches, loss: 0.0110Epoch 6/10: [================              ] 41/75 batches, loss: 0.0112Epoch 6/10: [================              ] 42/75 batches, loss: 0.0111Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0110Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0109Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0110Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0110Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0108Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0109Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0108Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0108Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0106Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0105Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0105Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0104Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0104Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0103Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0104Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0104Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0103Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0102Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0102Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0102Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0102Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0101Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0102Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0102Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0101Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0101Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0102Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0102Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0101Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0100Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0100Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0099Epoch 6/10: [==============================] 75/75 batches, loss: 0.0099
[2025-05-01 11:48:01,929][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0099
[2025-05-01 11:48:02,159][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0326, Metrics: {'mse': 0.03258148208260536, 'rmse': 0.18050341293893965, 'r2': 0.5030322074890137}
[2025-05-01 11:48:02,160][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0066Epoch 7/10: [                              ] 2/75 batches, loss: 0.0083Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0081Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0073Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0079Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0072Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0069Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0070Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0068Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0073Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0076Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0074Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0073Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0074Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0072Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0073Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0073Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0073Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0076Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0074Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0074Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0072Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0074Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0074Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0076Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0076Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0076Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0080Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0080Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0082Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0081Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0080Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0081Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0079Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0080Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0080Epoch 7/10: [================              ] 40/75 batches, loss: 0.0079Epoch 7/10: [================              ] 41/75 batches, loss: 0.0078Epoch 7/10: [================              ] 42/75 batches, loss: 0.0078Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0078Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0077Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0079Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0079Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0080Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0081Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0080Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0080Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0080Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0079Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0079Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0081Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0081Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0081Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0081Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0082Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0083Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0083Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0083Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0083Epoch 7/10: [==============================] 75/75 batches, loss: 0.0083
[2025-05-01 11:48:09,761][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0083
[2025-05-01 11:48:09,982][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0273, Metrics: {'mse': 0.027256017550826073, 'rmse': 0.16509396582197083, 'r2': 0.5842618942260742}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0106Epoch 8/10: [                              ] 2/75 batches, loss: 0.0070Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0100Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0100Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0087Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0088Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0080Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0073Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0077Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0074Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0071Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0072Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0076Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0073Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0073Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0072Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0071Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0069Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0072Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0075Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0074Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0074Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0076Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0076Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0075Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0080Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0082Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0081Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0081Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0080Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0080Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0081Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0080Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0080Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0079Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0078Epoch 8/10: [================              ] 40/75 batches, loss: 0.0077Epoch 8/10: [================              ] 41/75 batches, loss: 0.0077Epoch 8/10: [================              ] 42/75 batches, loss: 0.0077Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0077Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0078Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0078Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0079Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0079Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0078Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0078Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0077Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0077Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0077Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0076Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0078Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0078Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0077Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0077Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0076Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0076Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0077Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0077Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0078Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0078Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0078Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0077Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0078Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0079Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0079Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0079Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0079Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0078Epoch 8/10: [==============================] 75/75 batches, loss: 0.0079
[2025-05-01 11:48:18,002][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0079
[2025-05-01 11:48:18,236][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0256, Metrics: {'mse': 0.025504937395453453, 'rmse': 0.15970265306328962, 'r2': 0.610971212387085}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0075Epoch 9/10: [                              ] 2/75 batches, loss: 0.0073Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0067Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0061Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0065Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0070Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0065Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0060Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0061Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0062Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0058Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0057Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0056Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0057Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0059Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0059Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0059Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0060Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0059Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0059Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0059Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0059Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0059Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0061Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0063Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0063Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0065Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0065Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0064Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0064Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0064Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0063Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0062Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0066Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0066Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0067Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0068Epoch 9/10: [================              ] 40/75 batches, loss: 0.0067Epoch 9/10: [================              ] 41/75 batches, loss: 0.0067Epoch 9/10: [================              ] 42/75 batches, loss: 0.0066Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0066Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0066Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0066Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0065Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0066Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0067Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0066Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0069Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0068Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0068Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0068Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0070Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0069Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0069Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0068Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0068Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0067Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0067Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0067Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0067Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0066Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0067Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0067Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0066Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0066Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0066Epoch 9/10: [==============================] 75/75 batches, loss: 0.0066
[2025-05-01 11:48:26,218][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0066
[2025-05-01 11:48:26,451][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0370, Metrics: {'mse': 0.036957599222660065, 'rmse': 0.19224359345023714, 'r2': 0.4362828731536865}
[2025-05-01 11:48:26,451][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0074Epoch 10/10: [                              ] 2/75 batches, loss: 0.0076Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0110Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0093Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0093Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0086Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0079Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0075Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0078Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0073Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0072Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0070Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0068Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0072Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0071Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0070Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0069Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0067Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0065Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0067Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0067Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0072Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0071Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0069Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0069Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0069Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0070Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0070Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0071Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0071Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0074Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0072Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0075Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0076Epoch 10/10: [================              ] 40/75 batches, loss: 0.0075Epoch 10/10: [================              ] 41/75 batches, loss: 0.0076Epoch 10/10: [================              ] 42/75 batches, loss: 0.0075Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0075Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0074Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0074Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0074Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0073Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0073Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0072Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0072Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0071Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0071Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0070Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0070Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0069Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0071Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0071Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0070Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0070Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0070Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0069Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0069Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0069Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0069Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0070Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0069Epoch 10/10: [==============================] 75/75 batches, loss: 0.0070
[2025-05-01 11:48:34,044][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0070
[2025-05-01 11:48:34,285][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0246, Metrics: {'mse': 0.024519970640540123, 'rmse': 0.15658853930138095, 'r2': 0.6259950399398804}
[2025-05-01 11:48:34,662][src.training.lm_trainer][INFO] - Training completed in 81.71 seconds
[2025-05-01 11:48:34,662][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.004295824561268091, 'rmse': 0.06554254008861796, 'r2': 0.7874200940132141}
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.024519970640540123, 'rmse': 0.15658853930138095, 'r2': 0.6259950399398804}
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02496601827442646, 'rmse': 0.1580063868153008, 'r2': 0.3677409291267395}
[2025-05-01 11:48:39,245][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/fi/model.pt
[2025-05-01 11:48:39,250][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▁▁▁▁
wandb:     best_val_mse █▅▂▁▁▁▁
wandb:      best_val_r2 ▁▄▇████
wandb:    best_val_rmse █▅▃▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▇▇▇▇▇▇▇
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▃▁▂▁▁▂▁
wandb:          val_mse █▅▂▃▁▂▁▁▂▁
wandb:           val_r2 ▁▄▇▆█▇██▇█
wandb:         val_rmse █▅▃▃▂▂▁▁▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02459
wandb:     best_val_mse 0.02452
wandb:      best_val_r2 0.626
wandb:    best_val_rmse 0.15659
wandb:            epoch 10
wandb:   final_test_mse 0.02497
wandb:    final_test_r2 0.36774
wandb:  final_test_rmse 0.15801
wandb:  final_train_mse 0.0043
wandb:   final_train_r2 0.78742
wandb: final_train_rmse 0.06554
wandb:    final_val_mse 0.02452
wandb:     final_val_r2 0.626
wandb:   final_val_rmse 0.15659
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00696
wandb:       train_time 81.70911
wandb:         val_loss 0.02459
wandb:          val_mse 0.02452
wandb:           val_r2 0.626
wandb:         val_rmse 0.15659
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114702-okidsjni
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114702-okidsjni/logs
Experiment finetune_complexity_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/results.json
Running experiment: finetune_question_type_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:48:50,980][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/id
experiment_name: finetune_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:48:50,980][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:48:50,980][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:48:50,980][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:48:50,984][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-01 11:48:50,985][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:48:52,403][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:48:54,763][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:48:54,763][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:54,837][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:54,862][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:54,956][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-01 11:48:54,962][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:54,963][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-01 11:48:54,964][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:54,980][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,021][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-01 11:48:55,022][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:55,022][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-01 11:48:55,023][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:55,040][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,072][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,095][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-01 11:48:55,096][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:55,096][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-01 11:48:55,097][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-01 11:48:55,097][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,098][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-01 11:48:55,098][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,099][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:48:55,099][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,100][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:48:55,100][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:48:55,100][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:48:55,101][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:48:59,036][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:48:59,041][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:48:59,042][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:48:59,042][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:48:59,042][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-01 11:48:59,043][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:48:59,043][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.7521Epoch 1/10: [=                             ] 2/60 batches, loss: 0.7363Epoch 1/10: [=                             ] 3/60 batches, loss: 0.7513Epoch 1/10: [==                            ] 4/60 batches, loss: 0.7502Epoch 1/10: [==                            ] 5/60 batches, loss: 0.7440Epoch 1/10: [===                           ] 6/60 batches, loss: 0.7388Epoch 1/10: [===                           ] 7/60 batches, loss: 0.7312Epoch 1/10: [====                          ] 8/60 batches, loss: 0.7329Epoch 1/10: [====                          ] 9/60 batches, loss: 0.7377Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.7322Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.7280Epoch 1/10: [======                        ] 12/60 batches, loss: 0.7251Epoch 1/10: [======                        ] 13/60 batches, loss: 0.7240Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.7208Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.7172Epoch 1/10: [========                      ] 16/60 batches, loss: 0.7187Epoch 1/10: [========                      ] 17/60 batches, loss: 0.7190Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.7211Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.7205Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.7197Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.7221Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.7180Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.7199Epoch 1/10: [============                  ] 24/60 batches, loss: 0.7207Epoch 1/10: [============                  ] 25/60 batches, loss: 0.7215Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.7217Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.7211Epoch 1/10: [==============                ] 28/60 batches, loss: 0.7209Epoch 1/10: [==============                ] 29/60 batches, loss: 0.7225Epoch 1/10: [===============               ] 30/60 batches, loss: 0.7256Epoch 1/10: [===============               ] 31/60 batches, loss: 0.7264Epoch 1/10: [================              ] 32/60 batches, loss: 0.7270Epoch 1/10: [================              ] 33/60 batches, loss: 0.7273Epoch 1/10: [=================             ] 34/60 batches, loss: 0.7298Epoch 1/10: [=================             ] 35/60 batches, loss: 0.7279Epoch 1/10: [==================            ] 36/60 batches, loss: 0.7283Epoch 1/10: [==================            ] 37/60 batches, loss: 0.7273Epoch 1/10: [===================           ] 38/60 batches, loss: 0.7290Epoch 1/10: [===================           ] 39/60 batches, loss: 0.7277Epoch 1/10: [====================          ] 40/60 batches, loss: 0.7283Epoch 1/10: [====================          ] 41/60 batches, loss: 0.7298Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.7310Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.7290Epoch 1/10: [======================        ] 44/60 batches, loss: 0.7269Epoch 1/10: [======================        ] 45/60 batches, loss: 0.7273Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.7251Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.7280Epoch 1/10: [========================      ] 48/60 batches, loss: 0.7275Epoch 1/10: [========================      ] 49/60 batches, loss: 0.7270Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.7260Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.7233Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.7238Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.7239Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.7246Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.7253Epoch 1/10: [============================  ] 56/60 batches, loss: 0.7240Epoch 1/10: [============================  ] 57/60 batches, loss: 0.7223Epoch 1/10: [============================= ] 58/60 batches, loss: 0.7226Epoch 1/10: [============================= ] 59/60 batches, loss: 0.7223Epoch 1/10: [==============================] 60/60 batches, loss: 0.7213
[2025-05-01 11:49:07,386][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7213
[2025-05-01 11:49:07,622][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7009, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.6761Epoch 2/10: [=                             ] 2/60 batches, loss: 0.6724Epoch 2/10: [=                             ] 3/60 batches, loss: 0.6818Epoch 2/10: [==                            ] 4/60 batches, loss: 0.6797Epoch 2/10: [==                            ] 5/60 batches, loss: 0.6778Epoch 2/10: [===                           ] 6/60 batches, loss: 0.6730Epoch 2/10: [===                           ] 7/60 batches, loss: 0.6691Epoch 2/10: [====                          ] 8/60 batches, loss: 0.6773Epoch 2/10: [====                          ] 9/60 batches, loss: 0.6744Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.6687Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.6715Epoch 2/10: [======                        ] 12/60 batches, loss: 0.6692Epoch 2/10: [======                        ] 13/60 batches, loss: 0.6658Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.6655Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.6668Epoch 2/10: [========                      ] 16/60 batches, loss: 0.6661Epoch 2/10: [========                      ] 17/60 batches, loss: 0.6658Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.6608Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.6580Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.6576Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.6569Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.6568Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.6539Epoch 2/10: [============                  ] 24/60 batches, loss: 0.6527Epoch 2/10: [============                  ] 25/60 batches, loss: 0.6526Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.6506Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.6477Epoch 2/10: [==============                ] 28/60 batches, loss: 0.6461Epoch 2/10: [==============                ] 29/60 batches, loss: 0.6448Epoch 2/10: [===============               ] 30/60 batches, loss: 0.6436Epoch 2/10: [===============               ] 31/60 batches, loss: 0.6414Epoch 2/10: [================              ] 32/60 batches, loss: 0.6390Epoch 2/10: [================              ] 33/60 batches, loss: 0.6353Epoch 2/10: [=================             ] 34/60 batches, loss: 0.6345Epoch 2/10: [=================             ] 35/60 batches, loss: 0.6327Epoch 2/10: [==================            ] 36/60 batches, loss: 0.6305Epoch 2/10: [==================            ] 37/60 batches, loss: 0.6301Epoch 2/10: [===================           ] 38/60 batches, loss: 0.6281Epoch 2/10: [===================           ] 39/60 batches, loss: 0.6278Epoch 2/10: [====================          ] 40/60 batches, loss: 0.6267Epoch 2/10: [====================          ] 41/60 batches, loss: 0.6269Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.6254Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.6251Epoch 2/10: [======================        ] 44/60 batches, loss: 0.6224Epoch 2/10: [======================        ] 45/60 batches, loss: 0.6206Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.6201Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.6186Epoch 2/10: [========================      ] 48/60 batches, loss: 0.6177Epoch 2/10: [========================      ] 49/60 batches, loss: 0.6170Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.6173Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.6158Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.6151Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.6148Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.6123Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.6098Epoch 2/10: [============================  ] 56/60 batches, loss: 0.6089Epoch 2/10: [============================  ] 57/60 batches, loss: 0.6094Epoch 2/10: [============================= ] 58/60 batches, loss: 0.6088Epoch 2/10: [============================= ] 59/60 batches, loss: 0.6083Epoch 2/10: [==============================] 60/60 batches, loss: 0.6082
[2025-05-01 11:49:14,091][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6082
[2025-05-01 11:49:14,335][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6023, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.5746Epoch 3/10: [=                             ] 2/60 batches, loss: 0.5856Epoch 3/10: [=                             ] 3/60 batches, loss: 0.5772Epoch 3/10: [==                            ] 4/60 batches, loss: 0.5617Epoch 3/10: [==                            ] 5/60 batches, loss: 0.5615Epoch 3/10: [===                           ] 6/60 batches, loss: 0.5676Epoch 3/10: [===                           ] 7/60 batches, loss: 0.5694Epoch 3/10: [====                          ] 8/60 batches, loss: 0.5549Epoch 3/10: [====                          ] 9/60 batches, loss: 0.5505Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.5467Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.5433Epoch 3/10: [======                        ] 12/60 batches, loss: 0.5436Epoch 3/10: [======                        ] 13/60 batches, loss: 0.5429Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.5424Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.5434Epoch 3/10: [========                      ] 16/60 batches, loss: 0.5437Epoch 3/10: [========                      ] 17/60 batches, loss: 0.5459Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.5464Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.5481Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.5500Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.5482Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.5464Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.5458Epoch 3/10: [============                  ] 24/60 batches, loss: 0.5445Epoch 3/10: [============                  ] 25/60 batches, loss: 0.5448Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.5446Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.5459Epoch 3/10: [==============                ] 28/60 batches, loss: 0.5462Epoch 3/10: [==============                ] 29/60 batches, loss: 0.5448Epoch 3/10: [===============               ] 30/60 batches, loss: 0.5451Epoch 3/10: [===============               ] 31/60 batches, loss: 0.5446Epoch 3/10: [================              ] 32/60 batches, loss: 0.5456Epoch 3/10: [================              ] 33/60 batches, loss: 0.5466Epoch 3/10: [=================             ] 34/60 batches, loss: 0.5440Epoch 3/10: [=================             ] 35/60 batches, loss: 0.5430Epoch 3/10: [==================            ] 36/60 batches, loss: 0.5454Epoch 3/10: [==================            ] 37/60 batches, loss: 0.5444Epoch 3/10: [===================           ] 38/60 batches, loss: 0.5427Epoch 3/10: [===================           ] 39/60 batches, loss: 0.5418Epoch 3/10: [====================          ] 40/60 batches, loss: 0.5420Epoch 3/10: [====================          ] 41/60 batches, loss: 0.5400Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.5397Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.5372Epoch 3/10: [======================        ] 44/60 batches, loss: 0.5359Epoch 3/10: [======================        ] 45/60 batches, loss: 0.5363Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.5356Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.5365Epoch 3/10: [========================      ] 48/60 batches, loss: 0.5344Epoch 3/10: [========================      ] 49/60 batches, loss: 0.5338Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.5342Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.5345Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.5344Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.5339Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.5325Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.5324Epoch 3/10: [============================  ] 56/60 batches, loss: 0.5324Epoch 3/10: [============================  ] 57/60 batches, loss: 0.5319Epoch 3/10: [============================= ] 58/60 batches, loss: 0.5318Epoch 3/10: [============================= ] 59/60 batches, loss: 0.5310Epoch 3/10: [==============================] 60/60 batches, loss: 0.5318
[2025-05-01 11:49:20,864][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5318
[2025-05-01 11:49:21,126][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6003, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.4824Epoch 4/10: [=                             ] 2/60 batches, loss: 0.5049Epoch 4/10: [=                             ] 3/60 batches, loss: 0.4810Epoch 4/10: [==                            ] 4/60 batches, loss: 0.5049Epoch 4/10: [==                            ] 5/60 batches, loss: 0.5141Epoch 4/10: [===                           ] 6/60 batches, loss: 0.5124Epoch 4/10: [===                           ] 7/60 batches, loss: 0.5145Epoch 4/10: [====                          ] 8/60 batches, loss: 0.5073Epoch 4/10: [====                          ] 9/60 batches, loss: 0.5203Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.5188Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.5239Epoch 4/10: [======                        ] 12/60 batches, loss: 0.5301Epoch 4/10: [======                        ] 13/60 batches, loss: 0.5336Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.5332Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.5297Epoch 4/10: [========                      ] 16/60 batches, loss: 0.5325Epoch 4/10: [========                      ] 17/60 batches, loss: 0.5308Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.5307Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.5350Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.5348Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.5323Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.5332Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.5351Epoch 4/10: [============                  ] 24/60 batches, loss: 0.5348Epoch 4/10: [============                  ] 25/60 batches, loss: 0.5335Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.5351Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.5340Epoch 4/10: [==============                ] 28/60 batches, loss: 0.5338Epoch 4/10: [==============                ] 29/60 batches, loss: 0.5357Epoch 4/10: [===============               ] 30/60 batches, loss: 0.5375Epoch 4/10: [===============               ] 31/60 batches, loss: 0.5387Epoch 4/10: [================              ] 32/60 batches, loss: 0.5391Epoch 4/10: [================              ] 33/60 batches, loss: 0.5395Epoch 4/10: [=================             ] 34/60 batches, loss: 0.5384Epoch 4/10: [=================             ] 35/60 batches, loss: 0.5368Epoch 4/10: [==================            ] 36/60 batches, loss: 0.5352Epoch 4/10: [==================            ] 37/60 batches, loss: 0.5370Epoch 4/10: [===================           ] 38/60 batches, loss: 0.5367Epoch 4/10: [===================           ] 39/60 batches, loss: 0.5353Epoch 4/10: [====================          ] 40/60 batches, loss: 0.5363Epoch 4/10: [====================          ] 41/60 batches, loss: 0.5367Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.5364Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.5357Epoch 4/10: [======================        ] 44/60 batches, loss: 0.5333Epoch 4/10: [======================        ] 45/60 batches, loss: 0.5316Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.5321Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.5299Epoch 4/10: [========================      ] 48/60 batches, loss: 0.5309Epoch 4/10: [========================      ] 49/60 batches, loss: 0.5294Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.5284Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.5293Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.5284Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.5270Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.5266Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.5262Epoch 4/10: [============================  ] 56/60 batches, loss: 0.5266Epoch 4/10: [============================  ] 57/60 batches, loss: 0.5266Epoch 4/10: [============================= ] 58/60 batches, loss: 0.5263Epoch 4/10: [============================= ] 59/60 batches, loss: 0.5251Epoch 4/10: [==============================] 60/60 batches, loss: 0.5234
[2025-05-01 11:49:27,589][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5234
[2025-05-01 11:49:27,864][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5991, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.4564Epoch 5/10: [=                             ] 2/60 batches, loss: 0.4452Epoch 5/10: [=                             ] 3/60 batches, loss: 0.4647Epoch 5/10: [==                            ] 4/60 batches, loss: 0.4808Epoch 5/10: [==                            ] 5/60 batches, loss: 0.5046Epoch 5/10: [===                           ] 6/60 batches, loss: 0.5126Epoch 5/10: [===                           ] 7/60 batches, loss: 0.5214Epoch 5/10: [====                          ] 8/60 batches, loss: 0.5193Epoch 5/10: [====                          ] 9/60 batches, loss: 0.5255Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.5209Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.5236Epoch 5/10: [======                        ] 12/60 batches, loss: 0.5279Epoch 5/10: [======                        ] 13/60 batches, loss: 0.5297Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.5295Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.5278Epoch 5/10: [========                      ] 16/60 batches, loss: 0.5263Epoch 5/10: [========                      ] 17/60 batches, loss: 0.5250Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.5212Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.5178Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.5169Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.5185Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.5229Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.5262Epoch 5/10: [============                  ] 24/60 batches, loss: 0.5262Epoch 5/10: [============                  ] 25/60 batches, loss: 0.5234Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.5254Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.5264Epoch 5/10: [==============                ] 28/60 batches, loss: 0.5230Epoch 5/10: [==============                ] 29/60 batches, loss: 0.5240Epoch 5/10: [===============               ] 30/60 batches, loss: 0.5273Epoch 5/10: [===============               ] 31/60 batches, loss: 0.5250Epoch 5/10: [================              ] 32/60 batches, loss: 0.5213Epoch 5/10: [================              ] 33/60 batches, loss: 0.5208Epoch 5/10: [=================             ] 34/60 batches, loss: 0.5203Epoch 5/10: [=================             ] 35/60 batches, loss: 0.5212Epoch 5/10: [==================            ] 36/60 batches, loss: 0.5213Epoch 5/10: [==================            ] 37/60 batches, loss: 0.5247Epoch 5/10: [===================           ] 38/60 batches, loss: 0.5241Epoch 5/10: [===================           ] 39/60 batches, loss: 0.5230Epoch 5/10: [====================          ] 40/60 batches, loss: 0.5219Epoch 5/10: [====================          ] 41/60 batches, loss: 0.5236Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.5254Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.5243Epoch 5/10: [======================        ] 44/60 batches, loss: 0.5271Epoch 5/10: [======================        ] 45/60 batches, loss: 0.5260Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.5271Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.5266Epoch 5/10: [========================      ] 48/60 batches, loss: 0.5266Epoch 5/10: [========================      ] 49/60 batches, loss: 0.5266Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.5271Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.5252Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.5248Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.5235Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.5240Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.5237Epoch 5/10: [============================  ] 56/60 batches, loss: 0.5237Epoch 5/10: [============================  ] 57/60 batches, loss: 0.5234Epoch 5/10: [============================= ] 58/60 batches, loss: 0.5230Epoch 5/10: [============================= ] 59/60 batches, loss: 0.5227Epoch 5/10: [==============================] 60/60 batches, loss: 0.5237
[2025-05-01 11:49:34,446][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5237
[2025-05-01 11:49:34,720][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5851, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.5516Epoch 6/10: [=                             ] 2/60 batches, loss: 0.4919Epoch 6/10: [=                             ] 3/60 batches, loss: 0.4957Epoch 6/10: [==                            ] 4/60 batches, loss: 0.4977Epoch 6/10: [==                            ] 5/60 batches, loss: 0.5083Epoch 6/10: [===                           ] 6/60 batches, loss: 0.5076Epoch 6/10: [===                           ] 7/60 batches, loss: 0.5036Epoch 6/10: [====                          ] 8/60 batches, loss: 0.5037Epoch 6/10: [====                          ] 9/60 batches, loss: 0.5090Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.5084Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.5101Epoch 6/10: [======                        ] 12/60 batches, loss: 0.5187Epoch 6/10: [======                        ] 13/60 batches, loss: 0.5157Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.5165Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.5220Epoch 6/10: [========                      ] 16/60 batches, loss: 0.5224Epoch 6/10: [========                      ] 17/60 batches, loss: 0.5184Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.5176Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.5194Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.5209Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.5212Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.5215Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.5207Epoch 6/10: [============                  ] 24/60 batches, loss: 0.5230Epoch 6/10: [============                  ] 25/60 batches, loss: 0.5212Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.5233Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.5226Epoch 6/10: [==============                ] 28/60 batches, loss: 0.5219Epoch 6/10: [==============                ] 29/60 batches, loss: 0.5180Epoch 6/10: [===============               ] 30/60 batches, loss: 0.5159Epoch 6/10: [===============               ] 31/60 batches, loss: 0.5140Epoch 6/10: [================              ] 32/60 batches, loss: 0.5152Epoch 6/10: [================              ] 33/60 batches, loss: 0.5134Epoch 6/10: [=================             ] 34/60 batches, loss: 0.5138Epoch 6/10: [=================             ] 35/60 batches, loss: 0.5121Epoch 6/10: [==================            ] 36/60 batches, loss: 0.5125Epoch 6/10: [==================            ] 37/60 batches, loss: 0.5142Epoch 6/10: [===================           ] 38/60 batches, loss: 0.5146Epoch 6/10: [===================           ] 39/60 batches, loss: 0.5143Epoch 6/10: [====================          ] 40/60 batches, loss: 0.5158Epoch 6/10: [====================          ] 41/60 batches, loss: 0.5178Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.5169Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.5171Epoch 6/10: [======================        ] 44/60 batches, loss: 0.5174Epoch 6/10: [======================        ] 45/60 batches, loss: 0.5190Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.5192Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.5209Epoch 6/10: [========================      ] 48/60 batches, loss: 0.5195Epoch 6/10: [========================      ] 49/60 batches, loss: 0.5206Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.5231Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.5246Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.5247Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.5229Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.5221Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.5226Epoch 6/10: [============================  ] 56/60 batches, loss: 0.5214Epoch 6/10: [============================  ] 57/60 batches, loss: 0.5211Epoch 6/10: [============================= ] 58/60 batches, loss: 0.5216Epoch 6/10: [============================= ] 59/60 batches, loss: 0.5225Epoch 6/10: [==============================] 60/60 batches, loss: 0.5235
[2025-05-01 11:49:41,218][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5235
[2025-05-01 11:49:41,487][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5836, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.5750Epoch 7/10: [=                             ] 2/60 batches, loss: 0.5748Epoch 7/10: [=                             ] 3/60 batches, loss: 0.5905Epoch 7/10: [==                            ] 4/60 batches, loss: 0.5829Epoch 7/10: [==                            ] 5/60 batches, loss: 0.5765Epoch 7/10: [===                           ] 6/60 batches, loss: 0.5762Epoch 7/10: [===                           ] 7/60 batches, loss: 0.5794Epoch 7/10: [====                          ] 8/60 batches, loss: 0.5877Epoch 7/10: [====                          ] 9/60 batches, loss: 0.5948Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.5905Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.5912Epoch 7/10: [======                        ] 12/60 batches, loss: 0.5800Epoch 7/10: [======                        ] 13/60 batches, loss: 0.5723Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.5691Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.5695Epoch 7/10: [========                      ] 16/60 batches, loss: 0.5662Epoch 7/10: [========                      ] 17/60 batches, loss: 0.5667Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.5652Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.5595Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.5594Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.5534Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.5511Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.5504Epoch 7/10: [============                  ] 24/60 batches, loss: 0.5455Epoch 7/10: [============                  ] 25/60 batches, loss: 0.5467Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.5441Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.5417Epoch 7/10: [==============                ] 28/60 batches, loss: 0.5412Epoch 7/10: [==============                ] 29/60 batches, loss: 0.5408Epoch 7/10: [===============               ] 30/60 batches, loss: 0.5405Epoch 7/10: [===============               ] 31/60 batches, loss: 0.5417Epoch 7/10: [================              ] 32/60 batches, loss: 0.5397Epoch 7/10: [================              ] 33/60 batches, loss: 0.5387Epoch 7/10: [=================             ] 34/60 batches, loss: 0.5411Epoch 7/10: [=================             ] 35/60 batches, loss: 0.5407Epoch 7/10: [==================            ] 36/60 batches, loss: 0.5430Epoch 7/10: [==================            ] 37/60 batches, loss: 0.5426Epoch 7/10: [===================           ] 38/60 batches, loss: 0.5422Epoch 7/10: [===================           ] 39/60 batches, loss: 0.5424Epoch 7/10: [====================          ] 40/60 batches, loss: 0.5426Epoch 7/10: [====================          ] 41/60 batches, loss: 0.5428Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.5430Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.5422Epoch 7/10: [======================        ] 44/60 batches, loss: 0.5438Epoch 7/10: [======================        ] 45/60 batches, loss: 0.5419Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.5425Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.5432Epoch 7/10: [========================      ] 48/60 batches, loss: 0.5429Epoch 7/10: [========================      ] 49/60 batches, loss: 0.5411Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.5399Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.5392Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.5385Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.5369Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.5372Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.5345Epoch 7/10: [============================  ] 56/60 batches, loss: 0.5348Epoch 7/10: [============================  ] 57/60 batches, loss: 0.5346Epoch 7/10: [============================= ] 58/60 batches, loss: 0.5322Epoch 7/10: [============================= ] 59/60 batches, loss: 0.5317Epoch 7/10: [==============================] 60/60 batches, loss: 0.5313
[2025-05-01 11:49:47,987][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5313
[2025-05-01 11:49:48,239][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5935, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7142857142857143, 'precision': 1.0, 'recall': 0.5555555555555556}
[2025-05-01 11:49:48,241][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/60 batches, loss: 0.5508Epoch 8/10: [=                             ] 2/60 batches, loss: 0.5272Epoch 8/10: [=                             ] 3/60 batches, loss: 0.5112Epoch 8/10: [==                            ] 4/60 batches, loss: 0.5389Epoch 8/10: [==                            ] 5/60 batches, loss: 0.5365Epoch 8/10: [===                           ] 6/60 batches, loss: 0.5072Epoch 8/10: [===                           ] 7/60 batches, loss: 0.5237Epoch 8/10: [====                          ] 8/60 batches, loss: 0.5271Epoch 8/10: [====                          ] 9/60 batches, loss: 0.5271Epoch 8/10: [=====                         ] 10/60 batches, loss: 0.5271Epoch 8/10: [=====                         ] 11/60 batches, loss: 0.5315Epoch 8/10: [======                        ] 12/60 batches, loss: 0.5331Epoch 8/10: [======                        ] 13/60 batches, loss: 0.5345Epoch 8/10: [=======                       ] 14/60 batches, loss: 0.5272Epoch 8/10: [=======                       ] 15/60 batches, loss: 0.5209Epoch 8/10: [========                      ] 16/60 batches, loss: 0.5228Epoch 8/10: [========                      ] 17/60 batches, loss: 0.5230Epoch 8/10: [=========                     ] 18/60 batches, loss: 0.5246Epoch 8/10: [=========                     ] 19/60 batches, loss: 0.5247Epoch 8/10: [==========                    ] 20/60 batches, loss: 0.5249Epoch 8/10: [==========                    ] 21/60 batches, loss: 0.5257Epoch 8/10: [===========                   ] 22/60 batches, loss: 0.5204Epoch 8/10: [===========                   ] 23/60 batches, loss: 0.5207Epoch 8/10: [============                  ] 24/60 batches, loss: 0.5220Epoch 8/10: [============                  ] 25/60 batches, loss: 0.5193Epoch 8/10: [=============                 ] 26/60 batches, loss: 0.5196Epoch 8/10: [=============                 ] 27/60 batches, loss: 0.5190Epoch 8/10: [==============                ] 28/60 batches, loss: 0.5193Epoch 8/10: [==============                ] 29/60 batches, loss: 0.5180Epoch 8/10: [===============               ] 30/60 batches, loss: 0.5183Epoch 8/10: [===============               ] 31/60 batches, loss: 0.5201Epoch 8/10: [================              ] 32/60 batches, loss: 0.5196Epoch 8/10: [================              ] 33/60 batches, loss: 0.5184Epoch 8/10: [=================             ] 34/60 batches, loss: 0.5207Epoch 8/10: [=================             ] 35/60 batches, loss: 0.5209Epoch 8/10: [==================            ] 36/60 batches, loss: 0.5198Epoch 8/10: [==================            ] 37/60 batches, loss: 0.5187Epoch 8/10: [===================           ] 38/60 batches, loss: 0.5189Epoch 8/10: [===================           ] 39/60 batches, loss: 0.5173Epoch 8/10: [====================          ] 40/60 batches, loss: 0.5169Epoch 8/10: [====================          ] 41/60 batches, loss: 0.5155Epoch 8/10: [=====================         ] 42/60 batches, loss: 0.5135Epoch 8/10: [=====================         ] 43/60 batches, loss: 0.5147Epoch 8/10: [======================        ] 44/60 batches, loss: 0.5139Epoch 8/10: [======================        ] 45/60 batches, loss: 0.5168Epoch 8/10: [=======================       ] 46/60 batches, loss: 0.5181Epoch 8/10: [=======================       ] 47/60 batches, loss: 0.5183Epoch 8/10: [========================      ] 48/60 batches, loss: 0.5180Epoch 8/10: [========================      ] 49/60 batches, loss: 0.5187Epoch 8/10: [=========================     ] 50/60 batches, loss: 0.5188Epoch 8/10: [=========================     ] 51/60 batches, loss: 0.5190Epoch 8/10: [==========================    ] 52/60 batches, loss: 0.5191Epoch 8/10: [==========================    ] 53/60 batches, loss: 0.5197Epoch 8/10: [===========================   ] 54/60 batches, loss: 0.5203Epoch 8/10: [===========================   ] 55/60 batches, loss: 0.5196Epoch 8/10: [============================  ] 56/60 batches, loss: 0.5210Epoch 8/10: [============================  ] 57/60 batches, loss: 0.5228Epoch 8/10: [============================= ] 58/60 batches, loss: 0.5228Epoch 8/10: [============================= ] 59/60 batches, loss: 0.5229Epoch 8/10: [==============================] 60/60 batches, loss: 0.5226
[2025-05-01 11:49:54,342][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5226
[2025-05-01 11:49:54,591][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5860, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:49:54,591][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/60 batches, loss: 0.5271Epoch 9/10: [=                             ] 2/60 batches, loss: 0.5034Epoch 9/10: [=                             ] 3/60 batches, loss: 0.5113Epoch 9/10: [==                            ] 4/60 batches, loss: 0.5152Epoch 9/10: [==                            ] 5/60 batches, loss: 0.5271Epoch 9/10: [===                           ] 6/60 batches, loss: 0.5271Epoch 9/10: [===                           ] 7/60 batches, loss: 0.5271Epoch 9/10: [====                          ] 8/60 batches, loss: 0.5241Epoch 9/10: [====                          ] 9/60 batches, loss: 0.5192Epoch 9/10: [=====                         ] 10/60 batches, loss: 0.5200Epoch 9/10: [=====                         ] 11/60 batches, loss: 0.5228Epoch 9/10: [======                        ] 12/60 batches, loss: 0.5251Epoch 9/10: [======                        ] 13/60 batches, loss: 0.5326Epoch 9/10: [=======                       ] 14/60 batches, loss: 0.5356Epoch 9/10: [=======                       ] 15/60 batches, loss: 0.5303Epoch 9/10: [========                      ] 16/60 batches, loss: 0.5325Epoch 9/10: [========                      ] 17/60 batches, loss: 0.5308Epoch 9/10: [=========                     ] 18/60 batches, loss: 0.5319Epoch 9/10: [=========                     ] 19/60 batches, loss: 0.5329Epoch 9/10: [==========                    ] 20/60 batches, loss: 0.5326Epoch 9/10: [==========                    ] 21/60 batches, loss: 0.5301Epoch 9/10: [===========                   ] 22/60 batches, loss: 0.5289Epoch 9/10: [===========                   ] 23/60 batches, loss: 0.5298Epoch 9/10: [============                  ] 24/60 batches, loss: 0.5278Epoch 9/10: [============                  ] 25/60 batches, loss: 0.5287Epoch 9/10: [=============                 ] 26/60 batches, loss: 0.5305Epoch 9/10: [=============                 ] 27/60 batches, loss: 0.5295Epoch 9/10: [==============                ] 28/60 batches, loss: 0.5277Epoch 9/10: [==============                ] 29/60 batches, loss: 0.5252Epoch 9/10: [===============               ] 30/60 batches, loss: 0.5221Epoch 9/10: [===============               ] 31/60 batches, loss: 0.5253Epoch 9/10: [================              ] 32/60 batches, loss: 0.5254Epoch 9/10: [================              ] 33/60 batches, loss: 0.5276Epoch 9/10: [=================             ] 34/60 batches, loss: 0.5269Epoch 9/10: [=================             ] 35/60 batches, loss: 0.5262Epoch 9/10: [==================            ] 36/60 batches, loss: 0.5262Epoch 9/10: [==================            ] 37/60 batches, loss: 0.5256Epoch 9/10: [===================           ] 38/60 batches, loss: 0.5250Epoch 9/10: [===================           ] 39/60 batches, loss: 0.5232Epoch 9/10: [====================          ] 40/60 batches, loss: 0.5216Epoch 9/10: [====================          ] 41/60 batches, loss: 0.5234Epoch 9/10: [=====================         ] 42/60 batches, loss: 0.5241Epoch 9/10: [=====================         ] 43/60 batches, loss: 0.5253Epoch 9/10: [======================        ] 44/60 batches, loss: 0.5243Epoch 9/10: [======================        ] 45/60 batches, loss: 0.5254Epoch 9/10: [=======================       ] 46/60 batches, loss: 0.5249Epoch 9/10: [=======================       ] 47/60 batches, loss: 0.5239Epoch 9/10: [========================      ] 48/60 batches, loss: 0.5235Epoch 9/10: [========================      ] 49/60 batches, loss: 0.5231Epoch 9/10: [=========================     ] 50/60 batches, loss: 0.5236Epoch 9/10: [=========================     ] 51/60 batches, loss: 0.5254Epoch 9/10: [==========================    ] 52/60 batches, loss: 0.5250Epoch 9/10: [==========================    ] 53/60 batches, loss: 0.5241Epoch 9/10: [===========================   ] 54/60 batches, loss: 0.5229Epoch 9/10: [===========================   ] 55/60 batches, loss: 0.5229Epoch 9/10: [============================  ] 56/60 batches, loss: 0.5226Epoch 9/10: [============================  ] 57/60 batches, loss: 0.5232Epoch 9/10: [============================= ] 58/60 batches, loss: 0.5228Epoch 9/10: [============================= ] 59/60 batches, loss: 0.5225Epoch 9/10: [==============================] 60/60 batches, loss: 0.5228
[2025-05-01 11:50:00,675][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5228
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5873, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-01 11:50:00,928][src.training.lm_trainer][INFO] - Training completed in 60.31 seconds
[2025-05-01 11:50:00,928][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9727463312368972, 'f1': 0.970917225950783, 'precision': 0.9931350114416476, 'recall': 0.949671772428884}
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6363636363636364, 'f1': 0.4594594594594595, 'precision': 0.8947368421052632, 'recall': 0.3090909090909091}
[2025-05-01 11:50:05,080][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/id/id/model.pt
[2025-05-01 11:50:05,085][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▇▇▇██
wandb:           best_val_f1 ▁█████
wandb:         best_val_loss █▂▂▂▁▁
wandb:    best_val_precision ▁█████
wandb:       best_val_recall ▁▇▇▇██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃▃▃▃▃
wandb:            train_loss █▄▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▇▇▇█████
wandb:                val_f1 ▁████████
wandb:              val_loss █▂▂▂▁▁▂▁▁
wandb:         val_precision ▁████████
wandb:            val_recall ▁▇▇▇█████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.79167
wandb:           best_val_f1 0.73684
wandb:         best_val_loss 0.58361
wandb:    best_val_precision 1
wandb:       best_val_recall 0.58333
wandb:      early_stop_epoch 9
wandb:                 epoch 9
wandb:   final_test_accuracy 0.63636
wandb:         final_test_f1 0.45946
wandb:  final_test_precision 0.89474
wandb:     final_test_recall 0.30909
wandb:  final_train_accuracy 0.97275
wandb:        final_train_f1 0.97092
wandb: final_train_precision 0.99314
wandb:    final_train_recall 0.94967
wandb:    final_val_accuracy 0.79167
wandb:          final_val_f1 0.73684
wandb:   final_val_precision 1
wandb:      final_val_recall 0.58333
wandb:         learning_rate 2e-05
wandb:            train_loss 0.52279
wandb:            train_time 60.31436
wandb:          val_accuracy 0.79167
wandb:                val_f1 0.73684
wandb:              val_loss 0.5873
wandb:         val_precision 1
wandb:            val_recall 0.58333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114851-cz3tzhts
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114851-cz3tzhts/logs
Experiment finetune_question_type_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/id/results.json
Running experiment: finetune_complexity_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:50:18,594][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/id
experiment_name: finetune_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:50:18,594][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:50:18,594][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:50:18,594][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:50:18,598][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-01 11:50:18,598][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:50:20,498][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:50:22,999][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:50:23,000][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,086][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,225][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-01 11:50:23,237][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,239][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-01 11:50:23,240][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,262][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,295][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,310][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-01 11:50:23,311][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,312][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-01 11:50:23,316][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,344][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,376][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,388][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-01 11:50:23,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,390][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,392][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,392][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,394][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:50:23,394][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:50:23,394][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:50:27,785][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:50:27,791][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:50:27,791][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:50:27,792][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:50:27,792][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-01 11:50:27,792][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:50:27,793][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.2579Epoch 1/10: [=                             ] 2/60 batches, loss: 0.2191Epoch 1/10: [=                             ] 3/60 batches, loss: 0.2082Epoch 1/10: [==                            ] 4/60 batches, loss: 0.2013Epoch 1/10: [==                            ] 5/60 batches, loss: 0.2025Epoch 1/10: [===                           ] 6/60 batches, loss: 0.2152Epoch 1/10: [===                           ] 7/60 batches, loss: 0.2010Epoch 1/10: [====                          ] 8/60 batches, loss: 0.1926Epoch 1/10: [====                          ] 9/60 batches, loss: 0.1969Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.1970Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.1979Epoch 1/10: [======                        ] 12/60 batches, loss: 0.1986Epoch 1/10: [======                        ] 13/60 batches, loss: 0.1957Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.1949Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.1985Epoch 1/10: [========                      ] 16/60 batches, loss: 0.1944Epoch 1/10: [========                      ] 17/60 batches, loss: 0.1921Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.1862Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.1837Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.1801Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.1760Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.1727Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.1679Epoch 1/10: [============                  ] 24/60 batches, loss: 0.1654Epoch 1/10: [============                  ] 25/60 batches, loss: 0.1630Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.1595Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.1572Epoch 1/10: [==============                ] 28/60 batches, loss: 0.1574Epoch 1/10: [==============                ] 29/60 batches, loss: 0.1550Epoch 1/10: [===============               ] 30/60 batches, loss: 0.1542Epoch 1/10: [===============               ] 31/60 batches, loss: 0.1532Epoch 1/10: [================              ] 32/60 batches, loss: 0.1547Epoch 1/10: [================              ] 33/60 batches, loss: 0.1546Epoch 1/10: [=================             ] 34/60 batches, loss: 0.1536Epoch 1/10: [=================             ] 35/60 batches, loss: 0.1534Epoch 1/10: [==================            ] 36/60 batches, loss: 0.1523Epoch 1/10: [==================            ] 37/60 batches, loss: 0.1512Epoch 1/10: [===================           ] 38/60 batches, loss: 0.1510Epoch 1/10: [===================           ] 39/60 batches, loss: 0.1515Epoch 1/10: [====================          ] 40/60 batches, loss: 0.1514Epoch 1/10: [====================          ] 41/60 batches, loss: 0.1508Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.1500Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.1493Epoch 1/10: [======================        ] 44/60 batches, loss: 0.1492Epoch 1/10: [======================        ] 45/60 batches, loss: 0.1478Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.1481Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.1472Epoch 1/10: [========================      ] 48/60 batches, loss: 0.1462Epoch 1/10: [========================      ] 49/60 batches, loss: 0.1451Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.1441Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.1440Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.1434Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.1420Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.1412Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.1392Epoch 1/10: [============================  ] 56/60 batches, loss: 0.1374Epoch 1/10: [============================  ] 57/60 batches, loss: 0.1364Epoch 1/10: [============================= ] 58/60 batches, loss: 0.1346Epoch 1/10: [============================= ] 59/60 batches, loss: 0.1336Epoch 1/10: [==============================] 60/60 batches, loss: 0.1319
[2025-05-01 11:50:36,680][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1319
[2025-05-01 11:50:36,923][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0766, Metrics: {'mse': 0.07300561666488647, 'rmse': 0.2701955156269002, 'r2': -0.7461632490158081}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.0302Epoch 2/10: [=                             ] 2/60 batches, loss: 0.0383Epoch 2/10: [=                             ] 3/60 batches, loss: 0.0332Epoch 2/10: [==                            ] 4/60 batches, loss: 0.0395Epoch 2/10: [==                            ] 5/60 batches, loss: 0.0391Epoch 2/10: [===                           ] 6/60 batches, loss: 0.0452Epoch 2/10: [===                           ] 7/60 batches, loss: 0.0439Epoch 2/10: [====                          ] 8/60 batches, loss: 0.0445Epoch 2/10: [====                          ] 9/60 batches, loss: 0.0426Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.0426Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.0425Epoch 2/10: [======                        ] 12/60 batches, loss: 0.0423Epoch 2/10: [======                        ] 13/60 batches, loss: 0.0423Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.0440Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.0441Epoch 2/10: [========                      ] 16/60 batches, loss: 0.0452Epoch 2/10: [========                      ] 17/60 batches, loss: 0.0442Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.0437Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.0435Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.0422Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.0415Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.0405Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.0402Epoch 2/10: [============                  ] 24/60 batches, loss: 0.0392Epoch 2/10: [============                  ] 25/60 batches, loss: 0.0387Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.0387Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.0384Epoch 2/10: [==============                ] 28/60 batches, loss: 0.0378Epoch 2/10: [==============                ] 29/60 batches, loss: 0.0386Epoch 2/10: [===============               ] 30/60 batches, loss: 0.0387Epoch 2/10: [===============               ] 31/60 batches, loss: 0.0389Epoch 2/10: [================              ] 32/60 batches, loss: 0.0383Epoch 2/10: [================              ] 33/60 batches, loss: 0.0381Epoch 2/10: [=================             ] 34/60 batches, loss: 0.0375Epoch 2/10: [=================             ] 35/60 batches, loss: 0.0375Epoch 2/10: [==================            ] 36/60 batches, loss: 0.0374Epoch 2/10: [==================            ] 37/60 batches, loss: 0.0369Epoch 2/10: [===================           ] 38/60 batches, loss: 0.0371Epoch 2/10: [===================           ] 39/60 batches, loss: 0.0367Epoch 2/10: [====================          ] 40/60 batches, loss: 0.0366Epoch 2/10: [====================          ] 41/60 batches, loss: 0.0362Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.0358Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.0353Epoch 2/10: [======================        ] 44/60 batches, loss: 0.0352Epoch 2/10: [======================        ] 45/60 batches, loss: 0.0348Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.0350Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.0348Epoch 2/10: [========================      ] 48/60 batches, loss: 0.0344Epoch 2/10: [========================      ] 49/60 batches, loss: 0.0339Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.0338Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.0341Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.0339Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.0335Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.0333Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.0334Epoch 2/10: [============================  ] 56/60 batches, loss: 0.0333Epoch 2/10: [============================  ] 57/60 batches, loss: 0.0332Epoch 2/10: [============================= ] 58/60 batches, loss: 0.0333Epoch 2/10: [============================= ] 59/60 batches, loss: 0.0333Epoch 2/10: [==============================] 60/60 batches, loss: 0.0330
[2025-05-01 11:50:43,406][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0330
[2025-05-01 11:50:43,652][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0379, Metrics: {'mse': 0.03619665652513504, 'rmse': 0.19025418924463935, 'r2': 0.134240984916687}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.0261Epoch 3/10: [=                             ] 2/60 batches, loss: 0.0197Epoch 3/10: [=                             ] 3/60 batches, loss: 0.0214Epoch 3/10: [==                            ] 4/60 batches, loss: 0.0214Epoch 3/10: [==                            ] 5/60 batches, loss: 0.0204Epoch 3/10: [===                           ] 6/60 batches, loss: 0.0202Epoch 3/10: [===                           ] 7/60 batches, loss: 0.0210Epoch 3/10: [====                          ] 8/60 batches, loss: 0.0216Epoch 3/10: [====                          ] 9/60 batches, loss: 0.0231Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.0229Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.0248Epoch 3/10: [======                        ] 12/60 batches, loss: 0.0256Epoch 3/10: [======                        ] 13/60 batches, loss: 0.0243Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.0243Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.0237Epoch 3/10: [========                      ] 16/60 batches, loss: 0.0244Epoch 3/10: [========                      ] 17/60 batches, loss: 0.0238Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.0235Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.0229Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.0225Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.0226Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.0220Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.0218Epoch 3/10: [============                  ] 24/60 batches, loss: 0.0214Epoch 3/10: [============                  ] 25/60 batches, loss: 0.0213Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.0217Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.0213Epoch 3/10: [==============                ] 28/60 batches, loss: 0.0220Epoch 3/10: [==============                ] 29/60 batches, loss: 0.0218Epoch 3/10: [===============               ] 30/60 batches, loss: 0.0216Epoch 3/10: [===============               ] 31/60 batches, loss: 0.0212Epoch 3/10: [================              ] 32/60 batches, loss: 0.0209Epoch 3/10: [================              ] 33/60 batches, loss: 0.0207Epoch 3/10: [=================             ] 34/60 batches, loss: 0.0205Epoch 3/10: [=================             ] 35/60 batches, loss: 0.0207Epoch 3/10: [==================            ] 36/60 batches, loss: 0.0206Epoch 3/10: [==================            ] 37/60 batches, loss: 0.0204Epoch 3/10: [===================           ] 38/60 batches, loss: 0.0202Epoch 3/10: [===================           ] 39/60 batches, loss: 0.0199Epoch 3/10: [====================          ] 40/60 batches, loss: 0.0198Epoch 3/10: [====================          ] 41/60 batches, loss: 0.0196Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.0194Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.0193Epoch 3/10: [======================        ] 44/60 batches, loss: 0.0193Epoch 3/10: [======================        ] 45/60 batches, loss: 0.0191Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.0195Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.0195Epoch 3/10: [========================      ] 48/60 batches, loss: 0.0193Epoch 3/10: [========================      ] 49/60 batches, loss: 0.0191Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.0189Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.0190Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.0192Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.0191Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.0191Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.0191Epoch 3/10: [============================  ] 56/60 batches, loss: 0.0190Epoch 3/10: [============================  ] 57/60 batches, loss: 0.0188Epoch 3/10: [============================= ] 58/60 batches, loss: 0.0189Epoch 3/10: [============================= ] 59/60 batches, loss: 0.0190Epoch 3/10: [==============================] 60/60 batches, loss: 0.0188
[2025-05-01 11:50:50,142][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0188
[2025-05-01 11:50:50,388][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0184, Metrics: {'mse': 0.017388159409165382, 'rmse': 0.13186417030097822, 'r2': 0.5841064453125}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.0095Epoch 4/10: [=                             ] 2/60 batches, loss: 0.0088Epoch 4/10: [=                             ] 3/60 batches, loss: 0.0088Epoch 4/10: [==                            ] 4/60 batches, loss: 0.0109Epoch 4/10: [==                            ] 5/60 batches, loss: 0.0125Epoch 4/10: [===                           ] 6/60 batches, loss: 0.0135Epoch 4/10: [===                           ] 7/60 batches, loss: 0.0165Epoch 4/10: [====                          ] 8/60 batches, loss: 0.0170Epoch 4/10: [====                          ] 9/60 batches, loss: 0.0163Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.0155Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.0151Epoch 4/10: [======                        ] 12/60 batches, loss: 0.0152Epoch 4/10: [======                        ] 13/60 batches, loss: 0.0148Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.0146Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.0144Epoch 4/10: [========                      ] 16/60 batches, loss: 0.0151Epoch 4/10: [========                      ] 17/60 batches, loss: 0.0149Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.0146Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.0143Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.0141Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.0139Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.0137Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.0141Epoch 4/10: [============                  ] 24/60 batches, loss: 0.0143Epoch 4/10: [============                  ] 25/60 batches, loss: 0.0146Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.0143Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.0142Epoch 4/10: [==============                ] 28/60 batches, loss: 0.0142Epoch 4/10: [==============                ] 29/60 batches, loss: 0.0147Epoch 4/10: [===============               ] 30/60 batches, loss: 0.0149Epoch 4/10: [===============               ] 31/60 batches, loss: 0.0150Epoch 4/10: [================              ] 32/60 batches, loss: 0.0148Epoch 4/10: [================              ] 33/60 batches, loss: 0.0146Epoch 4/10: [=================             ] 34/60 batches, loss: 0.0145Epoch 4/10: [=================             ] 35/60 batches, loss: 0.0148Epoch 4/10: [==================            ] 36/60 batches, loss: 0.0146Epoch 4/10: [==================            ] 37/60 batches, loss: 0.0145Epoch 4/10: [===================           ] 38/60 batches, loss: 0.0146Epoch 4/10: [===================           ] 39/60 batches, loss: 0.0148Epoch 4/10: [====================          ] 40/60 batches, loss: 0.0147Epoch 4/10: [====================          ] 41/60 batches, loss: 0.0145Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.0144Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.0146Epoch 4/10: [======================        ] 44/60 batches, loss: 0.0145Epoch 4/10: [======================        ] 45/60 batches, loss: 0.0145Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.0145Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.0143Epoch 4/10: [========================      ] 48/60 batches, loss: 0.0142Epoch 4/10: [========================      ] 49/60 batches, loss: 0.0141Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.0141Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.0139Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.0138Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.0140Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.0144Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.0143Epoch 4/10: [============================  ] 56/60 batches, loss: 0.0144Epoch 4/10: [============================  ] 57/60 batches, loss: 0.0143Epoch 4/10: [============================= ] 58/60 batches, loss: 0.0143Epoch 4/10: [============================= ] 59/60 batches, loss: 0.0143Epoch 4/10: [==============================] 60/60 batches, loss: 0.0143
[2025-05-01 11:50:56,852][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0143
[2025-05-01 11:50:57,111][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0164, Metrics: {'mse': 0.015578867867588997, 'rmse': 0.1248153350658043, 'r2': 0.627381443977356}
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.0099Epoch 5/10: [=                             ] 2/60 batches, loss: 0.0141Epoch 5/10: [=                             ] 3/60 batches, loss: 0.0170Epoch 5/10: [==                            ] 4/60 batches, loss: 0.0160Epoch 5/10: [==                            ] 5/60 batches, loss: 0.0146Epoch 5/10: [===                           ] 6/60 batches, loss: 0.0128Epoch 5/10: [===                           ] 7/60 batches, loss: 0.0120Epoch 5/10: [====                          ] 8/60 batches, loss: 0.0115Epoch 5/10: [====                          ] 9/60 batches, loss: 0.0118Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.0118Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.0118Epoch 5/10: [======                        ] 12/60 batches, loss: 0.0112Epoch 5/10: [======                        ] 13/60 batches, loss: 0.0108Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.0108Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.0105Epoch 5/10: [========                      ] 16/60 batches, loss: 0.0110Epoch 5/10: [========                      ] 17/60 batches, loss: 0.0112Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.0111Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.0113Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.0112Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.0114Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.0116Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.0117Epoch 5/10: [============                  ] 24/60 batches, loss: 0.0118Epoch 5/10: [============                  ] 25/60 batches, loss: 0.0119Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.0118Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.0117Epoch 5/10: [==============                ] 28/60 batches, loss: 0.0115Epoch 5/10: [==============                ] 29/60 batches, loss: 0.0114Epoch 5/10: [===============               ] 30/60 batches, loss: 0.0112Epoch 5/10: [===============               ] 31/60 batches, loss: 0.0113Epoch 5/10: [================              ] 32/60 batches, loss: 0.0114Epoch 5/10: [================              ] 33/60 batches, loss: 0.0115Epoch 5/10: [=================             ] 34/60 batches, loss: 0.0113Epoch 5/10: [=================             ] 35/60 batches, loss: 0.0112Epoch 5/10: [==================            ] 36/60 batches, loss: 0.0111Epoch 5/10: [==================            ] 37/60 batches, loss: 0.0111Epoch 5/10: [===================           ] 38/60 batches, loss: 0.0112Epoch 5/10: [===================           ] 39/60 batches, loss: 0.0114Epoch 5/10: [====================          ] 40/60 batches, loss: 0.0115Epoch 5/10: [====================          ] 41/60 batches, loss: 0.0116Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.0117Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.0120Epoch 5/10: [======================        ] 44/60 batches, loss: 0.0119Epoch 5/10: [======================        ] 45/60 batches, loss: 0.0120Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.0118Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.0118Epoch 5/10: [========================      ] 48/60 batches, loss: 0.0118Epoch 5/10: [========================      ] 49/60 batches, loss: 0.0117Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.0116Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.0117Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.0117Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.0116Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.0116Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.0115Epoch 5/10: [============================  ] 56/60 batches, loss: 0.0116Epoch 5/10: [============================  ] 57/60 batches, loss: 0.0115Epoch 5/10: [============================= ] 58/60 batches, loss: 0.0115Epoch 5/10: [============================= ] 59/60 batches, loss: 0.0114Epoch 5/10: [==============================] 60/60 batches, loss: 0.0115
[2025-05-01 11:51:03,560][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0115
[2025-05-01 11:51:03,819][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0124, Metrics: {'mse': 0.011687989346683025, 'rmse': 0.108111004743657, 'r2': 0.7204443216323853}
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.0095Epoch 6/10: [=                             ] 2/60 batches, loss: 0.0120Epoch 6/10: [=                             ] 3/60 batches, loss: 0.0096Epoch 6/10: [==                            ] 4/60 batches, loss: 0.0094Epoch 6/10: [==                            ] 5/60 batches, loss: 0.0088Epoch 6/10: [===                           ] 6/60 batches, loss: 0.0093Epoch 6/10: [===                           ] 7/60 batches, loss: 0.0090Epoch 6/10: [====                          ] 8/60 batches, loss: 0.0087Epoch 6/10: [====                          ] 9/60 batches, loss: 0.0082Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.0084Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.0082Epoch 6/10: [======                        ] 12/60 batches, loss: 0.0083Epoch 6/10: [======                        ] 13/60 batches, loss: 0.0085Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.0084Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.0085Epoch 6/10: [========                      ] 16/60 batches, loss: 0.0086Epoch 6/10: [========                      ] 17/60 batches, loss: 0.0087Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.0087Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.0089Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.0086Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.0086Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.0087Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.0085Epoch 6/10: [============                  ] 24/60 batches, loss: 0.0086Epoch 6/10: [============                  ] 25/60 batches, loss: 0.0086Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.0085Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.0085Epoch 6/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 6/10: [==============                ] 29/60 batches, loss: 0.0082Epoch 6/10: [===============               ] 30/60 batches, loss: 0.0084Epoch 6/10: [===============               ] 31/60 batches, loss: 0.0084Epoch 6/10: [================              ] 32/60 batches, loss: 0.0083Epoch 6/10: [================              ] 33/60 batches, loss: 0.0083Epoch 6/10: [=================             ] 34/60 batches, loss: 0.0083Epoch 6/10: [=================             ] 35/60 batches, loss: 0.0083Epoch 6/10: [==================            ] 36/60 batches, loss: 0.0083Epoch 6/10: [==================            ] 37/60 batches, loss: 0.0082Epoch 6/10: [===================           ] 38/60 batches, loss: 0.0082Epoch 6/10: [===================           ] 39/60 batches, loss: 0.0082Epoch 6/10: [====================          ] 40/60 batches, loss: 0.0083Epoch 6/10: [====================          ] 41/60 batches, loss: 0.0082Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.0083Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.0085Epoch 6/10: [======================        ] 44/60 batches, loss: 0.0084Epoch 6/10: [======================        ] 45/60 batches, loss: 0.0084Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.0084Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.0083Epoch 6/10: [========================      ] 48/60 batches, loss: 0.0083Epoch 6/10: [========================      ] 49/60 batches, loss: 0.0082Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.0082Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.0082Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.0082Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.0082Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.0082Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.0082Epoch 6/10: [============================  ] 56/60 batches, loss: 0.0082Epoch 6/10: [============================  ] 57/60 batches, loss: 0.0082Epoch 6/10: [============================= ] 58/60 batches, loss: 0.0083Epoch 6/10: [============================= ] 59/60 batches, loss: 0.0083Epoch 6/10: [==============================] 60/60 batches, loss: 0.0083
[2025-05-01 11:51:10,301][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0083
[2025-05-01 11:51:10,560][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0232, Metrics: {'mse': 0.023061146959662437, 'rmse': 0.151858970626244, 'r2': 0.4484187960624695}
[2025-05-01 11:51:10,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.0059Epoch 7/10: [=                             ] 2/60 batches, loss: 0.0071Epoch 7/10: [=                             ] 3/60 batches, loss: 0.0069Epoch 7/10: [==                            ] 4/60 batches, loss: 0.0080Epoch 7/10: [==                            ] 5/60 batches, loss: 0.0076Epoch 7/10: [===                           ] 6/60 batches, loss: 0.0074Epoch 7/10: [===                           ] 7/60 batches, loss: 0.0074Epoch 7/10: [====                          ] 8/60 batches, loss: 0.0074Epoch 7/10: [====                          ] 9/60 batches, loss: 0.0071Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.0074Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.0075Epoch 7/10: [======                        ] 12/60 batches, loss: 0.0078Epoch 7/10: [======                        ] 13/60 batches, loss: 0.0078Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.0077Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.0079Epoch 7/10: [========                      ] 16/60 batches, loss: 0.0079Epoch 7/10: [========                      ] 17/60 batches, loss: 0.0079Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.0078Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.0076Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.0075Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.0074Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.0076Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.0075Epoch 7/10: [============                  ] 24/60 batches, loss: 0.0077Epoch 7/10: [============                  ] 25/60 batches, loss: 0.0075Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.0075Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.0080Epoch 7/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 7/10: [==============                ] 29/60 batches, loss: 0.0083Epoch 7/10: [===============               ] 30/60 batches, loss: 0.0081Epoch 7/10: [===============               ] 31/60 batches, loss: 0.0080Epoch 7/10: [================              ] 32/60 batches, loss: 0.0081Epoch 7/10: [================              ] 33/60 batches, loss: 0.0080Epoch 7/10: [=================             ] 34/60 batches, loss: 0.0080Epoch 7/10: [=================             ] 35/60 batches, loss: 0.0079Epoch 7/10: [==================            ] 36/60 batches, loss: 0.0078Epoch 7/10: [==================            ] 37/60 batches, loss: 0.0078Epoch 7/10: [===================           ] 38/60 batches, loss: 0.0077Epoch 7/10: [===================           ] 39/60 batches, loss: 0.0077Epoch 7/10: [====================          ] 40/60 batches, loss: 0.0077Epoch 7/10: [====================          ] 41/60 batches, loss: 0.0076Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.0077Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.0077Epoch 7/10: [======================        ] 44/60 batches, loss: 0.0077Epoch 7/10: [======================        ] 45/60 batches, loss: 0.0077Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.0076Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.0076Epoch 7/10: [========================      ] 48/60 batches, loss: 0.0076Epoch 7/10: [========================      ] 49/60 batches, loss: 0.0075Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.0076Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.0076Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.0075Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.0075Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.0074Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.0074Epoch 7/10: [============================  ] 56/60 batches, loss: 0.0074Epoch 7/10: [============================  ] 57/60 batches, loss: 0.0074Epoch 7/10: [============================= ] 58/60 batches, loss: 0.0073Epoch 7/10: [============================= ] 59/60 batches, loss: 0.0073Epoch 7/10: [==============================] 60/60 batches, loss: 0.0073
[2025-05-01 11:51:16,688][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0073
[2025-05-01 11:51:16,936][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0121, Metrics: {'mse': 0.011142720468342304, 'rmse': 0.10555908520038577, 'r2': 0.7334861159324646}
Epoch 8/10: [Epoch 8/10: [                              ] 1/60 batches, loss: 0.0039Epoch 8/10: [=                             ] 2/60 batches, loss: 0.0071Epoch 8/10: [=                             ] 3/60 batches, loss: 0.0107Epoch 8/10: [==                            ] 4/60 batches, loss: 0.0110Epoch 8/10: [==                            ] 5/60 batches, loss: 0.0106Epoch 8/10: [===                           ] 6/60 batches, loss: 0.0101Epoch 8/10: [===                           ] 7/60 batches, loss: 0.0096Epoch 8/10: [====                          ] 8/60 batches, loss: 0.0093Epoch 8/10: [====                          ] 9/60 batches, loss: 0.0088Epoch 8/10: [=====                         ] 10/60 batches, loss: 0.0090Epoch 8/10: [=====                         ] 11/60 batches, loss: 0.0086Epoch 8/10: [======                        ] 12/60 batches, loss: 0.0085Epoch 8/10: [======                        ] 13/60 batches, loss: 0.0084Epoch 8/10: [=======                       ] 14/60 batches, loss: 0.0080Epoch 8/10: [=======                       ] 15/60 batches, loss: 0.0081Epoch 8/10: [========                      ] 16/60 batches, loss: 0.0079Epoch 8/10: [========                      ] 17/60 batches, loss: 0.0078Epoch 8/10: [=========                     ] 18/60 batches, loss: 0.0075Epoch 8/10: [=========                     ] 19/60 batches, loss: 0.0074Epoch 8/10: [==========                    ] 20/60 batches, loss: 0.0078Epoch 8/10: [==========                    ] 21/60 batches, loss: 0.0081Epoch 8/10: [===========                   ] 22/60 batches, loss: 0.0080Epoch 8/10: [===========                   ] 23/60 batches, loss: 0.0080Epoch 8/10: [============                  ] 24/60 batches, loss: 0.0082Epoch 8/10: [============                  ] 25/60 batches, loss: 0.0082Epoch 8/10: [=============                 ] 26/60 batches, loss: 0.0085Epoch 8/10: [=============                 ] 27/60 batches, loss: 0.0084Epoch 8/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 8/10: [==============                ] 29/60 batches, loss: 0.0081Epoch 8/10: [===============               ] 30/60 batches, loss: 0.0081Epoch 8/10: [===============               ] 31/60 batches, loss: 0.0080Epoch 8/10: [================              ] 32/60 batches, loss: 0.0081Epoch 8/10: [================              ] 33/60 batches, loss: 0.0080Epoch 8/10: [=================             ] 34/60 batches, loss: 0.0082Epoch 8/10: [=================             ] 35/60 batches, loss: 0.0082Epoch 8/10: [==================            ] 36/60 batches, loss: 0.0081Epoch 8/10: [==================            ] 37/60 batches, loss: 0.0081Epoch 8/10: [===================           ] 38/60 batches, loss: 0.0082Epoch 8/10: [===================           ] 39/60 batches, loss: 0.0081Epoch 8/10: [====================          ] 40/60 batches, loss: 0.0082Epoch 8/10: [====================          ] 41/60 batches, loss: 0.0081Epoch 8/10: [=====================         ] 42/60 batches, loss: 0.0082Epoch 8/10: [=====================         ] 43/60 batches, loss: 0.0083Epoch 8/10: [======================        ] 44/60 batches, loss: 0.0086Epoch 8/10: [======================        ] 45/60 batches, loss: 0.0084Epoch 8/10: [=======================       ] 46/60 batches, loss: 0.0084Epoch 8/10: [=======================       ] 47/60 batches, loss: 0.0084Epoch 8/10: [========================      ] 48/60 batches, loss: 0.0084Epoch 8/10: [========================      ] 49/60 batches, loss: 0.0083Epoch 8/10: [=========================     ] 50/60 batches, loss: 0.0082Epoch 8/10: [=========================     ] 51/60 batches, loss: 0.0081Epoch 8/10: [==========================    ] 52/60 batches, loss: 0.0080Epoch 8/10: [==========================    ] 53/60 batches, loss: 0.0080Epoch 8/10: [===========================   ] 54/60 batches, loss: 0.0080Epoch 8/10: [===========================   ] 55/60 batches, loss: 0.0079Epoch 8/10: [============================  ] 56/60 batches, loss: 0.0079Epoch 8/10: [============================  ] 57/60 batches, loss: 0.0078Epoch 8/10: [============================= ] 58/60 batches, loss: 0.0078Epoch 8/10: [============================= ] 59/60 batches, loss: 0.0079Epoch 8/10: [==============================] 60/60 batches, loss: 0.0078
[2025-05-01 11:51:23,464][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0078
[2025-05-01 11:51:23,740][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0127, Metrics: {'mse': 0.012340009212493896, 'rmse': 0.11108559408174354, 'r2': 0.704849123954773}
[2025-05-01 11:51:23,741][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/60 batches, loss: 0.0062Epoch 9/10: [=                             ] 2/60 batches, loss: 0.0072Epoch 9/10: [=                             ] 3/60 batches, loss: 0.0069Epoch 9/10: [==                            ] 4/60 batches, loss: 0.0072Epoch 9/10: [==                            ] 5/60 batches, loss: 0.0067Epoch 9/10: [===                           ] 6/60 batches, loss: 0.0062Epoch 9/10: [===                           ] 7/60 batches, loss: 0.0058Epoch 9/10: [====                          ] 8/60 batches, loss: 0.0064Epoch 9/10: [====                          ] 9/60 batches, loss: 0.0063Epoch 9/10: [=====                         ] 10/60 batches, loss: 0.0059Epoch 9/10: [=====                         ] 11/60 batches, loss: 0.0057Epoch 9/10: [======                        ] 12/60 batches, loss: 0.0056Epoch 9/10: [======                        ] 13/60 batches, loss: 0.0056Epoch 9/10: [=======                       ] 14/60 batches, loss: 0.0055Epoch 9/10: [=======                       ] 15/60 batches, loss: 0.0058Epoch 9/10: [========                      ] 16/60 batches, loss: 0.0058Epoch 9/10: [========                      ] 17/60 batches, loss: 0.0063Epoch 9/10: [=========                     ] 18/60 batches, loss: 0.0061Epoch 9/10: [=========                     ] 19/60 batches, loss: 0.0060Epoch 9/10: [==========                    ] 20/60 batches, loss: 0.0058Epoch 9/10: [==========                    ] 21/60 batches, loss: 0.0058Epoch 9/10: [===========                   ] 22/60 batches, loss: 0.0057Epoch 9/10: [===========                   ] 23/60 batches, loss: 0.0059Epoch 9/10: [============                  ] 24/60 batches, loss: 0.0058Epoch 9/10: [============                  ] 25/60 batches, loss: 0.0058Epoch 9/10: [=============                 ] 26/60 batches, loss: 0.0056Epoch 9/10: [=============                 ] 27/60 batches, loss: 0.0056Epoch 9/10: [==============                ] 28/60 batches, loss: 0.0057Epoch 9/10: [==============                ] 29/60 batches, loss: 0.0056Epoch 9/10: [===============               ] 30/60 batches, loss: 0.0056Epoch 9/10: [===============               ] 31/60 batches, loss: 0.0055Epoch 9/10: [================              ] 32/60 batches, loss: 0.0055Epoch 9/10: [================              ] 33/60 batches, loss: 0.0054Epoch 9/10: [=================             ] 34/60 batches, loss: 0.0054Epoch 9/10: [=================             ] 35/60 batches, loss: 0.0055Epoch 9/10: [==================            ] 36/60 batches, loss: 0.0055Epoch 9/10: [==================            ] 37/60 batches, loss: 0.0055Epoch 9/10: [===================           ] 38/60 batches, loss: 0.0054Epoch 9/10: [===================           ] 39/60 batches, loss: 0.0055Epoch 9/10: [====================          ] 40/60 batches, loss: 0.0056Epoch 9/10: [====================          ] 41/60 batches, loss: 0.0056Epoch 9/10: [=====================         ] 42/60 batches, loss: 0.0056Epoch 9/10: [=====================         ] 43/60 batches, loss: 0.0055Epoch 9/10: [======================        ] 44/60 batches, loss: 0.0055Epoch 9/10: [======================        ] 45/60 batches, loss: 0.0054Epoch 9/10: [=======================       ] 46/60 batches, loss: 0.0055Epoch 9/10: [=======================       ] 47/60 batches, loss: 0.0055Epoch 9/10: [========================      ] 48/60 batches, loss: 0.0054Epoch 9/10: [========================      ] 49/60 batches, loss: 0.0055Epoch 9/10: [=========================     ] 50/60 batches, loss: 0.0055Epoch 9/10: [=========================     ] 51/60 batches, loss: 0.0055Epoch 9/10: [==========================    ] 52/60 batches, loss: 0.0057Epoch 9/10: [==========================    ] 53/60 batches, loss: 0.0056Epoch 9/10: [===========================   ] 54/60 batches, loss: 0.0056Epoch 9/10: [===========================   ] 55/60 batches, loss: 0.0056Epoch 9/10: [============================  ] 56/60 batches, loss: 0.0056Epoch 9/10: [============================  ] 57/60 batches, loss: 0.0055Epoch 9/10: [============================= ] 58/60 batches, loss: 0.0055Epoch 9/10: [============================= ] 59/60 batches, loss: 0.0055Epoch 9/10: [==============================] 60/60 batches, loss: 0.0055
[2025-05-01 11:51:29,846][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0055
[2025-05-01 11:51:30,095][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0142, Metrics: {'mse': 0.012903592549264431, 'rmse': 0.1135939811313277, 'r2': 0.6913692355155945}
[2025-05-01 11:51:30,096][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/60 batches, loss: 0.0022Epoch 10/10: [=                             ] 2/60 batches, loss: 0.0062Epoch 10/10: [=                             ] 3/60 batches, loss: 0.0066Epoch 10/10: [==                            ] 4/60 batches, loss: 0.0069Epoch 10/10: [==                            ] 5/60 batches, loss: 0.0064Epoch 10/10: [===                           ] 6/60 batches, loss: 0.0060Epoch 10/10: [===                           ] 7/60 batches, loss: 0.0058Epoch 10/10: [====                          ] 8/60 batches, loss: 0.0059Epoch 10/10: [====                          ] 9/60 batches, loss: 0.0060Epoch 10/10: [=====                         ] 10/60 batches, loss: 0.0060Epoch 10/10: [=====                         ] 11/60 batches, loss: 0.0061Epoch 10/10: [======                        ] 12/60 batches, loss: 0.0062Epoch 10/10: [======                        ] 13/60 batches, loss: 0.0061Epoch 10/10: [=======                       ] 14/60 batches, loss: 0.0060Epoch 10/10: [=======                       ] 15/60 batches, loss: 0.0058Epoch 10/10: [========                      ] 16/60 batches, loss: 0.0057Epoch 10/10: [========                      ] 17/60 batches, loss: 0.0057Epoch 10/10: [=========                     ] 18/60 batches, loss: 0.0056Epoch 10/10: [=========                     ] 19/60 batches, loss: 0.0056Epoch 10/10: [==========                    ] 20/60 batches, loss: 0.0056Epoch 10/10: [==========                    ] 21/60 batches, loss: 0.0057Epoch 10/10: [===========                   ] 22/60 batches, loss: 0.0056Epoch 10/10: [===========                   ] 23/60 batches, loss: 0.0056Epoch 10/10: [============                  ] 24/60 batches, loss: 0.0055Epoch 10/10: [============                  ] 25/60 batches, loss: 0.0055Epoch 10/10: [=============                 ] 26/60 batches, loss: 0.0054Epoch 10/10: [=============                 ] 27/60 batches, loss: 0.0053Epoch 10/10: [==============                ] 28/60 batches, loss: 0.0054Epoch 10/10: [==============                ] 29/60 batches, loss: 0.0055Epoch 10/10: [===============               ] 30/60 batches, loss: 0.0055Epoch 10/10: [===============               ] 31/60 batches, loss: 0.0054Epoch 10/10: [================              ] 32/60 batches, loss: 0.0055Epoch 10/10: [================              ] 33/60 batches, loss: 0.0056Epoch 10/10: [=================             ] 34/60 batches, loss: 0.0056Epoch 10/10: [=================             ] 35/60 batches, loss: 0.0055Epoch 10/10: [==================            ] 36/60 batches, loss: 0.0055Epoch 10/10: [==================            ] 37/60 batches, loss: 0.0057Epoch 10/10: [===================           ] 38/60 batches, loss: 0.0057Epoch 10/10: [===================           ] 39/60 batches, loss: 0.0057Epoch 10/10: [====================          ] 40/60 batches, loss: 0.0057Epoch 10/10: [====================          ] 41/60 batches, loss: 0.0057Epoch 10/10: [=====================         ] 42/60 batches, loss: 0.0058Epoch 10/10: [=====================         ] 43/60 batches, loss: 0.0057Epoch 10/10: [======================        ] 44/60 batches, loss: 0.0057Epoch 10/10: [======================        ] 45/60 batches, loss: 0.0056Epoch 10/10: [=======================       ] 46/60 batches, loss: 0.0056Epoch 10/10: [=======================       ] 47/60 batches, loss: 0.0056Epoch 10/10: [========================      ] 48/60 batches, loss: 0.0055Epoch 10/10: [========================      ] 49/60 batches, loss: 0.0055Epoch 10/10: [=========================     ] 50/60 batches, loss: 0.0056Epoch 10/10: [=========================     ] 51/60 batches, loss: 0.0055Epoch 10/10: [==========================    ] 52/60 batches, loss: 0.0055Epoch 10/10: [==========================    ] 53/60 batches, loss: 0.0055Epoch 10/10: [===========================   ] 54/60 batches, loss: 0.0054Epoch 10/10: [===========================   ] 55/60 batches, loss: 0.0054Epoch 10/10: [============================  ] 56/60 batches, loss: 0.0054Epoch 10/10: [============================  ] 57/60 batches, loss: 0.0054Epoch 10/10: [============================= ] 58/60 batches, loss: 0.0053Epoch 10/10: [============================= ] 59/60 batches, loss: 0.0053Epoch 10/10: [==============================] 60/60 batches, loss: 0.0054
[2025-05-01 11:51:36,204][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0054
[2025-05-01 11:51:36,466][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0163, Metrics: {'mse': 0.015271814540028572, 'rmse': 0.12357918327950129, 'r2': 0.6347256302833557}
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Training completed in 66.71 seconds
[2025-05-01 11:51:36,468][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:51:38,977][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.00696216756477952, 'rmse': 0.083439604294241, 'r2': 0.8081346750259399}
[2025-05-01 11:51:38,978][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.011142720468342304, 'rmse': 0.10555908520038577, 'r2': 0.7334861159324646}
[2025-05-01 11:51:38,978][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03317911922931671, 'rmse': 0.18215136351209868, 'r2': 0.1862698793411255}
[2025-05-01 11:51:40,665][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/id/id/model.pt
[2025-05-01 11:51:40,671][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▂▂▁▁
wandb:      best_val_r2 ▁▅▇▇██
wandb:    best_val_rmse █▅▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆███▇███
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▂▁▁▁▁
wandb:          val_mse █▄▂▂▁▂▁▁▁▁
wandb:           val_r2 ▁▅▇▇█▇████
wandb:         val_rmse █▅▂▂▁▃▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01211
wandb:     best_val_mse 0.01114
wandb:      best_val_r2 0.73349
wandb:    best_val_rmse 0.10556
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.03318
wandb:    final_test_r2 0.18627
wandb:  final_test_rmse 0.18215
wandb:  final_train_mse 0.00696
wandb:   final_train_r2 0.80813
wandb: final_train_rmse 0.08344
wandb:    final_val_mse 0.01114
wandb:     final_val_r2 0.73349
wandb:   final_val_rmse 0.10556
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00536
wandb:       train_time 66.715
wandb:         val_loss 0.01625
wandb:          val_mse 0.01527
wandb:           val_r2 0.63473
wandb:         val_rmse 0.12358
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115018-ippeu9hq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115018-ippeu9hq/logs
Experiment finetune_complexity_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/id/results.json
Running experiment: finetune_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ja"         "wandb.mode=offline"
