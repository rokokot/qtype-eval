SLURM_JOB_ID: 64435680
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Thu May  1 11:40:00 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:40:16,265][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:40:16,265][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:40:16,265][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:40:16,265][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:40:16,269][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 11:40:16,270][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:40:18,264][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:40:20,510][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:40:20,511][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,649][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,678][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,773][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 11:40:20,780][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,781][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 11:40:20,781][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,820][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,831][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 11:40:20,832][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,832][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 11:40:20,833][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:40:20,848][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,869][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:40:20,879][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 11:40:20,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:40:20,880][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 11:40:20,881][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,882][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 11:40:20,882][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 11:40:20,882][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,883][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 11:40:20,883][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:40:20,883][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:40:20,884][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 11:40:20,884][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:40:20,884][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:40:20,885][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:40:20,885][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:40:24,798][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:40:24,799][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:40:24,803][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:40:24,804][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:40:24,804][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:40:24,804][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 11:40:24,805][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:40:24,805][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.8145Epoch 1/10: [                              ] 2/63 batches, loss: 0.8446Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7718Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7524Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7532Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7585Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7449Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7424Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7300Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7407Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7360Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7349Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7268Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7327Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7320Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7330Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7372Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7427Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7398Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7346Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7308Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7302Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7258Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7253Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7248Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7245Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7265Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7232Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7250Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7245Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7214Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7210Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7208Epoch 1/10: [================              ] 34/63 batches, loss: 0.7221Epoch 1/10: [================              ] 35/63 batches, loss: 0.7219Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7234Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7241Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7213Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7219Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7217Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7207Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7211Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7201Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7200Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7206Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7215Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7212Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7190Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7185Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7166Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7153Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7172Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7159Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7157Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7160Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7148Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7164Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7137Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7126Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7117Epoch 1/10: [==============================] 63/63 batches, loss: 0.7132
[2025-05-01 11:40:33,805][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7132
[2025-05-01 11:40:33,982][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7450, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961, 'precision': 0.6451612903225806, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7625Epoch 2/10: [                              ] 2/63 batches, loss: 0.7733Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7537Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7262Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7238Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7200Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7277Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7272Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7212Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7156Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7228Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7157Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7044Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6984Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6949Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6984Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6938Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6917Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6919Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6906Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6866Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6902Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6886Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6851Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6877Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6925Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6933Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6917Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6880Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6829Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6802Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6788Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6767Epoch 2/10: [================              ] 34/63 batches, loss: 0.6758Epoch 2/10: [================              ] 35/63 batches, loss: 0.6757Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6719Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6684Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6657Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6629Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6609Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6594Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6601Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6576Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6539Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6510Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6495Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6459Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6408Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6394Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6372Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6370Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6357Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6345Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6338Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6323Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6305Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6284Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6270Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6262Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6235Epoch 2/10: [==============================] 63/63 batches, loss: 0.6238
[2025-05-01 11:40:40,671][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6238
[2025-05-01 11:40:40,850][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5717, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6418Epoch 3/10: [                              ] 2/63 batches, loss: 0.6040Epoch 3/10: [=                             ] 3/63 batches, loss: 0.5731Epoch 3/10: [=                             ] 4/63 batches, loss: 0.5429Epoch 3/10: [==                            ] 5/63 batches, loss: 0.5245Epoch 3/10: [==                            ] 6/63 batches, loss: 0.5065Epoch 3/10: [===                           ] 7/63 batches, loss: 0.5174Epoch 3/10: [===                           ] 8/63 batches, loss: 0.5236Epoch 3/10: [====                          ] 9/63 batches, loss: 0.5277Epoch 3/10: [====                          ] 10/63 batches, loss: 0.5290Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.5317Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.5225Epoch 3/10: [======                        ] 13/63 batches, loss: 0.5197Epoch 3/10: [======                        ] 14/63 batches, loss: 0.5229Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.5190Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.5170Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5166Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5137Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5125Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5101Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5132Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5148Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5146Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5160Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5191Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5195Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5167Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5141Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5139Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5144Epoch 3/10: [================              ] 34/63 batches, loss: 0.5156Epoch 3/10: [================              ] 35/63 batches, loss: 0.5174Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5164Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5162Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5159Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5138Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5137Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5140Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5139Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5125Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5146Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5133Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5137Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5130Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5138Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5137Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5130Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5115Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5114Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5122Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5120Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5136Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5131Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5125Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5123Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5126Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5121Epoch 3/10: [============================= ] 61/63 batches, loss: 0.5128Epoch 3/10: [============================= ] 62/63 batches, loss: 0.5115Epoch 3/10: [==============================] 63/63 batches, loss: 0.5144
[2025-05-01 11:40:47,593][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5144
[2025-05-01 11:40:47,784][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5252, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.4093Epoch 4/10: [                              ] 2/63 batches, loss: 0.4450Epoch 4/10: [=                             ] 3/63 batches, loss: 0.4568Epoch 4/10: [=                             ] 4/63 batches, loss: 0.4627Epoch 4/10: [==                            ] 5/63 batches, loss: 0.4666Epoch 4/10: [==                            ] 6/63 batches, loss: 0.4730Epoch 4/10: [===                           ] 7/63 batches, loss: 0.4810Epoch 4/10: [===                           ] 8/63 batches, loss: 0.4779Epoch 4/10: [====                          ] 9/63 batches, loss: 0.4808Epoch 4/10: [====                          ] 10/63 batches, loss: 0.4856Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.4808Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.4828Epoch 4/10: [======                        ] 13/63 batches, loss: 0.4863Epoch 4/10: [======                        ] 14/63 batches, loss: 0.4876Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.4887Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.4881Epoch 4/10: [========                      ] 17/63 batches, loss: 0.4878Epoch 4/10: [========                      ] 18/63 batches, loss: 0.4860Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.4882Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.4878Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.4897Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.4936Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.4967Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.4989Epoch 4/10: [============                  ] 26/63 batches, loss: 0.4973Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4967Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.5004Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4981Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4991Epoch 4/10: [==============                ] 31/63 batches, loss: 0.5000Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4994Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4989Epoch 4/10: [================              ] 34/63 batches, loss: 0.5006Epoch 4/10: [================              ] 35/63 batches, loss: 0.5007Epoch 4/10: [=================             ] 36/63 batches, loss: 0.5008Epoch 4/10: [=================             ] 37/63 batches, loss: 0.5028Epoch 4/10: [==================            ] 38/63 batches, loss: 0.5029Epoch 4/10: [==================            ] 39/63 batches, loss: 0.5041Epoch 4/10: [===================           ] 40/63 batches, loss: 0.5036Epoch 4/10: [===================           ] 41/63 batches, loss: 0.5053Epoch 4/10: [====================          ] 42/63 batches, loss: 0.5047Epoch 4/10: [====================          ] 43/63 batches, loss: 0.5059Epoch 4/10: [====================          ] 44/63 batches, loss: 0.5064Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.5074Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.5089Epoch 4/10: [======================        ] 47/63 batches, loss: 0.5073Epoch 4/10: [======================        ] 48/63 batches, loss: 0.5052Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.5047Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.5047Epoch 4/10: [========================      ] 51/63 batches, loss: 0.5056Epoch 4/10: [========================      ] 52/63 batches, loss: 0.5042Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.5047Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.5051Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.5047Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.5051Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.5063Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.5063Epoch 4/10: [============================  ] 59/63 batches, loss: 0.5067Epoch 4/10: [============================  ] 60/63 batches, loss: 0.5066Epoch 4/10: [============================= ] 61/63 batches, loss: 0.5058Epoch 4/10: [============================= ] 62/63 batches, loss: 0.5058Epoch 4/10: [==============================] 63/63 batches, loss: 0.5067
[2025-05-01 11:40:54,489][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5067
[2025-05-01 11:40:54,680][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5243, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6225Epoch 5/10: [                              ] 2/63 batches, loss: 0.6108Epoch 5/10: [=                             ] 3/63 batches, loss: 0.5751Epoch 5/10: [=                             ] 4/63 batches, loss: 0.5577Epoch 5/10: [==                            ] 5/63 batches, loss: 0.5426Epoch 5/10: [==                            ] 6/63 batches, loss: 0.5322Epoch 5/10: [===                           ] 7/63 batches, loss: 0.5180Epoch 5/10: [===                           ] 8/63 batches, loss: 0.5192Epoch 5/10: [====                          ] 9/63 batches, loss: 0.5176Epoch 5/10: [====                          ] 10/63 batches, loss: 0.5186Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.5152Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.5143Epoch 5/10: [======                        ] 13/63 batches, loss: 0.5135Epoch 5/10: [======                        ] 14/63 batches, loss: 0.5128Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.5072Epoch 5/10: [========                      ] 17/63 batches, loss: 0.5112Epoch 5/10: [========                      ] 18/63 batches, loss: 0.5095Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.5079Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.5018Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.5019Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.4998Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.5021Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.5022Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.5013Epoch 5/10: [============                  ] 26/63 batches, loss: 0.5023Epoch 5/10: [============                  ] 27/63 batches, loss: 0.5024Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.5016Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.5041Epoch 5/10: [==============                ] 30/63 batches, loss: 0.5065Epoch 5/10: [==============                ] 31/63 batches, loss: 0.5087Epoch 5/10: [===============               ] 32/63 batches, loss: 0.5093Epoch 5/10: [===============               ] 33/63 batches, loss: 0.5084Epoch 5/10: [================              ] 34/63 batches, loss: 0.5082Epoch 5/10: [================              ] 35/63 batches, loss: 0.5095Epoch 5/10: [=================             ] 36/63 batches, loss: 0.5060Epoch 5/10: [=================             ] 37/63 batches, loss: 0.5053Epoch 5/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 5/10: [==================            ] 39/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 40/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 41/63 batches, loss: 0.5057Epoch 5/10: [====================          ] 42/63 batches, loss: 0.5046Epoch 5/10: [====================          ] 43/63 batches, loss: 0.5023Epoch 5/10: [====================          ] 44/63 batches, loss: 0.5040Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.5035Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.5040Epoch 5/10: [======================        ] 47/63 batches, loss: 0.5045Epoch 5/10: [======================        ] 48/63 batches, loss: 0.5055Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.5050Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.5040Epoch 5/10: [========================      ] 51/63 batches, loss: 0.5049Epoch 5/10: [========================      ] 52/63 batches, loss: 0.5063Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.5073Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.5068Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.5063Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.5062Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.5074Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.5070Epoch 5/10: [============================  ] 59/63 batches, loss: 0.5065Epoch 5/10: [============================  ] 60/63 batches, loss: 0.5061Epoch 5/10: [============================= ] 61/63 batches, loss: 0.5053Epoch 5/10: [============================= ] 62/63 batches, loss: 0.5053Epoch 5/10: [==============================] 63/63 batches, loss: 0.5042
[2025-05-01 11:41:01,369][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5042
[2025-05-01 11:41:01,565][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5520, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 11:41:01,565][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.5647Epoch 6/10: [                              ] 2/63 batches, loss: 0.5578Epoch 6/10: [=                             ] 3/63 batches, loss: 0.5329Epoch 6/10: [=                             ] 4/63 batches, loss: 0.5315Epoch 6/10: [==                            ] 5/63 batches, loss: 0.5164Epoch 6/10: [==                            ] 6/63 batches, loss: 0.5104Epoch 6/10: [===                           ] 7/63 batches, loss: 0.5196Epoch 6/10: [===                           ] 8/63 batches, loss: 0.5116Epoch 6/10: [====                          ] 9/63 batches, loss: 0.5071Epoch 6/10: [====                          ] 10/63 batches, loss: 0.5068Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.5003Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.5026Epoch 6/10: [======                        ] 13/63 batches, loss: 0.5045Epoch 6/10: [======                        ] 14/63 batches, loss: 0.5044Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.5087Epoch 6/10: [========                      ] 17/63 batches, loss: 0.5070Epoch 6/10: [========                      ] 18/63 batches, loss: 0.5082Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.5117Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.5137Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.5155Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.5139Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.5120Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.5079Epoch 6/10: [============                  ] 26/63 batches, loss: 0.5059Epoch 6/10: [============                  ] 27/63 batches, loss: 0.5076Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.5074Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.5089Epoch 6/10: [==============                ] 30/63 batches, loss: 0.5056Epoch 6/10: [==============                ] 31/63 batches, loss: 0.5040Epoch 6/10: [===============               ] 32/63 batches, loss: 0.5062Epoch 6/10: [===============               ] 33/63 batches, loss: 0.5061Epoch 6/10: [================              ] 34/63 batches, loss: 0.5089Epoch 6/10: [================              ] 35/63 batches, loss: 0.5094Epoch 6/10: [=================             ] 36/63 batches, loss: 0.5099Epoch 6/10: [=================             ] 37/63 batches, loss: 0.5097Epoch 6/10: [==================            ] 38/63 batches, loss: 0.5102Epoch 6/10: [==================            ] 39/63 batches, loss: 0.5088Epoch 6/10: [===================           ] 40/63 batches, loss: 0.5093Epoch 6/10: [===================           ] 41/63 batches, loss: 0.5097Epoch 6/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 6/10: [====================          ] 43/63 batches, loss: 0.5095Epoch 6/10: [====================          ] 44/63 batches, loss: 0.5099Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.5102Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.5101Epoch 6/10: [======================        ] 47/63 batches, loss: 0.5115Epoch 6/10: [======================        ] 48/63 batches, loss: 0.5118Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.5107Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.5105Epoch 6/10: [========================      ] 51/63 batches, loss: 0.5099Epoch 6/10: [========================      ] 52/63 batches, loss: 0.5084Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.5074Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.5069Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.5064Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.5077Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.5080Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.5059Epoch 6/10: [============================  ] 59/63 batches, loss: 0.5055Epoch 6/10: [============================  ] 60/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 61/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 62/63 batches, loss: 0.5054Epoch 6/10: [==============================] 63/63 batches, loss: 0.5064
[2025-05-01 11:41:07,928][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5064
[2025-05-01 11:41:08,121][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5391, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561, 'precision': 0.9523809523809523, 'recall': 1.0}
[2025-05-01 11:41:08,122][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.4559Epoch 7/10: [                              ] 2/63 batches, loss: 0.5157Epoch 7/10: [=                             ] 3/63 batches, loss: 0.5037Epoch 7/10: [=                             ] 4/63 batches, loss: 0.5037Epoch 7/10: [==                            ] 5/63 batches, loss: 0.4941Epoch 7/10: [==                            ] 6/63 batches, loss: 0.4918Epoch 7/10: [===                           ] 7/63 batches, loss: 0.4907Epoch 7/10: [===                           ] 8/63 batches, loss: 0.4923Epoch 7/10: [====                          ] 9/63 batches, loss: 0.4883Epoch 7/10: [====                          ] 10/63 batches, loss: 0.4947Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.4976Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.5061Epoch 7/10: [======                        ] 13/63 batches, loss: 0.5095Epoch 7/10: [======                        ] 14/63 batches, loss: 0.5041Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.5025Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.4996Epoch 7/10: [========                      ] 17/63 batches, loss: 0.5013Epoch 7/10: [========                      ] 18/63 batches, loss: 0.4987Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.4965Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.4968Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.4960Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.4974Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.4967Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.4990Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 26/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 27/63 batches, loss: 0.5012Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.5013Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.5030Epoch 7/10: [==============                ] 30/63 batches, loss: 0.5046Epoch 7/10: [==============                ] 31/63 batches, loss: 0.5061Epoch 7/10: [===============               ] 32/63 batches, loss: 0.5069Epoch 7/10: [===============               ] 33/63 batches, loss: 0.5090Epoch 7/10: [================              ] 34/63 batches, loss: 0.5074Epoch 7/10: [================              ] 35/63 batches, loss: 0.5060Epoch 7/10: [=================             ] 36/63 batches, loss: 0.5073Epoch 7/10: [=================             ] 37/63 batches, loss: 0.5065Epoch 7/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 7/10: [==================            ] 39/63 batches, loss: 0.5045Epoch 7/10: [===================           ] 40/63 batches, loss: 0.5057Epoch 7/10: [===================           ] 41/63 batches, loss: 0.5051Epoch 7/10: [====================          ] 42/63 batches, loss: 0.5067Epoch 7/10: [====================          ] 43/63 batches, loss: 0.5072Epoch 7/10: [====================          ] 44/63 batches, loss: 0.5077Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.5092Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.5080Epoch 7/10: [======================        ] 47/63 batches, loss: 0.5089Epoch 7/10: [======================        ] 48/63 batches, loss: 0.5083Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.5077Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 7/10: [========================      ] 51/63 batches, loss: 0.5066Epoch 7/10: [========================      ] 52/63 batches, loss: 0.5066Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.5061Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.5047Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.5051Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.5042Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.5046Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.5050Epoch 7/10: [============================  ] 59/63 batches, loss: 0.5046Epoch 7/10: [============================  ] 60/63 batches, loss: 0.5042Epoch 7/10: [============================= ] 61/63 batches, loss: 0.5046Epoch 7/10: [============================= ] 62/63 batches, loss: 0.5038Epoch 7/10: [==============================] 63/63 batches, loss: 0.5048
[2025-05-01 11:41:14,488][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5048
[2025-05-01 11:41:14,674][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5218, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.5040Epoch 8/10: [                              ] 2/63 batches, loss: 0.5392Epoch 8/10: [=                             ] 3/63 batches, loss: 0.5116Epoch 8/10: [=                             ] 4/63 batches, loss: 0.5333Epoch 8/10: [==                            ] 5/63 batches, loss: 0.5131Epoch 8/10: [==                            ] 6/63 batches, loss: 0.5114Epoch 8/10: [===                           ] 7/63 batches, loss: 0.5137Epoch 8/10: [===                           ] 8/63 batches, loss: 0.5065Epoch 8/10: [====                          ] 9/63 batches, loss: 0.5009Epoch 8/10: [====                          ] 10/63 batches, loss: 0.4988Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.4970Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.4996Epoch 8/10: [======                        ] 13/63 batches, loss: 0.4926Epoch 8/10: [======                        ] 14/63 batches, loss: 0.4917Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.4941Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.4888Epoch 8/10: [========                      ] 17/63 batches, loss: 0.4883Epoch 8/10: [========                      ] 18/63 batches, loss: 0.4931Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.4936Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.4977Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.4979Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.4960Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.4947Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.4969Epoch 8/10: [============                  ] 26/63 batches, loss: 0.4954Epoch 8/10: [============                  ] 27/63 batches, loss: 0.4948Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.4952Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 30/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 31/63 batches, loss: 0.4973Epoch 8/10: [===============               ] 32/63 batches, loss: 0.4960Epoch 8/10: [===============               ] 33/63 batches, loss: 0.4941Epoch 8/10: [================              ] 34/63 batches, loss: 0.4951Epoch 8/10: [================              ] 35/63 batches, loss: 0.4953Epoch 8/10: [=================             ] 36/63 batches, loss: 0.4949Epoch 8/10: [=================             ] 37/63 batches, loss: 0.4964Epoch 8/10: [==================            ] 38/63 batches, loss: 0.4972Epoch 8/10: [==================            ] 39/63 batches, loss: 0.4992Epoch 8/10: [===================           ] 40/63 batches, loss: 0.5011Epoch 8/10: [===================           ] 41/63 batches, loss: 0.5012Epoch 8/10: [====================          ] 42/63 batches, loss: 0.5013Epoch 8/10: [====================          ] 43/63 batches, loss: 0.5019Epoch 8/10: [====================          ] 44/63 batches, loss: 0.5030Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.5036Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.5025Epoch 8/10: [======================        ] 47/63 batches, loss: 0.5036Epoch 8/10: [======================        ] 48/63 batches, loss: 0.5036Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.5022Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.5013Epoch 8/10: [========================      ] 51/63 batches, loss: 0.5024Epoch 8/10: [========================      ] 52/63 batches, loss: 0.5019Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.5015Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.5011Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.5016Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.5008Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.5009Epoch 8/10: [============================  ] 59/63 batches, loss: 0.5013Epoch 8/10: [============================  ] 60/63 batches, loss: 0.5017Epoch 8/10: [============================= ] 61/63 batches, loss: 0.5037Epoch 8/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 8/10: [==============================] 63/63 batches, loss: 0.5071
[2025-05-01 11:41:21,444][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5071
[2025-05-01 11:41:21,644][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5239, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:21,645][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.4561Epoch 9/10: [                              ] 2/63 batches, loss: 0.5154Epoch 9/10: [=                             ] 3/63 batches, loss: 0.5036Epoch 9/10: [=                             ] 4/63 batches, loss: 0.5214Epoch 9/10: [==                            ] 5/63 batches, loss: 0.5225Epoch 9/10: [==                            ] 6/63 batches, loss: 0.5272Epoch 9/10: [===                           ] 7/63 batches, loss: 0.5102Epoch 9/10: [===                           ] 8/63 batches, loss: 0.5005Epoch 9/10: [====                          ] 9/63 batches, loss: 0.5061Epoch 9/10: [====                          ] 10/63 batches, loss: 0.5034Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.5014Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.5075Epoch 9/10: [======                        ] 13/63 batches, loss: 0.5090Epoch 9/10: [======                        ] 14/63 batches, loss: 0.5069Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.5082Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.5079Epoch 9/10: [========                      ] 17/63 batches, loss: 0.5077Epoch 9/10: [========                      ] 18/63 batches, loss: 0.5062Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.5073Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.5083Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.5092Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.5100Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.5107Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.5075Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.5054Epoch 9/10: [============                  ] 26/63 batches, loss: 0.5044Epoch 9/10: [============                  ] 27/63 batches, loss: 0.5026Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.5035Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.5018Epoch 9/10: [==============                ] 30/63 batches, loss: 0.5019Epoch 9/10: [==============                ] 31/63 batches, loss: 0.5019Epoch 9/10: [===============               ] 32/63 batches, loss: 0.5042Epoch 9/10: [===============               ] 33/63 batches, loss: 0.5049Epoch 9/10: [================              ] 34/63 batches, loss: 0.5070Epoch 9/10: [================              ] 35/63 batches, loss: 0.5055Epoch 9/10: [=================             ] 36/63 batches, loss: 0.5081Epoch 9/10: [=================             ] 37/63 batches, loss: 0.5079Epoch 9/10: [==================            ] 38/63 batches, loss: 0.5091Epoch 9/10: [==================            ] 39/63 batches, loss: 0.5077Epoch 9/10: [===================           ] 40/63 batches, loss: 0.5094Epoch 9/10: [===================           ] 41/63 batches, loss: 0.5104Epoch 9/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 9/10: [====================          ] 43/63 batches, loss: 0.5084Epoch 9/10: [====================          ] 44/63 batches, loss: 0.5094Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.5098Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.5096Epoch 9/10: [======================        ] 47/63 batches, loss: 0.5085Epoch 9/10: [======================        ] 48/63 batches, loss: 0.5079Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.5078Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 51/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 52/63 batches, loss: 0.5067Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.5062Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.5061Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.5052Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.5052Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.5060Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.5052Epoch 9/10: [============================  ] 59/63 batches, loss: 0.5048Epoch 9/10: [============================  ] 60/63 batches, loss: 0.5047Epoch 9/10: [============================= ] 61/63 batches, loss: 0.5043Epoch 9/10: [============================= ] 62/63 batches, loss: 0.5036Epoch 9/10: [==============================] 63/63 batches, loss: 0.5046
[2025-05-01 11:41:27,997][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5046
[2025-05-01 11:41:28,189][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5224, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:28,189][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.5508Epoch 10/10: [                              ] 2/63 batches, loss: 0.5033Epoch 10/10: [=                             ] 3/63 batches, loss: 0.5191Epoch 10/10: [=                             ] 4/63 batches, loss: 0.5211Epoch 10/10: [==                            ] 5/63 batches, loss: 0.5081Epoch 10/10: [==                            ] 6/63 batches, loss: 0.5073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.5000Epoch 10/10: [===                           ] 8/63 batches, loss: 0.5123Epoch 10/10: [====                          ] 9/63 batches, loss: 0.5140Epoch 10/10: [====                          ] 10/63 batches, loss: 0.5153Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.5056Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.4995Epoch 10/10: [======                        ] 13/63 batches, loss: 0.5017Epoch 10/10: [======                        ] 14/63 batches, loss: 0.4968Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.5035Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.5006Epoch 10/10: [========                      ] 17/63 batches, loss: 0.5035Epoch 10/10: [========                      ] 18/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.5035Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.5058Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.5057Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.5077Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.5085Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.5092Epoch 10/10: [============                  ] 26/63 batches, loss: 0.5090Epoch 10/10: [============                  ] 27/63 batches, loss: 0.5097Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.5078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.5061Epoch 10/10: [==============                ] 30/63 batches, loss: 0.5044Epoch 10/10: [==============                ] 31/63 batches, loss: 0.5051Epoch 10/10: [===============               ] 32/63 batches, loss: 0.5036Epoch 10/10: [===============               ] 33/63 batches, loss: 0.5043Epoch 10/10: [================              ] 34/63 batches, loss: 0.5029Epoch 10/10: [================              ] 35/63 batches, loss: 0.5022Epoch 10/10: [=================             ] 36/63 batches, loss: 0.5023Epoch 10/10: [=================             ] 37/63 batches, loss: 0.5017Epoch 10/10: [==================            ] 38/63 batches, loss: 0.5023Epoch 10/10: [==================            ] 39/63 batches, loss: 0.4993Epoch 10/10: [===================           ] 40/63 batches, loss: 0.4988Epoch 10/10: [===================           ] 41/63 batches, loss: 0.4984Epoch 10/10: [====================          ] 42/63 batches, loss: 0.4996Epoch 10/10: [====================          ] 43/63 batches, loss: 0.4997Epoch 10/10: [====================          ] 44/63 batches, loss: 0.4998Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.5004Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 47/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 48/63 batches, loss: 0.5016Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.5018Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.5027Epoch 10/10: [========================      ] 51/63 batches, loss: 0.5023Epoch 10/10: [========================      ] 52/63 batches, loss: 0.5005Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.5001Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.5006Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.5020Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.5037Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 59/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 60/63 batches, loss: 0.5037Epoch 10/10: [============================= ] 61/63 batches, loss: 0.5033Epoch 10/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 10/10: [==============================] 63/63 batches, loss: 0.5030
[2025-05-01 11:41:34,543][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5030
[2025-05-01 11:41:34,739][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.5531, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 11:41:34,739][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Training completed in 67.94 seconds
[2025-05-01 11:41:34,740][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:41:37,179][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.922077922077922, 'f1': 0.875, 'precision': 0.8076923076923077, 'recall': 0.9545454545454546}
[2025-05-01 11:41:38,869][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
[2025-05-01 11:41:38,874][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▁▅███
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▅▅▄▄▅▅▅
wandb:            train_loss █▅▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆██▇▇███▇
wandb:                val_f1 ▁▆██▆▇███▆
wandb:              val_loss █▃▁▁▂▂▁▁▁▂
wandb:         val_precision ▁▅██▆▇███▆
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.5218
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 10
wandb:                 epoch 10
wandb:   final_test_accuracy 0.92208
wandb:         final_test_f1 0.875
wandb:  final_test_precision 0.80769
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 1
wandb:        final_train_f1 1
wandb: final_train_precision 1
wandb:    final_train_recall 1
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50304
wandb:            train_time 67.94414
wandb:          val_accuracy 0.95455
wandb:                val_f1 0.95238
wandb:              val_loss 0.55315
wandb:         val_precision 0.90909
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114016-07wlukwc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114016-07wlukwc/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:41:50,537][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:41:50,537][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:41:50,537][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:41:50,538][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:41:50,542][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 11:41:50,542][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:41:52,060][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:41:54,310][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:41:54,311][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,388][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,482][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 11:41:54,489][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,490][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 11:41:54,491][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,507][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,534][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,547][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 11:41:54,549][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:41:54,564][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,592][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:41:54,605][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 11:41:54,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:41:54,606][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 11:41:54,607][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,608][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,608][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 11:41:54,608][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,609][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,609][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:41:54,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:41:54,610][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:41:54,610][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 11:41:54,610][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:41:54,611][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:41:54,611][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:41:54,611][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:41:58,426][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:41:58,428][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:41:58,432][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:41:58,433][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:41:58,433][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:41:58,433][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 11:41:58,434][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:41:58,434][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.2303Epoch 1/10: [                              ] 2/63 batches, loss: 0.2078Epoch 1/10: [=                             ] 3/63 batches, loss: 0.2220Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2301Epoch 1/10: [==                            ] 5/63 batches, loss: 0.2037Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1926Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1881Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1857Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1875Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1871Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1827Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1822Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1838Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1789Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1721Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1689Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1630Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1590Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1582Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1561Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1553Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1519Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1510Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1471Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1437Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1411Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1407Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1398Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1378Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1350Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1325Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1296Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1272Epoch 1/10: [================              ] 34/63 batches, loss: 0.1249Epoch 1/10: [================              ] 35/63 batches, loss: 0.1232Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1213Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1195Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1187Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1166Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1147Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1134Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1116Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1112Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1099Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1088Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1069Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1060Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1051Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1037Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1024Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1008Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0993Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0983Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0969Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0962Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0953Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0942Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0937Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0928Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0916Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0906Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0896Epoch 1/10: [==============================] 63/63 batches, loss: 0.0889
[2025-05-01 11:42:07,289][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0889
[2025-05-01 11:42:07,458][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0799, Metrics: {'mse': 0.08098221570253372, 'rmse': 0.2845737438741208, 'r2': -0.248205304145813}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0437Epoch 2/10: [                              ] 2/63 batches, loss: 0.0360Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0361Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0326Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0318Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0303Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0295Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0285Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0291Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0275Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0278Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0271Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0285Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0289Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0293Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0286Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0287Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0277Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0273Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0269Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0263Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0260Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0256Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0252Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0252Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0251Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0251Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0250Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0250Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0247Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0247Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0250Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0250Epoch 2/10: [================              ] 34/63 batches, loss: 0.0253Epoch 2/10: [================              ] 35/63 batches, loss: 0.0251Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0250Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0246Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0241Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0241Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0238Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0238Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0242Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0239Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0237Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0239Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0237Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0236Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0233Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0232Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0231Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0230Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0228Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0227Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0224Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0222Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0221Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0219Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0216Epoch 2/10: [==============================] 63/63 batches, loss: 0.0215
[2025-05-01 11:42:14,213][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0215
[2025-05-01 11:42:14,393][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0622, Metrics: {'mse': 0.06337393075227737, 'rmse': 0.2517417938131795, 'r2': 0.0231969952583313}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0210Epoch 3/10: [                              ] 2/63 batches, loss: 0.0156Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0158Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0144Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0153Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0155Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0146Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0145Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0144Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0142Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0141Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0138Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0157Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0157Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0154Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0158Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0156Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0158Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0159Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0162Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0157Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0156Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0157Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0157Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0161Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0161Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0160Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0160Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0159Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0160Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0169Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0170Epoch 3/10: [================              ] 34/63 batches, loss: 0.0167Epoch 3/10: [================              ] 35/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0166Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0164Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0163Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0163Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0161Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0160Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0161Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0162Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0162Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0163Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0165Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0166Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0167Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0166Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0165Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0164Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0163Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0161Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0162Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0163Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0164Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0163Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0162Epoch 3/10: [==============================] 63/63 batches, loss: 0.0161
[2025-05-01 11:42:21,172][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0161
[2025-05-01 11:42:21,359][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0502, Metrics: {'mse': 0.05127166584134102, 'rmse': 0.22643247523564508, 'r2': 0.20973312854766846}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0150Epoch 4/10: [                              ] 2/63 batches, loss: 0.0187Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0129Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0135Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0136Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0134Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0132Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0162Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0168Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0178Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0179Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0173Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0177Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0182Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0182Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0178Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0172Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0169Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0165Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0164Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0162Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0167Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0169Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0167Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0165Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0163Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0165Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0164Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0162Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0160Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0158Epoch 4/10: [================              ] 34/63 batches, loss: 0.0158Epoch 4/10: [================              ] 35/63 batches, loss: 0.0158Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0157Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0156Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0154Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0151Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0150Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0151Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0150Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0151Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0150Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0151Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0150Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0150Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0152Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0151Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0152Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0152Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0154Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0154Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0152Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0152Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0150Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0151Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0153Epoch 4/10: [==============================] 63/63 batches, loss: 0.0153
[2025-05-01 11:42:28,067][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0153
[2025-05-01 11:42:28,249][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0382, Metrics: {'mse': 0.03820837289094925, 'rmse': 0.19546962140176474, 'r2': 0.4110819101333618}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0113Epoch 5/10: [                              ] 2/63 batches, loss: 0.0082Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0114Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0153Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0170Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0162Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0168Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0171Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0166Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0175Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0167Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0165Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0162Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0154Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0150Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0146Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0145Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0141Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0139Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0138Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0136Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0138Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0135Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0134Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0136Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0134Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0132Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0136Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0135Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0134Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0135Epoch 5/10: [================              ] 34/63 batches, loss: 0.0135Epoch 5/10: [================              ] 35/63 batches, loss: 0.0134Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0133Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0132Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0131Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0130Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0132Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0132Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0131Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0134Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0133Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0131Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0131Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0130Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0131Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0130Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0130Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0131Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0132Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0133Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0132Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0134Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0133Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0134Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0133Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0135Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0136Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0135Epoch 5/10: [==============================] 63/63 batches, loss: 0.0134
[2025-05-01 11:42:34,974][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0134
[2025-05-01 11:42:35,165][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0359, Metrics: {'mse': 0.03661568462848663, 'rmse': 0.19135225273951345, 'r2': 0.43563055992126465}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0175Epoch 6/10: [                              ] 2/63 batches, loss: 0.0172Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0150Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0135Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0130Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0130Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0124Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0123Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0127Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0122Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0117Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0131Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0130Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0134Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0139Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0137Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0132Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0130Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0125Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0127Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0124Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0124Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0121Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0122Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0120Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0119Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0117Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0116Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0115Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0112Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0111Epoch 6/10: [================              ] 34/63 batches, loss: 0.0111Epoch 6/10: [================              ] 35/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0110Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0108Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0107Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0108Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0109Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0109Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0108Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0107Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0106Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0105Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0105Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0105Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0106Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0107Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0106Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0105Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0106Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0105Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0105Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0106Epoch 6/10: [==============================] 63/63 batches, loss: 0.0104
[2025-05-01 11:42:41,905][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0104
[2025-05-01 11:42:42,100][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0371, Metrics: {'mse': 0.03692213073372841, 'rmse': 0.19215132248758635, 'r2': 0.4309071898460388}
[2025-05-01 11:42:42,100][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0070Epoch 7/10: [                              ] 2/63 batches, loss: 0.0090Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0095Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0104Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0096Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0110Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0111Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0107Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0100Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0102Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0096Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0095Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0091Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0093Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0096Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0095Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0091Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0094Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0093Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0092Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0091Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0089Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0088Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0087Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0085Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0089Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0087Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0088Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0088Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0087Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0087Epoch 7/10: [================              ] 34/63 batches, loss: 0.0087Epoch 7/10: [================              ] 35/63 batches, loss: 0.0087Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0086Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0087Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0086Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0087Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0087Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0086Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0085Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0084Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0083Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0082Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0084Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0083Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0084Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0084Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0083Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0084Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0085Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0085Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0085Epoch 7/10: [==============================] 63/63 batches, loss: 0.0084
[2025-05-01 11:42:48,462][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0084
[2025-05-01 11:42:48,650][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0329, Metrics: {'mse': 0.03260713443160057, 'rmse': 0.18057445675288786, 'r2': 0.4974156618118286}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0032Epoch 8/10: [                              ] 2/63 batches, loss: 0.0052Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0054Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0054Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0060Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0059Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0061Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0068Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0073Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0075Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0081Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0087Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0090Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0090Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0087Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0083Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0082Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0083Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0081Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0083Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0087Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0090Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0091Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0094Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0100Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0098Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0097Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0096Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0096Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0097Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0097Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0096Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0098Epoch 8/10: [================              ] 34/63 batches, loss: 0.0096Epoch 8/10: [================              ] 35/63 batches, loss: 0.0095Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0096Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0096Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0097Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0098Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0097Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0097Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0099Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0101Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0102Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0102Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0101Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0101Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0103Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0104Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0105Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0104Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0105Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0105Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0106Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0105Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0104Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0104Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0103Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0103Epoch 8/10: [==============================] 63/63 batches, loss: 0.0106
[2025-05-01 11:42:55,447][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0106
[2025-05-01 11:42:55,641][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0266, Metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.0077Epoch 9/10: [                              ] 2/63 batches, loss: 0.0091Epoch 9/10: [=                             ] 3/63 batches, loss: 0.0088Epoch 9/10: [=                             ] 4/63 batches, loss: 0.0091Epoch 9/10: [==                            ] 5/63 batches, loss: 0.0097Epoch 9/10: [==                            ] 6/63 batches, loss: 0.0097Epoch 9/10: [===                           ] 7/63 batches, loss: 0.0090Epoch 9/10: [===                           ] 8/63 batches, loss: 0.0091Epoch 9/10: [====                          ] 9/63 batches, loss: 0.0086Epoch 9/10: [====                          ] 10/63 batches, loss: 0.0085Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.0082Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.0082Epoch 9/10: [======                        ] 13/63 batches, loss: 0.0080Epoch 9/10: [======                        ] 14/63 batches, loss: 0.0078Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.0080Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.0077Epoch 9/10: [========                      ] 17/63 batches, loss: 0.0076Epoch 9/10: [========                      ] 18/63 batches, loss: 0.0077Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.0075Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.0077Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.0076Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.0077Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.0078Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 26/63 batches, loss: 0.0080Epoch 9/10: [============                  ] 27/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.0079Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.0081Epoch 9/10: [==============                ] 30/63 batches, loss: 0.0080Epoch 9/10: [==============                ] 31/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 32/63 batches, loss: 0.0079Epoch 9/10: [===============               ] 33/63 batches, loss: 0.0079Epoch 9/10: [================              ] 34/63 batches, loss: 0.0079Epoch 9/10: [================              ] 35/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 36/63 batches, loss: 0.0080Epoch 9/10: [=================             ] 37/63 batches, loss: 0.0081Epoch 9/10: [==================            ] 38/63 batches, loss: 0.0082Epoch 9/10: [==================            ] 39/63 batches, loss: 0.0082Epoch 9/10: [===================           ] 40/63 batches, loss: 0.0081Epoch 9/10: [===================           ] 41/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 42/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 43/63 batches, loss: 0.0081Epoch 9/10: [====================          ] 44/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.0081Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 47/63 batches, loss: 0.0080Epoch 9/10: [======================        ] 48/63 batches, loss: 0.0080Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.0081Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 51/63 batches, loss: 0.0081Epoch 9/10: [========================      ] 52/63 batches, loss: 0.0081Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.0082Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.0081Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.0081Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 59/63 batches, loss: 0.0081Epoch 9/10: [============================  ] 60/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 61/63 batches, loss: 0.0080Epoch 9/10: [============================= ] 62/63 batches, loss: 0.0080Epoch 9/10: [==============================] 63/63 batches, loss: 0.0079
[2025-05-01 11:43:02,377][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0079
[2025-05-01 11:43:02,576][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0328, Metrics: {'mse': 0.03263333812355995, 'rmse': 0.18064699865638498, 'r2': 0.4970117211341858}
[2025-05-01 11:43:02,577][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.0058Epoch 10/10: [                              ] 2/63 batches, loss: 0.0090Epoch 10/10: [=                             ] 3/63 batches, loss: 0.0076Epoch 10/10: [=                             ] 4/63 batches, loss: 0.0071Epoch 10/10: [==                            ] 5/63 batches, loss: 0.0074Epoch 10/10: [==                            ] 6/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.0073Epoch 10/10: [===                           ] 8/63 batches, loss: 0.0069Epoch 10/10: [====                          ] 9/63 batches, loss: 0.0071Epoch 10/10: [====                          ] 10/63 batches, loss: 0.0071Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.0072Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.0071Epoch 10/10: [======                        ] 13/63 batches, loss: 0.0072Epoch 10/10: [======                        ] 14/63 batches, loss: 0.0072Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.0070Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 17/63 batches, loss: 0.0069Epoch 10/10: [========                      ] 18/63 batches, loss: 0.0070Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.0072Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.0071Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.0073Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.0075Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.0075Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.0076Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.0077Epoch 10/10: [============                  ] 26/63 batches, loss: 0.0079Epoch 10/10: [============                  ] 27/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.0078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 30/63 batches, loss: 0.0077Epoch 10/10: [==============                ] 31/63 batches, loss: 0.0076Epoch 10/10: [===============               ] 32/63 batches, loss: 0.0078Epoch 10/10: [===============               ] 33/63 batches, loss: 0.0080Epoch 10/10: [================              ] 34/63 batches, loss: 0.0079Epoch 10/10: [================              ] 35/63 batches, loss: 0.0078Epoch 10/10: [=================             ] 36/63 batches, loss: 0.0077Epoch 10/10: [=================             ] 37/63 batches, loss: 0.0078Epoch 10/10: [==================            ] 38/63 batches, loss: 0.0077Epoch 10/10: [==================            ] 39/63 batches, loss: 0.0078Epoch 10/10: [===================           ] 40/63 batches, loss: 0.0079Epoch 10/10: [===================           ] 41/63 batches, loss: 0.0078Epoch 10/10: [====================          ] 42/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 43/63 batches, loss: 0.0077Epoch 10/10: [====================          ] 44/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.0076Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 47/63 batches, loss: 0.0076Epoch 10/10: [======================        ] 48/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.0076Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.0075Epoch 10/10: [========================      ] 51/63 batches, loss: 0.0074Epoch 10/10: [========================      ] 52/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.0073Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.0073Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.0074Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.0073Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 59/63 batches, loss: 0.0073Epoch 10/10: [============================  ] 60/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 61/63 batches, loss: 0.0072Epoch 10/10: [============================= ] 62/63 batches, loss: 0.0073Epoch 10/10: [==============================] 63/63 batches, loss: 0.0072
[2025-05-01 11:43:08,935][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0072
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0373, Metrics: {'mse': 0.0370272658765316, 'rmse': 0.1924247018356313, 'r2': 0.4292866587638855}
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Training completed in 69.08 seconds
[2025-05-01 11:43:09,125][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007992316037416458, 'rmse': 0.089399754123915, 'r2': 0.7396416068077087}
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026841821148991585, 'rmse': 0.16383473730864156, 'r2': 0.5862782597541809}
[2025-05-01 11:43:11,593][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07233811914920807, 'rmse': 0.26895746717503133, 'r2': -0.24707698822021484}
[2025-05-01 11:43:13,231][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/ar/model.pt
[2025-05-01 11:43:13,236][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁
wandb:     best_val_mse █▆▄▂▂▂▁
wandb:      best_val_r2 ▁▃▅▇▇▇█
wandb:    best_val_rmse █▆▅▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▆▇▆▇▇▇
wandb:       train_loss █▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▂▁▂▂
wandb:          val_mse █▆▄▂▂▂▂▁▂▂
wandb:           val_r2 ▁▃▅▇▇▇▇█▇▇
wandb:         val_rmse █▆▅▃▃▃▂▁▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02665
wandb:     best_val_mse 0.02684
wandb:      best_val_r2 0.58628
wandb:    best_val_rmse 0.16383
wandb:            epoch 10
wandb:   final_test_mse 0.07234
wandb:    final_test_r2 -0.24708
wandb:  final_test_rmse 0.26896
wandb:  final_train_mse 0.00799
wandb:   final_train_r2 0.73964
wandb: final_train_rmse 0.0894
wandb:    final_val_mse 0.02684
wandb:     final_val_r2 0.58628
wandb:   final_val_rmse 0.16383
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00717
wandb:       train_time 69.07752
wandb:         val_loss 0.03734
wandb:          val_mse 0.03703
wandb:           val_r2 0.42929
wandb:         val_rmse 0.19242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114150-fr4hh7tz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114150-fr4hh7tz/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:43:24,183][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:43:24,183][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:43:24,183][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:43:24,183][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:43:24,187][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-05-01 11:43:24,188][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:43:25,852][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:43:28,380][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:43:28,380][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,428][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,456][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,576][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 11:43:28,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,585][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 11:43:28,586][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,601][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,629][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,641][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 11:43:28,642][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,642][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 11:43:28,643][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:43:28,660][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,689][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:43:28,701][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 11:43:28,703][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:43:28,703][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 11:43:28,704][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,705][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-05-01 11:43:28,705][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:43:28,705][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,706][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:43:28,706][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:43:28,706][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:43:28,707][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:43:28,707][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:43:28,707][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:43:28,707][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:43:28,708][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:43:32,576][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:43:32,577][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:43:32,582][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:43:32,582][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:43:32,583][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:43:32,583][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 11:43:32,583][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:43:32,584][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7244Epoch 1/10: [                              ] 2/75 batches, loss: 0.7709Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7345Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7469Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7655Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7285Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7406Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7416Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7494Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7309Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7394Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7360Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7374Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7416Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7414Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7419Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7397Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7407Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7387Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7353Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7348Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7330Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7302Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7263Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7272Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7254Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7283Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7299Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7300Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7288Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7278Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7264Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7268Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7256Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7222Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7218Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7207Epoch 1/10: [================              ] 40/75 batches, loss: 0.7197Epoch 1/10: [================              ] 41/75 batches, loss: 0.7194Epoch 1/10: [================              ] 42/75 batches, loss: 0.7208Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7211Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7192Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7178Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7169Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7182Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7161Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7159Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7154Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7168Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7172Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7176Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7172Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7161Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7151Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7146Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7146Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7156Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7145Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7131Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7118Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7114Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7120Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7115Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7095Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7084Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7064Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7060Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7046Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7022Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7006Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6999Epoch 1/10: [==============================] 75/75 batches, loss: 0.6998
[2025-05-01 11:43:42,515][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6998
[2025-05-01 11:43:42,756][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6192, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.96, 'precision': 0.9230769230769231, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5898Epoch 2/10: [                              ] 2/75 batches, loss: 0.6357Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6189Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6576Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6427Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6484Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6562Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6519Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6442Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6339Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6304Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6263Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6254Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6207Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6138Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6117Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6112Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6107Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6047Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6032Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5990Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5964Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5949Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5953Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5908Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5898Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5912Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5897Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5896Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5882Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5892Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5884Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5876Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5861Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5834Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5833Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5826Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5795Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5760Epoch 2/10: [================              ] 40/75 batches, loss: 0.5735Epoch 2/10: [================              ] 41/75 batches, loss: 0.5713Epoch 2/10: [================              ] 42/75 batches, loss: 0.5692Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5681Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5665Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5643Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5630Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5609Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5606Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5595Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5605Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5604Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5594Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5589Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5581Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5563Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5555Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5559Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5552Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5545Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5542Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5535Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5514Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5510Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5503Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5514Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5500Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5486Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5481Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5491Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5492Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5482Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5477Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5471Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5462Epoch 2/10: [==============================] 75/75 batches, loss: 0.5457
[2025-05-01 11:43:50,751][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5457
[2025-05-01 11:43:50,990][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5355, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5078Epoch 3/10: [                              ] 2/75 batches, loss: 0.4961Epoch 3/10: [=                             ] 3/75 batches, loss: 0.4835Epoch 3/10: [=                             ] 4/75 batches, loss: 0.4891Epoch 3/10: [==                            ] 5/75 batches, loss: 0.4831Epoch 3/10: [==                            ] 6/75 batches, loss: 0.4988Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5000Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5039Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5068Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5067Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5066Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5084Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5119Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5063Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5078Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5066Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5015Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5042Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5031Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5021Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.4991Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.4994Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.4987Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.4990Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5036Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5053Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5045Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5066Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5094Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5072Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5071Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5050Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5056Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5057Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5069Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5074Epoch 3/10: [================              ] 40/75 batches, loss: 0.5068Epoch 3/10: [================              ] 41/75 batches, loss: 0.5067Epoch 3/10: [================              ] 42/75 batches, loss: 0.5067Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5061Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5066Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5056Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5061Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5055Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5065Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5074Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5078Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5055Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5064Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5055Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5054Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5057Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5065Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5065Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5061Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5061Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5061Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5064Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5060Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5067Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5063Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5060Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5063Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5064Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5071Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5077Epoch 3/10: [==============================] 75/75 batches, loss: 0.5083
[2025-05-01 11:43:58,973][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5083
[2025-05-01 11:43:59,221][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5405, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:43:59,221][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5045Epoch 4/10: [                              ] 2/75 batches, loss: 0.5169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5049Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5106Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5143Epoch 4/10: [==                            ] 6/75 batches, loss: 0.4930Epoch 4/10: [==                            ] 7/75 batches, loss: 0.4981Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5019Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5022Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5048Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5039Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4982Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4951Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4940Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4900Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4864Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4884Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4906Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4926Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4957Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4972Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4965Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4989Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5027Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.4980Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.4992Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5011Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5012Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5013Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5022Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5045Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5052Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5066Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5072Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5079Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5078Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5096Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5095Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5087Epoch 4/10: [================              ] 40/75 batches, loss: 0.5056Epoch 4/10: [================              ] 41/75 batches, loss: 0.5056Epoch 4/10: [================              ] 42/75 batches, loss: 0.5055Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5060Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5060Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5049Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5044Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5034Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5030Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5025Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5021Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5030Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5026Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5018Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5002Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5015Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5012Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5012Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5020Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5024Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5030Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5027Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5045Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5038Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5048Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5051Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5058Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5054Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5061Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5060Epoch 4/10: [==============================] 75/75 batches, loss: 0.5073
[2025-05-01 11:44:06,786][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5073
[2025-05-01 11:44:07,034][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5446, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:44:07,034][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4084Epoch 5/10: [                              ] 2/75 batches, loss: 0.4206Epoch 5/10: [=                             ] 3/75 batches, loss: 0.4087Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4324Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4277Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4523Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4698Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4800Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4826Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4864Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4898Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4836Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4816Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4847Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4873Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4925Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4958Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4974Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4930Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4935Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4907Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4934Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4958Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4923Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4960Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4945Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4932Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4927Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4915Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4949Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4937Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4947Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4950Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4946Epoch 5/10: [==============                ] 36/75 batches, loss: 0.4948Epoch 5/10: [==============                ] 37/75 batches, loss: 0.4963Epoch 5/10: [===============               ] 38/75 batches, loss: 0.4972Epoch 5/10: [===============               ] 39/75 batches, loss: 0.4985Epoch 5/10: [================              ] 40/75 batches, loss: 0.5004Epoch 5/10: [================              ] 41/75 batches, loss: 0.5017Epoch 5/10: [================              ] 42/75 batches, loss: 0.5017Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5018Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5029Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5019Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5033Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5038Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5033Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5023Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5009Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5005Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5015Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5011Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5027Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5032Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5036Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5032Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5036Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5048Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5040Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5048Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5059Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5066Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5073Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5065Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5065Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5054Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5060Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5063Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5049Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5058Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5064Epoch 5/10: [==============================] 75/75 batches, loss: 0.5057
[2025-05-01 11:44:14,591][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5057
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5448, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:44:14,834][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 11:44:14,835][src.training.lm_trainer][INFO] - Training completed in 40.57 seconds
[2025-05-01 11:44:14,835][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:44:17,739][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488, 'precision': 0.9933333333333333, 'recall': 1.0}
[2025-05-01 11:44:17,740][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
[2025-05-01 11:44:17,740][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9363636363636364, 'f1': 0.9369369369369369, 'precision': 0.9285714285714286, 'recall': 0.9454545454545454}
[2025-05-01 11:44:19,392][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
[2025-05-01 11:44:19,397][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy █▁
wandb:           best_val_f1 █▁
wandb:         best_val_loss █▁
wandb:    best_val_precision █▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃
wandb:            train_loss █▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy █▄▁▁▁
wandb:                val_f1 █▄▁▁▁
wandb:              val_loss █▁▁▂▂
wandb:         val_precision █▄▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.94444
wandb:           best_val_f1 0.94737
wandb:         best_val_loss 0.53551
wandb:    best_val_precision 0.9
wandb:       best_val_recall 1
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.93636
wandb:         final_test_f1 0.93694
wandb:  final_test_precision 0.92857
wandb:     final_test_recall 0.94545
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99666
wandb: final_train_precision 0.99333
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.94444
wandb:          final_val_f1 0.94737
wandb:   final_val_precision 0.9
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50574
wandb:            train_time 40.57217
wandb:          val_accuracy 0.93056
wandb:                val_f1 0.93506
wandb:              val_loss 0.54483
wandb:         val_precision 0.87805
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114324-v27te4ak
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114324-v27te4ak/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:44:30,992][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/en
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:44:30,992][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:44:30,992][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:44:30,992][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:44:30,996][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-05-01 11:44:30,997][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:44:32,367][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:44:34,572][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:44:34,572][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,733][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 11:44:34,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,742][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 11:44:34,743][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,759][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,800][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 11:44:34,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,802][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 11:44:34,802][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:44:34,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,848][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:44:34,860][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 11:44:34,861][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:44:34,862][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,863][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,864][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,864][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:44:34,865][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:44:34,865][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:44:34,866][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:44:34,866][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:44:34,866][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:44:34,867][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:44:38,700][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:44:38,701][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:44:38,706][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:44:38,706][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 11:44:38,707][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:44:38,707][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.2066Epoch 1/10: [                              ] 2/75 batches, loss: 0.1540Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1667Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1615Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1548Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1549Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1526Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1525Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1538Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1519Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1514Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1493Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1487Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1461Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1438Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1426Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1370Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1344Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1307Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1295Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1290Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1307Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1324Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1332Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1346Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1334Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1322Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1296Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1286Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1269Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1253Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1240Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1229Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1238Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1226Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1207Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1200Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1187Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1176Epoch 1/10: [================              ] 40/75 batches, loss: 0.1170Epoch 1/10: [================              ] 41/75 batches, loss: 0.1176Epoch 1/10: [================              ] 42/75 batches, loss: 0.1187Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1177Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1176Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1167Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1151Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1138Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1125Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1113Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1096Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1083Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1075Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1064Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.1054Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1045Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1032Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1028Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1014Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.1008Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0998Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0991Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0980Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0970Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0963Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0956Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0948Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0939Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0930Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0918Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0910Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0904Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0895Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0890Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0881Epoch 1/10: [==============================] 75/75 batches, loss: 0.0873
[2025-05-01 11:44:48,736][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0873
[2025-05-01 11:44:48,957][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0408, Metrics: {'mse': 0.04292728006839752, 'rmse': 0.2071889960118479, 'r2': -0.025711774826049805}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0591Epoch 2/10: [                              ] 2/75 batches, loss: 0.0610Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0468Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0415Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0435Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0436Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0406Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0380Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0350Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0334Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0317Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0312Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0310Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0307Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0304Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0310Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0303Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0304Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0308Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0299Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0294Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0303Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0305Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0305Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0309Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0308Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0304Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0320Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0323Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0330Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0327Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0321Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0316Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0313Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0310Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0309Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0313Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0309Epoch 2/10: [================              ] 40/75 batches, loss: 0.0305Epoch 2/10: [================              ] 41/75 batches, loss: 0.0307Epoch 2/10: [================              ] 42/75 batches, loss: 0.0308Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0304Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0300Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0298Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0296Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0301Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0300Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0302Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0306Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0305Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0308Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0311Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0307Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0305Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0303Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0302Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0300Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0297Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0298Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0296Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0294Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0293Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0291Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0291Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0289Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0288Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0290Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0289Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0287Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0286Epoch 2/10: [==============================] 75/75 batches, loss: 0.0283
[2025-05-01 11:44:56,904][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0283
[2025-05-01 11:44:57,132][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0278, Metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0207Epoch 3/10: [                              ] 2/75 batches, loss: 0.0363Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0314Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0312Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0305Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0275Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0283Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0270Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0298Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0279Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0285Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0297Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0301Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0299Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0297Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0313Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0308Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0298Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0297Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0287Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0283Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0285Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0280Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0276Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0275Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0275Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0273Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0266Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0262Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0258Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0257Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0256Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0257Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0255Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0255Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0253Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0253Epoch 3/10: [================              ] 40/75 batches, loss: 0.0248Epoch 3/10: [================              ] 41/75 batches, loss: 0.0248Epoch 3/10: [================              ] 42/75 batches, loss: 0.0249Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0248Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0246Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0250Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0249Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0250Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0251Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0248Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0249Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0250Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0249Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0247Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0249Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0251Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0250Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0250Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0249Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0249Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0247Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0250Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0250Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0252Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0251Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0249Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0249Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0248Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0246Epoch 3/10: [==============================] 75/75 batches, loss: 0.0247
[2025-05-01 11:45:05,108][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0247
[2025-05-01 11:45:05,616][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0504, Metrics: {'mse': 0.04933957755565643, 'rmse': 0.2221251394049229, 'r2': -0.17892837524414062}
[2025-05-01 11:45:05,617][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0155Epoch 4/10: [                              ] 2/75 batches, loss: 0.0169Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0168Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0182Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0193Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0183Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0175Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0171Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0169Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0175Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0178Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0177Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0173Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0175Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0179Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0176Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0175Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0175Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0181Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0178Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0178Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0176Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0174Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0172Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0178Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0176Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0175Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0173Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0174Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0172Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0172Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0171Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0170Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0172Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0171Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0175Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0175Epoch 4/10: [================              ] 40/75 batches, loss: 0.0175Epoch 4/10: [================              ] 41/75 batches, loss: 0.0176Epoch 4/10: [================              ] 42/75 batches, loss: 0.0175Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0174Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0174Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0173Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0171Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0173Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0174Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0176Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0174Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0178Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0177Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0178Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0179Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0178Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0179Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0179Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0178Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0181Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0181Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0182Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0182Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0181Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0179Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0180Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0180Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0179Epoch 4/10: [==============================] 75/75 batches, loss: 0.0178
[2025-05-01 11:45:13,568][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0178
[2025-05-01 11:45:13,953][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0353, Metrics: {'mse': 0.03462854400277138, 'rmse': 0.18608746331435488, 'r2': 0.17257964611053467}
[2025-05-01 11:45:13,954][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0090Epoch 5/10: [                              ] 2/75 batches, loss: 0.0128Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0166Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0164Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0165Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0166Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0165Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0183Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0186Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0185Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0180Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0175Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0172Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0167Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0163Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0159Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0167Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0162Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0160Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0164Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0165Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0162Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0160Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0166Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0163Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0167Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0166Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0163Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0165Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0162Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0169Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0171Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0175Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0174Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0173Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0173Epoch 5/10: [================              ] 40/75 batches, loss: 0.0173Epoch 5/10: [================              ] 41/75 batches, loss: 0.0178Epoch 5/10: [================              ] 42/75 batches, loss: 0.0179Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0181Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0182Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0184Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0183Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0185Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0184Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0184Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0190Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0191Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0192Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0191Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0191Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0190Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0188Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0187Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0187Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0186Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0184Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0184Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0183Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0183Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0184Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0183Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0182Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0181Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0182Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0183Epoch 5/10: [==============================] 75/75 batches, loss: 0.0183
[2025-05-01 11:45:21,521][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0183
[2025-05-01 11:45:21,783][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0332, Metrics: {'mse': 0.03250707685947418, 'rmse': 0.1802971903815314, 'r2': 0.22327035665512085}
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Training completed in 41.52 seconds
[2025-05-01 11:45:21,784][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02293914183974266, 'rmse': 0.15145673256657383, 'r2': 0.14497649669647217}
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.027115143835544586, 'rmse': 0.16466676603232538, 'r2': 0.35210609436035156}
[2025-05-01 11:45:24,656][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.022624490782618523, 'rmse': 0.15041439685953775, 'r2': 0.4129396677017212}
[2025-05-01 11:45:26,297][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/en/model.pt
[2025-05-01 11:45:26,303][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▆▁▅
wandb:       train_loss █▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▁█▃▃
wandb:          val_mse ▆▁█▃▃
wandb:           val_r2 ▃█▁▆▆
wandb:         val_rmse ▆▁█▄▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02783
wandb:     best_val_mse 0.02712
wandb:      best_val_r2 0.35211
wandb:    best_val_rmse 0.16467
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.02262
wandb:    final_test_r2 0.41294
wandb:  final_test_rmse 0.15041
wandb:  final_train_mse 0.02294
wandb:   final_train_r2 0.14498
wandb: final_train_rmse 0.15146
wandb:    final_val_mse 0.02712
wandb:     final_val_r2 0.35211
wandb:   final_val_rmse 0.16467
wandb:    learning_rate 2e-05
wandb:       train_loss 0.01835
wandb:       train_time 41.5229
wandb:         val_loss 0.03321
wandb:          val_mse 0.03251
wandb:           val_r2 0.22327
wandb:         val_rmse 0.1803
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114431-aylozcfq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114431-aylozcfq/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:45:38,669][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:45:38,670][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:45:38,670][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:45:38,670][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:45:38,675][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-01 11:45:38,675][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:45:40,096][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:45:42,504][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:45:42,505][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,564][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,685][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 11:45:42,694][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,695][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 11:45:42,696][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,719][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,749][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,762][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 11:45:42,764][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,764][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 11:45:42,765][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:45:42,786][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,812][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:45:42,824][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 11:45:42,826][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:45:42,826][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 11:45:42,827][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-01 11:45:42,829][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:45:42,830][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:45:42,830][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 11:45:42,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:45:42,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:45:42,831][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:45:42,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:45:47,018][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:45:47,019][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:45:47,023][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:45:47,024][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:45:47,024][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:45:47,024][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 11:45:47,025][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:45:47,025][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7251Epoch 1/10: [                              ] 2/75 batches, loss: 0.7237Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7232Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7154Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6918Epoch 1/10: [==                            ] 6/75 batches, loss: 0.6868Epoch 1/10: [==                            ] 7/75 batches, loss: 0.6918Epoch 1/10: [===                           ] 8/75 batches, loss: 0.6915Epoch 1/10: [===                           ] 9/75 batches, loss: 0.6982Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7075Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7120Epoch 1/10: [====                          ] 12/75 batches, loss: 0.6995Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7033Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7046Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7039Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7053Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7037Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7076Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7071Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7057Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7142Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7131Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7165Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7166Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7206Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7235Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7237Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7236Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7235Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7244Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7224Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7214Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7231Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7249Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7237Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7219Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7227Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7220Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7210Epoch 1/10: [================              ] 40/75 batches, loss: 0.7223Epoch 1/10: [================              ] 41/75 batches, loss: 0.7245Epoch 1/10: [================              ] 42/75 batches, loss: 0.7255Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7263Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7245Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7277Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7288Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7291Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7306Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7283Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7270Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7248Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7230Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7232Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7218Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7223Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7216Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7220Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7204Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7205Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7194Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7199Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7211Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7209Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7196Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7210Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7202Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7205Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7209Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7220Epoch 1/10: [============================= ] 74/75 batches, loss: 0.7203Epoch 1/10: [==============================] 75/75 batches, loss: 0.7186
[2025-05-01 11:45:57,421][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7186
[2025-05-01 11:45:57,626][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7320, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.7242Epoch 2/10: [                              ] 2/75 batches, loss: 0.7205Epoch 2/10: [=                             ] 3/75 batches, loss: 0.7064Epoch 2/10: [=                             ] 4/75 batches, loss: 0.7046Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6933Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6944Epoch 2/10: [==                            ] 7/75 batches, loss: 0.7067Epoch 2/10: [===                           ] 8/75 batches, loss: 0.7179Epoch 2/10: [===                           ] 9/75 batches, loss: 0.7155Epoch 2/10: [====                          ] 10/75 batches, loss: 0.7152Epoch 2/10: [====                          ] 11/75 batches, loss: 0.7094Epoch 2/10: [====                          ] 12/75 batches, loss: 0.7034Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.7042Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6998Epoch 2/10: [======                        ] 15/75 batches, loss: 0.7004Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6975Epoch 2/10: [======                        ] 17/75 batches, loss: 0.7006Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.7019Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.7028Epoch 2/10: [========                      ] 20/75 batches, loss: 0.7052Epoch 2/10: [========                      ] 21/75 batches, loss: 0.7100Epoch 2/10: [========                      ] 22/75 batches, loss: 0.7144Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.7172Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.7195Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.7193Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.7207Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.7200Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.7176Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.7156Epoch 2/10: [============                  ] 30/75 batches, loss: 0.7145Epoch 2/10: [============                  ] 31/75 batches, loss: 0.7127Epoch 2/10: [============                  ] 32/75 batches, loss: 0.7154Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.7116Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.7072Epoch 2/10: [==============                ] 35/75 batches, loss: 0.7090Epoch 2/10: [==============                ] 36/75 batches, loss: 0.7053Epoch 2/10: [==============                ] 37/75 batches, loss: 0.7053Epoch 2/10: [===============               ] 38/75 batches, loss: 0.7035Epoch 2/10: [===============               ] 39/75 batches, loss: 0.7028Epoch 2/10: [================              ] 40/75 batches, loss: 0.7020Epoch 2/10: [================              ] 41/75 batches, loss: 0.7016Epoch 2/10: [================              ] 42/75 batches, loss: 0.6999Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6986Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6967Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6976Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6984Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6991Epoch 2/10: [===================           ] 48/75 batches, loss: 0.7012Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6995Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6988Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6982Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6985Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6979Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6983Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6974Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6972Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6975Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6968Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6970Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6967Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6946Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6951Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6940Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6936Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6914Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6917Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6911Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6892Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6876Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6872Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6871Epoch 2/10: [==============================] 75/75 batches, loss: 0.6854
[2025-05-01 11:46:05,612][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6854
[2025-05-01 11:46:05,832][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6544, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315, 'precision': 1.0, 'recall': 0.9}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6111Epoch 3/10: [                              ] 2/75 batches, loss: 0.6070Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5885Epoch 3/10: [=                             ] 4/75 batches, loss: 0.6017Epoch 3/10: [==                            ] 5/75 batches, loss: 0.6115Epoch 3/10: [==                            ] 6/75 batches, loss: 0.6209Epoch 3/10: [==                            ] 7/75 batches, loss: 0.6121Epoch 3/10: [===                           ] 8/75 batches, loss: 0.6182Epoch 3/10: [===                           ] 9/75 batches, loss: 0.6020Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5966Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5901Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5919Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5863Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5812Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5789Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5707Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5708Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5712Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5739Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5699Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5725Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5714Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5681Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5681Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5671Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5655Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5620Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5629Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5591Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5578Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5541Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5548Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5513Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5509Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5497Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5499Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5494Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5479Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5482Epoch 3/10: [================              ] 40/75 batches, loss: 0.5496Epoch 3/10: [================              ] 41/75 batches, loss: 0.5474Epoch 3/10: [================              ] 42/75 batches, loss: 0.5487Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5488Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5489Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5486Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5483Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5464Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5466Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5454Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5441Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5439Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5438Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5440Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5425Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5415Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5408Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5403Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5389Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5380Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5363Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5358Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5361Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5359Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5358Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5344Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5336Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5337Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5336Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5334Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5327Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5323Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5315Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5312Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5315Epoch 3/10: [==============================] 75/75 batches, loss: 0.5327
[2025-05-01 11:46:13,829][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5327
[2025-05-01 11:46:14,062][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5255, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5542Epoch 4/10: [                              ] 2/75 batches, loss: 0.5660Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5297Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5116Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5151Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5095Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5043Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5046Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5074Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5072Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5004Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5028Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5091Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5170Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5162Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5127Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5150Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5106Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5091Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5108Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5127Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5150Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5155Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5180Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5184Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5135Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5178Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5182Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5183Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5185Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5189Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5204Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5192Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5175Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5172Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5182Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5189Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5204Epoch 4/10: [================              ] 40/75 batches, loss: 0.5206Epoch 4/10: [================              ] 41/75 batches, loss: 0.5199Epoch 4/10: [================              ] 42/75 batches, loss: 0.5179Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5194Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5192Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5206Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5208Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5200Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5206Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5194Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5182Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5188Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5208Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5196Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5185Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5191Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5180Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5174Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5160Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5154Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5156Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5147Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5149Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5144Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5157Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5151Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5160Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5159Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5150Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5145Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5144Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5142Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5141Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5146Epoch 4/10: [==============================] 75/75 batches, loss: 0.5140
[2025-05-01 11:46:22,037][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5140
[2025-05-01 11:46:22,278][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5248, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.5751Epoch 5/10: [                              ] 2/75 batches, loss: 0.5396Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5120Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4922Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4898Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4762Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4667Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4654Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4617Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4659Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4628Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4643Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4619Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4648Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4690Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4771Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4801Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4828Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4839Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4822Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4858Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4856Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4895Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4941Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4945Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4922Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4944Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4964Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4975Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4985Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4986Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4973Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4961Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4970Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4972Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5007Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5027Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5034Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5034Epoch 5/10: [================              ] 40/75 batches, loss: 0.5046Epoch 5/10: [================              ] 41/75 batches, loss: 0.5046Epoch 5/10: [================              ] 42/75 batches, loss: 0.5079Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5067Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5072Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5092Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5086Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5090Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5079Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5098Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5087Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5086Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5094Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5102Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5100Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5091Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5103Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5097Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5096Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5111Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5118Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5128Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5127Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5122Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5105Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5104Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5096Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5102Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5108Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5107Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5116Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5105Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5104Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5100Epoch 5/10: [==============================] 75/75 batches, loss: 0.5088
[2025-05-01 11:46:30,237][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5088
[2025-05-01 11:46:30,478][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5249, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:30,479][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.5510Epoch 6/10: [                              ] 2/75 batches, loss: 0.4920Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5119Epoch 6/10: [=                             ] 4/75 batches, loss: 0.4921Epoch 6/10: [==                            ] 5/75 batches, loss: 0.4991Epoch 6/10: [==                            ] 6/75 batches, loss: 0.4840Epoch 6/10: [==                            ] 7/75 batches, loss: 0.4969Epoch 6/10: [===                           ] 8/75 batches, loss: 0.4978Epoch 6/10: [===                           ] 9/75 batches, loss: 0.5090Epoch 6/10: [====                          ] 10/75 batches, loss: 0.5132Epoch 6/10: [====                          ] 11/75 batches, loss: 0.5102Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5155Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5146Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5172Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5115Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5110Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5106Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5116Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5124Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5108Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5104Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5069Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5058Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5047Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5075Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5064Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5037Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5071Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5078Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5085Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5068Epoch 6/10: [============                  ] 32/75 batches, loss: 0.5059Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.5051Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.5079Epoch 6/10: [==============                ] 35/75 batches, loss: 0.5091Epoch 6/10: [==============                ] 36/75 batches, loss: 0.5109Epoch 6/10: [==============                ] 37/75 batches, loss: 0.5101Epoch 6/10: [===============               ] 38/75 batches, loss: 0.5087Epoch 6/10: [===============               ] 39/75 batches, loss: 0.5086Epoch 6/10: [================              ] 40/75 batches, loss: 0.5091Epoch 6/10: [================              ] 41/75 batches, loss: 0.5090Epoch 6/10: [================              ] 42/75 batches, loss: 0.5077Epoch 6/10: [=================             ] 43/75 batches, loss: 0.5071Epoch 6/10: [=================             ] 44/75 batches, loss: 0.5059Epoch 6/10: [==================            ] 45/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 46/75 batches, loss: 0.5037Epoch 6/10: [==================            ] 47/75 batches, loss: 0.5032Epoch 6/10: [===================           ] 48/75 batches, loss: 0.5037Epoch 6/10: [===================           ] 49/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 50/75 batches, loss: 0.5042Epoch 6/10: [====================          ] 51/75 batches, loss: 0.5047Epoch 6/10: [====================          ] 52/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.5042Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 55/75 batches, loss: 0.5037Epoch 6/10: [======================        ] 56/75 batches, loss: 0.5033Epoch 6/10: [======================        ] 57/75 batches, loss: 0.5041Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.5049Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5061Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5073Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5064Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5064Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5063Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5052Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5048Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5055Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5061Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5064Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5074Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5077Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5089Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5085Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5078Epoch 6/10: [==============================] 75/75 batches, loss: 0.5071
[2025-05-01 11:46:38,077][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5071
[2025-05-01 11:46:38,296][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5256, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:38,297][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.4801Epoch 7/10: [                              ] 2/75 batches, loss: 0.4917Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5196Epoch 7/10: [=                             ] 4/75 batches, loss: 0.4919Epoch 7/10: [==                            ] 5/75 batches, loss: 0.4895Epoch 7/10: [==                            ] 6/75 batches, loss: 0.4959Epoch 7/10: [==                            ] 7/75 batches, loss: 0.4902Epoch 7/10: [===                           ] 8/75 batches, loss: 0.4859Epoch 7/10: [===                           ] 9/75 batches, loss: 0.4852Epoch 7/10: [====                          ] 10/75 batches, loss: 0.4847Epoch 7/10: [====                          ] 11/75 batches, loss: 0.4886Epoch 7/10: [====                          ] 12/75 batches, loss: 0.4858Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.4909Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.4969Epoch 7/10: [======                        ] 15/75 batches, loss: 0.4989Epoch 7/10: [======                        ] 16/75 batches, loss: 0.4992Epoch 7/10: [======                        ] 17/75 batches, loss: 0.5009Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.5089Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.5050Epoch 7/10: [========                      ] 20/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 21/75 batches, loss: 0.5026Epoch 7/10: [========                      ] 22/75 batches, loss: 0.5026Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.5047Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5037Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5065Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5082Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5072Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.5096Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.5102Epoch 7/10: [============                  ] 30/75 batches, loss: 0.5084Epoch 7/10: [============                  ] 31/75 batches, loss: 0.5052Epoch 7/10: [============                  ] 32/75 batches, loss: 0.5044Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.5037Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.5058Epoch 7/10: [==============                ] 35/75 batches, loss: 0.5057Epoch 7/10: [==============                ] 36/75 batches, loss: 0.5069Epoch 7/10: [==============                ] 37/75 batches, loss: 0.5068Epoch 7/10: [===============               ] 38/75 batches, loss: 0.5074Epoch 7/10: [===============               ] 39/75 batches, loss: 0.5079Epoch 7/10: [================              ] 40/75 batches, loss: 0.5101Epoch 7/10: [================              ] 41/75 batches, loss: 0.5100Epoch 7/10: [================              ] 42/75 batches, loss: 0.5093Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5086Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5074Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5089Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5093Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5096Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5100Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5099Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5102Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5092Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5100Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5101Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5082Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5090Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5080Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5080Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5083Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5066Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5069Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5073Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5065Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5068Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5052Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5052Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5059Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5062Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5055Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5061Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5074Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5067Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5070Epoch 7/10: [==============================] 75/75 batches, loss: 0.5062
[2025-05-01 11:46:45,886][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5062
[2025-05-01 11:46:46,114][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5262, Metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Training completed in 57.06 seconds
[2025-05-01 11:46:46,115][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:46:48,974][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9874476987447699, 'f1': 0.9872988992379339, 'precision': 1.0, 'recall': 0.9749163879598662}
[2025-05-01 11:46:48,975][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9682539682539683, 'f1': 0.9655172413793104, 'precision': 1.0, 'recall': 0.9333333333333333}
[2025-05-01 11:46:48,975][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9074074074074074, 'precision': 0.9245283018867925, 'recall': 0.8909090909090909}
[2025-05-01 11:46:50,649][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/fi/model.pt
[2025-05-01 11:46:50,654][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁███
wandb:           best_val_f1 ▁███
wandb:         best_val_loss █▅▁▁
wandb:    best_val_precision ▁███
wandb:       best_val_recall ▁███
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▄▄▄▄
wandb:            train_loss █▇▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁██████
wandb:                val_f1 ▁██████
wandb:              val_loss █▅▁▁▁▁▁
wandb:         val_precision ▁██████
wandb:            val_recall ▁██████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.96825
wandb:           best_val_f1 0.96552
wandb:         best_val_loss 0.52477
wandb:    best_val_precision 1
wandb:       best_val_recall 0.93333
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.90909
wandb:         final_test_f1 0.90741
wandb:  final_test_precision 0.92453
wandb:     final_test_recall 0.89091
wandb:  final_train_accuracy 0.98745
wandb:        final_train_f1 0.9873
wandb: final_train_precision 1
wandb:    final_train_recall 0.97492
wandb:    final_val_accuracy 0.96825
wandb:          final_val_f1 0.96552
wandb:   final_val_precision 1
wandb:      final_val_recall 0.93333
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50621
wandb:            train_time 57.06461
wandb:          val_accuracy 0.96825
wandb:                val_f1 0.96552
wandb:              val_loss 0.52622
wandb:         val_precision 1
wandb:            val_recall 0.93333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114538-y3q4fxeo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114538-y3q4fxeo/logs
Experiment finetune_question_type_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi/results.json
Running experiment: finetune_complexity_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:47:02,909][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi
experiment_name: finetune_complexity_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:47:02,909][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:47:02,910][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:47:02,910][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:47:02,914][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-01 11:47:02,914][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:47:04,441][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:47:06,683][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:47:06,683][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:06,742][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,768][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,919][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-01 11:47:06,928][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:06,929][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-01 11:47:06,930][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:06,950][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,980][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:06,992][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-01 11:47:06,993][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:06,993][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-01 11:47:06,994][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:47:07,015][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:07,045][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:47:07,059][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-01 11:47:07,061][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:47:07,061][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,062][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,063][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,063][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,063][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:47:07,064][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:47:07,064][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:47:07,065][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:47:07,065][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:47:07,065][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:47:07,065][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:47:11,073][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:47:11,075][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:47:11,079][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:47:11,080][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:47:11,080][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:47:11,080][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-01 11:47:11,081][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:47:11,081][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1450Epoch 1/10: [                              ] 2/75 batches, loss: 0.1576Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1505Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1437Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1353Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1373Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1349Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1362Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1400Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1379Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1364Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1313Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1318Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1303Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1276Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1253Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1244Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1274Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1266Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1242Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1213Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1182Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1176Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1157Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1135Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1126Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1126Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1108Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1082Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1064Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1054Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1033Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1019Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1006Epoch 1/10: [==============                ] 35/75 batches, loss: 0.0997Epoch 1/10: [==============                ] 36/75 batches, loss: 0.0985Epoch 1/10: [==============                ] 37/75 batches, loss: 0.0969Epoch 1/10: [===============               ] 38/75 batches, loss: 0.0959Epoch 1/10: [===============               ] 39/75 batches, loss: 0.0951Epoch 1/10: [================              ] 40/75 batches, loss: 0.0934Epoch 1/10: [================              ] 41/75 batches, loss: 0.0925Epoch 1/10: [================              ] 42/75 batches, loss: 0.0910Epoch 1/10: [=================             ] 43/75 batches, loss: 0.0903Epoch 1/10: [=================             ] 44/75 batches, loss: 0.0899Epoch 1/10: [==================            ] 45/75 batches, loss: 0.0885Epoch 1/10: [==================            ] 46/75 batches, loss: 0.0881Epoch 1/10: [==================            ] 47/75 batches, loss: 0.0873Epoch 1/10: [===================           ] 48/75 batches, loss: 0.0862Epoch 1/10: [===================           ] 49/75 batches, loss: 0.0856Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0846Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0836Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0828Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0822Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0819Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0810Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0803Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0793Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0785Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0778Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0769Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0762Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0755Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0745Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0736Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0728Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0722Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0713Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0707Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0700Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0693Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0684Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0679Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0673Epoch 1/10: [==============================] 75/75 batches, loss: 0.0666
[2025-05-01 11:47:21,275][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0666
[2025-05-01 11:47:21,483][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0897, Metrics: {'mse': 0.08961991965770721, 'rmse': 0.2993658625456604, 'r2': -0.3669794797897339}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0185Epoch 2/10: [                              ] 2/75 batches, loss: 0.0216Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0224Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0247Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0270Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0252Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0243Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0235Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0239Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0246Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0251Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0246Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0246Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0241Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0252Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0249Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0241Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0248Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0242Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0240Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0237Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0232Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0233Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0229Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0226Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0224Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0225Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0223Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0221Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0218Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0215Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0216Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0219Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0219Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0217Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0215Epoch 2/10: [================              ] 40/75 batches, loss: 0.0215Epoch 2/10: [================              ] 41/75 batches, loss: 0.0216Epoch 2/10: [================              ] 42/75 batches, loss: 0.0214Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0213Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0213Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0212Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0211Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0212Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0213Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0212Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0211Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0212Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0212Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0214Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0213Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0212Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0211Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0211Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0210Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0212Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0211Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0212Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0211Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0213Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0215Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0216Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0218Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0217Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0216Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0215Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0213Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0213Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0213Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0214Epoch 2/10: [==============================] 75/75 batches, loss: 0.0215
[2025-05-01 11:47:29,466][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0215
[2025-05-01 11:47:29,688][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0582, Metrics: {'mse': 0.05815792828798294, 'rmse': 0.24115954944389603, 'r2': 0.11291265487670898}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0139Epoch 3/10: [                              ] 2/75 batches, loss: 0.0152Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0157Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0206Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0213Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0199Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0197Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0194Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0190Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0187Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0181Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0180Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0186Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0185Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0191Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0201Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0204Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0211Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0216Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0212Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0210Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0208Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0208Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0205Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0203Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0203Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0203Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0202Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0200Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0197Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0197Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0198Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0195Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0194Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0195Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0195Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0194Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0192Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0190Epoch 3/10: [================              ] 40/75 batches, loss: 0.0189Epoch 3/10: [================              ] 41/75 batches, loss: 0.0192Epoch 3/10: [================              ] 42/75 batches, loss: 0.0190Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0189Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0189Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0188Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0187Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0185Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0185Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0184Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0186Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0185Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0185Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0184Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0183Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0185Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0184Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0182Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0183Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0183Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0184Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0183Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0181Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0182Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0181Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0183Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0182Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0182Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0184Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0182Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0181Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0182Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0182Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0182Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0182Epoch 3/10: [==============================] 75/75 batches, loss: 0.0181
[2025-05-01 11:47:37,703][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0181
[2025-05-01 11:47:37,941][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0381, Metrics: {'mse': 0.03821783885359764, 'rmse': 0.19549383328790104, 'r2': 0.41706037521362305}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0304Epoch 4/10: [                              ] 2/75 batches, loss: 0.0258Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0251Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0206Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0201Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0202Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0213Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0205Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0197Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0184Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0181Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0177Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0186Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0182Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0183Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0182Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0181Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0189Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0184Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0185Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0182Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0180Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0180Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0178Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0176Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0177Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0177Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0175Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0180Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0179Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0176Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0173Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0171Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0169Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0166Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0171Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0170Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0169Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0168Epoch 4/10: [================              ] 40/75 batches, loss: 0.0165Epoch 4/10: [================              ] 41/75 batches, loss: 0.0164Epoch 4/10: [================              ] 42/75 batches, loss: 0.0164Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0163Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0161Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0163Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0165Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0165Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0167Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0165Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0164Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0165Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0163Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0163Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0163Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0163Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0162Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0162Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0163Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0162Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0160Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0159Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0159Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0161Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0160Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0158Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0157Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0157Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0158Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0158Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0159Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0160Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0159Epoch 4/10: [==============================] 75/75 batches, loss: 0.0158
[2025-05-01 11:47:45,886][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0158
[2025-05-01 11:47:46,120][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0404, Metrics: {'mse': 0.04041058570146561, 'rmse': 0.20102384361429768, 'r2': 0.3836142420768738}
[2025-05-01 11:47:46,121][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0189Epoch 5/10: [                              ] 2/75 batches, loss: 0.0134Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0116Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0109Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0097Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0093Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0107Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0109Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0107Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0105Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0111Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0111Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0111Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0110Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0107Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0107Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0106Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0105Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0116Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0114Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0118Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0116Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0116Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0117Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0117Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0115Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0112Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0114Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0113Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0114Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0115Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0113Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0112Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0112Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0111Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0111Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0112Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0110Epoch 5/10: [================              ] 40/75 batches, loss: 0.0111Epoch 5/10: [================              ] 41/75 batches, loss: 0.0111Epoch 5/10: [================              ] 42/75 batches, loss: 0.0112Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0113Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0112Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0112Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0113Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0113Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0111Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0111Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0117Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0117Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0117Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0116Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0115Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0115Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0114Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0114Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0114Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0113Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0113Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0114Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0113Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0113Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0112Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0112Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0111Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0111Epoch 5/10: [==============================] 75/75 batches, loss: 0.0110
[2025-05-01 11:47:53,717][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0110
[2025-05-01 11:47:53,949][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0284, Metrics: {'mse': 0.028346620500087738, 'rmse': 0.1683645464463577, 'r2': 0.5676268339157104}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0123Epoch 6/10: [                              ] 2/75 batches, loss: 0.0126Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0146Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0127Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0115Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0119Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0118Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0117Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0120Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0115Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0114Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0114Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0117Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0114Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0113Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0112Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0109Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0116Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0118Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0115Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0113Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0113Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0113Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0111Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0113Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0112Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0110Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0112Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0112Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0111Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0114Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0113Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0112Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0112Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0111Epoch 6/10: [================              ] 40/75 batches, loss: 0.0110Epoch 6/10: [================              ] 41/75 batches, loss: 0.0112Epoch 6/10: [================              ] 42/75 batches, loss: 0.0111Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0110Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0109Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0110Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0110Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0108Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0109Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0108Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0108Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0106Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0105Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0105Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0104Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0104Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0103Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0104Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0104Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0103Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0102Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0102Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0102Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0102Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0101Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0102Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0102Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0101Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0101Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0102Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0102Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0101Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0100Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0100Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0099Epoch 6/10: [==============================] 75/75 batches, loss: 0.0099
[2025-05-01 11:48:01,929][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0099
[2025-05-01 11:48:02,159][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0326, Metrics: {'mse': 0.03258148208260536, 'rmse': 0.18050341293893965, 'r2': 0.5030322074890137}
[2025-05-01 11:48:02,160][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0066Epoch 7/10: [                              ] 2/75 batches, loss: 0.0083Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0081Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0073Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0079Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0072Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0069Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0070Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0068Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0073Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0076Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0074Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0073Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0074Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0072Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0073Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0073Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0073Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0076Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0074Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0074Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0072Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0074Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0074Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0076Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0076Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0076Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0080Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0080Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0082Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0081Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0080Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0081Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0080Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0079Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0080Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0080Epoch 7/10: [================              ] 40/75 batches, loss: 0.0079Epoch 7/10: [================              ] 41/75 batches, loss: 0.0078Epoch 7/10: [================              ] 42/75 batches, loss: 0.0078Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0078Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0077Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0077Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0079Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0079Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0080Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0081Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0080Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0080Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0080Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0079Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0079Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0081Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0081Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0081Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0081Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0081Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0083Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0082Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0083Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0084Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0083Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0083Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0083Epoch 7/10: [==============================] 75/75 batches, loss: 0.0083
[2025-05-01 11:48:09,761][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0083
[2025-05-01 11:48:09,982][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0273, Metrics: {'mse': 0.027256017550826073, 'rmse': 0.16509396582197083, 'r2': 0.5842618942260742}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0106Epoch 8/10: [                              ] 2/75 batches, loss: 0.0070Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0100Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0100Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0087Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0088Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0080Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0073Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0077Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0074Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0071Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0072Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0076Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0073Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0073Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0072Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0071Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0069Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0072Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0075Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0074Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0074Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0076Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0077Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0076Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0075Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0080Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0082Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0081Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0081Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0080Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0080Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0081Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0080Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0080Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0079Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0078Epoch 8/10: [================              ] 40/75 batches, loss: 0.0077Epoch 8/10: [================              ] 41/75 batches, loss: 0.0077Epoch 8/10: [================              ] 42/75 batches, loss: 0.0077Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0077Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0078Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0078Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0079Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0079Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0078Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0078Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0077Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0077Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0077Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0076Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0076Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0078Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0078Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0077Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0077Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0076Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0076Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0077Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0077Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0078Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0078Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0078Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0077Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0078Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0079Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0079Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0079Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0079Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0078Epoch 8/10: [==============================] 75/75 batches, loss: 0.0079
[2025-05-01 11:48:18,002][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0079
[2025-05-01 11:48:18,236][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0256, Metrics: {'mse': 0.025504937395453453, 'rmse': 0.15970265306328962, 'r2': 0.610971212387085}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0075Epoch 9/10: [                              ] 2/75 batches, loss: 0.0073Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0067Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0061Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0065Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0070Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0065Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0060Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0061Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0062Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0058Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0057Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0056Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0057Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0059Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0059Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0059Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0060Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0059Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0059Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0059Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0059Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0059Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0061Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0063Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0063Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0065Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0065Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0064Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0064Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0064Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0063Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0062Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0066Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0066Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0067Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0068Epoch 9/10: [================              ] 40/75 batches, loss: 0.0067Epoch 9/10: [================              ] 41/75 batches, loss: 0.0067Epoch 9/10: [================              ] 42/75 batches, loss: 0.0066Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0066Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0066Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0066Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0065Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0066Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0067Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0066Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0069Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0068Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0068Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0068Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0068Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0070Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0069Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0069Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0068Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0068Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0067Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0067Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0067Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0067Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0066Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0067Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0067Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0067Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0066Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0066Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0066Epoch 9/10: [==============================] 75/75 batches, loss: 0.0066
[2025-05-01 11:48:26,218][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0066
[2025-05-01 11:48:26,451][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0370, Metrics: {'mse': 0.036957599222660065, 'rmse': 0.19224359345023714, 'r2': 0.4362828731536865}
[2025-05-01 11:48:26,451][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0074Epoch 10/10: [                              ] 2/75 batches, loss: 0.0076Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0110Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0093Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0093Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0086Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0079Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0075Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0078Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0073Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0072Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0070Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0068Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0072Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0071Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0070Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0069Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0067Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0065Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0067Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0067Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0072Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0071Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0070Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0069Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0069Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0069Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0070Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0070Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0071Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0071Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0074Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0072Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0075Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0076Epoch 10/10: [================              ] 40/75 batches, loss: 0.0075Epoch 10/10: [================              ] 41/75 batches, loss: 0.0076Epoch 10/10: [================              ] 42/75 batches, loss: 0.0075Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0075Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0074Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0074Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0074Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0074Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0073Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0073Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0072Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0072Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0071Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0071Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0070Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0070Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0069Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0071Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0071Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0070Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0070Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0070Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0069Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0069Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0069Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0069Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0070Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0069Epoch 10/10: [==============================] 75/75 batches, loss: 0.0070
[2025-05-01 11:48:34,044][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0070
[2025-05-01 11:48:34,285][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0246, Metrics: {'mse': 0.024519970640540123, 'rmse': 0.15658853930138095, 'r2': 0.6259950399398804}
[2025-05-01 11:48:34,662][src.training.lm_trainer][INFO] - Training completed in 81.71 seconds
[2025-05-01 11:48:34,662][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.004295824561268091, 'rmse': 0.06554254008861796, 'r2': 0.7874200940132141}
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.024519970640540123, 'rmse': 0.15658853930138095, 'r2': 0.6259950399398804}
[2025-05-01 11:48:37,579][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02496601827442646, 'rmse': 0.1580063868153008, 'r2': 0.3677409291267395}
[2025-05-01 11:48:39,245][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/fi/model.pt
[2025-05-01 11:48:39,250][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▁▁▁▁
wandb:     best_val_mse █▅▂▁▁▁▁
wandb:      best_val_r2 ▁▄▇████
wandb:    best_val_rmse █▅▃▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▇▇▇▇▇▇▇
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▃▁▂▁▁▂▁
wandb:          val_mse █▅▂▃▁▂▁▁▂▁
wandb:           val_r2 ▁▄▇▆█▇██▇█
wandb:         val_rmse █▅▃▃▂▂▁▁▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02459
wandb:     best_val_mse 0.02452
wandb:      best_val_r2 0.626
wandb:    best_val_rmse 0.15659
wandb:            epoch 10
wandb:   final_test_mse 0.02497
wandb:    final_test_r2 0.36774
wandb:  final_test_rmse 0.15801
wandb:  final_train_mse 0.0043
wandb:   final_train_r2 0.78742
wandb: final_train_rmse 0.06554
wandb:    final_val_mse 0.02452
wandb:     final_val_r2 0.626
wandb:   final_val_rmse 0.15659
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00696
wandb:       train_time 81.70911
wandb:         val_loss 0.02459
wandb:          val_mse 0.02452
wandb:           val_r2 0.626
wandb:         val_rmse 0.15659
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114702-okidsjni
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114702-okidsjni/logs
Experiment finetune_complexity_fi completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/fi/results.json
Running experiment: finetune_question_type_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:48:50,980][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/id
experiment_name: finetune_question_type_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:48:50,980][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:48:50,980][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:48:50,980][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:48:50,984][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-01 11:48:50,985][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:48:52,403][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:48:54,763][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:48:54,763][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:54,837][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:54,862][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:54,956][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-01 11:48:54,962][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:54,963][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-01 11:48:54,964][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:54,980][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,021][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-01 11:48:55,022][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:55,022][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-01 11:48:55,023][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:48:55,040][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,072][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:48:55,095][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-01 11:48:55,096][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:48:55,096][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-01 11:48:55,097][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-01 11:48:55,097][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,098][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-01 11:48:55,098][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,098][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,099][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:48:55,099][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:48:55,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:48:55,100][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:48:55,100][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:48:55,100][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:48:55,100][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:48:55,101][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:48:59,036][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:48:59,037][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:48:59,041][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:48:59,042][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:48:59,042][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:48:59,042][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-01 11:48:59,043][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:48:59,043][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.7521Epoch 1/10: [=                             ] 2/60 batches, loss: 0.7363Epoch 1/10: [=                             ] 3/60 batches, loss: 0.7513Epoch 1/10: [==                            ] 4/60 batches, loss: 0.7502Epoch 1/10: [==                            ] 5/60 batches, loss: 0.7440Epoch 1/10: [===                           ] 6/60 batches, loss: 0.7388Epoch 1/10: [===                           ] 7/60 batches, loss: 0.7312Epoch 1/10: [====                          ] 8/60 batches, loss: 0.7329Epoch 1/10: [====                          ] 9/60 batches, loss: 0.7377Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.7322Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.7280Epoch 1/10: [======                        ] 12/60 batches, loss: 0.7251Epoch 1/10: [======                        ] 13/60 batches, loss: 0.7240Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.7208Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.7172Epoch 1/10: [========                      ] 16/60 batches, loss: 0.7187Epoch 1/10: [========                      ] 17/60 batches, loss: 0.7190Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.7211Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.7205Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.7197Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.7221Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.7180Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.7199Epoch 1/10: [============                  ] 24/60 batches, loss: 0.7207Epoch 1/10: [============                  ] 25/60 batches, loss: 0.7215Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.7217Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.7211Epoch 1/10: [==============                ] 28/60 batches, loss: 0.7209Epoch 1/10: [==============                ] 29/60 batches, loss: 0.7225Epoch 1/10: [===============               ] 30/60 batches, loss: 0.7256Epoch 1/10: [===============               ] 31/60 batches, loss: 0.7264Epoch 1/10: [================              ] 32/60 batches, loss: 0.7270Epoch 1/10: [================              ] 33/60 batches, loss: 0.7273Epoch 1/10: [=================             ] 34/60 batches, loss: 0.7298Epoch 1/10: [=================             ] 35/60 batches, loss: 0.7279Epoch 1/10: [==================            ] 36/60 batches, loss: 0.7283Epoch 1/10: [==================            ] 37/60 batches, loss: 0.7273Epoch 1/10: [===================           ] 38/60 batches, loss: 0.7290Epoch 1/10: [===================           ] 39/60 batches, loss: 0.7277Epoch 1/10: [====================          ] 40/60 batches, loss: 0.7283Epoch 1/10: [====================          ] 41/60 batches, loss: 0.7298Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.7310Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.7290Epoch 1/10: [======================        ] 44/60 batches, loss: 0.7269Epoch 1/10: [======================        ] 45/60 batches, loss: 0.7273Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.7251Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.7280Epoch 1/10: [========================      ] 48/60 batches, loss: 0.7275Epoch 1/10: [========================      ] 49/60 batches, loss: 0.7270Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.7260Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.7233Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.7238Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.7239Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.7246Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.7253Epoch 1/10: [============================  ] 56/60 batches, loss: 0.7240Epoch 1/10: [============================  ] 57/60 batches, loss: 0.7223Epoch 1/10: [============================= ] 58/60 batches, loss: 0.7226Epoch 1/10: [============================= ] 59/60 batches, loss: 0.7223Epoch 1/10: [==============================] 60/60 batches, loss: 0.7213
[2025-05-01 11:49:07,386][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7213
[2025-05-01 11:49:07,622][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7009, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.6761Epoch 2/10: [=                             ] 2/60 batches, loss: 0.6724Epoch 2/10: [=                             ] 3/60 batches, loss: 0.6818Epoch 2/10: [==                            ] 4/60 batches, loss: 0.6797Epoch 2/10: [==                            ] 5/60 batches, loss: 0.6778Epoch 2/10: [===                           ] 6/60 batches, loss: 0.6730Epoch 2/10: [===                           ] 7/60 batches, loss: 0.6691Epoch 2/10: [====                          ] 8/60 batches, loss: 0.6773Epoch 2/10: [====                          ] 9/60 batches, loss: 0.6744Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.6687Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.6715Epoch 2/10: [======                        ] 12/60 batches, loss: 0.6692Epoch 2/10: [======                        ] 13/60 batches, loss: 0.6658Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.6655Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.6668Epoch 2/10: [========                      ] 16/60 batches, loss: 0.6661Epoch 2/10: [========                      ] 17/60 batches, loss: 0.6658Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.6608Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.6580Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.6576Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.6569Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.6568Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.6539Epoch 2/10: [============                  ] 24/60 batches, loss: 0.6527Epoch 2/10: [============                  ] 25/60 batches, loss: 0.6526Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.6506Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.6477Epoch 2/10: [==============                ] 28/60 batches, loss: 0.6461Epoch 2/10: [==============                ] 29/60 batches, loss: 0.6448Epoch 2/10: [===============               ] 30/60 batches, loss: 0.6436Epoch 2/10: [===============               ] 31/60 batches, loss: 0.6414Epoch 2/10: [================              ] 32/60 batches, loss: 0.6390Epoch 2/10: [================              ] 33/60 batches, loss: 0.6353Epoch 2/10: [=================             ] 34/60 batches, loss: 0.6345Epoch 2/10: [=================             ] 35/60 batches, loss: 0.6327Epoch 2/10: [==================            ] 36/60 batches, loss: 0.6305Epoch 2/10: [==================            ] 37/60 batches, loss: 0.6301Epoch 2/10: [===================           ] 38/60 batches, loss: 0.6281Epoch 2/10: [===================           ] 39/60 batches, loss: 0.6278Epoch 2/10: [====================          ] 40/60 batches, loss: 0.6267Epoch 2/10: [====================          ] 41/60 batches, loss: 0.6269Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.6254Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.6251Epoch 2/10: [======================        ] 44/60 batches, loss: 0.6224Epoch 2/10: [======================        ] 45/60 batches, loss: 0.6206Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.6201Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.6186Epoch 2/10: [========================      ] 48/60 batches, loss: 0.6177Epoch 2/10: [========================      ] 49/60 batches, loss: 0.6170Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.6173Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.6158Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.6151Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.6148Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.6123Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.6098Epoch 2/10: [============================  ] 56/60 batches, loss: 0.6089Epoch 2/10: [============================  ] 57/60 batches, loss: 0.6094Epoch 2/10: [============================= ] 58/60 batches, loss: 0.6088Epoch 2/10: [============================= ] 59/60 batches, loss: 0.6083Epoch 2/10: [==============================] 60/60 batches, loss: 0.6082
[2025-05-01 11:49:14,091][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6082
[2025-05-01 11:49:14,335][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6023, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.5746Epoch 3/10: [=                             ] 2/60 batches, loss: 0.5856Epoch 3/10: [=                             ] 3/60 batches, loss: 0.5772Epoch 3/10: [==                            ] 4/60 batches, loss: 0.5617Epoch 3/10: [==                            ] 5/60 batches, loss: 0.5615Epoch 3/10: [===                           ] 6/60 batches, loss: 0.5676Epoch 3/10: [===                           ] 7/60 batches, loss: 0.5694Epoch 3/10: [====                          ] 8/60 batches, loss: 0.5549Epoch 3/10: [====                          ] 9/60 batches, loss: 0.5505Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.5467Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.5433Epoch 3/10: [======                        ] 12/60 batches, loss: 0.5436Epoch 3/10: [======                        ] 13/60 batches, loss: 0.5429Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.5424Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.5434Epoch 3/10: [========                      ] 16/60 batches, loss: 0.5437Epoch 3/10: [========                      ] 17/60 batches, loss: 0.5459Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.5464Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.5481Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.5500Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.5482Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.5464Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.5458Epoch 3/10: [============                  ] 24/60 batches, loss: 0.5445Epoch 3/10: [============                  ] 25/60 batches, loss: 0.5448Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.5446Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.5459Epoch 3/10: [==============                ] 28/60 batches, loss: 0.5462Epoch 3/10: [==============                ] 29/60 batches, loss: 0.5448Epoch 3/10: [===============               ] 30/60 batches, loss: 0.5451Epoch 3/10: [===============               ] 31/60 batches, loss: 0.5446Epoch 3/10: [================              ] 32/60 batches, loss: 0.5456Epoch 3/10: [================              ] 33/60 batches, loss: 0.5466Epoch 3/10: [=================             ] 34/60 batches, loss: 0.5440Epoch 3/10: [=================             ] 35/60 batches, loss: 0.5430Epoch 3/10: [==================            ] 36/60 batches, loss: 0.5454Epoch 3/10: [==================            ] 37/60 batches, loss: 0.5444Epoch 3/10: [===================           ] 38/60 batches, loss: 0.5427Epoch 3/10: [===================           ] 39/60 batches, loss: 0.5418Epoch 3/10: [====================          ] 40/60 batches, loss: 0.5420Epoch 3/10: [====================          ] 41/60 batches, loss: 0.5400Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.5397Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.5372Epoch 3/10: [======================        ] 44/60 batches, loss: 0.5359Epoch 3/10: [======================        ] 45/60 batches, loss: 0.5363Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.5356Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.5365Epoch 3/10: [========================      ] 48/60 batches, loss: 0.5344Epoch 3/10: [========================      ] 49/60 batches, loss: 0.5338Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.5342Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.5345Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.5344Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.5339Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.5325Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.5324Epoch 3/10: [============================  ] 56/60 batches, loss: 0.5324Epoch 3/10: [============================  ] 57/60 batches, loss: 0.5319Epoch 3/10: [============================= ] 58/60 batches, loss: 0.5318Epoch 3/10: [============================= ] 59/60 batches, loss: 0.5310Epoch 3/10: [==============================] 60/60 batches, loss: 0.5318
[2025-05-01 11:49:20,864][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5318
[2025-05-01 11:49:21,126][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6003, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.4824Epoch 4/10: [=                             ] 2/60 batches, loss: 0.5049Epoch 4/10: [=                             ] 3/60 batches, loss: 0.4810Epoch 4/10: [==                            ] 4/60 batches, loss: 0.5049Epoch 4/10: [==                            ] 5/60 batches, loss: 0.5141Epoch 4/10: [===                           ] 6/60 batches, loss: 0.5124Epoch 4/10: [===                           ] 7/60 batches, loss: 0.5145Epoch 4/10: [====                          ] 8/60 batches, loss: 0.5073Epoch 4/10: [====                          ] 9/60 batches, loss: 0.5203Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.5188Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.5239Epoch 4/10: [======                        ] 12/60 batches, loss: 0.5301Epoch 4/10: [======                        ] 13/60 batches, loss: 0.5336Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.5332Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.5297Epoch 4/10: [========                      ] 16/60 batches, loss: 0.5325Epoch 4/10: [========                      ] 17/60 batches, loss: 0.5308Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.5307Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.5350Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.5348Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.5323Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.5332Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.5351Epoch 4/10: [============                  ] 24/60 batches, loss: 0.5348Epoch 4/10: [============                  ] 25/60 batches, loss: 0.5335Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.5351Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.5340Epoch 4/10: [==============                ] 28/60 batches, loss: 0.5338Epoch 4/10: [==============                ] 29/60 batches, loss: 0.5357Epoch 4/10: [===============               ] 30/60 batches, loss: 0.5375Epoch 4/10: [===============               ] 31/60 batches, loss: 0.5387Epoch 4/10: [================              ] 32/60 batches, loss: 0.5391Epoch 4/10: [================              ] 33/60 batches, loss: 0.5395Epoch 4/10: [=================             ] 34/60 batches, loss: 0.5384Epoch 4/10: [=================             ] 35/60 batches, loss: 0.5368Epoch 4/10: [==================            ] 36/60 batches, loss: 0.5352Epoch 4/10: [==================            ] 37/60 batches, loss: 0.5370Epoch 4/10: [===================           ] 38/60 batches, loss: 0.5367Epoch 4/10: [===================           ] 39/60 batches, loss: 0.5353Epoch 4/10: [====================          ] 40/60 batches, loss: 0.5363Epoch 4/10: [====================          ] 41/60 batches, loss: 0.5367Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.5364Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.5357Epoch 4/10: [======================        ] 44/60 batches, loss: 0.5333Epoch 4/10: [======================        ] 45/60 batches, loss: 0.5316Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.5321Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.5299Epoch 4/10: [========================      ] 48/60 batches, loss: 0.5309Epoch 4/10: [========================      ] 49/60 batches, loss: 0.5294Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.5284Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.5293Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.5284Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.5270Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.5266Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.5262Epoch 4/10: [============================  ] 56/60 batches, loss: 0.5266Epoch 4/10: [============================  ] 57/60 batches, loss: 0.5266Epoch 4/10: [============================= ] 58/60 batches, loss: 0.5263Epoch 4/10: [============================= ] 59/60 batches, loss: 0.5251Epoch 4/10: [==============================] 60/60 batches, loss: 0.5234
[2025-05-01 11:49:27,589][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5234
[2025-05-01 11:49:27,864][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5991, Metrics: {'accuracy': 0.7638888888888888, 'f1': 0.6909090909090909, 'precision': 1.0, 'recall': 0.5277777777777778}
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.4564Epoch 5/10: [=                             ] 2/60 batches, loss: 0.4452Epoch 5/10: [=                             ] 3/60 batches, loss: 0.4647Epoch 5/10: [==                            ] 4/60 batches, loss: 0.4808Epoch 5/10: [==                            ] 5/60 batches, loss: 0.5046Epoch 5/10: [===                           ] 6/60 batches, loss: 0.5126Epoch 5/10: [===                           ] 7/60 batches, loss: 0.5214Epoch 5/10: [====                          ] 8/60 batches, loss: 0.5193Epoch 5/10: [====                          ] 9/60 batches, loss: 0.5255Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.5209Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.5236Epoch 5/10: [======                        ] 12/60 batches, loss: 0.5279Epoch 5/10: [======                        ] 13/60 batches, loss: 0.5297Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.5295Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.5278Epoch 5/10: [========                      ] 16/60 batches, loss: 0.5263Epoch 5/10: [========                      ] 17/60 batches, loss: 0.5250Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.5212Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.5178Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.5169Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.5185Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.5229Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.5262Epoch 5/10: [============                  ] 24/60 batches, loss: 0.5262Epoch 5/10: [============                  ] 25/60 batches, loss: 0.5234Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.5254Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.5264Epoch 5/10: [==============                ] 28/60 batches, loss: 0.5230Epoch 5/10: [==============                ] 29/60 batches, loss: 0.5240Epoch 5/10: [===============               ] 30/60 batches, loss: 0.5273Epoch 5/10: [===============               ] 31/60 batches, loss: 0.5250Epoch 5/10: [================              ] 32/60 batches, loss: 0.5213Epoch 5/10: [================              ] 33/60 batches, loss: 0.5208Epoch 5/10: [=================             ] 34/60 batches, loss: 0.5203Epoch 5/10: [=================             ] 35/60 batches, loss: 0.5212Epoch 5/10: [==================            ] 36/60 batches, loss: 0.5213Epoch 5/10: [==================            ] 37/60 batches, loss: 0.5247Epoch 5/10: [===================           ] 38/60 batches, loss: 0.5241Epoch 5/10: [===================           ] 39/60 batches, loss: 0.5230Epoch 5/10: [====================          ] 40/60 batches, loss: 0.5219Epoch 5/10: [====================          ] 41/60 batches, loss: 0.5236Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.5254Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.5243Epoch 5/10: [======================        ] 44/60 batches, loss: 0.5271Epoch 5/10: [======================        ] 45/60 batches, loss: 0.5260Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.5271Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.5266Epoch 5/10: [========================      ] 48/60 batches, loss: 0.5266Epoch 5/10: [========================      ] 49/60 batches, loss: 0.5266Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.5271Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.5252Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.5248Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.5235Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.5240Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.5237Epoch 5/10: [============================  ] 56/60 batches, loss: 0.5237Epoch 5/10: [============================  ] 57/60 batches, loss: 0.5234Epoch 5/10: [============================= ] 58/60 batches, loss: 0.5230Epoch 5/10: [============================= ] 59/60 batches, loss: 0.5227Epoch 5/10: [==============================] 60/60 batches, loss: 0.5237
[2025-05-01 11:49:34,446][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5237
[2025-05-01 11:49:34,720][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5851, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.5516Epoch 6/10: [=                             ] 2/60 batches, loss: 0.4919Epoch 6/10: [=                             ] 3/60 batches, loss: 0.4957Epoch 6/10: [==                            ] 4/60 batches, loss: 0.4977Epoch 6/10: [==                            ] 5/60 batches, loss: 0.5083Epoch 6/10: [===                           ] 6/60 batches, loss: 0.5076Epoch 6/10: [===                           ] 7/60 batches, loss: 0.5036Epoch 6/10: [====                          ] 8/60 batches, loss: 0.5037Epoch 6/10: [====                          ] 9/60 batches, loss: 0.5090Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.5084Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.5101Epoch 6/10: [======                        ] 12/60 batches, loss: 0.5187Epoch 6/10: [======                        ] 13/60 batches, loss: 0.5157Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.5165Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.5220Epoch 6/10: [========                      ] 16/60 batches, loss: 0.5224Epoch 6/10: [========                      ] 17/60 batches, loss: 0.5184Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.5176Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.5194Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.5209Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.5212Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.5215Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.5207Epoch 6/10: [============                  ] 24/60 batches, loss: 0.5230Epoch 6/10: [============                  ] 25/60 batches, loss: 0.5212Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.5233Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.5226Epoch 6/10: [==============                ] 28/60 batches, loss: 0.5219Epoch 6/10: [==============                ] 29/60 batches, loss: 0.5180Epoch 6/10: [===============               ] 30/60 batches, loss: 0.5159Epoch 6/10: [===============               ] 31/60 batches, loss: 0.5140Epoch 6/10: [================              ] 32/60 batches, loss: 0.5152Epoch 6/10: [================              ] 33/60 batches, loss: 0.5134Epoch 6/10: [=================             ] 34/60 batches, loss: 0.5138Epoch 6/10: [=================             ] 35/60 batches, loss: 0.5121Epoch 6/10: [==================            ] 36/60 batches, loss: 0.5125Epoch 6/10: [==================            ] 37/60 batches, loss: 0.5142Epoch 6/10: [===================           ] 38/60 batches, loss: 0.5146Epoch 6/10: [===================           ] 39/60 batches, loss: 0.5143Epoch 6/10: [====================          ] 40/60 batches, loss: 0.5158Epoch 6/10: [====================          ] 41/60 batches, loss: 0.5178Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.5169Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.5171Epoch 6/10: [======================        ] 44/60 batches, loss: 0.5174Epoch 6/10: [======================        ] 45/60 batches, loss: 0.5190Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.5192Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.5209Epoch 6/10: [========================      ] 48/60 batches, loss: 0.5195Epoch 6/10: [========================      ] 49/60 batches, loss: 0.5206Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.5231Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.5246Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.5247Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.5229Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.5221Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.5226Epoch 6/10: [============================  ] 56/60 batches, loss: 0.5214Epoch 6/10: [============================  ] 57/60 batches, loss: 0.5211Epoch 6/10: [============================= ] 58/60 batches, loss: 0.5216Epoch 6/10: [============================= ] 59/60 batches, loss: 0.5225Epoch 6/10: [==============================] 60/60 batches, loss: 0.5235
[2025-05-01 11:49:41,218][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5235
[2025-05-01 11:49:41,487][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5836, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.5750Epoch 7/10: [=                             ] 2/60 batches, loss: 0.5748Epoch 7/10: [=                             ] 3/60 batches, loss: 0.5905Epoch 7/10: [==                            ] 4/60 batches, loss: 0.5829Epoch 7/10: [==                            ] 5/60 batches, loss: 0.5765Epoch 7/10: [===                           ] 6/60 batches, loss: 0.5762Epoch 7/10: [===                           ] 7/60 batches, loss: 0.5794Epoch 7/10: [====                          ] 8/60 batches, loss: 0.5877Epoch 7/10: [====                          ] 9/60 batches, loss: 0.5948Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.5905Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.5912Epoch 7/10: [======                        ] 12/60 batches, loss: 0.5800Epoch 7/10: [======                        ] 13/60 batches, loss: 0.5723Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.5691Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.5695Epoch 7/10: [========                      ] 16/60 batches, loss: 0.5662Epoch 7/10: [========                      ] 17/60 batches, loss: 0.5667Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.5652Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.5595Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.5594Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.5534Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.5511Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.5504Epoch 7/10: [============                  ] 24/60 batches, loss: 0.5455Epoch 7/10: [============                  ] 25/60 batches, loss: 0.5467Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.5441Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.5417Epoch 7/10: [==============                ] 28/60 batches, loss: 0.5412Epoch 7/10: [==============                ] 29/60 batches, loss: 0.5408Epoch 7/10: [===============               ] 30/60 batches, loss: 0.5405Epoch 7/10: [===============               ] 31/60 batches, loss: 0.5417Epoch 7/10: [================              ] 32/60 batches, loss: 0.5397Epoch 7/10: [================              ] 33/60 batches, loss: 0.5387Epoch 7/10: [=================             ] 34/60 batches, loss: 0.5411Epoch 7/10: [=================             ] 35/60 batches, loss: 0.5407Epoch 7/10: [==================            ] 36/60 batches, loss: 0.5430Epoch 7/10: [==================            ] 37/60 batches, loss: 0.5426Epoch 7/10: [===================           ] 38/60 batches, loss: 0.5422Epoch 7/10: [===================           ] 39/60 batches, loss: 0.5424Epoch 7/10: [====================          ] 40/60 batches, loss: 0.5426Epoch 7/10: [====================          ] 41/60 batches, loss: 0.5428Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.5430Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.5422Epoch 7/10: [======================        ] 44/60 batches, loss: 0.5438Epoch 7/10: [======================        ] 45/60 batches, loss: 0.5419Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.5425Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.5432Epoch 7/10: [========================      ] 48/60 batches, loss: 0.5429Epoch 7/10: [========================      ] 49/60 batches, loss: 0.5411Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.5399Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.5392Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.5385Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.5369Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.5372Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.5345Epoch 7/10: [============================  ] 56/60 batches, loss: 0.5348Epoch 7/10: [============================  ] 57/60 batches, loss: 0.5346Epoch 7/10: [============================= ] 58/60 batches, loss: 0.5322Epoch 7/10: [============================= ] 59/60 batches, loss: 0.5317Epoch 7/10: [==============================] 60/60 batches, loss: 0.5313
[2025-05-01 11:49:47,987][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5313
[2025-05-01 11:49:48,239][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5935, Metrics: {'accuracy': 0.7777777777777778, 'f1': 0.7142857142857143, 'precision': 1.0, 'recall': 0.5555555555555556}
[2025-05-01 11:49:48,241][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/60 batches, loss: 0.5508Epoch 8/10: [=                             ] 2/60 batches, loss: 0.5272Epoch 8/10: [=                             ] 3/60 batches, loss: 0.5112Epoch 8/10: [==                            ] 4/60 batches, loss: 0.5389Epoch 8/10: [==                            ] 5/60 batches, loss: 0.5365Epoch 8/10: [===                           ] 6/60 batches, loss: 0.5072Epoch 8/10: [===                           ] 7/60 batches, loss: 0.5237Epoch 8/10: [====                          ] 8/60 batches, loss: 0.5271Epoch 8/10: [====                          ] 9/60 batches, loss: 0.5271Epoch 8/10: [=====                         ] 10/60 batches, loss: 0.5271Epoch 8/10: [=====                         ] 11/60 batches, loss: 0.5315Epoch 8/10: [======                        ] 12/60 batches, loss: 0.5331Epoch 8/10: [======                        ] 13/60 batches, loss: 0.5345Epoch 8/10: [=======                       ] 14/60 batches, loss: 0.5272Epoch 8/10: [=======                       ] 15/60 batches, loss: 0.5209Epoch 8/10: [========                      ] 16/60 batches, loss: 0.5228Epoch 8/10: [========                      ] 17/60 batches, loss: 0.5230Epoch 8/10: [=========                     ] 18/60 batches, loss: 0.5246Epoch 8/10: [=========                     ] 19/60 batches, loss: 0.5247Epoch 8/10: [==========                    ] 20/60 batches, loss: 0.5249Epoch 8/10: [==========                    ] 21/60 batches, loss: 0.5257Epoch 8/10: [===========                   ] 22/60 batches, loss: 0.5204Epoch 8/10: [===========                   ] 23/60 batches, loss: 0.5207Epoch 8/10: [============                  ] 24/60 batches, loss: 0.5220Epoch 8/10: [============                  ] 25/60 batches, loss: 0.5193Epoch 8/10: [=============                 ] 26/60 batches, loss: 0.5196Epoch 8/10: [=============                 ] 27/60 batches, loss: 0.5190Epoch 8/10: [==============                ] 28/60 batches, loss: 0.5193Epoch 8/10: [==============                ] 29/60 batches, loss: 0.5180Epoch 8/10: [===============               ] 30/60 batches, loss: 0.5183Epoch 8/10: [===============               ] 31/60 batches, loss: 0.5201Epoch 8/10: [================              ] 32/60 batches, loss: 0.5196Epoch 8/10: [================              ] 33/60 batches, loss: 0.5184Epoch 8/10: [=================             ] 34/60 batches, loss: 0.5207Epoch 8/10: [=================             ] 35/60 batches, loss: 0.5209Epoch 8/10: [==================            ] 36/60 batches, loss: 0.5198Epoch 8/10: [==================            ] 37/60 batches, loss: 0.5187Epoch 8/10: [===================           ] 38/60 batches, loss: 0.5189Epoch 8/10: [===================           ] 39/60 batches, loss: 0.5173Epoch 8/10: [====================          ] 40/60 batches, loss: 0.5169Epoch 8/10: [====================          ] 41/60 batches, loss: 0.5155Epoch 8/10: [=====================         ] 42/60 batches, loss: 0.5135Epoch 8/10: [=====================         ] 43/60 batches, loss: 0.5147Epoch 8/10: [======================        ] 44/60 batches, loss: 0.5139Epoch 8/10: [======================        ] 45/60 batches, loss: 0.5168Epoch 8/10: [=======================       ] 46/60 batches, loss: 0.5181Epoch 8/10: [=======================       ] 47/60 batches, loss: 0.5183Epoch 8/10: [========================      ] 48/60 batches, loss: 0.5180Epoch 8/10: [========================      ] 49/60 batches, loss: 0.5187Epoch 8/10: [=========================     ] 50/60 batches, loss: 0.5188Epoch 8/10: [=========================     ] 51/60 batches, loss: 0.5190Epoch 8/10: [==========================    ] 52/60 batches, loss: 0.5191Epoch 8/10: [==========================    ] 53/60 batches, loss: 0.5197Epoch 8/10: [===========================   ] 54/60 batches, loss: 0.5203Epoch 8/10: [===========================   ] 55/60 batches, loss: 0.5196Epoch 8/10: [============================  ] 56/60 batches, loss: 0.5210Epoch 8/10: [============================  ] 57/60 batches, loss: 0.5228Epoch 8/10: [============================= ] 58/60 batches, loss: 0.5228Epoch 8/10: [============================= ] 59/60 batches, loss: 0.5229Epoch 8/10: [==============================] 60/60 batches, loss: 0.5226
[2025-05-01 11:49:54,342][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5226
[2025-05-01 11:49:54,591][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5860, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:49:54,591][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/60 batches, loss: 0.5271Epoch 9/10: [=                             ] 2/60 batches, loss: 0.5034Epoch 9/10: [=                             ] 3/60 batches, loss: 0.5113Epoch 9/10: [==                            ] 4/60 batches, loss: 0.5152Epoch 9/10: [==                            ] 5/60 batches, loss: 0.5271Epoch 9/10: [===                           ] 6/60 batches, loss: 0.5271Epoch 9/10: [===                           ] 7/60 batches, loss: 0.5271Epoch 9/10: [====                          ] 8/60 batches, loss: 0.5241Epoch 9/10: [====                          ] 9/60 batches, loss: 0.5192Epoch 9/10: [=====                         ] 10/60 batches, loss: 0.5200Epoch 9/10: [=====                         ] 11/60 batches, loss: 0.5228Epoch 9/10: [======                        ] 12/60 batches, loss: 0.5251Epoch 9/10: [======                        ] 13/60 batches, loss: 0.5326Epoch 9/10: [=======                       ] 14/60 batches, loss: 0.5356Epoch 9/10: [=======                       ] 15/60 batches, loss: 0.5303Epoch 9/10: [========                      ] 16/60 batches, loss: 0.5325Epoch 9/10: [========                      ] 17/60 batches, loss: 0.5308Epoch 9/10: [=========                     ] 18/60 batches, loss: 0.5319Epoch 9/10: [=========                     ] 19/60 batches, loss: 0.5329Epoch 9/10: [==========                    ] 20/60 batches, loss: 0.5326Epoch 9/10: [==========                    ] 21/60 batches, loss: 0.5301Epoch 9/10: [===========                   ] 22/60 batches, loss: 0.5289Epoch 9/10: [===========                   ] 23/60 batches, loss: 0.5298Epoch 9/10: [============                  ] 24/60 batches, loss: 0.5278Epoch 9/10: [============                  ] 25/60 batches, loss: 0.5287Epoch 9/10: [=============                 ] 26/60 batches, loss: 0.5305Epoch 9/10: [=============                 ] 27/60 batches, loss: 0.5295Epoch 9/10: [==============                ] 28/60 batches, loss: 0.5277Epoch 9/10: [==============                ] 29/60 batches, loss: 0.5252Epoch 9/10: [===============               ] 30/60 batches, loss: 0.5221Epoch 9/10: [===============               ] 31/60 batches, loss: 0.5253Epoch 9/10: [================              ] 32/60 batches, loss: 0.5254Epoch 9/10: [================              ] 33/60 batches, loss: 0.5276Epoch 9/10: [=================             ] 34/60 batches, loss: 0.5269Epoch 9/10: [=================             ] 35/60 batches, loss: 0.5262Epoch 9/10: [==================            ] 36/60 batches, loss: 0.5262Epoch 9/10: [==================            ] 37/60 batches, loss: 0.5256Epoch 9/10: [===================           ] 38/60 batches, loss: 0.5250Epoch 9/10: [===================           ] 39/60 batches, loss: 0.5232Epoch 9/10: [====================          ] 40/60 batches, loss: 0.5216Epoch 9/10: [====================          ] 41/60 batches, loss: 0.5234Epoch 9/10: [=====================         ] 42/60 batches, loss: 0.5241Epoch 9/10: [=====================         ] 43/60 batches, loss: 0.5253Epoch 9/10: [======================        ] 44/60 batches, loss: 0.5243Epoch 9/10: [======================        ] 45/60 batches, loss: 0.5254Epoch 9/10: [=======================       ] 46/60 batches, loss: 0.5249Epoch 9/10: [=======================       ] 47/60 batches, loss: 0.5239Epoch 9/10: [========================      ] 48/60 batches, loss: 0.5235Epoch 9/10: [========================      ] 49/60 batches, loss: 0.5231Epoch 9/10: [=========================     ] 50/60 batches, loss: 0.5236Epoch 9/10: [=========================     ] 51/60 batches, loss: 0.5254Epoch 9/10: [==========================    ] 52/60 batches, loss: 0.5250Epoch 9/10: [==========================    ] 53/60 batches, loss: 0.5241Epoch 9/10: [===========================   ] 54/60 batches, loss: 0.5229Epoch 9/10: [===========================   ] 55/60 batches, loss: 0.5229Epoch 9/10: [============================  ] 56/60 batches, loss: 0.5226Epoch 9/10: [============================  ] 57/60 batches, loss: 0.5232Epoch 9/10: [============================= ] 58/60 batches, loss: 0.5228Epoch 9/10: [============================= ] 59/60 batches, loss: 0.5225Epoch 9/10: [==============================] 60/60 batches, loss: 0.5228
[2025-05-01 11:50:00,675][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5228
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5873, Metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:50:00,927][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-01 11:50:00,928][src.training.lm_trainer][INFO] - Training completed in 60.31 seconds
[2025-05-01 11:50:00,928][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9727463312368972, 'f1': 0.970917225950783, 'precision': 0.9931350114416476, 'recall': 0.949671772428884}
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.7916666666666666, 'f1': 0.7368421052631579, 'precision': 1.0, 'recall': 0.5833333333333334}
[2025-05-01 11:50:03,404][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6363636363636364, 'f1': 0.4594594594594595, 'precision': 0.8947368421052632, 'recall': 0.3090909090909091}
[2025-05-01 11:50:05,080][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/id/id/model.pt
[2025-05-01 11:50:05,085][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▇▇▇██
wandb:           best_val_f1 ▁█████
wandb:         best_val_loss █▂▂▂▁▁
wandb:    best_val_precision ▁█████
wandb:       best_val_recall ▁▇▇▇██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃▃▃▃▃
wandb:            train_loss █▄▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▇▇▇█████
wandb:                val_f1 ▁████████
wandb:              val_loss █▂▂▂▁▁▂▁▁
wandb:         val_precision ▁████████
wandb:            val_recall ▁▇▇▇█████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.79167
wandb:           best_val_f1 0.73684
wandb:         best_val_loss 0.58361
wandb:    best_val_precision 1
wandb:       best_val_recall 0.58333
wandb:      early_stop_epoch 9
wandb:                 epoch 9
wandb:   final_test_accuracy 0.63636
wandb:         final_test_f1 0.45946
wandb:  final_test_precision 0.89474
wandb:     final_test_recall 0.30909
wandb:  final_train_accuracy 0.97275
wandb:        final_train_f1 0.97092
wandb: final_train_precision 0.99314
wandb:    final_train_recall 0.94967
wandb:    final_val_accuracy 0.79167
wandb:          final_val_f1 0.73684
wandb:   final_val_precision 1
wandb:      final_val_recall 0.58333
wandb:         learning_rate 2e-05
wandb:            train_loss 0.52279
wandb:            train_time 60.31436
wandb:          val_accuracy 0.79167
wandb:                val_f1 0.73684
wandb:              val_loss 0.5873
wandb:         val_precision 1
wandb:            val_recall 0.58333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114851-cz3tzhts
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_114851-cz3tzhts/logs
Experiment finetune_question_type_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/id/results.json
Running experiment: finetune_complexity_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_id"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/id"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:50:18,594][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/id
experiment_name: finetune_complexity_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:50:18,594][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:50:18,594][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:50:18,594][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:50:18,598][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-01 11:50:18,598][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:50:20,498][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:50:22,999][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:50:23,000][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,086][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,225][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-01 11:50:23,237][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,239][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-01 11:50:23,240][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,262][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,295][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,310][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-01 11:50:23,311][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,312][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-01 11:50:23,316][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:50:23,344][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,376][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:50:23,388][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-01 11:50:23,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:50:23,390][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,391][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,392][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,392][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,392][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:50:23,393][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:50:23,393][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:50:23,394][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:50:23,394][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:50:23,394][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:50:23,394][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:50:27,785][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:50:27,786][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:50:27,791][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:50:27,791][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:50:27,792][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:50:27,792][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-01 11:50:27,792][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:50:27,793][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/60 batches, loss: 0.2579Epoch 1/10: [=                             ] 2/60 batches, loss: 0.2191Epoch 1/10: [=                             ] 3/60 batches, loss: 0.2082Epoch 1/10: [==                            ] 4/60 batches, loss: 0.2013Epoch 1/10: [==                            ] 5/60 batches, loss: 0.2025Epoch 1/10: [===                           ] 6/60 batches, loss: 0.2152Epoch 1/10: [===                           ] 7/60 batches, loss: 0.2010Epoch 1/10: [====                          ] 8/60 batches, loss: 0.1926Epoch 1/10: [====                          ] 9/60 batches, loss: 0.1969Epoch 1/10: [=====                         ] 10/60 batches, loss: 0.1970Epoch 1/10: [=====                         ] 11/60 batches, loss: 0.1979Epoch 1/10: [======                        ] 12/60 batches, loss: 0.1986Epoch 1/10: [======                        ] 13/60 batches, loss: 0.1957Epoch 1/10: [=======                       ] 14/60 batches, loss: 0.1949Epoch 1/10: [=======                       ] 15/60 batches, loss: 0.1985Epoch 1/10: [========                      ] 16/60 batches, loss: 0.1944Epoch 1/10: [========                      ] 17/60 batches, loss: 0.1921Epoch 1/10: [=========                     ] 18/60 batches, loss: 0.1862Epoch 1/10: [=========                     ] 19/60 batches, loss: 0.1837Epoch 1/10: [==========                    ] 20/60 batches, loss: 0.1801Epoch 1/10: [==========                    ] 21/60 batches, loss: 0.1760Epoch 1/10: [===========                   ] 22/60 batches, loss: 0.1727Epoch 1/10: [===========                   ] 23/60 batches, loss: 0.1679Epoch 1/10: [============                  ] 24/60 batches, loss: 0.1654Epoch 1/10: [============                  ] 25/60 batches, loss: 0.1630Epoch 1/10: [=============                 ] 26/60 batches, loss: 0.1595Epoch 1/10: [=============                 ] 27/60 batches, loss: 0.1572Epoch 1/10: [==============                ] 28/60 batches, loss: 0.1574Epoch 1/10: [==============                ] 29/60 batches, loss: 0.1550Epoch 1/10: [===============               ] 30/60 batches, loss: 0.1542Epoch 1/10: [===============               ] 31/60 batches, loss: 0.1532Epoch 1/10: [================              ] 32/60 batches, loss: 0.1547Epoch 1/10: [================              ] 33/60 batches, loss: 0.1546Epoch 1/10: [=================             ] 34/60 batches, loss: 0.1536Epoch 1/10: [=================             ] 35/60 batches, loss: 0.1534Epoch 1/10: [==================            ] 36/60 batches, loss: 0.1523Epoch 1/10: [==================            ] 37/60 batches, loss: 0.1512Epoch 1/10: [===================           ] 38/60 batches, loss: 0.1510Epoch 1/10: [===================           ] 39/60 batches, loss: 0.1515Epoch 1/10: [====================          ] 40/60 batches, loss: 0.1514Epoch 1/10: [====================          ] 41/60 batches, loss: 0.1508Epoch 1/10: [=====================         ] 42/60 batches, loss: 0.1500Epoch 1/10: [=====================         ] 43/60 batches, loss: 0.1493Epoch 1/10: [======================        ] 44/60 batches, loss: 0.1492Epoch 1/10: [======================        ] 45/60 batches, loss: 0.1478Epoch 1/10: [=======================       ] 46/60 batches, loss: 0.1481Epoch 1/10: [=======================       ] 47/60 batches, loss: 0.1472Epoch 1/10: [========================      ] 48/60 batches, loss: 0.1462Epoch 1/10: [========================      ] 49/60 batches, loss: 0.1451Epoch 1/10: [=========================     ] 50/60 batches, loss: 0.1441Epoch 1/10: [=========================     ] 51/60 batches, loss: 0.1440Epoch 1/10: [==========================    ] 52/60 batches, loss: 0.1434Epoch 1/10: [==========================    ] 53/60 batches, loss: 0.1420Epoch 1/10: [===========================   ] 54/60 batches, loss: 0.1412Epoch 1/10: [===========================   ] 55/60 batches, loss: 0.1392Epoch 1/10: [============================  ] 56/60 batches, loss: 0.1374Epoch 1/10: [============================  ] 57/60 batches, loss: 0.1364Epoch 1/10: [============================= ] 58/60 batches, loss: 0.1346Epoch 1/10: [============================= ] 59/60 batches, loss: 0.1336Epoch 1/10: [==============================] 60/60 batches, loss: 0.1319
[2025-05-01 11:50:36,680][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1319
[2025-05-01 11:50:36,923][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0766, Metrics: {'mse': 0.07300561666488647, 'rmse': 0.2701955156269002, 'r2': -0.7461632490158081}
Epoch 2/10: [Epoch 2/10: [                              ] 1/60 batches, loss: 0.0302Epoch 2/10: [=                             ] 2/60 batches, loss: 0.0383Epoch 2/10: [=                             ] 3/60 batches, loss: 0.0332Epoch 2/10: [==                            ] 4/60 batches, loss: 0.0395Epoch 2/10: [==                            ] 5/60 batches, loss: 0.0391Epoch 2/10: [===                           ] 6/60 batches, loss: 0.0452Epoch 2/10: [===                           ] 7/60 batches, loss: 0.0439Epoch 2/10: [====                          ] 8/60 batches, loss: 0.0445Epoch 2/10: [====                          ] 9/60 batches, loss: 0.0426Epoch 2/10: [=====                         ] 10/60 batches, loss: 0.0426Epoch 2/10: [=====                         ] 11/60 batches, loss: 0.0425Epoch 2/10: [======                        ] 12/60 batches, loss: 0.0423Epoch 2/10: [======                        ] 13/60 batches, loss: 0.0423Epoch 2/10: [=======                       ] 14/60 batches, loss: 0.0440Epoch 2/10: [=======                       ] 15/60 batches, loss: 0.0441Epoch 2/10: [========                      ] 16/60 batches, loss: 0.0452Epoch 2/10: [========                      ] 17/60 batches, loss: 0.0442Epoch 2/10: [=========                     ] 18/60 batches, loss: 0.0437Epoch 2/10: [=========                     ] 19/60 batches, loss: 0.0435Epoch 2/10: [==========                    ] 20/60 batches, loss: 0.0422Epoch 2/10: [==========                    ] 21/60 batches, loss: 0.0415Epoch 2/10: [===========                   ] 22/60 batches, loss: 0.0405Epoch 2/10: [===========                   ] 23/60 batches, loss: 0.0402Epoch 2/10: [============                  ] 24/60 batches, loss: 0.0392Epoch 2/10: [============                  ] 25/60 batches, loss: 0.0387Epoch 2/10: [=============                 ] 26/60 batches, loss: 0.0387Epoch 2/10: [=============                 ] 27/60 batches, loss: 0.0384Epoch 2/10: [==============                ] 28/60 batches, loss: 0.0378Epoch 2/10: [==============                ] 29/60 batches, loss: 0.0386Epoch 2/10: [===============               ] 30/60 batches, loss: 0.0387Epoch 2/10: [===============               ] 31/60 batches, loss: 0.0389Epoch 2/10: [================              ] 32/60 batches, loss: 0.0383Epoch 2/10: [================              ] 33/60 batches, loss: 0.0381Epoch 2/10: [=================             ] 34/60 batches, loss: 0.0375Epoch 2/10: [=================             ] 35/60 batches, loss: 0.0375Epoch 2/10: [==================            ] 36/60 batches, loss: 0.0374Epoch 2/10: [==================            ] 37/60 batches, loss: 0.0369Epoch 2/10: [===================           ] 38/60 batches, loss: 0.0371Epoch 2/10: [===================           ] 39/60 batches, loss: 0.0367Epoch 2/10: [====================          ] 40/60 batches, loss: 0.0366Epoch 2/10: [====================          ] 41/60 batches, loss: 0.0362Epoch 2/10: [=====================         ] 42/60 batches, loss: 0.0358Epoch 2/10: [=====================         ] 43/60 batches, loss: 0.0353Epoch 2/10: [======================        ] 44/60 batches, loss: 0.0352Epoch 2/10: [======================        ] 45/60 batches, loss: 0.0348Epoch 2/10: [=======================       ] 46/60 batches, loss: 0.0350Epoch 2/10: [=======================       ] 47/60 batches, loss: 0.0348Epoch 2/10: [========================      ] 48/60 batches, loss: 0.0344Epoch 2/10: [========================      ] 49/60 batches, loss: 0.0339Epoch 2/10: [=========================     ] 50/60 batches, loss: 0.0338Epoch 2/10: [=========================     ] 51/60 batches, loss: 0.0341Epoch 2/10: [==========================    ] 52/60 batches, loss: 0.0339Epoch 2/10: [==========================    ] 53/60 batches, loss: 0.0335Epoch 2/10: [===========================   ] 54/60 batches, loss: 0.0333Epoch 2/10: [===========================   ] 55/60 batches, loss: 0.0334Epoch 2/10: [============================  ] 56/60 batches, loss: 0.0333Epoch 2/10: [============================  ] 57/60 batches, loss: 0.0332Epoch 2/10: [============================= ] 58/60 batches, loss: 0.0333Epoch 2/10: [============================= ] 59/60 batches, loss: 0.0333Epoch 2/10: [==============================] 60/60 batches, loss: 0.0330
[2025-05-01 11:50:43,406][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0330
[2025-05-01 11:50:43,652][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0379, Metrics: {'mse': 0.03619665652513504, 'rmse': 0.19025418924463935, 'r2': 0.134240984916687}
Epoch 3/10: [Epoch 3/10: [                              ] 1/60 batches, loss: 0.0261Epoch 3/10: [=                             ] 2/60 batches, loss: 0.0197Epoch 3/10: [=                             ] 3/60 batches, loss: 0.0214Epoch 3/10: [==                            ] 4/60 batches, loss: 0.0214Epoch 3/10: [==                            ] 5/60 batches, loss: 0.0204Epoch 3/10: [===                           ] 6/60 batches, loss: 0.0202Epoch 3/10: [===                           ] 7/60 batches, loss: 0.0210Epoch 3/10: [====                          ] 8/60 batches, loss: 0.0216Epoch 3/10: [====                          ] 9/60 batches, loss: 0.0231Epoch 3/10: [=====                         ] 10/60 batches, loss: 0.0229Epoch 3/10: [=====                         ] 11/60 batches, loss: 0.0248Epoch 3/10: [======                        ] 12/60 batches, loss: 0.0256Epoch 3/10: [======                        ] 13/60 batches, loss: 0.0243Epoch 3/10: [=======                       ] 14/60 batches, loss: 0.0243Epoch 3/10: [=======                       ] 15/60 batches, loss: 0.0237Epoch 3/10: [========                      ] 16/60 batches, loss: 0.0244Epoch 3/10: [========                      ] 17/60 batches, loss: 0.0238Epoch 3/10: [=========                     ] 18/60 batches, loss: 0.0235Epoch 3/10: [=========                     ] 19/60 batches, loss: 0.0229Epoch 3/10: [==========                    ] 20/60 batches, loss: 0.0225Epoch 3/10: [==========                    ] 21/60 batches, loss: 0.0226Epoch 3/10: [===========                   ] 22/60 batches, loss: 0.0220Epoch 3/10: [===========                   ] 23/60 batches, loss: 0.0218Epoch 3/10: [============                  ] 24/60 batches, loss: 0.0214Epoch 3/10: [============                  ] 25/60 batches, loss: 0.0213Epoch 3/10: [=============                 ] 26/60 batches, loss: 0.0217Epoch 3/10: [=============                 ] 27/60 batches, loss: 0.0213Epoch 3/10: [==============                ] 28/60 batches, loss: 0.0220Epoch 3/10: [==============                ] 29/60 batches, loss: 0.0218Epoch 3/10: [===============               ] 30/60 batches, loss: 0.0216Epoch 3/10: [===============               ] 31/60 batches, loss: 0.0212Epoch 3/10: [================              ] 32/60 batches, loss: 0.0209Epoch 3/10: [================              ] 33/60 batches, loss: 0.0207Epoch 3/10: [=================             ] 34/60 batches, loss: 0.0205Epoch 3/10: [=================             ] 35/60 batches, loss: 0.0207Epoch 3/10: [==================            ] 36/60 batches, loss: 0.0206Epoch 3/10: [==================            ] 37/60 batches, loss: 0.0204Epoch 3/10: [===================           ] 38/60 batches, loss: 0.0202Epoch 3/10: [===================           ] 39/60 batches, loss: 0.0199Epoch 3/10: [====================          ] 40/60 batches, loss: 0.0198Epoch 3/10: [====================          ] 41/60 batches, loss: 0.0196Epoch 3/10: [=====================         ] 42/60 batches, loss: 0.0194Epoch 3/10: [=====================         ] 43/60 batches, loss: 0.0193Epoch 3/10: [======================        ] 44/60 batches, loss: 0.0193Epoch 3/10: [======================        ] 45/60 batches, loss: 0.0191Epoch 3/10: [=======================       ] 46/60 batches, loss: 0.0195Epoch 3/10: [=======================       ] 47/60 batches, loss: 0.0195Epoch 3/10: [========================      ] 48/60 batches, loss: 0.0193Epoch 3/10: [========================      ] 49/60 batches, loss: 0.0191Epoch 3/10: [=========================     ] 50/60 batches, loss: 0.0189Epoch 3/10: [=========================     ] 51/60 batches, loss: 0.0190Epoch 3/10: [==========================    ] 52/60 batches, loss: 0.0192Epoch 3/10: [==========================    ] 53/60 batches, loss: 0.0191Epoch 3/10: [===========================   ] 54/60 batches, loss: 0.0191Epoch 3/10: [===========================   ] 55/60 batches, loss: 0.0191Epoch 3/10: [============================  ] 56/60 batches, loss: 0.0190Epoch 3/10: [============================  ] 57/60 batches, loss: 0.0188Epoch 3/10: [============================= ] 58/60 batches, loss: 0.0189Epoch 3/10: [============================= ] 59/60 batches, loss: 0.0190Epoch 3/10: [==============================] 60/60 batches, loss: 0.0188
[2025-05-01 11:50:50,142][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0188
[2025-05-01 11:50:50,388][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0184, Metrics: {'mse': 0.017388159409165382, 'rmse': 0.13186417030097822, 'r2': 0.5841064453125}
Epoch 4/10: [Epoch 4/10: [                              ] 1/60 batches, loss: 0.0095Epoch 4/10: [=                             ] 2/60 batches, loss: 0.0088Epoch 4/10: [=                             ] 3/60 batches, loss: 0.0088Epoch 4/10: [==                            ] 4/60 batches, loss: 0.0109Epoch 4/10: [==                            ] 5/60 batches, loss: 0.0125Epoch 4/10: [===                           ] 6/60 batches, loss: 0.0135Epoch 4/10: [===                           ] 7/60 batches, loss: 0.0165Epoch 4/10: [====                          ] 8/60 batches, loss: 0.0170Epoch 4/10: [====                          ] 9/60 batches, loss: 0.0163Epoch 4/10: [=====                         ] 10/60 batches, loss: 0.0155Epoch 4/10: [=====                         ] 11/60 batches, loss: 0.0151Epoch 4/10: [======                        ] 12/60 batches, loss: 0.0152Epoch 4/10: [======                        ] 13/60 batches, loss: 0.0148Epoch 4/10: [=======                       ] 14/60 batches, loss: 0.0146Epoch 4/10: [=======                       ] 15/60 batches, loss: 0.0144Epoch 4/10: [========                      ] 16/60 batches, loss: 0.0151Epoch 4/10: [========                      ] 17/60 batches, loss: 0.0149Epoch 4/10: [=========                     ] 18/60 batches, loss: 0.0146Epoch 4/10: [=========                     ] 19/60 batches, loss: 0.0143Epoch 4/10: [==========                    ] 20/60 batches, loss: 0.0141Epoch 4/10: [==========                    ] 21/60 batches, loss: 0.0139Epoch 4/10: [===========                   ] 22/60 batches, loss: 0.0137Epoch 4/10: [===========                   ] 23/60 batches, loss: 0.0141Epoch 4/10: [============                  ] 24/60 batches, loss: 0.0143Epoch 4/10: [============                  ] 25/60 batches, loss: 0.0146Epoch 4/10: [=============                 ] 26/60 batches, loss: 0.0143Epoch 4/10: [=============                 ] 27/60 batches, loss: 0.0142Epoch 4/10: [==============                ] 28/60 batches, loss: 0.0142Epoch 4/10: [==============                ] 29/60 batches, loss: 0.0147Epoch 4/10: [===============               ] 30/60 batches, loss: 0.0149Epoch 4/10: [===============               ] 31/60 batches, loss: 0.0150Epoch 4/10: [================              ] 32/60 batches, loss: 0.0148Epoch 4/10: [================              ] 33/60 batches, loss: 0.0146Epoch 4/10: [=================             ] 34/60 batches, loss: 0.0145Epoch 4/10: [=================             ] 35/60 batches, loss: 0.0148Epoch 4/10: [==================            ] 36/60 batches, loss: 0.0146Epoch 4/10: [==================            ] 37/60 batches, loss: 0.0145Epoch 4/10: [===================           ] 38/60 batches, loss: 0.0146Epoch 4/10: [===================           ] 39/60 batches, loss: 0.0148Epoch 4/10: [====================          ] 40/60 batches, loss: 0.0147Epoch 4/10: [====================          ] 41/60 batches, loss: 0.0145Epoch 4/10: [=====================         ] 42/60 batches, loss: 0.0144Epoch 4/10: [=====================         ] 43/60 batches, loss: 0.0146Epoch 4/10: [======================        ] 44/60 batches, loss: 0.0145Epoch 4/10: [======================        ] 45/60 batches, loss: 0.0145Epoch 4/10: [=======================       ] 46/60 batches, loss: 0.0145Epoch 4/10: [=======================       ] 47/60 batches, loss: 0.0143Epoch 4/10: [========================      ] 48/60 batches, loss: 0.0142Epoch 4/10: [========================      ] 49/60 batches, loss: 0.0141Epoch 4/10: [=========================     ] 50/60 batches, loss: 0.0141Epoch 4/10: [=========================     ] 51/60 batches, loss: 0.0139Epoch 4/10: [==========================    ] 52/60 batches, loss: 0.0138Epoch 4/10: [==========================    ] 53/60 batches, loss: 0.0140Epoch 4/10: [===========================   ] 54/60 batches, loss: 0.0144Epoch 4/10: [===========================   ] 55/60 batches, loss: 0.0143Epoch 4/10: [============================  ] 56/60 batches, loss: 0.0144Epoch 4/10: [============================  ] 57/60 batches, loss: 0.0143Epoch 4/10: [============================= ] 58/60 batches, loss: 0.0143Epoch 4/10: [============================= ] 59/60 batches, loss: 0.0143Epoch 4/10: [==============================] 60/60 batches, loss: 0.0143
[2025-05-01 11:50:56,852][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0143
[2025-05-01 11:50:57,111][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0164, Metrics: {'mse': 0.015578867867588997, 'rmse': 0.1248153350658043, 'r2': 0.627381443977356}
Epoch 5/10: [Epoch 5/10: [                              ] 1/60 batches, loss: 0.0099Epoch 5/10: [=                             ] 2/60 batches, loss: 0.0141Epoch 5/10: [=                             ] 3/60 batches, loss: 0.0170Epoch 5/10: [==                            ] 4/60 batches, loss: 0.0160Epoch 5/10: [==                            ] 5/60 batches, loss: 0.0146Epoch 5/10: [===                           ] 6/60 batches, loss: 0.0128Epoch 5/10: [===                           ] 7/60 batches, loss: 0.0120Epoch 5/10: [====                          ] 8/60 batches, loss: 0.0115Epoch 5/10: [====                          ] 9/60 batches, loss: 0.0118Epoch 5/10: [=====                         ] 10/60 batches, loss: 0.0118Epoch 5/10: [=====                         ] 11/60 batches, loss: 0.0118Epoch 5/10: [======                        ] 12/60 batches, loss: 0.0112Epoch 5/10: [======                        ] 13/60 batches, loss: 0.0108Epoch 5/10: [=======                       ] 14/60 batches, loss: 0.0108Epoch 5/10: [=======                       ] 15/60 batches, loss: 0.0105Epoch 5/10: [========                      ] 16/60 batches, loss: 0.0110Epoch 5/10: [========                      ] 17/60 batches, loss: 0.0112Epoch 5/10: [=========                     ] 18/60 batches, loss: 0.0111Epoch 5/10: [=========                     ] 19/60 batches, loss: 0.0113Epoch 5/10: [==========                    ] 20/60 batches, loss: 0.0112Epoch 5/10: [==========                    ] 21/60 batches, loss: 0.0114Epoch 5/10: [===========                   ] 22/60 batches, loss: 0.0116Epoch 5/10: [===========                   ] 23/60 batches, loss: 0.0117Epoch 5/10: [============                  ] 24/60 batches, loss: 0.0118Epoch 5/10: [============                  ] 25/60 batches, loss: 0.0119Epoch 5/10: [=============                 ] 26/60 batches, loss: 0.0118Epoch 5/10: [=============                 ] 27/60 batches, loss: 0.0117Epoch 5/10: [==============                ] 28/60 batches, loss: 0.0115Epoch 5/10: [==============                ] 29/60 batches, loss: 0.0114Epoch 5/10: [===============               ] 30/60 batches, loss: 0.0112Epoch 5/10: [===============               ] 31/60 batches, loss: 0.0113Epoch 5/10: [================              ] 32/60 batches, loss: 0.0114Epoch 5/10: [================              ] 33/60 batches, loss: 0.0115Epoch 5/10: [=================             ] 34/60 batches, loss: 0.0113Epoch 5/10: [=================             ] 35/60 batches, loss: 0.0112Epoch 5/10: [==================            ] 36/60 batches, loss: 0.0111Epoch 5/10: [==================            ] 37/60 batches, loss: 0.0111Epoch 5/10: [===================           ] 38/60 batches, loss: 0.0112Epoch 5/10: [===================           ] 39/60 batches, loss: 0.0114Epoch 5/10: [====================          ] 40/60 batches, loss: 0.0115Epoch 5/10: [====================          ] 41/60 batches, loss: 0.0116Epoch 5/10: [=====================         ] 42/60 batches, loss: 0.0117Epoch 5/10: [=====================         ] 43/60 batches, loss: 0.0120Epoch 5/10: [======================        ] 44/60 batches, loss: 0.0119Epoch 5/10: [======================        ] 45/60 batches, loss: 0.0120Epoch 5/10: [=======================       ] 46/60 batches, loss: 0.0118Epoch 5/10: [=======================       ] 47/60 batches, loss: 0.0118Epoch 5/10: [========================      ] 48/60 batches, loss: 0.0118Epoch 5/10: [========================      ] 49/60 batches, loss: 0.0117Epoch 5/10: [=========================     ] 50/60 batches, loss: 0.0116Epoch 5/10: [=========================     ] 51/60 batches, loss: 0.0117Epoch 5/10: [==========================    ] 52/60 batches, loss: 0.0117Epoch 5/10: [==========================    ] 53/60 batches, loss: 0.0116Epoch 5/10: [===========================   ] 54/60 batches, loss: 0.0116Epoch 5/10: [===========================   ] 55/60 batches, loss: 0.0115Epoch 5/10: [============================  ] 56/60 batches, loss: 0.0116Epoch 5/10: [============================  ] 57/60 batches, loss: 0.0115Epoch 5/10: [============================= ] 58/60 batches, loss: 0.0115Epoch 5/10: [============================= ] 59/60 batches, loss: 0.0114Epoch 5/10: [==============================] 60/60 batches, loss: 0.0115
[2025-05-01 11:51:03,560][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0115
[2025-05-01 11:51:03,819][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0124, Metrics: {'mse': 0.011687989346683025, 'rmse': 0.108111004743657, 'r2': 0.7204443216323853}
Epoch 6/10: [Epoch 6/10: [                              ] 1/60 batches, loss: 0.0095Epoch 6/10: [=                             ] 2/60 batches, loss: 0.0120Epoch 6/10: [=                             ] 3/60 batches, loss: 0.0096Epoch 6/10: [==                            ] 4/60 batches, loss: 0.0094Epoch 6/10: [==                            ] 5/60 batches, loss: 0.0088Epoch 6/10: [===                           ] 6/60 batches, loss: 0.0093Epoch 6/10: [===                           ] 7/60 batches, loss: 0.0090Epoch 6/10: [====                          ] 8/60 batches, loss: 0.0087Epoch 6/10: [====                          ] 9/60 batches, loss: 0.0082Epoch 6/10: [=====                         ] 10/60 batches, loss: 0.0084Epoch 6/10: [=====                         ] 11/60 batches, loss: 0.0082Epoch 6/10: [======                        ] 12/60 batches, loss: 0.0083Epoch 6/10: [======                        ] 13/60 batches, loss: 0.0085Epoch 6/10: [=======                       ] 14/60 batches, loss: 0.0084Epoch 6/10: [=======                       ] 15/60 batches, loss: 0.0085Epoch 6/10: [========                      ] 16/60 batches, loss: 0.0086Epoch 6/10: [========                      ] 17/60 batches, loss: 0.0087Epoch 6/10: [=========                     ] 18/60 batches, loss: 0.0087Epoch 6/10: [=========                     ] 19/60 batches, loss: 0.0089Epoch 6/10: [==========                    ] 20/60 batches, loss: 0.0086Epoch 6/10: [==========                    ] 21/60 batches, loss: 0.0086Epoch 6/10: [===========                   ] 22/60 batches, loss: 0.0087Epoch 6/10: [===========                   ] 23/60 batches, loss: 0.0085Epoch 6/10: [============                  ] 24/60 batches, loss: 0.0086Epoch 6/10: [============                  ] 25/60 batches, loss: 0.0086Epoch 6/10: [=============                 ] 26/60 batches, loss: 0.0085Epoch 6/10: [=============                 ] 27/60 batches, loss: 0.0085Epoch 6/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 6/10: [==============                ] 29/60 batches, loss: 0.0082Epoch 6/10: [===============               ] 30/60 batches, loss: 0.0084Epoch 6/10: [===============               ] 31/60 batches, loss: 0.0084Epoch 6/10: [================              ] 32/60 batches, loss: 0.0083Epoch 6/10: [================              ] 33/60 batches, loss: 0.0083Epoch 6/10: [=================             ] 34/60 batches, loss: 0.0083Epoch 6/10: [=================             ] 35/60 batches, loss: 0.0083Epoch 6/10: [==================            ] 36/60 batches, loss: 0.0083Epoch 6/10: [==================            ] 37/60 batches, loss: 0.0082Epoch 6/10: [===================           ] 38/60 batches, loss: 0.0082Epoch 6/10: [===================           ] 39/60 batches, loss: 0.0082Epoch 6/10: [====================          ] 40/60 batches, loss: 0.0083Epoch 6/10: [====================          ] 41/60 batches, loss: 0.0082Epoch 6/10: [=====================         ] 42/60 batches, loss: 0.0083Epoch 6/10: [=====================         ] 43/60 batches, loss: 0.0085Epoch 6/10: [======================        ] 44/60 batches, loss: 0.0084Epoch 6/10: [======================        ] 45/60 batches, loss: 0.0084Epoch 6/10: [=======================       ] 46/60 batches, loss: 0.0084Epoch 6/10: [=======================       ] 47/60 batches, loss: 0.0083Epoch 6/10: [========================      ] 48/60 batches, loss: 0.0083Epoch 6/10: [========================      ] 49/60 batches, loss: 0.0082Epoch 6/10: [=========================     ] 50/60 batches, loss: 0.0082Epoch 6/10: [=========================     ] 51/60 batches, loss: 0.0082Epoch 6/10: [==========================    ] 52/60 batches, loss: 0.0082Epoch 6/10: [==========================    ] 53/60 batches, loss: 0.0082Epoch 6/10: [===========================   ] 54/60 batches, loss: 0.0082Epoch 6/10: [===========================   ] 55/60 batches, loss: 0.0082Epoch 6/10: [============================  ] 56/60 batches, loss: 0.0082Epoch 6/10: [============================  ] 57/60 batches, loss: 0.0082Epoch 6/10: [============================= ] 58/60 batches, loss: 0.0083Epoch 6/10: [============================= ] 59/60 batches, loss: 0.0083Epoch 6/10: [==============================] 60/60 batches, loss: 0.0083
[2025-05-01 11:51:10,301][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0083
[2025-05-01 11:51:10,560][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0232, Metrics: {'mse': 0.023061146959662437, 'rmse': 0.151858970626244, 'r2': 0.4484187960624695}
[2025-05-01 11:51:10,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/60 batches, loss: 0.0059Epoch 7/10: [=                             ] 2/60 batches, loss: 0.0071Epoch 7/10: [=                             ] 3/60 batches, loss: 0.0069Epoch 7/10: [==                            ] 4/60 batches, loss: 0.0080Epoch 7/10: [==                            ] 5/60 batches, loss: 0.0076Epoch 7/10: [===                           ] 6/60 batches, loss: 0.0074Epoch 7/10: [===                           ] 7/60 batches, loss: 0.0074Epoch 7/10: [====                          ] 8/60 batches, loss: 0.0074Epoch 7/10: [====                          ] 9/60 batches, loss: 0.0071Epoch 7/10: [=====                         ] 10/60 batches, loss: 0.0074Epoch 7/10: [=====                         ] 11/60 batches, loss: 0.0075Epoch 7/10: [======                        ] 12/60 batches, loss: 0.0078Epoch 7/10: [======                        ] 13/60 batches, loss: 0.0078Epoch 7/10: [=======                       ] 14/60 batches, loss: 0.0077Epoch 7/10: [=======                       ] 15/60 batches, loss: 0.0079Epoch 7/10: [========                      ] 16/60 batches, loss: 0.0079Epoch 7/10: [========                      ] 17/60 batches, loss: 0.0079Epoch 7/10: [=========                     ] 18/60 batches, loss: 0.0078Epoch 7/10: [=========                     ] 19/60 batches, loss: 0.0076Epoch 7/10: [==========                    ] 20/60 batches, loss: 0.0075Epoch 7/10: [==========                    ] 21/60 batches, loss: 0.0074Epoch 7/10: [===========                   ] 22/60 batches, loss: 0.0076Epoch 7/10: [===========                   ] 23/60 batches, loss: 0.0075Epoch 7/10: [============                  ] 24/60 batches, loss: 0.0077Epoch 7/10: [============                  ] 25/60 batches, loss: 0.0075Epoch 7/10: [=============                 ] 26/60 batches, loss: 0.0075Epoch 7/10: [=============                 ] 27/60 batches, loss: 0.0080Epoch 7/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 7/10: [==============                ] 29/60 batches, loss: 0.0083Epoch 7/10: [===============               ] 30/60 batches, loss: 0.0081Epoch 7/10: [===============               ] 31/60 batches, loss: 0.0080Epoch 7/10: [================              ] 32/60 batches, loss: 0.0081Epoch 7/10: [================              ] 33/60 batches, loss: 0.0080Epoch 7/10: [=================             ] 34/60 batches, loss: 0.0080Epoch 7/10: [=================             ] 35/60 batches, loss: 0.0079Epoch 7/10: [==================            ] 36/60 batches, loss: 0.0078Epoch 7/10: [==================            ] 37/60 batches, loss: 0.0078Epoch 7/10: [===================           ] 38/60 batches, loss: 0.0077Epoch 7/10: [===================           ] 39/60 batches, loss: 0.0077Epoch 7/10: [====================          ] 40/60 batches, loss: 0.0077Epoch 7/10: [====================          ] 41/60 batches, loss: 0.0076Epoch 7/10: [=====================         ] 42/60 batches, loss: 0.0077Epoch 7/10: [=====================         ] 43/60 batches, loss: 0.0077Epoch 7/10: [======================        ] 44/60 batches, loss: 0.0077Epoch 7/10: [======================        ] 45/60 batches, loss: 0.0077Epoch 7/10: [=======================       ] 46/60 batches, loss: 0.0076Epoch 7/10: [=======================       ] 47/60 batches, loss: 0.0076Epoch 7/10: [========================      ] 48/60 batches, loss: 0.0076Epoch 7/10: [========================      ] 49/60 batches, loss: 0.0075Epoch 7/10: [=========================     ] 50/60 batches, loss: 0.0076Epoch 7/10: [=========================     ] 51/60 batches, loss: 0.0076Epoch 7/10: [==========================    ] 52/60 batches, loss: 0.0075Epoch 7/10: [==========================    ] 53/60 batches, loss: 0.0075Epoch 7/10: [===========================   ] 54/60 batches, loss: 0.0074Epoch 7/10: [===========================   ] 55/60 batches, loss: 0.0074Epoch 7/10: [============================  ] 56/60 batches, loss: 0.0074Epoch 7/10: [============================  ] 57/60 batches, loss: 0.0074Epoch 7/10: [============================= ] 58/60 batches, loss: 0.0073Epoch 7/10: [============================= ] 59/60 batches, loss: 0.0073Epoch 7/10: [==============================] 60/60 batches, loss: 0.0073
[2025-05-01 11:51:16,688][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0073
[2025-05-01 11:51:16,936][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0121, Metrics: {'mse': 0.011142720468342304, 'rmse': 0.10555908520038577, 'r2': 0.7334861159324646}
Epoch 8/10: [Epoch 8/10: [                              ] 1/60 batches, loss: 0.0039Epoch 8/10: [=                             ] 2/60 batches, loss: 0.0071Epoch 8/10: [=                             ] 3/60 batches, loss: 0.0107Epoch 8/10: [==                            ] 4/60 batches, loss: 0.0110Epoch 8/10: [==                            ] 5/60 batches, loss: 0.0106Epoch 8/10: [===                           ] 6/60 batches, loss: 0.0101Epoch 8/10: [===                           ] 7/60 batches, loss: 0.0096Epoch 8/10: [====                          ] 8/60 batches, loss: 0.0093Epoch 8/10: [====                          ] 9/60 batches, loss: 0.0088Epoch 8/10: [=====                         ] 10/60 batches, loss: 0.0090Epoch 8/10: [=====                         ] 11/60 batches, loss: 0.0086Epoch 8/10: [======                        ] 12/60 batches, loss: 0.0085Epoch 8/10: [======                        ] 13/60 batches, loss: 0.0084Epoch 8/10: [=======                       ] 14/60 batches, loss: 0.0080Epoch 8/10: [=======                       ] 15/60 batches, loss: 0.0081Epoch 8/10: [========                      ] 16/60 batches, loss: 0.0079Epoch 8/10: [========                      ] 17/60 batches, loss: 0.0078Epoch 8/10: [=========                     ] 18/60 batches, loss: 0.0075Epoch 8/10: [=========                     ] 19/60 batches, loss: 0.0074Epoch 8/10: [==========                    ] 20/60 batches, loss: 0.0078Epoch 8/10: [==========                    ] 21/60 batches, loss: 0.0081Epoch 8/10: [===========                   ] 22/60 batches, loss: 0.0080Epoch 8/10: [===========                   ] 23/60 batches, loss: 0.0080Epoch 8/10: [============                  ] 24/60 batches, loss: 0.0082Epoch 8/10: [============                  ] 25/60 batches, loss: 0.0082Epoch 8/10: [=============                 ] 26/60 batches, loss: 0.0085Epoch 8/10: [=============                 ] 27/60 batches, loss: 0.0084Epoch 8/10: [==============                ] 28/60 batches, loss: 0.0083Epoch 8/10: [==============                ] 29/60 batches, loss: 0.0081Epoch 8/10: [===============               ] 30/60 batches, loss: 0.0081Epoch 8/10: [===============               ] 31/60 batches, loss: 0.0080Epoch 8/10: [================              ] 32/60 batches, loss: 0.0081Epoch 8/10: [================              ] 33/60 batches, loss: 0.0080Epoch 8/10: [=================             ] 34/60 batches, loss: 0.0082Epoch 8/10: [=================             ] 35/60 batches, loss: 0.0082Epoch 8/10: [==================            ] 36/60 batches, loss: 0.0081Epoch 8/10: [==================            ] 37/60 batches, loss: 0.0081Epoch 8/10: [===================           ] 38/60 batches, loss: 0.0082Epoch 8/10: [===================           ] 39/60 batches, loss: 0.0081Epoch 8/10: [====================          ] 40/60 batches, loss: 0.0082Epoch 8/10: [====================          ] 41/60 batches, loss: 0.0081Epoch 8/10: [=====================         ] 42/60 batches, loss: 0.0082Epoch 8/10: [=====================         ] 43/60 batches, loss: 0.0083Epoch 8/10: [======================        ] 44/60 batches, loss: 0.0086Epoch 8/10: [======================        ] 45/60 batches, loss: 0.0084Epoch 8/10: [=======================       ] 46/60 batches, loss: 0.0084Epoch 8/10: [=======================       ] 47/60 batches, loss: 0.0084Epoch 8/10: [========================      ] 48/60 batches, loss: 0.0084Epoch 8/10: [========================      ] 49/60 batches, loss: 0.0083Epoch 8/10: [=========================     ] 50/60 batches, loss: 0.0082Epoch 8/10: [=========================     ] 51/60 batches, loss: 0.0081Epoch 8/10: [==========================    ] 52/60 batches, loss: 0.0080Epoch 8/10: [==========================    ] 53/60 batches, loss: 0.0080Epoch 8/10: [===========================   ] 54/60 batches, loss: 0.0080Epoch 8/10: [===========================   ] 55/60 batches, loss: 0.0079Epoch 8/10: [============================  ] 56/60 batches, loss: 0.0079Epoch 8/10: [============================  ] 57/60 batches, loss: 0.0078Epoch 8/10: [============================= ] 58/60 batches, loss: 0.0078Epoch 8/10: [============================= ] 59/60 batches, loss: 0.0079Epoch 8/10: [==============================] 60/60 batches, loss: 0.0078
[2025-05-01 11:51:23,464][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0078
[2025-05-01 11:51:23,740][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0127, Metrics: {'mse': 0.012340009212493896, 'rmse': 0.11108559408174354, 'r2': 0.704849123954773}
[2025-05-01 11:51:23,741][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/60 batches, loss: 0.0062Epoch 9/10: [=                             ] 2/60 batches, loss: 0.0072Epoch 9/10: [=                             ] 3/60 batches, loss: 0.0069Epoch 9/10: [==                            ] 4/60 batches, loss: 0.0072Epoch 9/10: [==                            ] 5/60 batches, loss: 0.0067Epoch 9/10: [===                           ] 6/60 batches, loss: 0.0062Epoch 9/10: [===                           ] 7/60 batches, loss: 0.0058Epoch 9/10: [====                          ] 8/60 batches, loss: 0.0064Epoch 9/10: [====                          ] 9/60 batches, loss: 0.0063Epoch 9/10: [=====                         ] 10/60 batches, loss: 0.0059Epoch 9/10: [=====                         ] 11/60 batches, loss: 0.0057Epoch 9/10: [======                        ] 12/60 batches, loss: 0.0056Epoch 9/10: [======                        ] 13/60 batches, loss: 0.0056Epoch 9/10: [=======                       ] 14/60 batches, loss: 0.0055Epoch 9/10: [=======                       ] 15/60 batches, loss: 0.0058Epoch 9/10: [========                      ] 16/60 batches, loss: 0.0058Epoch 9/10: [========                      ] 17/60 batches, loss: 0.0063Epoch 9/10: [=========                     ] 18/60 batches, loss: 0.0061Epoch 9/10: [=========                     ] 19/60 batches, loss: 0.0060Epoch 9/10: [==========                    ] 20/60 batches, loss: 0.0058Epoch 9/10: [==========                    ] 21/60 batches, loss: 0.0058Epoch 9/10: [===========                   ] 22/60 batches, loss: 0.0057Epoch 9/10: [===========                   ] 23/60 batches, loss: 0.0059Epoch 9/10: [============                  ] 24/60 batches, loss: 0.0058Epoch 9/10: [============                  ] 25/60 batches, loss: 0.0058Epoch 9/10: [=============                 ] 26/60 batches, loss: 0.0056Epoch 9/10: [=============                 ] 27/60 batches, loss: 0.0056Epoch 9/10: [==============                ] 28/60 batches, loss: 0.0057Epoch 9/10: [==============                ] 29/60 batches, loss: 0.0056Epoch 9/10: [===============               ] 30/60 batches, loss: 0.0056Epoch 9/10: [===============               ] 31/60 batches, loss: 0.0055Epoch 9/10: [================              ] 32/60 batches, loss: 0.0055Epoch 9/10: [================              ] 33/60 batches, loss: 0.0054Epoch 9/10: [=================             ] 34/60 batches, loss: 0.0054Epoch 9/10: [=================             ] 35/60 batches, loss: 0.0055Epoch 9/10: [==================            ] 36/60 batches, loss: 0.0055Epoch 9/10: [==================            ] 37/60 batches, loss: 0.0055Epoch 9/10: [===================           ] 38/60 batches, loss: 0.0054Epoch 9/10: [===================           ] 39/60 batches, loss: 0.0055Epoch 9/10: [====================          ] 40/60 batches, loss: 0.0056Epoch 9/10: [====================          ] 41/60 batches, loss: 0.0056Epoch 9/10: [=====================         ] 42/60 batches, loss: 0.0056Epoch 9/10: [=====================         ] 43/60 batches, loss: 0.0055Epoch 9/10: [======================        ] 44/60 batches, loss: 0.0055Epoch 9/10: [======================        ] 45/60 batches, loss: 0.0054Epoch 9/10: [=======================       ] 46/60 batches, loss: 0.0055Epoch 9/10: [=======================       ] 47/60 batches, loss: 0.0055Epoch 9/10: [========================      ] 48/60 batches, loss: 0.0054Epoch 9/10: [========================      ] 49/60 batches, loss: 0.0055Epoch 9/10: [=========================     ] 50/60 batches, loss: 0.0055Epoch 9/10: [=========================     ] 51/60 batches, loss: 0.0055Epoch 9/10: [==========================    ] 52/60 batches, loss: 0.0057Epoch 9/10: [==========================    ] 53/60 batches, loss: 0.0056Epoch 9/10: [===========================   ] 54/60 batches, loss: 0.0056Epoch 9/10: [===========================   ] 55/60 batches, loss: 0.0056Epoch 9/10: [============================  ] 56/60 batches, loss: 0.0056Epoch 9/10: [============================  ] 57/60 batches, loss: 0.0055Epoch 9/10: [============================= ] 58/60 batches, loss: 0.0055Epoch 9/10: [============================= ] 59/60 batches, loss: 0.0055Epoch 9/10: [==============================] 60/60 batches, loss: 0.0055
[2025-05-01 11:51:29,846][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0055
[2025-05-01 11:51:30,095][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0142, Metrics: {'mse': 0.012903592549264431, 'rmse': 0.1135939811313277, 'r2': 0.6913692355155945}
[2025-05-01 11:51:30,096][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/60 batches, loss: 0.0022Epoch 10/10: [=                             ] 2/60 batches, loss: 0.0062Epoch 10/10: [=                             ] 3/60 batches, loss: 0.0066Epoch 10/10: [==                            ] 4/60 batches, loss: 0.0069Epoch 10/10: [==                            ] 5/60 batches, loss: 0.0064Epoch 10/10: [===                           ] 6/60 batches, loss: 0.0060Epoch 10/10: [===                           ] 7/60 batches, loss: 0.0058Epoch 10/10: [====                          ] 8/60 batches, loss: 0.0059Epoch 10/10: [====                          ] 9/60 batches, loss: 0.0060Epoch 10/10: [=====                         ] 10/60 batches, loss: 0.0060Epoch 10/10: [=====                         ] 11/60 batches, loss: 0.0061Epoch 10/10: [======                        ] 12/60 batches, loss: 0.0062Epoch 10/10: [======                        ] 13/60 batches, loss: 0.0061Epoch 10/10: [=======                       ] 14/60 batches, loss: 0.0060Epoch 10/10: [=======                       ] 15/60 batches, loss: 0.0058Epoch 10/10: [========                      ] 16/60 batches, loss: 0.0057Epoch 10/10: [========                      ] 17/60 batches, loss: 0.0057Epoch 10/10: [=========                     ] 18/60 batches, loss: 0.0056Epoch 10/10: [=========                     ] 19/60 batches, loss: 0.0056Epoch 10/10: [==========                    ] 20/60 batches, loss: 0.0056Epoch 10/10: [==========                    ] 21/60 batches, loss: 0.0057Epoch 10/10: [===========                   ] 22/60 batches, loss: 0.0056Epoch 10/10: [===========                   ] 23/60 batches, loss: 0.0056Epoch 10/10: [============                  ] 24/60 batches, loss: 0.0055Epoch 10/10: [============                  ] 25/60 batches, loss: 0.0055Epoch 10/10: [=============                 ] 26/60 batches, loss: 0.0054Epoch 10/10: [=============                 ] 27/60 batches, loss: 0.0053Epoch 10/10: [==============                ] 28/60 batches, loss: 0.0054Epoch 10/10: [==============                ] 29/60 batches, loss: 0.0055Epoch 10/10: [===============               ] 30/60 batches, loss: 0.0055Epoch 10/10: [===============               ] 31/60 batches, loss: 0.0054Epoch 10/10: [================              ] 32/60 batches, loss: 0.0055Epoch 10/10: [================              ] 33/60 batches, loss: 0.0056Epoch 10/10: [=================             ] 34/60 batches, loss: 0.0056Epoch 10/10: [=================             ] 35/60 batches, loss: 0.0055Epoch 10/10: [==================            ] 36/60 batches, loss: 0.0055Epoch 10/10: [==================            ] 37/60 batches, loss: 0.0057Epoch 10/10: [===================           ] 38/60 batches, loss: 0.0057Epoch 10/10: [===================           ] 39/60 batches, loss: 0.0057Epoch 10/10: [====================          ] 40/60 batches, loss: 0.0057Epoch 10/10: [====================          ] 41/60 batches, loss: 0.0057Epoch 10/10: [=====================         ] 42/60 batches, loss: 0.0058Epoch 10/10: [=====================         ] 43/60 batches, loss: 0.0057Epoch 10/10: [======================        ] 44/60 batches, loss: 0.0057Epoch 10/10: [======================        ] 45/60 batches, loss: 0.0056Epoch 10/10: [=======================       ] 46/60 batches, loss: 0.0056Epoch 10/10: [=======================       ] 47/60 batches, loss: 0.0056Epoch 10/10: [========================      ] 48/60 batches, loss: 0.0055Epoch 10/10: [========================      ] 49/60 batches, loss: 0.0055Epoch 10/10: [=========================     ] 50/60 batches, loss: 0.0056Epoch 10/10: [=========================     ] 51/60 batches, loss: 0.0055Epoch 10/10: [==========================    ] 52/60 batches, loss: 0.0055Epoch 10/10: [==========================    ] 53/60 batches, loss: 0.0055Epoch 10/10: [===========================   ] 54/60 batches, loss: 0.0054Epoch 10/10: [===========================   ] 55/60 batches, loss: 0.0054Epoch 10/10: [============================  ] 56/60 batches, loss: 0.0054Epoch 10/10: [============================  ] 57/60 batches, loss: 0.0054Epoch 10/10: [============================= ] 58/60 batches, loss: 0.0053Epoch 10/10: [============================= ] 59/60 batches, loss: 0.0053Epoch 10/10: [==============================] 60/60 batches, loss: 0.0054
[2025-05-01 11:51:36,204][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0054
[2025-05-01 11:51:36,466][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0163, Metrics: {'mse': 0.015271814540028572, 'rmse': 0.12357918327950129, 'r2': 0.6347256302833557}
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 11:51:36,467][src.training.lm_trainer][INFO] - Training completed in 66.71 seconds
[2025-05-01 11:51:36,468][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:51:38,977][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.00696216756477952, 'rmse': 0.083439604294241, 'r2': 0.8081346750259399}
[2025-05-01 11:51:38,978][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.011142720468342304, 'rmse': 0.10555908520038577, 'r2': 0.7334861159324646}
[2025-05-01 11:51:38,978][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03317911922931671, 'rmse': 0.18215136351209868, 'r2': 0.1862698793411255}
[2025-05-01 11:51:40,665][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/id/id/model.pt
[2025-05-01 11:51:40,671][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▂▂▁▁
wandb:      best_val_r2 ▁▅▇▇██
wandb:    best_val_rmse █▅▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆███▇███
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▂▁▁▁▁
wandb:          val_mse █▄▂▂▁▂▁▁▁▁
wandb:           val_r2 ▁▅▇▇█▇████
wandb:         val_rmse █▅▂▂▁▃▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01211
wandb:     best_val_mse 0.01114
wandb:      best_val_r2 0.73349
wandb:    best_val_rmse 0.10556
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.03318
wandb:    final_test_r2 0.18627
wandb:  final_test_rmse 0.18215
wandb:  final_train_mse 0.00696
wandb:   final_train_r2 0.80813
wandb: final_train_rmse 0.08344
wandb:    final_val_mse 0.01114
wandb:     final_val_r2 0.73349
wandb:   final_val_rmse 0.10556
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00536
wandb:       train_time 66.715
wandb:         val_loss 0.01625
wandb:          val_mse 0.01527
wandb:           val_r2 0.63473
wandb:         val_rmse 0.12358
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115018-ippeu9hq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115018-ippeu9hq/logs
Experiment finetune_complexity_id completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/id/results.json
Running experiment: finetune_question_type_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ja"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:51:52,320][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ja
experiment_name: finetune_question_type_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:51:52,320][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:51:52,320][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:51:52,320][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:51:52,325][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-01 11:51:52,325][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:51:54,114][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:51:56,491][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:51:56,491][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:51:56,553][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,673][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-01 11:51:56,684][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:51:56,686][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-01 11:51:56,687][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:51:56,706][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,734][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,748][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-01 11:51:56,749][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:51:56,749][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-01 11:51:56,750][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:51:56,769][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,799][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:51:56,812][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-01 11:51:56,813][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:51:56,813][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-01 11:51:56,814][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:51:56,815][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-01 11:51:56,815][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-01 11:51:56,815][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:51:56,816][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-01 11:51:56,816][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:51:56,816][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:51:56,817][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-01 11:51:56,817][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:51:56,817][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:51:56,818][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:51:56,818][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:52:00,938][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:52:00,939][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:52:00,939][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:52:00,939][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:52:00,944][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:52:00,944][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:52:00,945][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:52:00,945][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-01 11:52:00,946][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:52:00,946][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.6602Epoch 1/10: [                              ] 2/75 batches, loss: 0.6899Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7402Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7522Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7252Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7208Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7294Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7218Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7158Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7169Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7132Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7149Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7137Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7192Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7260Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7311Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7302Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7289Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7290Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7267Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7288Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7290Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7277Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7276Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7253Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7257Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7261Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7258Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7296Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7298Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7266Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7264Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7255Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7270Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7262Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7246Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7245Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7221Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7225Epoch 1/10: [================              ] 40/75 batches, loss: 0.7232Epoch 1/10: [================              ] 41/75 batches, loss: 0.7216Epoch 1/10: [================              ] 42/75 batches, loss: 0.7199Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7184Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7187Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7180Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7170Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7170Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7185Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7192Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7153Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7140Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7122Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7099Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7089Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7076Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7080Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7059Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7069Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7066Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7069Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7047Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7054Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7055Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7053Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7058Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7042Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7031Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7025Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7006Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7007Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7004Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7011Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7002Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6992Epoch 1/10: [==============================] 75/75 batches, loss: 0.6990
[2025-05-01 11:52:10,889][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6990
[2025-05-01 11:52:11,084][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.5979, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5398Epoch 2/10: [                              ] 2/75 batches, loss: 0.5878Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6048Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6110Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6090Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6022Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6042Epoch 2/10: [===                           ] 8/75 batches, loss: 0.5986Epoch 2/10: [===                           ] 9/75 batches, loss: 0.5930Epoch 2/10: [====                          ] 10/75 batches, loss: 0.5966Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6014Epoch 2/10: [====                          ] 12/75 batches, loss: 0.5914Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5906Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5911Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5913Epoch 2/10: [======                        ] 16/75 batches, loss: 0.5860Epoch 2/10: [======                        ] 17/75 batches, loss: 0.5834Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5759Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5772Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5749Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5711Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5671Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5663Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5697Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5694Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5690Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5672Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5672Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5668Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5662Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5660Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5654Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5668Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5641Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5635Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5622Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5602Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5599Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5572Epoch 2/10: [================              ] 40/75 batches, loss: 0.5561Epoch 2/10: [================              ] 41/75 batches, loss: 0.5538Epoch 2/10: [================              ] 42/75 batches, loss: 0.5527Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5514Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5516Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5513Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5515Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5501Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5497Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5488Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5480Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5464Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5461Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5459Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5466Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5459Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5447Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5433Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5440Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5437Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5431Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5429Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5423Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5417Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5419Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5421Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5416Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5406Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5405Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5400Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5381Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5370Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5366Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5365Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5354Epoch 2/10: [==============================] 75/75 batches, loss: 0.5339
[2025-05-01 11:52:19,077][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5339
[2025-05-01 11:52:19,279][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4980, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5771Epoch 3/10: [                              ] 2/75 batches, loss: 0.5409Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5452Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5231Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5290Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5251Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5154Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5080Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5025Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5051Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5051Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5131Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5107Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5138Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5133Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5202Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5153Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5107Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5117Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5137Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5155Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5161Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5177Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5161Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5157Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5162Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5166Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5137Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5167Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5131Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5113Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5104Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5124Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5098Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5096Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5114Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5100Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5098Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5084Epoch 3/10: [================              ] 40/75 batches, loss: 0.5101Epoch 3/10: [================              ] 41/75 batches, loss: 0.5095Epoch 3/10: [================              ] 42/75 batches, loss: 0.5099Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5076Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5075Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5085Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5100Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5084Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5083Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5087Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5082Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5081Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5066Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5070Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5074Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5052Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5060Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5060Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5080Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5068Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5075Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5094Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5094Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5078Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5074Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5080Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5076Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5083Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5075Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5078Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5067Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5070Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5073Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5072Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5066Epoch 3/10: [==============================] 75/75 batches, loss: 0.5069
[2025-05-01 11:52:27,279][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5069
[2025-05-01 11:52:27,482][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4940, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5045Epoch 4/10: [                              ] 2/75 batches, loss: 0.5162Epoch 4/10: [=                             ] 3/75 batches, loss: 0.4967Epoch 4/10: [=                             ] 4/75 batches, loss: 0.4689Epoch 4/10: [==                            ] 5/75 batches, loss: 0.4762Epoch 4/10: [==                            ] 6/75 batches, loss: 0.4849Epoch 4/10: [==                            ] 7/75 batches, loss: 0.4878Epoch 4/10: [===                           ] 8/75 batches, loss: 0.4927Epoch 4/10: [===                           ] 9/75 batches, loss: 0.4914Epoch 4/10: [====                          ] 10/75 batches, loss: 0.4903Epoch 4/10: [====                          ] 11/75 batches, loss: 0.4851Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4859Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4873Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4868Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4895Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4949Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4926Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4960Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4964Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4956Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4960Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4965Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4958Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.4951Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.4964Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.4967Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.4987Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.4998Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.4991Epoch 4/10: [============                  ] 30/75 batches, loss: 0.4977Epoch 4/10: [============                  ] 31/75 batches, loss: 0.4963Epoch 4/10: [============                  ] 32/75 batches, loss: 0.4973Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.4975Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.4995Epoch 4/10: [==============                ] 35/75 batches, loss: 0.4969Epoch 4/10: [==============                ] 36/75 batches, loss: 0.4978Epoch 4/10: [==============                ] 37/75 batches, loss: 0.4990Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5017Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5024Epoch 4/10: [================              ] 40/75 batches, loss: 0.5018Epoch 4/10: [================              ] 41/75 batches, loss: 0.5031Epoch 4/10: [================              ] 42/75 batches, loss: 0.5048Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5053Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5064Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5053Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5052Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5047Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5052Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5056Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5062Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5047Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5038Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5043Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5034Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5029Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5049Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5053Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5068Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5086Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5085Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5081Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5080Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5079Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5086Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5089Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5092Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5102Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5105Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5100Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5103Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5099Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5098Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5107Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5106Epoch 4/10: [==============================] 75/75 batches, loss: 0.5101
[2025-05-01 11:52:35,416][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5101
[2025-05-01 11:52:35,629][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5038, Metrics: {'accuracy': 0.9782608695652174, 'f1': 0.9787234042553191, 'precision': 1.0, 'recall': 0.9583333333333334}
[2025-05-01 11:52:35,630][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4798Epoch 5/10: [                              ] 2/75 batches, loss: 0.5036Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5196Epoch 5/10: [=                             ] 4/75 batches, loss: 0.5277Epoch 5/10: [==                            ] 5/75 batches, loss: 0.5134Epoch 5/10: [==                            ] 6/75 batches, loss: 0.5157Epoch 5/10: [==                            ] 7/75 batches, loss: 0.5038Epoch 5/10: [===                           ] 8/75 batches, loss: 0.5009Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4985Epoch 5/10: [====                          ] 10/75 batches, loss: 0.5014Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4995Epoch 5/10: [====                          ] 12/75 batches, loss: 0.5019Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.5057Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.5089Epoch 5/10: [======                        ] 15/75 batches, loss: 0.5133Epoch 5/10: [======                        ] 16/75 batches, loss: 0.5113Epoch 5/10: [======                        ] 17/75 batches, loss: 0.5108Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.5118Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.5134Epoch 5/10: [========                      ] 20/75 batches, loss: 0.5093Epoch 5/10: [========                      ] 21/75 batches, loss: 0.5068Epoch 5/10: [========                      ] 22/75 batches, loss: 0.5056Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.5065Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.5064Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.5063Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.5071Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.5061Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.5052Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.5059Epoch 5/10: [============                  ] 30/75 batches, loss: 0.5074Epoch 5/10: [============                  ] 31/75 batches, loss: 0.5058Epoch 5/10: [============                  ] 32/75 batches, loss: 0.5065Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.5079Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.5056Epoch 5/10: [==============                ] 35/75 batches, loss: 0.5069Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5062Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5062Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5062Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5040Epoch 5/10: [================              ] 40/75 batches, loss: 0.5011Epoch 5/10: [================              ] 41/75 batches, loss: 0.5006Epoch 5/10: [================              ] 42/75 batches, loss: 0.5006Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5001Epoch 5/10: [=================             ] 44/75 batches, loss: 0.4991Epoch 5/10: [==================            ] 45/75 batches, loss: 0.4987Epoch 5/10: [==================            ] 46/75 batches, loss: 0.4978Epoch 5/10: [==================            ] 47/75 batches, loss: 0.4984Epoch 5/10: [===================           ] 48/75 batches, loss: 0.4996Epoch 5/10: [===================           ] 49/75 batches, loss: 0.4996Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5007Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5018Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5021Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5026Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5030Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5026Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5022Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5026Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5022Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5033Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5033Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5026Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5026Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5033Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5037Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5052Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5055Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5055Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5058Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5047Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5050Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5044Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5059Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5068Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5081Epoch 5/10: [==============================] 75/75 batches, loss: 0.5076
[2025-05-01 11:52:43,214][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5076
[2025-05-01 11:52:43,432][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.4934, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.5208Epoch 6/10: [                              ] 2/75 batches, loss: 0.5240Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5411Epoch 6/10: [=                             ] 4/75 batches, loss: 0.5495Epoch 6/10: [==                            ] 5/75 batches, loss: 0.5545Epoch 6/10: [==                            ] 6/75 batches, loss: 0.5490Epoch 6/10: [==                            ] 7/75 batches, loss: 0.5446Epoch 6/10: [===                           ] 8/75 batches, loss: 0.5395Epoch 6/10: [===                           ] 9/75 batches, loss: 0.5355Epoch 6/10: [====                          ] 10/75 batches, loss: 0.5323Epoch 6/10: [====                          ] 11/75 batches, loss: 0.5340Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5216Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5202Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5207Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5243Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5275Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5219Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5213Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5229Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5234Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5202Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5184Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5177Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5161Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5147Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5142Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5121Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5109Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5099Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5104Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5095Epoch 6/10: [============                  ] 32/75 batches, loss: 0.5093Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.5098Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.5075Epoch 6/10: [==============                ] 35/75 batches, loss: 0.5081Epoch 6/10: [==============                ] 36/75 batches, loss: 0.5106Epoch 6/10: [==============                ] 37/75 batches, loss: 0.5104Epoch 6/10: [===============               ] 38/75 batches, loss: 0.5109Epoch 6/10: [===============               ] 39/75 batches, loss: 0.5094Epoch 6/10: [================              ] 40/75 batches, loss: 0.5099Epoch 6/10: [================              ] 41/75 batches, loss: 0.5091Epoch 6/10: [================              ] 42/75 batches, loss: 0.5090Epoch 6/10: [=================             ] 43/75 batches, loss: 0.5072Epoch 6/10: [=================             ] 44/75 batches, loss: 0.5066Epoch 6/10: [==================            ] 45/75 batches, loss: 0.5071Epoch 6/10: [==================            ] 46/75 batches, loss: 0.5075Epoch 6/10: [==================            ] 47/75 batches, loss: 0.5074Epoch 6/10: [===================           ] 48/75 batches, loss: 0.5059Epoch 6/10: [===================           ] 49/75 batches, loss: 0.5063Epoch 6/10: [====================          ] 50/75 batches, loss: 0.5077Epoch 6/10: [====================          ] 51/75 batches, loss: 0.5067Epoch 6/10: [====================          ] 52/75 batches, loss: 0.5071Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.5083Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.5087Epoch 6/10: [======================        ] 55/75 batches, loss: 0.5082Epoch 6/10: [======================        ] 56/75 batches, loss: 0.5085Epoch 6/10: [======================        ] 57/75 batches, loss: 0.5080Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.5083Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5082Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5078Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5085Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5080Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5076Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5068Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5071Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5074Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5077Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5073Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5072Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5061Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5068Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5067Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5067Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5057Epoch 6/10: [==============================] 75/75 batches, loss: 0.5060
[2025-05-01 11:52:51,416][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5060
[2025-05-01 11:52:51,618][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4934, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.5034Epoch 7/10: [                              ] 2/75 batches, loss: 0.4916Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5034Epoch 7/10: [=                             ] 4/75 batches, loss: 0.4915Epoch 7/10: [==                            ] 5/75 batches, loss: 0.4987Epoch 7/10: [==                            ] 6/75 batches, loss: 0.4955Epoch 7/10: [==                            ] 7/75 batches, loss: 0.4967Epoch 7/10: [===                           ] 8/75 batches, loss: 0.5065Epoch 7/10: [===                           ] 9/75 batches, loss: 0.5114Epoch 7/10: [====                          ] 10/75 batches, loss: 0.5082Epoch 7/10: [====                          ] 11/75 batches, loss: 0.5121Epoch 7/10: [====                          ] 12/75 batches, loss: 0.5094Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.5089Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.5068Epoch 7/10: [======                        ] 15/75 batches, loss: 0.4987Epoch 7/10: [======                        ] 16/75 batches, loss: 0.4960Epoch 7/10: [======                        ] 17/75 batches, loss: 0.4922Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.4929Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.4947Epoch 7/10: [========                      ] 20/75 batches, loss: 0.4951Epoch 7/10: [========                      ] 21/75 batches, loss: 0.4967Epoch 7/10: [========                      ] 22/75 batches, loss: 0.4970Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.4993Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5015Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5006Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5021Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5004Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.4988Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.4990Epoch 7/10: [============                  ] 30/75 batches, loss: 0.4999Epoch 7/10: [============                  ] 31/75 batches, loss: 0.4985Epoch 7/10: [============                  ] 32/75 batches, loss: 0.4979Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.4966Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.4954Epoch 7/10: [==============                ] 35/75 batches, loss: 0.4963Epoch 7/10: [==============                ] 36/75 batches, loss: 0.4965Epoch 7/10: [==============                ] 37/75 batches, loss: 0.4967Epoch 7/10: [===============               ] 38/75 batches, loss: 0.4969Epoch 7/10: [===============               ] 39/75 batches, loss: 0.4983Epoch 7/10: [================              ] 40/75 batches, loss: 0.4990Epoch 7/10: [================              ] 41/75 batches, loss: 0.5009Epoch 7/10: [================              ] 42/75 batches, loss: 0.5009Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5032Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5032Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5032Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5037Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5022Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5022Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5037Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5042Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5043Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5043Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5043Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5051Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5051Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5047Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5046Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5046Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5062Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5066Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5065Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5057Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5064Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5060Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5056Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5066Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5062Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5058Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5064Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5059Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5055Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5055Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5058Epoch 7/10: [==============================] 75/75 batches, loss: 0.5047
[2025-05-01 11:52:59,635][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5047
[2025-05-01 11:52:59,852][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.4934, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:52:59,853][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.4558Epoch 8/10: [                              ] 2/75 batches, loss: 0.4796Epoch 8/10: [=                             ] 3/75 batches, loss: 0.4954Epoch 8/10: [=                             ] 4/75 batches, loss: 0.5033Epoch 8/10: [==                            ] 5/75 batches, loss: 0.4938Epoch 8/10: [==                            ] 6/75 batches, loss: 0.4954Epoch 8/10: [==                            ] 7/75 batches, loss: 0.4999Epoch 8/10: [===                           ] 8/75 batches, loss: 0.5033Epoch 8/10: [===                           ] 9/75 batches, loss: 0.5060Epoch 8/10: [====                          ] 10/75 batches, loss: 0.5106Epoch 8/10: [====                          ] 11/75 batches, loss: 0.5078Epoch 8/10: [====                          ] 12/75 batches, loss: 0.5034Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.4998Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.5035Epoch 8/10: [======                        ] 15/75 batches, loss: 0.5098Epoch 8/10: [======                        ] 16/75 batches, loss: 0.5109Epoch 8/10: [======                        ] 17/75 batches, loss: 0.5119Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.5114Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.5118Epoch 8/10: [========                      ] 20/75 batches, loss: 0.5113Epoch 8/10: [========                      ] 21/75 batches, loss: 0.5087Epoch 8/10: [========                      ] 22/75 batches, loss: 0.5063Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.5062Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.5041Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.5041Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.5040Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.5022Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.5031Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.5015Epoch 8/10: [============                  ] 30/75 batches, loss: 0.5031Epoch 8/10: [============                  ] 31/75 batches, loss: 0.5032Epoch 8/10: [============                  ] 32/75 batches, loss: 0.5054Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.5068Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.5046Epoch 8/10: [==============                ] 35/75 batches, loss: 0.5052Epoch 8/10: [==============                ] 36/75 batches, loss: 0.5065Epoch 8/10: [==============                ] 37/75 batches, loss: 0.5077Epoch 8/10: [===============               ] 38/75 batches, loss: 0.5076Epoch 8/10: [===============               ] 39/75 batches, loss: 0.5081Epoch 8/10: [================              ] 40/75 batches, loss: 0.5080Epoch 8/10: [================              ] 41/75 batches, loss: 0.5084Epoch 8/10: [================              ] 42/75 batches, loss: 0.5077Epoch 8/10: [=================             ] 43/75 batches, loss: 0.5071Epoch 8/10: [=================             ] 44/75 batches, loss: 0.5070Epoch 8/10: [==================            ] 45/75 batches, loss: 0.5080Epoch 8/10: [==================            ] 46/75 batches, loss: 0.5074Epoch 8/10: [==================            ] 47/75 batches, loss: 0.5063Epoch 8/10: [===================           ] 48/75 batches, loss: 0.5052Epoch 8/10: [===================           ] 49/75 batches, loss: 0.5052Epoch 8/10: [====================          ] 50/75 batches, loss: 0.5042Epoch 8/10: [====================          ] 51/75 batches, loss: 0.5037Epoch 8/10: [====================          ] 52/75 batches, loss: 0.5037Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.5046Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.5050Epoch 8/10: [======================        ] 55/75 batches, loss: 0.5054Epoch 8/10: [======================        ] 56/75 batches, loss: 0.5045Epoch 8/10: [======================        ] 57/75 batches, loss: 0.5045Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.5049Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.5057Epoch 8/10: [========================      ] 60/75 batches, loss: 0.5056Epoch 8/10: [========================      ] 61/75 batches, loss: 0.5060Epoch 8/10: [========================      ] 62/75 batches, loss: 0.5060Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.5048Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.5059Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.5058Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.5058Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.5047Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.5043Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.5040Epoch 8/10: [============================  ] 70/75 batches, loss: 0.5043Epoch 8/10: [============================  ] 71/75 batches, loss: 0.5056Epoch 8/10: [============================  ] 72/75 batches, loss: 0.5059Epoch 8/10: [============================= ] 73/75 batches, loss: 0.5056Epoch 8/10: [============================= ] 74/75 batches, loss: 0.5055Epoch 8/10: [==============================] 75/75 batches, loss: 0.5051
[2025-05-01 11:53:07,417][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5051
[2025-05-01 11:53:07,629][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4933, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.4558Epoch 9/10: [                              ] 2/75 batches, loss: 0.4796Epoch 9/10: [=                             ] 3/75 batches, loss: 0.4796Epoch 9/10: [=                             ] 4/75 batches, loss: 0.5093Epoch 9/10: [==                            ] 5/75 batches, loss: 0.5176Epoch 9/10: [==                            ] 6/75 batches, loss: 0.5231Epoch 9/10: [==                            ] 7/75 batches, loss: 0.5203Epoch 9/10: [===                           ] 8/75 batches, loss: 0.5212Epoch 9/10: [===                           ] 9/75 batches, loss: 0.5165Epoch 9/10: [====                          ] 10/75 batches, loss: 0.5176Epoch 9/10: [====                          ] 11/75 batches, loss: 0.5206Epoch 9/10: [====                          ] 12/75 batches, loss: 0.5152Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.5161Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.5197Epoch 9/10: [======                        ] 15/75 batches, loss: 0.5202Epoch 9/10: [======                        ] 16/75 batches, loss: 0.5176Epoch 9/10: [======                        ] 17/75 batches, loss: 0.5154Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.5160Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.5141Epoch 9/10: [========                      ] 20/75 batches, loss: 0.5100Epoch 9/10: [========                      ] 21/75 batches, loss: 0.5086Epoch 9/10: [========                      ] 22/75 batches, loss: 0.5040Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.5050Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.5059Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.5058Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.5021Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.5030Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.5030Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.5047Epoch 9/10: [============                  ] 30/75 batches, loss: 0.5030Epoch 9/10: [============                  ] 31/75 batches, loss: 0.5007Epoch 9/10: [============                  ] 32/75 batches, loss: 0.5001Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.5002Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.4996Epoch 9/10: [==============                ] 35/75 batches, loss: 0.5004Epoch 9/10: [==============                ] 36/75 batches, loss: 0.4998Epoch 9/10: [==============                ] 37/75 batches, loss: 0.5005Epoch 9/10: [===============               ] 38/75 batches, loss: 0.4993Epoch 9/10: [===============               ] 39/75 batches, loss: 0.5007Epoch 9/10: [================              ] 40/75 batches, loss: 0.5001Epoch 9/10: [================              ] 41/75 batches, loss: 0.5025Epoch 9/10: [================              ] 42/75 batches, loss: 0.5031Epoch 9/10: [=================             ] 43/75 batches, loss: 0.5026Epoch 9/10: [=================             ] 44/75 batches, loss: 0.5042Epoch 9/10: [==================            ] 45/75 batches, loss: 0.5037Epoch 9/10: [==================            ] 46/75 batches, loss: 0.5021Epoch 9/10: [==================            ] 47/75 batches, loss: 0.5032Epoch 9/10: [===================           ] 48/75 batches, loss: 0.5046Epoch 9/10: [===================           ] 49/75 batches, loss: 0.5041Epoch 9/10: [====================          ] 50/75 batches, loss: 0.5041Epoch 9/10: [====================          ] 51/75 batches, loss: 0.5027Epoch 9/10: [====================          ] 52/75 batches, loss: 0.5050Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.5032Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.5032Epoch 9/10: [======================        ] 55/75 batches, loss: 0.5036Epoch 9/10: [======================        ] 56/75 batches, loss: 0.5023Epoch 9/10: [======================        ] 57/75 batches, loss: 0.5036Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.5040Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.5052Epoch 9/10: [========================      ] 60/75 batches, loss: 0.5060Epoch 9/10: [========================      ] 61/75 batches, loss: 0.5063Epoch 9/10: [========================      ] 62/75 batches, loss: 0.5067Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.5070Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.5062Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.5054Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.5065Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.5053Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.5053Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.5046Epoch 9/10: [============================  ] 70/75 batches, loss: 0.5046Epoch 9/10: [============================  ] 71/75 batches, loss: 0.5052Epoch 9/10: [============================  ] 72/75 batches, loss: 0.5049Epoch 9/10: [============================= ] 73/75 batches, loss: 0.5052Epoch 9/10: [============================= ] 74/75 batches, loss: 0.5051Epoch 9/10: [==============================] 75/75 batches, loss: 0.5055
[2025-05-01 11:53:15,589][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5055
[2025-05-01 11:53:15,800][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4931, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.5033Epoch 10/10: [                              ] 2/75 batches, loss: 0.5032Epoch 10/10: [=                             ] 3/75 batches, loss: 0.4874Epoch 10/10: [=                             ] 4/75 batches, loss: 0.4914Epoch 10/10: [==                            ] 5/75 batches, loss: 0.4843Epoch 10/10: [==                            ] 6/75 batches, loss: 0.4993Epoch 10/10: [==                            ] 7/75 batches, loss: 0.5033Epoch 10/10: [===                           ] 8/75 batches, loss: 0.5033Epoch 10/10: [===                           ] 9/75 batches, loss: 0.5033Epoch 10/10: [====                          ] 10/75 batches, loss: 0.5033Epoch 10/10: [====                          ] 11/75 batches, loss: 0.5119Epoch 10/10: [====                          ] 12/75 batches, loss: 0.5132Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.5143Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.5135Epoch 10/10: [======                        ] 15/75 batches, loss: 0.5191Epoch 10/10: [======                        ] 16/75 batches, loss: 0.5167Epoch 10/10: [======                        ] 17/75 batches, loss: 0.5173Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.5139Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.5108Epoch 10/10: [========                      ] 20/75 batches, loss: 0.5057Epoch 10/10: [========                      ] 21/75 batches, loss: 0.5033Epoch 10/10: [========                      ] 22/75 batches, loss: 0.5066Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.5105Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.5093Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.5090Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.5097Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.5068Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.5059Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.5079Epoch 10/10: [============                  ] 30/75 batches, loss: 0.5078Epoch 10/10: [============                  ] 31/75 batches, loss: 0.5076Epoch 10/10: [============                  ] 32/75 batches, loss: 0.5075Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.5074Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.5065Epoch 10/10: [==============                ] 35/75 batches, loss: 0.5078Epoch 10/10: [==============                ] 36/75 batches, loss: 0.5097Epoch 10/10: [==============                ] 37/75 batches, loss: 0.5082Epoch 10/10: [===============               ] 38/75 batches, loss: 0.5087Epoch 10/10: [===============               ] 39/75 batches, loss: 0.5086Epoch 10/10: [================              ] 40/75 batches, loss: 0.5090Epoch 10/10: [================              ] 41/75 batches, loss: 0.5089Epoch 10/10: [================              ] 42/75 batches, loss: 0.5099Epoch 10/10: [=================             ] 43/75 batches, loss: 0.5097Epoch 10/10: [=================             ] 44/75 batches, loss: 0.5107Epoch 10/10: [==================            ] 45/75 batches, loss: 0.5126Epoch 10/10: [==================            ] 46/75 batches, loss: 0.5134Epoch 10/10: [==================            ] 47/75 batches, loss: 0.5112Epoch 10/10: [===================           ] 48/75 batches, loss: 0.5096Epoch 10/10: [===================           ] 49/75 batches, loss: 0.5099Epoch 10/10: [====================          ] 50/75 batches, loss: 0.5103Epoch 10/10: [====================          ] 51/75 batches, loss: 0.5106Epoch 10/10: [====================          ] 52/75 batches, loss: 0.5082Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.5081Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.5071Epoch 10/10: [======================        ] 55/75 batches, loss: 0.5088Epoch 10/10: [======================        ] 56/75 batches, loss: 0.5083Epoch 10/10: [======================        ] 57/75 batches, loss: 0.5090Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.5097Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.5088Epoch 10/10: [========================      ] 60/75 batches, loss: 0.5091Epoch 10/10: [========================      ] 61/75 batches, loss: 0.5090Epoch 10/10: [========================      ] 62/75 batches, loss: 0.5082Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.5081Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.5069Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.5068Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.5068Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.5060Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.5070Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.5063Epoch 10/10: [============================  ] 70/75 batches, loss: 0.5059Epoch 10/10: [============================  ] 71/75 batches, loss: 0.5069Epoch 10/10: [============================  ] 72/75 batches, loss: 0.5058Epoch 10/10: [============================= ] 73/75 batches, loss: 0.5055Epoch 10/10: [============================= ] 74/75 batches, loss: 0.5055Epoch 10/10: [==============================] 75/75 batches, loss: 0.5051
[2025-05-01 11:53:23,760][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5051
[2025-05-01 11:53:23,981][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4933, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:53:23,982][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-01 11:53:23,982][src.training.lm_trainer][INFO] - Training completed in 81.55 seconds
[2025-05-01 11:53:23,982][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:53:26,822][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9949622166246851, 'f1': 0.9949494949494949, 'precision': 0.9983108108108109, 'recall': 0.9916107382550335}
[2025-05-01 11:53:26,822][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 11:53:26,822][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8586956521739131, 'f1': 0.8869565217391304, 'precision': 0.85, 'recall': 0.9272727272727272}
[2025-05-01 11:53:28,577][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ja/ja/model.pt
[2025-05-01 11:53:28,582][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁▁▁
wandb:         best_val_loss █▁▁▁▁▁▁
wandb:    best_val_precision ▁▁▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁▁▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▃▃▃▃▃▃▃▃
wandb:            train_loss █▂▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ███▁██████
wandb:                val_f1 ███▁██████
wandb:              val_loss █▁▁▂▁▁▁▁▁▁
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ███▁██████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.49314
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:                 epoch 10
wandb:   final_test_accuracy 0.8587
wandb:         final_test_f1 0.88696
wandb:  final_test_precision 0.85
wandb:     final_test_recall 0.92727
wandb:  final_train_accuracy 0.99496
wandb:        final_train_f1 0.99495
wandb: final_train_precision 0.99831
wandb:    final_train_recall 0.99161
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50506
wandb:            train_time 81.55367
wandb:          val_accuracy 1
wandb:                val_f1 1
wandb:              val_loss 0.49327
wandb:         val_precision 1
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115152-wkj11ig3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115152-wkj11ig3/logs
Experiment finetune_question_type_ja completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ja/results.json
Running experiment: finetune_complexity_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ja"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ja"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:53:40,823][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ja
experiment_name: finetune_complexity_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:53:40,823][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:53:40,823][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:53:40,823][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:53:40,827][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-01 11:53:40,827][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:53:42,161][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:53:44,510][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:53:44,510][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:53:44,641][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,665][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,755][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-01 11:53:44,764][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:53:44,764][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-01 11:53:44,765][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:53:44,781][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,821][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-01 11:53:44,822][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:53:44,822][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-01 11:53:44,823][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:53:44,840][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,866][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:53:44,878][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-01 11:53:44,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:53:44,880][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-01 11:53:44,881][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-01 11:53:44,881][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:53:44,882][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:53:44,882][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-05-01 11:53:44,882][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:53:44,883][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:53:44,883][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:53:44,883][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:53:44,884][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:53:44,884][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:53:44,884][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:53:44,885][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:53:44,885][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:53:48,917][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:53:48,919][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:53:48,919][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:53:48,919][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:53:48,924][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:53:48,924][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:53:48,924][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:53:48,924][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-01 11:53:48,925][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:53:48,925][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1778Epoch 1/10: [                              ] 2/75 batches, loss: 0.2100Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1859Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1916Epoch 1/10: [==                            ] 5/75 batches, loss: 0.2065Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1979Epoch 1/10: [==                            ] 7/75 batches, loss: 0.2170Epoch 1/10: [===                           ] 8/75 batches, loss: 0.2128Epoch 1/10: [===                           ] 9/75 batches, loss: 0.2150Epoch 1/10: [====                          ] 10/75 batches, loss: 0.2242Epoch 1/10: [====                          ] 11/75 batches, loss: 0.2253Epoch 1/10: [====                          ] 12/75 batches, loss: 0.2248Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.2205Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.2170Epoch 1/10: [======                        ] 15/75 batches, loss: 0.2122Epoch 1/10: [======                        ] 16/75 batches, loss: 0.2100Epoch 1/10: [======                        ] 17/75 batches, loss: 0.2040Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.2036Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1984Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1975Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1963Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1931Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1894Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1850Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1809Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1785Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1758Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1732Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1751Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1715Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1687Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1680Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1672Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1657Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1648Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1650Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1646Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1617Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1602Epoch 1/10: [================              ] 40/75 batches, loss: 0.1587Epoch 1/10: [================              ] 41/75 batches, loss: 0.1592Epoch 1/10: [================              ] 42/75 batches, loss: 0.1608Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1602Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1606Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1612Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1602Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1592Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1595Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1602Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1597Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1603Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1600Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1599Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.1594Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1595Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1598Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1592Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1602Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.1590Epoch 1/10: [========================      ] 60/75 batches, loss: 0.1590Epoch 1/10: [========================      ] 61/75 batches, loss: 0.1582Epoch 1/10: [========================      ] 62/75 batches, loss: 0.1590Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.1584Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.1582Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.1576Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.1573Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.1571Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.1559Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.1541Epoch 1/10: [============================  ] 70/75 batches, loss: 0.1533Epoch 1/10: [============================  ] 71/75 batches, loss: 0.1524Epoch 1/10: [============================  ] 72/75 batches, loss: 0.1509Epoch 1/10: [============================= ] 73/75 batches, loss: 0.1496Epoch 1/10: [============================= ] 74/75 batches, loss: 0.1484Epoch 1/10: [==============================] 75/75 batches, loss: 0.1475
[2025-05-01 11:53:59,022][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1475
[2025-05-01 11:53:59,200][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0825, Metrics: {'mse': 0.08185435086488724, 'rmse': 0.2861019938149457, 'r2': -0.33405601978302}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0453Epoch 2/10: [                              ] 2/75 batches, loss: 0.0616Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0621Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0808Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0835Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0803Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0884Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0884Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0835Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0810Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0791Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0768Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0778Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0751Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0743Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0721Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0705Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0693Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0685Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0683Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0672Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0660Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0670Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0668Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0661Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0661Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0645Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0637Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0631Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0630Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0628Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0624Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0619Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0617Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0619Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0609Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0600Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0594Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0587Epoch 2/10: [================              ] 40/75 batches, loss: 0.0583Epoch 2/10: [================              ] 41/75 batches, loss: 0.0578Epoch 2/10: [================              ] 42/75 batches, loss: 0.0575Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0573Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0563Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0559Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0555Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0552Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0548Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0544Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0541Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0541Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0538Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0540Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0537Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0537Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0533Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0531Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0538Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0534Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0533Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0536Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0536Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0534Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0539Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0539Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0547Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0544Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0544Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0544Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0542Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0538Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0535Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0534Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0532Epoch 2/10: [==============================] 75/75 batches, loss: 0.0531
[2025-05-01 11:54:07,196][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-05-01 11:54:07,396][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0409, Metrics: {'mse': 0.04042927920818329, 'rmse': 0.20107033398336835, 'r2': 0.34108662605285645}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0282Epoch 3/10: [                              ] 2/75 batches, loss: 0.0324Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0301Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0294Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0336Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0337Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0315Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0357Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0331Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0333Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0347Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0334Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0330Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0319Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0323Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0326Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0320Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0331Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0322Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0324Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0319Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0315Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0316Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0318Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0316Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0310Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0315Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0313Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0309Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0306Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0307Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0304Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0309Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0309Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0307Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0307Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0307Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0303Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0302Epoch 3/10: [================              ] 40/75 batches, loss: 0.0301Epoch 3/10: [================              ] 41/75 batches, loss: 0.0299Epoch 3/10: [================              ] 42/75 batches, loss: 0.0302Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0298Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0296Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0293Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0293Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0292Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0290Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0289Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0289Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0286Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0288Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0287Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0288Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0285Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0283Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0284Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0283Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0282Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0280Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0279Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0276Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0275Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0273Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0273Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0275Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0274Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0275Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0273Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0272Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0270Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0276Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0275Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0276Epoch 3/10: [==============================] 75/75 batches, loss: 0.0275
[2025-05-01 11:54:15,419][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0275
[2025-05-01 11:54:15,635][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0227, Metrics: {'mse': 0.022250032052397728, 'rmse': 0.14916444634160558, 'r2': 0.6373705863952637}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0072Epoch 4/10: [                              ] 2/75 batches, loss: 0.0182Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0222Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0239Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0275Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0247Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0230Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0235Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0236Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0229Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0228Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0248Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0249Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0249Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0238Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0237Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0229Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0228Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0234Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0234Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0236Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0230Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0226Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0224Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0220Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0218Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0214Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0210Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0210Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0209Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0206Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0204Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0203Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0203Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0205Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0204Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0202Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0200Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0197Epoch 4/10: [================              ] 40/75 batches, loss: 0.0199Epoch 4/10: [================              ] 41/75 batches, loss: 0.0198Epoch 4/10: [================              ] 42/75 batches, loss: 0.0197Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0196Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0195Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0193Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0192Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0191Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0189Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0188Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0188Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0186Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0184Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0184Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0183Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0182Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0182Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0182Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0181Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0180Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0179Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0179Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0177Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0176Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0178Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0177Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0177Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0177Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0177Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0178Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0179Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0178Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0177Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0177Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0177Epoch 4/10: [==============================] 75/75 batches, loss: 0.0177
[2025-05-01 11:54:23,577][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0177
[2025-05-01 11:54:23,782][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0224, Metrics: {'mse': 0.021939517930150032, 'rmse': 0.14811994440368262, 'r2': 0.642431378364563}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0205Epoch 5/10: [                              ] 2/75 batches, loss: 0.0178Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0170Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0176Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0158Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0151Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0154Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0150Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0161Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0178Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0175Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0186Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0187Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0182Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0185Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0184Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0178Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0172Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0168Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0164Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0164Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0168Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0173Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0170Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0168Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0167Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0163Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0163Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0161Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0164Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0166Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0164Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0164Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0161Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0159Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0156Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0157Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0157Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0154Epoch 5/10: [================              ] 40/75 batches, loss: 0.0153Epoch 5/10: [================              ] 41/75 batches, loss: 0.0154Epoch 5/10: [================              ] 42/75 batches, loss: 0.0153Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0155Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0156Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0155Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0156Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0158Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0157Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0156Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0156Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0157Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0156Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0156Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0154Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0153Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0152Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0152Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0151Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0151Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0153Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0153Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0153Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0152Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0153Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0152Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0152Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0151Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0150Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0150Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0151Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0150Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0150Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0151Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0152Epoch 5/10: [==============================] 75/75 batches, loss: 0.0152
[2025-05-01 11:54:31,753][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0152
[2025-05-01 11:54:31,959][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0171, Metrics: {'mse': 0.01683175005018711, 'rmse': 0.1297372346328806, 'r2': 0.7256773710250854}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0085Epoch 6/10: [                              ] 2/75 batches, loss: 0.0091Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0073Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0109Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0131Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0132Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0129Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0137Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0127Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0129Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0126Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0123Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0118Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0116Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0118Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0118Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0122Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0126Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0126Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0122Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0121Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0127Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0125Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0124Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0124Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0122Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0124Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0125Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0124Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0123Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0122Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0122Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0124Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0123Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0120Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0117Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0119Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0119Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0119Epoch 6/10: [================              ] 40/75 batches, loss: 0.0119Epoch 6/10: [================              ] 41/75 batches, loss: 0.0119Epoch 6/10: [================              ] 42/75 batches, loss: 0.0117Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0117Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0117Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0115Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0116Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0117Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0116Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0115Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0114Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0115Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0115Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0116Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0116Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0115Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0115Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0114Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0113Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0112Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0112Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0112Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0111Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0111Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0111Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0111Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0113Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0113Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0114Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0114Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0114Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0113Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0113Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0113Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0112Epoch 6/10: [==============================] 75/75 batches, loss: 0.0111
[2025-05-01 11:54:39,937][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0111
[2025-05-01 11:54:40,146][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0157, Metrics: {'mse': 0.01543536689132452, 'rmse': 0.12423915200662197, 'r2': 0.7484355568885803}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0104Epoch 7/10: [                              ] 2/75 batches, loss: 0.0069Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0099Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0103Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0117Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0118Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0113Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0120Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0115Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0118Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0128Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0125Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0120Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0124Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0118Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0120Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0119Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0117Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0115Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0113Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0114Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0115Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0112Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0114Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0112Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0110Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0111Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0110Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0108Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0107Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0108Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0110Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0109Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0108Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0106Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0104Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0104Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0106Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0104Epoch 7/10: [================              ] 40/75 batches, loss: 0.0103Epoch 7/10: [================              ] 41/75 batches, loss: 0.0102Epoch 7/10: [================              ] 42/75 batches, loss: 0.0102Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0101Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0100Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0099Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0098Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0099Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0099Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0098Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0097Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0097Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0097Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0097Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0096Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0095Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0095Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0094Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0094Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0095Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0095Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0095Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0096Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0095Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0096Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0095Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0095Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0094Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0094Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0094Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0095Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0097Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0096Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0097Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0097Epoch 7/10: [==============================] 75/75 batches, loss: 0.0099
[2025-05-01 11:54:48,195][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0099
[2025-05-01 11:54:48,407][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0134, Metrics: {'mse': 0.013211964629590511, 'rmse': 0.11494331050387627, 'r2': 0.7846723794937134}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0102Epoch 8/10: [                              ] 2/75 batches, loss: 0.0085Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0090Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0088Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0095Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0089Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0089Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0091Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0088Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0092Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0092Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0090Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0093Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0091Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0087Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0088Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0088Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0085Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0082Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0082Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0082Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0084Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0082Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0082Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0083Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0082Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0083Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0084Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0083Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0082Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0083Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0083Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0083Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0082Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0082Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0081Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0082Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0081Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0081Epoch 8/10: [================              ] 40/75 batches, loss: 0.0082Epoch 8/10: [================              ] 41/75 batches, loss: 0.0083Epoch 8/10: [================              ] 42/75 batches, loss: 0.0082Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0084Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0085Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0084Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0086Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0086Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0085Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0085Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0084Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0084Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0084Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0085Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0084Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0083Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0082Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0082Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0082Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0082Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0083Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0082Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0084Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0084Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0084Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0083Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0083Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0082Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0082Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0082Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0083Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0082Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0082Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0082Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0082Epoch 8/10: [==============================] 75/75 batches, loss: 0.0081
[2025-05-01 11:54:56,383][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0081
[2025-05-01 11:54:56,599][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0136, Metrics: {'mse': 0.013570292852818966, 'rmse': 0.11649159992385273, 'r2': 0.7788323760032654}
[2025-05-01 11:54:56,600][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0108Epoch 9/10: [                              ] 2/75 batches, loss: 0.0101Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0082Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0081Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0078Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0069Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0075Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0081Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0078Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0077Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0077Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0076Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0076Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0077Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0078Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0081Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0079Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0078Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0079Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0078Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0077Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0077Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0075Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0074Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0073Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0072Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0074Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0076Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0074Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0076Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0076Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0076Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0074Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0075Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0075Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0076Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0078Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0078Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0079Epoch 9/10: [================              ] 40/75 batches, loss: 0.0079Epoch 9/10: [================              ] 41/75 batches, loss: 0.0079Epoch 9/10: [================              ] 42/75 batches, loss: 0.0080Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0079Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0078Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0077Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0077Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0076Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0077Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0076Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0076Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0076Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0076Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0077Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0078Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0078Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0078Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0077Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0077Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0076Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0076Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0076Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0075Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0074Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0074Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0074Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0074Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0074Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0076Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0075Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0076Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0076Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0076Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0075Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0076Epoch 9/10: [==============================] 75/75 batches, loss: 0.0076
[2025-05-01 11:55:04,210][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0076
[2025-05-01 11:55:04,428][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0123, Metrics: {'mse': 0.012036764062941074, 'rmse': 0.10971218739475153, 'r2': 0.8038257360458374}
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0080Epoch 10/10: [                              ] 2/75 batches, loss: 0.0092Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0083Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0081Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0078Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0080Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0074Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0068Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0066Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0074Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0074Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0070Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0069Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0070Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0071Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0069Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0067Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0065Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0064Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0064Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0068Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0068Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0067Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0068Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0067Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0072Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0073Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0073Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0073Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0073Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0072Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0073Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0072Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0073Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0073Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0073Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0073Epoch 10/10: [================              ] 40/75 batches, loss: 0.0074Epoch 10/10: [================              ] 41/75 batches, loss: 0.0074Epoch 10/10: [================              ] 42/75 batches, loss: 0.0074Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0074Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0073Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0072Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0075Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0075Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0075Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0075Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0074Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0074Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0074Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0073Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0073Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0074Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0074Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0074Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0074Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0073Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0072Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0072Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0071Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0070Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0070Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0070Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0070Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0070Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0070Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0070Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0070Epoch 10/10: [==============================] 75/75 batches, loss: 0.0069
[2025-05-01 11:55:12,368][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0069
[2025-05-01 11:55:12,588][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0117, Metrics: {'mse': 0.011497599072754383, 'rmse': 0.10722685798229091, 'r2': 0.8126130104064941}
[2025-05-01 11:55:12,978][src.training.lm_trainer][INFO] - Training completed in 82.33 seconds
[2025-05-01 11:55:12,978][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:55:15,836][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.003721583168953657, 'rmse': 0.06100477988611759, 'r2': 0.9071121215820312}
[2025-05-01 11:55:15,837][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.011497599072754383, 'rmse': 0.10722685798229091, 'r2': 0.8126130104064941}
[2025-05-01 11:55:15,837][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02155463770031929, 'rmse': 0.14681497777924188, 'r2': 0.585953950881958}
[2025-05-01 11:55:17,462][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ja/ja/model.pt
[2025-05-01 11:55:17,467][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▂▁▁▁▁
wandb:     best_val_mse █▄▂▂▂▁▁▁▁
wandb:      best_val_r2 ▁▅▇▇▇████
wandb:    best_val_rmse █▅▃▃▂▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▇▇█████
wandb:       train_loss █▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▂▂▁▁▁▁▁
wandb:          val_mse █▄▂▂▂▁▁▁▁▁
wandb:           val_r2 ▁▅▇▇▇█████
wandb:         val_rmse █▅▃▃▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01169
wandb:     best_val_mse 0.0115
wandb:      best_val_r2 0.81261
wandb:    best_val_rmse 0.10723
wandb:            epoch 10
wandb:   final_test_mse 0.02155
wandb:    final_test_r2 0.58595
wandb:  final_test_rmse 0.14681
wandb:  final_train_mse 0.00372
wandb:   final_train_r2 0.90711
wandb: final_train_rmse 0.061
wandb:    final_val_mse 0.0115
wandb:     final_val_r2 0.81261
wandb:   final_val_rmse 0.10723
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0069
wandb:       train_time 82.33364
wandb:         val_loss 0.01169
wandb:          val_mse 0.0115
wandb:           val_r2 0.81261
wandb:         val_rmse 0.10723
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115340-mqf9gko3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115340-mqf9gko3/logs
Experiment finetune_complexity_ja completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ja/results.json
Running experiment: finetune_question_type_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ko"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:55:30,704][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ko
experiment_name: finetune_question_type_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:55:30,704][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:55:30,704][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:55:30,705][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:55:30,709][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-05-01 11:55:30,710][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:55:32,299][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:55:34,496][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:55:34,496][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:55:34,566][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,592][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,693][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-01 11:55:34,699][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:55:34,699][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-01 11:55:34,700][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:55:34,719][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,747][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,768][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-01 11:55:34,770][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:55:34,770][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-01 11:55:34,771][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:55:34,783][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:55:34,825][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-01 11:55:34,827][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:55:34,827][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-01 11:55:34,828][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-01 11:55:34,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:55:34,828][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:55:34,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:55:34,829][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-05-01 11:55:34,829][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:55:34,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:55:34,830][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:55:34,830][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:55:34,830][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:55:34,830][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:55:34,830][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-01 11:55:34,831][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:55:34,831][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-01 11:55:34,831][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:55:34,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:55:34,831][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:55:34,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:55:38,945][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:55:38,946][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:55:38,946][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:55:38,946][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:55:38,951][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:55:38,952][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:55:38,952][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:55:38,952][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-01 11:55:38,953][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:55:38,953][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/47 batches, loss: 0.8439Epoch 1/10: [=                             ] 2/47 batches, loss: 0.7969Epoch 1/10: [=                             ] 3/47 batches, loss: 0.7732Epoch 1/10: [==                            ] 4/47 batches, loss: 0.7537Epoch 1/10: [===                           ] 5/47 batches, loss: 0.7357Epoch 1/10: [===                           ] 6/47 batches, loss: 0.7438Epoch 1/10: [====                          ] 7/47 batches, loss: 0.7495Epoch 1/10: [=====                         ] 8/47 batches, loss: 0.7644Epoch 1/10: [=====                         ] 9/47 batches, loss: 0.7599Epoch 1/10: [======                        ] 10/47 batches, loss: 0.7622Epoch 1/10: [=======                       ] 11/47 batches, loss: 0.7612Epoch 1/10: [=======                       ] 12/47 batches, loss: 0.7633Epoch 1/10: [========                      ] 13/47 batches, loss: 0.7555Epoch 1/10: [========                      ] 14/47 batches, loss: 0.7554Epoch 1/10: [=========                     ] 15/47 batches, loss: 0.7509Epoch 1/10: [==========                    ] 16/47 batches, loss: 0.7488Epoch 1/10: [==========                    ] 17/47 batches, loss: 0.7452Epoch 1/10: [===========                   ] 18/47 batches, loss: 0.7438Epoch 1/10: [============                  ] 19/47 batches, loss: 0.7378Epoch 1/10: [============                  ] 20/47 batches, loss: 0.7370Epoch 1/10: [=============                 ] 21/47 batches, loss: 0.7378Epoch 1/10: [==============                ] 22/47 batches, loss: 0.7380Epoch 1/10: [==============                ] 23/47 batches, loss: 0.7358Epoch 1/10: [===============               ] 24/47 batches, loss: 0.7340Epoch 1/10: [===============               ] 25/47 batches, loss: 0.7309Epoch 1/10: [================              ] 26/47 batches, loss: 0.7313Epoch 1/10: [=================             ] 27/47 batches, loss: 0.7349Epoch 1/10: [=================             ] 28/47 batches, loss: 0.7352Epoch 1/10: [==================            ] 29/47 batches, loss: 0.7405Epoch 1/10: [===================           ] 30/47 batches, loss: 0.7374Epoch 1/10: [===================           ] 31/47 batches, loss: 0.7368Epoch 1/10: [====================          ] 32/47 batches, loss: 0.7366Epoch 1/10: [=====================         ] 33/47 batches, loss: 0.7385Epoch 1/10: [=====================         ] 34/47 batches, loss: 0.7400Epoch 1/10: [======================        ] 35/47 batches, loss: 0.7399Epoch 1/10: [======================        ] 36/47 batches, loss: 0.7399Epoch 1/10: [=======================       ] 37/47 batches, loss: 0.7400Epoch 1/10: [========================      ] 38/47 batches, loss: 0.7398Epoch 1/10: [========================      ] 39/47 batches, loss: 0.7363Epoch 1/10: [=========================     ] 40/47 batches, loss: 0.7351Epoch 1/10: [==========================    ] 41/47 batches, loss: 0.7341Epoch 1/10: [==========================    ] 42/47 batches, loss: 0.7350Epoch 1/10: [===========================   ] 43/47 batches, loss: 0.7341Epoch 1/10: [============================  ] 44/47 batches, loss: 0.7319Epoch 1/10: [============================  ] 45/47 batches, loss: 0.7304Epoch 1/10: [============================= ] 46/47 batches, loss: 0.7320Epoch 1/10: [==============================] 47/47 batches, loss: 0.7264
[2025-05-01 11:55:46,246][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7264
[2025-05-01 11:55:46,483][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7094, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919, 'precision': 0.8947368421052632, 'recall': 0.9444444444444444}
Epoch 2/10: [Epoch 2/10: [                              ] 1/47 batches, loss: 0.8124Epoch 2/10: [=                             ] 2/47 batches, loss: 0.7483Epoch 2/10: [=                             ] 3/47 batches, loss: 0.7408Epoch 2/10: [==                            ] 4/47 batches, loss: 0.7429Epoch 2/10: [===                           ] 5/47 batches, loss: 0.7521Epoch 2/10: [===                           ] 6/47 batches, loss: 0.7423Epoch 2/10: [====                          ] 7/47 batches, loss: 0.7504Epoch 2/10: [=====                         ] 8/47 batches, loss: 0.7542Epoch 2/10: [=====                         ] 9/47 batches, loss: 0.7509Epoch 2/10: [======                        ] 10/47 batches, loss: 0.7446Epoch 2/10: [=======                       ] 11/47 batches, loss: 0.7375Epoch 2/10: [=======                       ] 12/47 batches, loss: 0.7245Epoch 2/10: [========                      ] 13/47 batches, loss: 0.7275Epoch 2/10: [========                      ] 14/47 batches, loss: 0.7292Epoch 2/10: [=========                     ] 15/47 batches, loss: 0.7210Epoch 2/10: [==========                    ] 16/47 batches, loss: 0.7156Epoch 2/10: [==========                    ] 17/47 batches, loss: 0.7116Epoch 2/10: [===========                   ] 18/47 batches, loss: 0.7116Epoch 2/10: [============                  ] 19/47 batches, loss: 0.7097Epoch 2/10: [============                  ] 20/47 batches, loss: 0.7087Epoch 2/10: [=============                 ] 21/47 batches, loss: 0.7079Epoch 2/10: [==============                ] 22/47 batches, loss: 0.7037Epoch 2/10: [==============                ] 23/47 batches, loss: 0.7035Epoch 2/10: [===============               ] 24/47 batches, loss: 0.6999Epoch 2/10: [===============               ] 25/47 batches, loss: 0.6968Epoch 2/10: [================              ] 26/47 batches, loss: 0.6974Epoch 2/10: [=================             ] 27/47 batches, loss: 0.6966Epoch 2/10: [=================             ] 28/47 batches, loss: 0.6938Epoch 2/10: [==================            ] 29/47 batches, loss: 0.6900Epoch 2/10: [===================           ] 30/47 batches, loss: 0.6909Epoch 2/10: [===================           ] 31/47 batches, loss: 0.6905Epoch 2/10: [====================          ] 32/47 batches, loss: 0.6891Epoch 2/10: [=====================         ] 33/47 batches, loss: 0.6894Epoch 2/10: [=====================         ] 34/47 batches, loss: 0.6871Epoch 2/10: [======================        ] 35/47 batches, loss: 0.6865Epoch 2/10: [======================        ] 36/47 batches, loss: 0.6841Epoch 2/10: [=======================       ] 37/47 batches, loss: 0.6817Epoch 2/10: [========================      ] 38/47 batches, loss: 0.6777Epoch 2/10: [========================      ] 39/47 batches, loss: 0.6761Epoch 2/10: [=========================     ] 40/47 batches, loss: 0.6741Epoch 2/10: [==========================    ] 41/47 batches, loss: 0.6730Epoch 2/10: [==========================    ] 42/47 batches, loss: 0.6716Epoch 2/10: [===========================   ] 43/47 batches, loss: 0.6700Epoch 2/10: [============================  ] 44/47 batches, loss: 0.6684Epoch 2/10: [============================  ] 45/47 batches, loss: 0.6662Epoch 2/10: [============================= ] 46/47 batches, loss: 0.6643Epoch 2/10: [==============================] 47/47 batches, loss: 0.6622
[2025-05-01 11:55:51,652][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6622
[2025-05-01 11:55:51,891][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5669, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.8923076923076924, 'precision': 1.0, 'recall': 0.8055555555555556}
Epoch 3/10: [Epoch 3/10: [                              ] 1/47 batches, loss: 0.6013Epoch 3/10: [=                             ] 2/47 batches, loss: 0.5898Epoch 3/10: [=                             ] 3/47 batches, loss: 0.5729Epoch 3/10: [==                            ] 4/47 batches, loss: 0.5849Epoch 3/10: [===                           ] 5/47 batches, loss: 0.5756Epoch 3/10: [===                           ] 6/47 batches, loss: 0.5865Epoch 3/10: [====                          ] 7/47 batches, loss: 0.5790Epoch 3/10: [=====                         ] 8/47 batches, loss: 0.5789Epoch 3/10: [=====                         ] 9/47 batches, loss: 0.5797Epoch 3/10: [======                        ] 10/47 batches, loss: 0.5782Epoch 3/10: [=======                       ] 11/47 batches, loss: 0.5691Epoch 3/10: [=======                       ] 12/47 batches, loss: 0.5684Epoch 3/10: [========                      ] 13/47 batches, loss: 0.5644Epoch 3/10: [========                      ] 14/47 batches, loss: 0.5651Epoch 3/10: [=========                     ] 15/47 batches, loss: 0.5682Epoch 3/10: [==========                    ] 16/47 batches, loss: 0.5659Epoch 3/10: [==========                    ] 17/47 batches, loss: 0.5618Epoch 3/10: [===========                   ] 18/47 batches, loss: 0.5607Epoch 3/10: [============                  ] 19/47 batches, loss: 0.5540Epoch 3/10: [============                  ] 20/47 batches, loss: 0.5537Epoch 3/10: [=============                 ] 21/47 batches, loss: 0.5522Epoch 3/10: [==============                ] 22/47 batches, loss: 0.5543Epoch 3/10: [==============                ] 23/47 batches, loss: 0.5554Epoch 3/10: [===============               ] 24/47 batches, loss: 0.5545Epoch 3/10: [===============               ] 25/47 batches, loss: 0.5528Epoch 3/10: [================              ] 26/47 batches, loss: 0.5504Epoch 3/10: [=================             ] 27/47 batches, loss: 0.5488Epoch 3/10: [=================             ] 28/47 batches, loss: 0.5482Epoch 3/10: [==================            ] 29/47 batches, loss: 0.5476Epoch 3/10: [===================           ] 30/47 batches, loss: 0.5465Epoch 3/10: [===================           ] 31/47 batches, loss: 0.5446Epoch 3/10: [====================          ] 32/47 batches, loss: 0.5443Epoch 3/10: [=====================         ] 33/47 batches, loss: 0.5404Epoch 3/10: [=====================         ] 34/47 batches, loss: 0.5409Epoch 3/10: [======================        ] 35/47 batches, loss: 0.5393Epoch 3/10: [======================        ] 36/47 batches, loss: 0.5419Epoch 3/10: [=======================       ] 37/47 batches, loss: 0.5422Epoch 3/10: [========================      ] 38/47 batches, loss: 0.5432Epoch 3/10: [========================      ] 39/47 batches, loss: 0.5411Epoch 3/10: [=========================     ] 40/47 batches, loss: 0.5421Epoch 3/10: [==========================    ] 41/47 batches, loss: 0.5426Epoch 3/10: [==========================    ] 42/47 batches, loss: 0.5429Epoch 3/10: [===========================   ] 43/47 batches, loss: 0.5455Epoch 3/10: [============================  ] 44/47 batches, loss: 0.5448Epoch 3/10: [============================  ] 45/47 batches, loss: 0.5432Epoch 3/10: [============================= ] 46/47 batches, loss: 0.5440Epoch 3/10: [==============================] 47/47 batches, loss: 0.5420
[2025-05-01 11:55:57,045][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5420
[2025-05-01 11:55:57,300][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5357, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.8923076923076924, 'precision': 1.0, 'recall': 0.8055555555555556}
Epoch 4/10: [Epoch 4/10: [                              ] 1/47 batches, loss: 0.5816Epoch 4/10: [=                             ] 2/47 batches, loss: 0.5335Epoch 4/10: [=                             ] 3/47 batches, loss: 0.5415Epoch 4/10: [==                            ] 4/47 batches, loss: 0.5486Epoch 4/10: [===                           ] 5/47 batches, loss: 0.5394Epoch 4/10: [===                           ] 6/47 batches, loss: 0.5194Epoch 4/10: [====                          ] 7/47 batches, loss: 0.5309Epoch 4/10: [=====                         ] 8/47 batches, loss: 0.5307Epoch 4/10: [=====                         ] 9/47 batches, loss: 0.5307Epoch 4/10: [======                        ] 10/47 batches, loss: 0.5306Epoch 4/10: [=======                       ] 11/47 batches, loss: 0.5272Epoch 4/10: [=======                       ] 12/47 batches, loss: 0.5360Epoch 4/10: [========                      ] 13/47 batches, loss: 0.5300Epoch 4/10: [========                      ] 14/47 batches, loss: 0.5249Epoch 4/10: [=========                     ] 15/47 batches, loss: 0.5269Epoch 4/10: [==========                    ] 16/47 batches, loss: 0.5276Epoch 4/10: [==========                    ] 17/47 batches, loss: 0.5269Epoch 4/10: [===========                   ] 18/47 batches, loss: 0.5316Epoch 4/10: [============                  ] 19/47 batches, loss: 0.5290Epoch 4/10: [============                  ] 20/47 batches, loss: 0.5281Epoch 4/10: [=============                 ] 21/47 batches, loss: 0.5248Epoch 4/10: [==============                ] 22/47 batches, loss: 0.5228Epoch 4/10: [==============                ] 23/47 batches, loss: 0.5231Epoch 4/10: [===============               ] 24/47 batches, loss: 0.5223Epoch 4/10: [===============               ] 25/47 batches, loss: 0.5216Epoch 4/10: [================              ] 26/47 batches, loss: 0.5265Epoch 4/10: [=================             ] 27/47 batches, loss: 0.5266Epoch 4/10: [=================             ] 28/47 batches, loss: 0.5267Epoch 4/10: [==================            ] 29/47 batches, loss: 0.5292Epoch 4/10: [===================           ] 30/47 batches, loss: 0.5293Epoch 4/10: [===================           ] 31/47 batches, loss: 0.5323Epoch 4/10: [====================          ] 32/47 batches, loss: 0.5315Epoch 4/10: [=====================         ] 33/47 batches, loss: 0.5299Epoch 4/10: [=====================         ] 34/47 batches, loss: 0.5285Epoch 4/10: [======================        ] 35/47 batches, loss: 0.5312Epoch 4/10: [======================        ] 36/47 batches, loss: 0.5312Epoch 4/10: [=======================       ] 37/47 batches, loss: 0.5304Epoch 4/10: [========================      ] 38/47 batches, loss: 0.5310Epoch 4/10: [========================      ] 39/47 batches, loss: 0.5309Epoch 4/10: [=========================     ] 40/47 batches, loss: 0.5297Epoch 4/10: [==========================    ] 41/47 batches, loss: 0.5314Epoch 4/10: [==========================    ] 42/47 batches, loss: 0.5307Epoch 4/10: [===========================   ] 43/47 batches, loss: 0.5306Epoch 4/10: [============================  ] 44/47 batches, loss: 0.5311Epoch 4/10: [============================  ] 45/47 batches, loss: 0.5310Epoch 4/10: [============================= ] 46/47 batches, loss: 0.5310Epoch 4/10: [==============================] 47/47 batches, loss: 0.5290
[2025-05-01 11:56:02,407][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5290
[2025-05-01 11:56:02,674][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5192, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
Epoch 5/10: [Epoch 5/10: [                              ] 1/47 batches, loss: 0.5218Epoch 5/10: [=                             ] 2/47 batches, loss: 0.5606Epoch 5/10: [=                             ] 3/47 batches, loss: 0.5264Epoch 5/10: [==                            ] 4/47 batches, loss: 0.5386Epoch 5/10: [===                           ] 5/47 batches, loss: 0.5223Epoch 5/10: [===                           ] 6/47 batches, loss: 0.5233Epoch 5/10: [====                          ] 7/47 batches, loss: 0.5274Epoch 5/10: [=====                         ] 8/47 batches, loss: 0.5186Epoch 5/10: [=====                         ] 9/47 batches, loss: 0.5197Epoch 5/10: [======                        ] 10/47 batches, loss: 0.5229Epoch 5/10: [=======                       ] 11/47 batches, loss: 0.5190Epoch 5/10: [=======                       ] 12/47 batches, loss: 0.5219Epoch 5/10: [========                      ] 13/47 batches, loss: 0.5224Epoch 5/10: [========                      ] 14/47 batches, loss: 0.5194Epoch 5/10: [=========                     ] 15/47 batches, loss: 0.5196Epoch 5/10: [==========                    ] 16/47 batches, loss: 0.5186Epoch 5/10: [==========                    ] 17/47 batches, loss: 0.5222Epoch 5/10: [===========                   ] 18/47 batches, loss: 0.5160Epoch 5/10: [============                  ] 19/47 batches, loss: 0.5154Epoch 5/10: [============                  ] 20/47 batches, loss: 0.5184Epoch 5/10: [=============                 ] 21/47 batches, loss: 0.5200Epoch 5/10: [==============                ] 22/47 batches, loss: 0.5225Epoch 5/10: [==============                ] 23/47 batches, loss: 0.5279Epoch 5/10: [===============               ] 24/47 batches, loss: 0.5279Epoch 5/10: [===============               ] 25/47 batches, loss: 0.5279Epoch 5/10: [================              ] 26/47 batches, loss: 0.5307Epoch 5/10: [=================             ] 27/47 batches, loss: 0.5288Epoch 5/10: [=================             ] 28/47 batches, loss: 0.5288Epoch 5/10: [==================            ] 29/47 batches, loss: 0.5279Epoch 5/10: [===================           ] 30/47 batches, loss: 0.5256Epoch 5/10: [===================           ] 31/47 batches, loss: 0.5272Epoch 5/10: [====================          ] 32/47 batches, loss: 0.5280Epoch 5/10: [=====================         ] 33/47 batches, loss: 0.5294Epoch 5/10: [=====================         ] 34/47 batches, loss: 0.5294Epoch 5/10: [======================        ] 35/47 batches, loss: 0.5281Epoch 5/10: [======================        ] 36/47 batches, loss: 0.5287Epoch 5/10: [=======================       ] 37/47 batches, loss: 0.5274Epoch 5/10: [========================      ] 38/47 batches, loss: 0.5262Epoch 5/10: [========================      ] 39/47 batches, loss: 0.5250Epoch 5/10: [=========================     ] 40/47 batches, loss: 0.5233Epoch 5/10: [==========================    ] 41/47 batches, loss: 0.5231Epoch 5/10: [==========================    ] 42/47 batches, loss: 0.5238Epoch 5/10: [===========================   ] 43/47 batches, loss: 0.5228Epoch 5/10: [============================  ] 44/47 batches, loss: 0.5213Epoch 5/10: [============================  ] 45/47 batches, loss: 0.5220Epoch 5/10: [============================= ] 46/47 batches, loss: 0.5242Epoch 5/10: [==============================] 47/47 batches, loss: 0.5230
[2025-05-01 11:56:07,800][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5230
[2025-05-01 11:56:08,059][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5201, Metrics: {'accuracy': 0.9583333333333334, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-01 11:56:08,059][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/47 batches, loss: 0.5518Epoch 6/10: [=                             ] 2/47 batches, loss: 0.5173Epoch 6/10: [=                             ] 3/47 batches, loss: 0.5258Epoch 6/10: [==                            ] 4/47 batches, loss: 0.5265Epoch 6/10: [===                           ] 5/47 batches, loss: 0.5362Epoch 6/10: [===                           ] 6/47 batches, loss: 0.5310Epoch 6/10: [====                          ] 7/47 batches, loss: 0.5381Epoch 6/10: [=====                         ] 8/47 batches, loss: 0.5338Epoch 6/10: [=====                         ] 9/47 batches, loss: 0.5337Epoch 6/10: [======                        ] 10/47 batches, loss: 0.5392Epoch 6/10: [=======                       ] 11/47 batches, loss: 0.5398Epoch 6/10: [=======                       ] 12/47 batches, loss: 0.5468Epoch 6/10: [========                      ] 13/47 batches, loss: 0.5436Epoch 6/10: [========                      ] 14/47 batches, loss: 0.5390Epoch 6/10: [=========                     ] 15/47 batches, loss: 0.5367Epoch 6/10: [==========                    ] 16/47 batches, loss: 0.5361Epoch 6/10: [==========                    ] 17/47 batches, loss: 0.5315Epoch 6/10: [===========                   ] 18/47 batches, loss: 0.5300Epoch 6/10: [============                  ] 19/47 batches, loss: 0.5261Epoch 6/10: [============                  ] 20/47 batches, loss: 0.5261Epoch 6/10: [=============                 ] 21/47 batches, loss: 0.5218Epoch 6/10: [==============                ] 22/47 batches, loss: 0.5195Epoch 6/10: [==============                ] 23/47 batches, loss: 0.5188Epoch 6/10: [===============               ] 24/47 batches, loss: 0.5203Epoch 6/10: [===============               ] 25/47 batches, loss: 0.5178Epoch 6/10: [================              ] 26/47 batches, loss: 0.5191Epoch 6/10: [=================             ] 27/47 batches, loss: 0.5168Epoch 6/10: [=================             ] 28/47 batches, loss: 0.5164Epoch 6/10: [==================            ] 29/47 batches, loss: 0.5185Epoch 6/10: [===================           ] 30/47 batches, loss: 0.5206Epoch 6/10: [===================           ] 31/47 batches, loss: 0.5193Epoch 6/10: [====================          ] 32/47 batches, loss: 0.5188Epoch 6/10: [=====================         ] 33/47 batches, loss: 0.5206Epoch 6/10: [=====================         ] 34/47 batches, loss: 0.5222Epoch 6/10: [======================        ] 35/47 batches, loss: 0.5231Epoch 6/10: [======================        ] 36/47 batches, loss: 0.5259Epoch 6/10: [=======================       ] 37/47 batches, loss: 0.5260Epoch 6/10: [========================      ] 38/47 batches, loss: 0.5266Epoch 6/10: [========================      ] 39/47 batches, loss: 0.5273Epoch 6/10: [=========================     ] 40/47 batches, loss: 0.5279Epoch 6/10: [==========================    ] 41/47 batches, loss: 0.5285Epoch 6/10: [==========================    ] 42/47 batches, loss: 0.5285Epoch 6/10: [===========================   ] 43/47 batches, loss: 0.5299Epoch 6/10: [============================  ] 44/47 batches, loss: 0.5287Epoch 6/10: [============================  ] 45/47 batches, loss: 0.5288Epoch 6/10: [============================= ] 46/47 batches, loss: 0.5282Epoch 6/10: [==============================] 47/47 batches, loss: 0.5285
[2025-05-01 11:56:12,830][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5285
[2025-05-01 11:56:13,093][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5350, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9295774647887324, 'precision': 0.9428571428571428, 'recall': 0.9166666666666666}
[2025-05-01 11:56:13,094][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/47 batches, loss: 0.4802Epoch 7/10: [=                             ] 2/47 batches, loss: 0.4925Epoch 7/10: [=                             ] 3/47 batches, loss: 0.4886Epoch 7/10: [==                            ] 4/47 batches, loss: 0.4988Epoch 7/10: [===                           ] 5/47 batches, loss: 0.5001Epoch 7/10: [===                           ] 6/47 batches, loss: 0.5146Epoch 7/10: [====                          ] 7/47 batches, loss: 0.5063Epoch 7/10: [=====                         ] 8/47 batches, loss: 0.5070Epoch 7/10: [=====                         ] 9/47 batches, loss: 0.5067Epoch 7/10: [======                        ] 10/47 batches, loss: 0.5178Epoch 7/10: [=======                       ] 11/47 batches, loss: 0.5035Epoch 7/10: [=======                       ] 12/47 batches, loss: 0.5048Epoch 7/10: [========                      ] 13/47 batches, loss: 0.4993Epoch 7/10: [========                      ] 14/47 batches, loss: 0.4979Epoch 7/10: [=========                     ] 15/47 batches, loss: 0.5015Epoch 7/10: [==========                    ] 16/47 batches, loss: 0.5068Epoch 7/10: [==========                    ] 17/47 batches, loss: 0.5099Epoch 7/10: [===========                   ] 18/47 batches, loss: 0.5138Epoch 7/10: [============                  ] 19/47 batches, loss: 0.5158Epoch 7/10: [============                  ] 20/47 batches, loss: 0.5188Epoch 7/10: [=============                 ] 21/47 batches, loss: 0.5174Epoch 7/10: [==============                ] 22/47 batches, loss: 0.5212Epoch 7/10: [==============                ] 23/47 batches, loss: 0.5204Epoch 7/10: [===============               ] 24/47 batches, loss: 0.5178Epoch 7/10: [===============               ] 25/47 batches, loss: 0.5163Epoch 7/10: [================              ] 26/47 batches, loss: 0.5177Epoch 7/10: [=================             ] 27/47 batches, loss: 0.5215Epoch 7/10: [=================             ] 28/47 batches, loss: 0.5234Epoch 7/10: [==================            ] 29/47 batches, loss: 0.5244Epoch 7/10: [===================           ] 30/47 batches, loss: 0.5223Epoch 7/10: [===================           ] 31/47 batches, loss: 0.5217Epoch 7/10: [====================          ] 32/47 batches, loss: 0.5234Epoch 7/10: [=====================         ] 33/47 batches, loss: 0.5242Epoch 7/10: [=====================         ] 34/47 batches, loss: 0.5257Epoch 7/10: [======================        ] 35/47 batches, loss: 0.5264Epoch 7/10: [======================        ] 36/47 batches, loss: 0.5272Epoch 7/10: [=======================       ] 37/47 batches, loss: 0.5272Epoch 7/10: [========================      ] 38/47 batches, loss: 0.5284Epoch 7/10: [========================      ] 39/47 batches, loss: 0.5284Epoch 7/10: [=========================     ] 40/47 batches, loss: 0.5269Epoch 7/10: [==========================    ] 41/47 batches, loss: 0.5281Epoch 7/10: [==========================    ] 42/47 batches, loss: 0.5275Epoch 7/10: [===========================   ] 43/47 batches, loss: 0.5275Epoch 7/10: [============================  ] 44/47 batches, loss: 0.5270Epoch 7/10: [============================  ] 45/47 batches, loss: 0.5270Epoch 7/10: [============================= ] 46/47 batches, loss: 0.5270Epoch 7/10: [==============================] 47/47 batches, loss: 0.5305
[2025-05-01 11:56:17,855][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5305
[2025-05-01 11:56:18,108][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5270, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9428571428571428, 'precision': 0.9705882352941176, 'recall': 0.9166666666666666}
[2025-05-01 11:56:18,108][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 11:56:18,108][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 7
[2025-05-01 11:56:18,109][src.training.lm_trainer][INFO] - Training completed in 37.51 seconds
[2025-05-01 11:56:18,109][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:56:20,285][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9851150202976996, 'f1': 0.9836065573770492, 'precision': 1.0, 'recall': 0.967741935483871}
[2025-05-01 11:56:20,286][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9583333333333334, 'f1': 0.9565217391304348, 'precision': 1.0, 'recall': 0.9166666666666666}
[2025-05-01 11:56:20,286][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8421052631578947, 'precision': 1.0, 'recall': 0.7272727272727273}
[2025-05-01 11:56:21,968][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ko/ko/model.pt
[2025-05-01 11:56:21,973][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▃▁▁█
wandb:           best_val_f1 ▄▁▁█
wandb:         best_val_loss █▃▂▁
wandb:    best_val_precision ▁███
wandb:       best_val_recall █▁▁▇
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▄▄▄▄
wandb:            train_loss █▆▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▃▁▁██▅▆
wandb:                val_f1 ▄▁▁██▅▇
wandb:              val_loss █▃▂▁▁▂▁
wandb:         val_precision ▁████▄▆
wandb:            val_recall █▁▁▇▇▇▇
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.95833
wandb:           best_val_f1 0.95652
wandb:         best_val_loss 0.51924
wandb:    best_val_precision 1
wandb:       best_val_recall 0.91667
wandb:      early_stop_epoch 7
wandb:                 epoch 7
wandb:   final_test_accuracy 0.86364
wandb:         final_test_f1 0.84211
wandb:  final_test_precision 1
wandb:     final_test_recall 0.72727
wandb:  final_train_accuracy 0.98512
wandb:        final_train_f1 0.98361
wandb: final_train_precision 1
wandb:    final_train_recall 0.96774
wandb:    final_val_accuracy 0.95833
wandb:          final_val_f1 0.95652
wandb:   final_val_precision 1
wandb:      final_val_recall 0.91667
wandb:         learning_rate 2e-05
wandb:            train_loss 0.53053
wandb:            train_time 37.51131
wandb:          val_accuracy 0.94444
wandb:                val_f1 0.94286
wandb:              val_loss 0.52705
wandb:         val_precision 0.97059
wandb:            val_recall 0.91667
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115530-ctruj6oq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115530-ctruj6oq/logs
Experiment finetune_question_type_ko completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ko/results.json
Running experiment: finetune_complexity_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ko"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ko"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:56:33,482][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ko
experiment_name: finetune_complexity_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:56:33,482][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:56:33,482][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:56:33,482][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:56:33,486][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-01 11:56:33,486][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:56:34,819][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:56:37,111][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:56:37,112][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:56:37,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,184][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,274][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-01 11:56:37,280][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:56:37,280][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-01 11:56:37,281][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:56:37,299][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,322][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,335][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-01 11:56:37,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:56:37,337][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-01 11:56:37,338][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:56:37,353][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,378][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:56:37,390][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-01 11:56:37,392][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:56:37,392][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-01 11:56:37,392][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-01 11:56:37,393][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:56:37,393][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:56:37,393][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:56:37,393][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:56:37,393][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:56:37,394][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:56:37,394][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:56:37,394][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:56:37,395][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:56:37,395][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:56:37,395][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-01 11:56:37,395][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-01 11:56:37,396][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-01 11:56:37,396][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:56:37,396][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:56:37,396][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:56:37,397][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:56:41,178][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:56:41,179][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:56:41,179][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:56:41,179][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:56:41,183][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:56:41,184][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:56:41,184][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:56:41,184][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-01 11:56:41,185][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:56:41,185][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/47 batches, loss: 0.2153Epoch 1/10: [=                             ] 2/47 batches, loss: 0.2042Epoch 1/10: [=                             ] 3/47 batches, loss: 0.1880Epoch 1/10: [==                            ] 4/47 batches, loss: 0.1950Epoch 1/10: [===                           ] 5/47 batches, loss: 0.1900Epoch 1/10: [===                           ] 6/47 batches, loss: 0.1866Epoch 1/10: [====                          ] 7/47 batches, loss: 0.1749Epoch 1/10: [=====                         ] 8/47 batches, loss: 0.1730Epoch 1/10: [=====                         ] 9/47 batches, loss: 0.1764Epoch 1/10: [======                        ] 10/47 batches, loss: 0.1716Epoch 1/10: [=======                       ] 11/47 batches, loss: 0.1686Epoch 1/10: [=======                       ] 12/47 batches, loss: 0.1644Epoch 1/10: [========                      ] 13/47 batches, loss: 0.1659Epoch 1/10: [========                      ] 14/47 batches, loss: 0.1621Epoch 1/10: [=========                     ] 15/47 batches, loss: 0.1605Epoch 1/10: [==========                    ] 16/47 batches, loss: 0.1589Epoch 1/10: [==========                    ] 17/47 batches, loss: 0.1582Epoch 1/10: [===========                   ] 18/47 batches, loss: 0.1564Epoch 1/10: [============                  ] 19/47 batches, loss: 0.1546Epoch 1/10: [============                  ] 20/47 batches, loss: 0.1505Epoch 1/10: [=============                 ] 21/47 batches, loss: 0.1461Epoch 1/10: [==============                ] 22/47 batches, loss: 0.1436Epoch 1/10: [==============                ] 23/47 batches, loss: 0.1424Epoch 1/10: [===============               ] 24/47 batches, loss: 0.1391Epoch 1/10: [===============               ] 25/47 batches, loss: 0.1385Epoch 1/10: [================              ] 26/47 batches, loss: 0.1354Epoch 1/10: [=================             ] 27/47 batches, loss: 0.1346Epoch 1/10: [=================             ] 28/47 batches, loss: 0.1337Epoch 1/10: [==================            ] 29/47 batches, loss: 0.1307Epoch 1/10: [===================           ] 30/47 batches, loss: 0.1279Epoch 1/10: [===================           ] 31/47 batches, loss: 0.1255Epoch 1/10: [====================          ] 32/47 batches, loss: 0.1235Epoch 1/10: [=====================         ] 33/47 batches, loss: 0.1207Epoch 1/10: [=====================         ] 34/47 batches, loss: 0.1185Epoch 1/10: [======================        ] 35/47 batches, loss: 0.1173Epoch 1/10: [======================        ] 36/47 batches, loss: 0.1159Epoch 1/10: [=======================       ] 37/47 batches, loss: 0.1157Epoch 1/10: [========================      ] 38/47 batches, loss: 0.1154Epoch 1/10: [========================      ] 39/47 batches, loss: 0.1148Epoch 1/10: [=========================     ] 40/47 batches, loss: 0.1136Epoch 1/10: [==========================    ] 41/47 batches, loss: 0.1122Epoch 1/10: [==========================    ] 42/47 batches, loss: 0.1112Epoch 1/10: [===========================   ] 43/47 batches, loss: 0.1104Epoch 1/10: [============================  ] 44/47 batches, loss: 0.1096Epoch 1/10: [============================  ] 45/47 batches, loss: 0.1082Epoch 1/10: [============================= ] 46/47 batches, loss: 0.1066Epoch 1/10: [==============================] 47/47 batches, loss: 0.1051
[2025-05-01 11:56:48,254][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1051
[2025-05-01 11:56:48,483][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0812, Metrics: {'mse': 0.08214788883924484, 'rmse': 0.2866145300560403, 'r2': -0.7437127828598022}
Epoch 2/10: [Epoch 2/10: [                              ] 1/47 batches, loss: 0.0224Epoch 2/10: [=                             ] 2/47 batches, loss: 0.0327Epoch 2/10: [=                             ] 3/47 batches, loss: 0.0397Epoch 2/10: [==                            ] 4/47 batches, loss: 0.0414Epoch 2/10: [===                           ] 5/47 batches, loss: 0.0379Epoch 2/10: [===                           ] 6/47 batches, loss: 0.0392Epoch 2/10: [====                          ] 7/47 batches, loss: 0.0396Epoch 2/10: [=====                         ] 8/47 batches, loss: 0.0400Epoch 2/10: [=====                         ] 9/47 batches, loss: 0.0387Epoch 2/10: [======                        ] 10/47 batches, loss: 0.0414Epoch 2/10: [=======                       ] 11/47 batches, loss: 0.0424Epoch 2/10: [=======                       ] 12/47 batches, loss: 0.0421Epoch 2/10: [========                      ] 13/47 batches, loss: 0.0413Epoch 2/10: [========                      ] 14/47 batches, loss: 0.0402Epoch 2/10: [=========                     ] 15/47 batches, loss: 0.0411Epoch 2/10: [==========                    ] 16/47 batches, loss: 0.0402Epoch 2/10: [==========                    ] 17/47 batches, loss: 0.0386Epoch 2/10: [===========                   ] 18/47 batches, loss: 0.0390Epoch 2/10: [============                  ] 19/47 batches, loss: 0.0384Epoch 2/10: [============                  ] 20/47 batches, loss: 0.0374Epoch 2/10: [=============                 ] 21/47 batches, loss: 0.0374Epoch 2/10: [==============                ] 22/47 batches, loss: 0.0368Epoch 2/10: [==============                ] 23/47 batches, loss: 0.0362Epoch 2/10: [===============               ] 24/47 batches, loss: 0.0357Epoch 2/10: [===============               ] 25/47 batches, loss: 0.0353Epoch 2/10: [================              ] 26/47 batches, loss: 0.0347Epoch 2/10: [=================             ] 27/47 batches, loss: 0.0344Epoch 2/10: [=================             ] 28/47 batches, loss: 0.0342Epoch 2/10: [==================            ] 29/47 batches, loss: 0.0335Epoch 2/10: [===================           ] 30/47 batches, loss: 0.0333Epoch 2/10: [===================           ] 31/47 batches, loss: 0.0328Epoch 2/10: [====================          ] 32/47 batches, loss: 0.0323Epoch 2/10: [=====================         ] 33/47 batches, loss: 0.0318Epoch 2/10: [=====================         ] 34/47 batches, loss: 0.0319Epoch 2/10: [======================        ] 35/47 batches, loss: 0.0315Epoch 2/10: [======================        ] 36/47 batches, loss: 0.0310Epoch 2/10: [=======================       ] 37/47 batches, loss: 0.0308Epoch 2/10: [========================      ] 38/47 batches, loss: 0.0304Epoch 2/10: [========================      ] 39/47 batches, loss: 0.0301Epoch 2/10: [=========================     ] 40/47 batches, loss: 0.0298Epoch 2/10: [==========================    ] 41/47 batches, loss: 0.0297Epoch 2/10: [==========================    ] 42/47 batches, loss: 0.0296Epoch 2/10: [===========================   ] 43/47 batches, loss: 0.0296Epoch 2/10: [============================  ] 44/47 batches, loss: 0.0295Epoch 2/10: [============================  ] 45/47 batches, loss: 0.0292Epoch 2/10: [============================= ] 46/47 batches, loss: 0.0290Epoch 2/10: [==============================] 47/47 batches, loss: 0.0287
[2025-05-01 11:56:53,642][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0287
[2025-05-01 11:56:53,881][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0479, Metrics: {'mse': 0.050320252776145935, 'rmse': 0.22432176170881402, 'r2': -0.0681232213973999}
Epoch 3/10: [Epoch 3/10: [                              ] 1/47 batches, loss: 0.0171Epoch 3/10: [=                             ] 2/47 batches, loss: 0.0158Epoch 3/10: [=                             ] 3/47 batches, loss: 0.0169Epoch 3/10: [==                            ] 4/47 batches, loss: 0.0197Epoch 3/10: [===                           ] 5/47 batches, loss: 0.0199Epoch 3/10: [===                           ] 6/47 batches, loss: 0.0216Epoch 3/10: [====                          ] 7/47 batches, loss: 0.0237Epoch 3/10: [=====                         ] 8/47 batches, loss: 0.0245Epoch 3/10: [=====                         ] 9/47 batches, loss: 0.0239Epoch 3/10: [======                        ] 10/47 batches, loss: 0.0246Epoch 3/10: [=======                       ] 11/47 batches, loss: 0.0239Epoch 3/10: [=======                       ] 12/47 batches, loss: 0.0273Epoch 3/10: [========                      ] 13/47 batches, loss: 0.0270Epoch 3/10: [========                      ] 14/47 batches, loss: 0.0263Epoch 3/10: [=========                     ] 15/47 batches, loss: 0.0259Epoch 3/10: [==========                    ] 16/47 batches, loss: 0.0249Epoch 3/10: [==========                    ] 17/47 batches, loss: 0.0245Epoch 3/10: [===========                   ] 18/47 batches, loss: 0.0242Epoch 3/10: [============                  ] 19/47 batches, loss: 0.0235Epoch 3/10: [============                  ] 20/47 batches, loss: 0.0230Epoch 3/10: [=============                 ] 21/47 batches, loss: 0.0228Epoch 3/10: [==============                ] 22/47 batches, loss: 0.0223Epoch 3/10: [==============                ] 23/47 batches, loss: 0.0219Epoch 3/10: [===============               ] 24/47 batches, loss: 0.0216Epoch 3/10: [===============               ] 25/47 batches, loss: 0.0214Epoch 3/10: [================              ] 26/47 batches, loss: 0.0213Epoch 3/10: [=================             ] 27/47 batches, loss: 0.0209Epoch 3/10: [=================             ] 28/47 batches, loss: 0.0215Epoch 3/10: [==================            ] 29/47 batches, loss: 0.0217Epoch 3/10: [===================           ] 30/47 batches, loss: 0.0214Epoch 3/10: [===================           ] 31/47 batches, loss: 0.0214Epoch 3/10: [====================          ] 32/47 batches, loss: 0.0214Epoch 3/10: [=====================         ] 33/47 batches, loss: 0.0211Epoch 3/10: [=====================         ] 34/47 batches, loss: 0.0208Epoch 3/10: [======================        ] 35/47 batches, loss: 0.0204Epoch 3/10: [======================        ] 36/47 batches, loss: 0.0205Epoch 3/10: [=======================       ] 37/47 batches, loss: 0.0203Epoch 3/10: [========================      ] 38/47 batches, loss: 0.0202Epoch 3/10: [========================      ] 39/47 batches, loss: 0.0199Epoch 3/10: [=========================     ] 40/47 batches, loss: 0.0197Epoch 3/10: [==========================    ] 41/47 batches, loss: 0.0197Epoch 3/10: [==========================    ] 42/47 batches, loss: 0.0197Epoch 3/10: [===========================   ] 43/47 batches, loss: 0.0196Epoch 3/10: [============================  ] 44/47 batches, loss: 0.0194Epoch 3/10: [============================  ] 45/47 batches, loss: 0.0194Epoch 3/10: [============================= ] 46/47 batches, loss: 0.0192Epoch 3/10: [==============================] 47/47 batches, loss: 0.0194
[2025-05-01 11:56:59,061][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0194
[2025-05-01 11:56:59,306][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0217, Metrics: {'mse': 0.022335175424814224, 'rmse': 0.14944957485658575, 'r2': 0.525902271270752}
Epoch 4/10: [Epoch 4/10: [                              ] 1/47 batches, loss: 0.0136Epoch 4/10: [=                             ] 2/47 batches, loss: 0.0191Epoch 4/10: [=                             ] 3/47 batches, loss: 0.0175Epoch 4/10: [==                            ] 4/47 batches, loss: 0.0154Epoch 4/10: [===                           ] 5/47 batches, loss: 0.0137Epoch 4/10: [===                           ] 6/47 batches, loss: 0.0133Epoch 4/10: [====                          ] 7/47 batches, loss: 0.0129Epoch 4/10: [=====                         ] 8/47 batches, loss: 0.0135Epoch 4/10: [=====                         ] 9/47 batches, loss: 0.0140Epoch 4/10: [======                        ] 10/47 batches, loss: 0.0142Epoch 4/10: [=======                       ] 11/47 batches, loss: 0.0148Epoch 4/10: [=======                       ] 12/47 batches, loss: 0.0144Epoch 4/10: [========                      ] 13/47 batches, loss: 0.0152Epoch 4/10: [========                      ] 14/47 batches, loss: 0.0149Epoch 4/10: [=========                     ] 15/47 batches, loss: 0.0145Epoch 4/10: [==========                    ] 16/47 batches, loss: 0.0147Epoch 4/10: [==========                    ] 17/47 batches, loss: 0.0148Epoch 4/10: [===========                   ] 18/47 batches, loss: 0.0145Epoch 4/10: [============                  ] 19/47 batches, loss: 0.0143Epoch 4/10: [============                  ] 20/47 batches, loss: 0.0151Epoch 4/10: [=============                 ] 21/47 batches, loss: 0.0151Epoch 4/10: [==============                ] 22/47 batches, loss: 0.0155Epoch 4/10: [==============                ] 23/47 batches, loss: 0.0150Epoch 4/10: [===============               ] 24/47 batches, loss: 0.0151Epoch 4/10: [===============               ] 25/47 batches, loss: 0.0148Epoch 4/10: [================              ] 26/47 batches, loss: 0.0145Epoch 4/10: [=================             ] 27/47 batches, loss: 0.0146Epoch 4/10: [=================             ] 28/47 batches, loss: 0.0145Epoch 4/10: [==================            ] 29/47 batches, loss: 0.0143Epoch 4/10: [===================           ] 30/47 batches, loss: 0.0145Epoch 4/10: [===================           ] 31/47 batches, loss: 0.0143Epoch 4/10: [====================          ] 32/47 batches, loss: 0.0141Epoch 4/10: [=====================         ] 33/47 batches, loss: 0.0141Epoch 4/10: [=====================         ] 34/47 batches, loss: 0.0141Epoch 4/10: [======================        ] 35/47 batches, loss: 0.0139Epoch 4/10: [======================        ] 36/47 batches, loss: 0.0138Epoch 4/10: [=======================       ] 37/47 batches, loss: 0.0139Epoch 4/10: [========================      ] 38/47 batches, loss: 0.0142Epoch 4/10: [========================      ] 39/47 batches, loss: 0.0143Epoch 4/10: [=========================     ] 40/47 batches, loss: 0.0142Epoch 4/10: [==========================    ] 41/47 batches, loss: 0.0141Epoch 4/10: [==========================    ] 42/47 batches, loss: 0.0140Epoch 4/10: [===========================   ] 43/47 batches, loss: 0.0140Epoch 4/10: [============================  ] 44/47 batches, loss: 0.0142Epoch 4/10: [============================  ] 45/47 batches, loss: 0.0141Epoch 4/10: [============================= ] 46/47 batches, loss: 0.0142Epoch 4/10: [==============================] 47/47 batches, loss: 0.0144
[2025-05-01 11:57:04,413][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0144
[2025-05-01 11:57:04,656][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0205, Metrics: {'mse': 0.02122644893825054, 'rmse': 0.1456929955016731, 'r2': 0.5494365692138672}
Epoch 5/10: [Epoch 5/10: [                              ] 1/47 batches, loss: 0.0115Epoch 5/10: [=                             ] 2/47 batches, loss: 0.0107Epoch 5/10: [=                             ] 3/47 batches, loss: 0.0105Epoch 5/10: [==                            ] 4/47 batches, loss: 0.0100Epoch 5/10: [===                           ] 5/47 batches, loss: 0.0103Epoch 5/10: [===                           ] 6/47 batches, loss: 0.0101Epoch 5/10: [====                          ] 7/47 batches, loss: 0.0094Epoch 5/10: [=====                         ] 8/47 batches, loss: 0.0097Epoch 5/10: [=====                         ] 9/47 batches, loss: 0.0110Epoch 5/10: [======                        ] 10/47 batches, loss: 0.0110Epoch 5/10: [=======                       ] 11/47 batches, loss: 0.0127Epoch 5/10: [=======                       ] 12/47 batches, loss: 0.0124Epoch 5/10: [========                      ] 13/47 batches, loss: 0.0129Epoch 5/10: [========                      ] 14/47 batches, loss: 0.0131Epoch 5/10: [=========                     ] 15/47 batches, loss: 0.0133Epoch 5/10: [==========                    ] 16/47 batches, loss: 0.0134Epoch 5/10: [==========                    ] 17/47 batches, loss: 0.0132Epoch 5/10: [===========                   ] 18/47 batches, loss: 0.0134Epoch 5/10: [============                  ] 19/47 batches, loss: 0.0135Epoch 5/10: [============                  ] 20/47 batches, loss: 0.0134Epoch 5/10: [=============                 ] 21/47 batches, loss: 0.0135Epoch 5/10: [==============                ] 22/47 batches, loss: 0.0135Epoch 5/10: [==============                ] 23/47 batches, loss: 0.0134Epoch 5/10: [===============               ] 24/47 batches, loss: 0.0131Epoch 5/10: [===============               ] 25/47 batches, loss: 0.0130Epoch 5/10: [================              ] 26/47 batches, loss: 0.0131Epoch 5/10: [=================             ] 27/47 batches, loss: 0.0134Epoch 5/10: [=================             ] 28/47 batches, loss: 0.0133Epoch 5/10: [==================            ] 29/47 batches, loss: 0.0131Epoch 5/10: [===================           ] 30/47 batches, loss: 0.0130Epoch 5/10: [===================           ] 31/47 batches, loss: 0.0129Epoch 5/10: [====================          ] 32/47 batches, loss: 0.0130Epoch 5/10: [=====================         ] 33/47 batches, loss: 0.0130Epoch 5/10: [=====================         ] 34/47 batches, loss: 0.0129Epoch 5/10: [======================        ] 35/47 batches, loss: 0.0128Epoch 5/10: [======================        ] 36/47 batches, loss: 0.0126Epoch 5/10: [=======================       ] 37/47 batches, loss: 0.0124Epoch 5/10: [========================      ] 38/47 batches, loss: 0.0122Epoch 5/10: [========================      ] 39/47 batches, loss: 0.0122Epoch 5/10: [=========================     ] 40/47 batches, loss: 0.0121Epoch 5/10: [==========================    ] 41/47 batches, loss: 0.0119Epoch 5/10: [==========================    ] 42/47 batches, loss: 0.0118Epoch 5/10: [===========================   ] 43/47 batches, loss: 0.0118Epoch 5/10: [============================  ] 44/47 batches, loss: 0.0119Epoch 5/10: [============================  ] 45/47 batches, loss: 0.0118Epoch 5/10: [============================= ] 46/47 batches, loss: 0.0118Epoch 5/10: [==============================] 47/47 batches, loss: 0.0118
[2025-05-01 11:57:09,775][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0118
[2025-05-01 11:57:10,018][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0190, Metrics: {'mse': 0.019871346652507782, 'rmse': 0.14096576411493603, 'r2': 0.5782006978988647}
Epoch 6/10: [Epoch 6/10: [                              ] 1/47 batches, loss: 0.0041Epoch 6/10: [=                             ] 2/47 batches, loss: 0.0079Epoch 6/10: [=                             ] 3/47 batches, loss: 0.0102Epoch 6/10: [==                            ] 4/47 batches, loss: 0.0107Epoch 6/10: [===                           ] 5/47 batches, loss: 0.0100Epoch 6/10: [===                           ] 6/47 batches, loss: 0.0106Epoch 6/10: [====                          ] 7/47 batches, loss: 0.0111Epoch 6/10: [=====                         ] 8/47 batches, loss: 0.0111Epoch 6/10: [=====                         ] 9/47 batches, loss: 0.0105Epoch 6/10: [======                        ] 10/47 batches, loss: 0.0102Epoch 6/10: [=======                       ] 11/47 batches, loss: 0.0104Epoch 6/10: [=======                       ] 12/47 batches, loss: 0.0112Epoch 6/10: [========                      ] 13/47 batches, loss: 0.0109Epoch 6/10: [========                      ] 14/47 batches, loss: 0.0111Epoch 6/10: [=========                     ] 15/47 batches, loss: 0.0107Epoch 6/10: [==========                    ] 16/47 batches, loss: 0.0106Epoch 6/10: [==========                    ] 17/47 batches, loss: 0.0108Epoch 6/10: [===========                   ] 18/47 batches, loss: 0.0108Epoch 6/10: [============                  ] 19/47 batches, loss: 0.0106Epoch 6/10: [============                  ] 20/47 batches, loss: 0.0106Epoch 6/10: [=============                 ] 21/47 batches, loss: 0.0105Epoch 6/10: [==============                ] 22/47 batches, loss: 0.0103Epoch 6/10: [==============                ] 23/47 batches, loss: 0.0108Epoch 6/10: [===============               ] 24/47 batches, loss: 0.0106Epoch 6/10: [===============               ] 25/47 batches, loss: 0.0106Epoch 6/10: [================              ] 26/47 batches, loss: 0.0105Epoch 6/10: [=================             ] 27/47 batches, loss: 0.0105Epoch 6/10: [=================             ] 28/47 batches, loss: 0.0105Epoch 6/10: [==================            ] 29/47 batches, loss: 0.0103Epoch 6/10: [===================           ] 30/47 batches, loss: 0.0102Epoch 6/10: [===================           ] 31/47 batches, loss: 0.0100Epoch 6/10: [====================          ] 32/47 batches, loss: 0.0099Epoch 6/10: [=====================         ] 33/47 batches, loss: 0.0099Epoch 6/10: [=====================         ] 34/47 batches, loss: 0.0100Epoch 6/10: [======================        ] 35/47 batches, loss: 0.0099Epoch 6/10: [======================        ] 36/47 batches, loss: 0.0100Epoch 6/10: [=======================       ] 37/47 batches, loss: 0.0099Epoch 6/10: [========================      ] 38/47 batches, loss: 0.0098Epoch 6/10: [========================      ] 39/47 batches, loss: 0.0099Epoch 6/10: [=========================     ] 40/47 batches, loss: 0.0099Epoch 6/10: [==========================    ] 41/47 batches, loss: 0.0098Epoch 6/10: [==========================    ] 42/47 batches, loss: 0.0098Epoch 6/10: [===========================   ] 43/47 batches, loss: 0.0097Epoch 6/10: [============================  ] 44/47 batches, loss: 0.0097Epoch 6/10: [============================  ] 45/47 batches, loss: 0.0097Epoch 6/10: [============================= ] 46/47 batches, loss: 0.0097Epoch 6/10: [==============================] 47/47 batches, loss: 0.0096
[2025-05-01 11:57:15,147][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0096
[2025-05-01 11:57:15,396][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0162, Metrics: {'mse': 0.016976628452539444, 'rmse': 0.1302943914853569, 'r2': 0.6396454572677612}
Epoch 7/10: [Epoch 7/10: [                              ] 1/47 batches, loss: 0.0158Epoch 7/10: [=                             ] 2/47 batches, loss: 0.0131Epoch 7/10: [=                             ] 3/47 batches, loss: 0.0100Epoch 7/10: [==                            ] 4/47 batches, loss: 0.0094Epoch 7/10: [===                           ] 5/47 batches, loss: 0.0092Epoch 7/10: [===                           ] 6/47 batches, loss: 0.0112Epoch 7/10: [====                          ] 7/47 batches, loss: 0.0111Epoch 7/10: [=====                         ] 8/47 batches, loss: 0.0124Epoch 7/10: [=====                         ] 9/47 batches, loss: 0.0122Epoch 7/10: [======                        ] 10/47 batches, loss: 0.0124Epoch 7/10: [=======                       ] 11/47 batches, loss: 0.0122Epoch 7/10: [=======                       ] 12/47 batches, loss: 0.0117Epoch 7/10: [========                      ] 13/47 batches, loss: 0.0113Epoch 7/10: [========                      ] 14/47 batches, loss: 0.0116Epoch 7/10: [=========                     ] 15/47 batches, loss: 0.0116Epoch 7/10: [==========                    ] 16/47 batches, loss: 0.0115Epoch 7/10: [==========                    ] 17/47 batches, loss: 0.0112Epoch 7/10: [===========                   ] 18/47 batches, loss: 0.0111Epoch 7/10: [============                  ] 19/47 batches, loss: 0.0111Epoch 7/10: [============                  ] 20/47 batches, loss: 0.0110Epoch 7/10: [=============                 ] 21/47 batches, loss: 0.0110Epoch 7/10: [==============                ] 22/47 batches, loss: 0.0110Epoch 7/10: [==============                ] 23/47 batches, loss: 0.0111Epoch 7/10: [===============               ] 24/47 batches, loss: 0.0115Epoch 7/10: [===============               ] 25/47 batches, loss: 0.0115Epoch 7/10: [================              ] 26/47 batches, loss: 0.0115Epoch 7/10: [=================             ] 27/47 batches, loss: 0.0114Epoch 7/10: [=================             ] 28/47 batches, loss: 0.0111Epoch 7/10: [==================            ] 29/47 batches, loss: 0.0109Epoch 7/10: [===================           ] 30/47 batches, loss: 0.0109Epoch 7/10: [===================           ] 31/47 batches, loss: 0.0111Epoch 7/10: [====================          ] 32/47 batches, loss: 0.0112Epoch 7/10: [=====================         ] 33/47 batches, loss: 0.0111Epoch 7/10: [=====================         ] 34/47 batches, loss: 0.0109Epoch 7/10: [======================        ] 35/47 batches, loss: 0.0109Epoch 7/10: [======================        ] 36/47 batches, loss: 0.0109Epoch 7/10: [=======================       ] 37/47 batches, loss: 0.0108Epoch 7/10: [========================      ] 38/47 batches, loss: 0.0106Epoch 7/10: [========================      ] 39/47 batches, loss: 0.0106Epoch 7/10: [=========================     ] 40/47 batches, loss: 0.0106Epoch 7/10: [==========================    ] 41/47 batches, loss: 0.0105Epoch 7/10: [==========================    ] 42/47 batches, loss: 0.0104Epoch 7/10: [===========================   ] 43/47 batches, loss: 0.0103Epoch 7/10: [============================  ] 44/47 batches, loss: 0.0103Epoch 7/10: [============================  ] 45/47 batches, loss: 0.0102Epoch 7/10: [============================= ] 46/47 batches, loss: 0.0102Epoch 7/10: [==============================] 47/47 batches, loss: 0.0102
[2025-05-01 11:57:20,523][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0102
[2025-05-01 11:57:20,775][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0166, Metrics: {'mse': 0.017716139554977417, 'rmse': 0.1331019892975962, 'r2': 0.6239482164382935}
[2025-05-01 11:57:20,776][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/47 batches, loss: 0.0049Epoch 8/10: [=                             ] 2/47 batches, loss: 0.0064Epoch 8/10: [=                             ] 3/47 batches, loss: 0.0068Epoch 8/10: [==                            ] 4/47 batches, loss: 0.0067Epoch 8/10: [===                           ] 5/47 batches, loss: 0.0081Epoch 8/10: [===                           ] 6/47 batches, loss: 0.0085Epoch 8/10: [====                          ] 7/47 batches, loss: 0.0078Epoch 8/10: [=====                         ] 8/47 batches, loss: 0.0077Epoch 8/10: [=====                         ] 9/47 batches, loss: 0.0078Epoch 8/10: [======                        ] 10/47 batches, loss: 0.0078Epoch 8/10: [=======                       ] 11/47 batches, loss: 0.0080Epoch 8/10: [=======                       ] 12/47 batches, loss: 0.0078Epoch 8/10: [========                      ] 13/47 batches, loss: 0.0083Epoch 8/10: [========                      ] 14/47 batches, loss: 0.0086Epoch 8/10: [=========                     ] 15/47 batches, loss: 0.0085Epoch 8/10: [==========                    ] 16/47 batches, loss: 0.0089Epoch 8/10: [==========                    ] 17/47 batches, loss: 0.0086Epoch 8/10: [===========                   ] 18/47 batches, loss: 0.0087Epoch 8/10: [============                  ] 19/47 batches, loss: 0.0092Epoch 8/10: [============                  ] 20/47 batches, loss: 0.0091Epoch 8/10: [=============                 ] 21/47 batches, loss: 0.0090Epoch 8/10: [==============                ] 22/47 batches, loss: 0.0089Epoch 8/10: [==============                ] 23/47 batches, loss: 0.0089Epoch 8/10: [===============               ] 24/47 batches, loss: 0.0091Epoch 8/10: [===============               ] 25/47 batches, loss: 0.0091Epoch 8/10: [================              ] 26/47 batches, loss: 0.0092Epoch 8/10: [=================             ] 27/47 batches, loss: 0.0092Epoch 8/10: [=================             ] 28/47 batches, loss: 0.0090Epoch 8/10: [==================            ] 29/47 batches, loss: 0.0090Epoch 8/10: [===================           ] 30/47 batches, loss: 0.0091Epoch 8/10: [===================           ] 31/47 batches, loss: 0.0090Epoch 8/10: [====================          ] 32/47 batches, loss: 0.0091Epoch 8/10: [=====================         ] 33/47 batches, loss: 0.0092Epoch 8/10: [=====================         ] 34/47 batches, loss: 0.0091Epoch 8/10: [======================        ] 35/47 batches, loss: 0.0093Epoch 8/10: [======================        ] 36/47 batches, loss: 0.0095Epoch 8/10: [=======================       ] 37/47 batches, loss: 0.0094Epoch 8/10: [========================      ] 38/47 batches, loss: 0.0092Epoch 8/10: [========================      ] 39/47 batches, loss: 0.0092Epoch 8/10: [=========================     ] 40/47 batches, loss: 0.0094Epoch 8/10: [==========================    ] 41/47 batches, loss: 0.0094Epoch 8/10: [==========================    ] 42/47 batches, loss: 0.0096Epoch 8/10: [===========================   ] 43/47 batches, loss: 0.0095Epoch 8/10: [============================  ] 44/47 batches, loss: 0.0095Epoch 8/10: [============================  ] 45/47 batches, loss: 0.0094Epoch 8/10: [============================= ] 46/47 batches, loss: 0.0094Epoch 8/10: [==============================] 47/47 batches, loss: 0.0094
[2025-05-01 11:57:25,570][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0094
[2025-05-01 11:57:25,812][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0227, Metrics: {'mse': 0.024566398933529854, 'rmse': 0.1567367185235478, 'r2': 0.47854113578796387}
[2025-05-01 11:57:25,813][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/47 batches, loss: 0.0086Epoch 9/10: [=                             ] 2/47 batches, loss: 0.0081Epoch 9/10: [=                             ] 3/47 batches, loss: 0.0101Epoch 9/10: [==                            ] 4/47 batches, loss: 0.0100Epoch 9/10: [===                           ] 5/47 batches, loss: 0.0102Epoch 9/10: [===                           ] 6/47 batches, loss: 0.0109Epoch 9/10: [====                          ] 7/47 batches, loss: 0.0116Epoch 9/10: [=====                         ] 8/47 batches, loss: 0.0110Epoch 9/10: [=====                         ] 9/47 batches, loss: 0.0104Epoch 9/10: [======                        ] 10/47 batches, loss: 0.0103Epoch 9/10: [=======                       ] 11/47 batches, loss: 0.0100Epoch 9/10: [=======                       ] 12/47 batches, loss: 0.0096Epoch 9/10: [========                      ] 13/47 batches, loss: 0.0095Epoch 9/10: [========                      ] 14/47 batches, loss: 0.0092Epoch 9/10: [=========                     ] 15/47 batches, loss: 0.0102Epoch 9/10: [==========                    ] 16/47 batches, loss: 0.0105Epoch 9/10: [==========                    ] 17/47 batches, loss: 0.0106Epoch 9/10: [===========                   ] 18/47 batches, loss: 0.0105Epoch 9/10: [============                  ] 19/47 batches, loss: 0.0102Epoch 9/10: [============                  ] 20/47 batches, loss: 0.0100Epoch 9/10: [=============                 ] 21/47 batches, loss: 0.0100Epoch 9/10: [==============                ] 22/47 batches, loss: 0.0097Epoch 9/10: [==============                ] 23/47 batches, loss: 0.0096Epoch 9/10: [===============               ] 24/47 batches, loss: 0.0094Epoch 9/10: [===============               ] 25/47 batches, loss: 0.0092Epoch 9/10: [================              ] 26/47 batches, loss: 0.0093Epoch 9/10: [=================             ] 27/47 batches, loss: 0.0091Epoch 9/10: [=================             ] 28/47 batches, loss: 0.0090Epoch 9/10: [==================            ] 29/47 batches, loss: 0.0088Epoch 9/10: [===================           ] 30/47 batches, loss: 0.0087Epoch 9/10: [===================           ] 31/47 batches, loss: 0.0087Epoch 9/10: [====================          ] 32/47 batches, loss: 0.0086Epoch 9/10: [=====================         ] 33/47 batches, loss: 0.0086Epoch 9/10: [=====================         ] 34/47 batches, loss: 0.0085Epoch 9/10: [======================        ] 35/47 batches, loss: 0.0084Epoch 9/10: [======================        ] 36/47 batches, loss: 0.0084Epoch 9/10: [=======================       ] 37/47 batches, loss: 0.0083Epoch 9/10: [========================      ] 38/47 batches, loss: 0.0083Epoch 9/10: [========================      ] 39/47 batches, loss: 0.0082Epoch 9/10: [=========================     ] 40/47 batches, loss: 0.0082Epoch 9/10: [==========================    ] 41/47 batches, loss: 0.0081Epoch 9/10: [==========================    ] 42/47 batches, loss: 0.0080Epoch 9/10: [===========================   ] 43/47 batches, loss: 0.0079Epoch 9/10: [============================  ] 44/47 batches, loss: 0.0079Epoch 9/10: [============================  ] 45/47 batches, loss: 0.0079Epoch 9/10: [============================= ] 46/47 batches, loss: 0.0080Epoch 9/10: [==============================] 47/47 batches, loss: 0.0079
[2025-05-01 11:57:30,574][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0079
[2025-05-01 11:57:30,811][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0148, Metrics: {'mse': 0.015748990699648857, 'rmse': 0.12549498276683757, 'r2': 0.6657038927078247}
Epoch 10/10: [Epoch 10/10: [                              ] 1/47 batches, loss: 0.0049Epoch 10/10: [=                             ] 2/47 batches, loss: 0.0080Epoch 10/10: [=                             ] 3/47 batches, loss: 0.0074Epoch 10/10: [==                            ] 4/47 batches, loss: 0.0065Epoch 10/10: [===                           ] 5/47 batches, loss: 0.0091Epoch 10/10: [===                           ] 6/47 batches, loss: 0.0083Epoch 10/10: [====                          ] 7/47 batches, loss: 0.0075Epoch 10/10: [=====                         ] 8/47 batches, loss: 0.0076Epoch 10/10: [=====                         ] 9/47 batches, loss: 0.0087Epoch 10/10: [======                        ] 10/47 batches, loss: 0.0089Epoch 10/10: [=======                       ] 11/47 batches, loss: 0.0090Epoch 10/10: [=======                       ] 12/47 batches, loss: 0.0090Epoch 10/10: [========                      ] 13/47 batches, loss: 0.0086Epoch 10/10: [========                      ] 14/47 batches, loss: 0.0084Epoch 10/10: [=========                     ] 15/47 batches, loss: 0.0083Epoch 10/10: [==========                    ] 16/47 batches, loss: 0.0086Epoch 10/10: [==========                    ] 17/47 batches, loss: 0.0084Epoch 10/10: [===========                   ] 18/47 batches, loss: 0.0083Epoch 10/10: [============                  ] 19/47 batches, loss: 0.0082Epoch 10/10: [============                  ] 20/47 batches, loss: 0.0080Epoch 10/10: [=============                 ] 21/47 batches, loss: 0.0080Epoch 10/10: [==============                ] 22/47 batches, loss: 0.0079Epoch 10/10: [==============                ] 23/47 batches, loss: 0.0079Epoch 10/10: [===============               ] 24/47 batches, loss: 0.0077Epoch 10/10: [===============               ] 25/47 batches, loss: 0.0078Epoch 10/10: [================              ] 26/47 batches, loss: 0.0077Epoch 10/10: [=================             ] 27/47 batches, loss: 0.0077Epoch 10/10: [=================             ] 28/47 batches, loss: 0.0076Epoch 10/10: [==================            ] 29/47 batches, loss: 0.0076Epoch 10/10: [===================           ] 30/47 batches, loss: 0.0076Epoch 10/10: [===================           ] 31/47 batches, loss: 0.0076Epoch 10/10: [====================          ] 32/47 batches, loss: 0.0075Epoch 10/10: [=====================         ] 33/47 batches, loss: 0.0074Epoch 10/10: [=====================         ] 34/47 batches, loss: 0.0073Epoch 10/10: [======================        ] 35/47 batches, loss: 0.0075Epoch 10/10: [======================        ] 36/47 batches, loss: 0.0074Epoch 10/10: [=======================       ] 37/47 batches, loss: 0.0073Epoch 10/10: [========================      ] 38/47 batches, loss: 0.0074Epoch 10/10: [========================      ] 39/47 batches, loss: 0.0073Epoch 10/10: [=========================     ] 40/47 batches, loss: 0.0075Epoch 10/10: [==========================    ] 41/47 batches, loss: 0.0076Epoch 10/10: [==========================    ] 42/47 batches, loss: 0.0076Epoch 10/10: [===========================   ] 43/47 batches, loss: 0.0077Epoch 10/10: [============================  ] 44/47 batches, loss: 0.0077Epoch 10/10: [============================  ] 45/47 batches, loss: 0.0076Epoch 10/10: [============================= ] 46/47 batches, loss: 0.0077Epoch 10/10: [==============================] 47/47 batches, loss: 0.0076
[2025-05-01 11:57:36,022][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0076
[2025-05-01 11:57:36,280][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0176, Metrics: {'mse': 0.018797539174556732, 'rmse': 0.1371041180072894, 'r2': 0.6009938716888428}
[2025-05-01 11:57:36,281][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-01 11:57:36,281][src.training.lm_trainer][INFO] - Training completed in 53.66 seconds
[2025-05-01 11:57:36,281][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:57:38,431][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.005299539305269718, 'rmse': 0.0727979347596463, 'r2': 0.7619419097900391}
[2025-05-01 11:57:38,432][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.015748990699648857, 'rmse': 0.12549498276683757, 'r2': 0.6657038927078247}
[2025-05-01 11:57:38,432][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.01143182534724474, 'rmse': 0.10691971449290696, 'r2': 0.6450397372245789}
[2025-05-01 11:57:40,126][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ko/ko/model.pt
[2025-05-01 11:57:40,132][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▁▁▁
wandb:     best_val_mse █▅▂▂▁▁▁
wandb:      best_val_r2 ▁▄▇▇███
wandb:    best_val_rmse █▅▂▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆█████▇█
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▂▁▁▁▂▁▁
wandb:          val_mse █▅▂▂▁▁▁▂▁▁
wandb:           val_r2 ▁▄▇▇███▇██
wandb:         val_rmse █▅▂▂▂▁▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01483
wandb:     best_val_mse 0.01575
wandb:      best_val_r2 0.6657
wandb:    best_val_rmse 0.12549
wandb:            epoch 10
wandb:   final_test_mse 0.01143
wandb:    final_test_r2 0.64504
wandb:  final_test_rmse 0.10692
wandb:  final_train_mse 0.0053
wandb:   final_train_r2 0.76194
wandb: final_train_rmse 0.0728
wandb:    final_val_mse 0.01575
wandb:     final_val_r2 0.6657
wandb:   final_val_rmse 0.12549
wandb:    learning_rate 2e-05
wandb:       train_loss 0.00759
wandb:       train_time 53.65685
wandb:         val_loss 0.01757
wandb:          val_mse 0.0188
wandb:           val_r2 0.60099
wandb:         val_rmse 0.1371
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115633-hwqpx1yi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115633-hwqpx1yi/logs
Experiment finetune_complexity_ko completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ko/results.json
Running experiment: finetune_question_type_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ru"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:57:51,670][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ru
experiment_name: finetune_question_type_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 11:57:51,670][__main__][INFO] - Normalized task: question_type
[2025-05-01 11:57:51,670][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 11:57:51,670][__main__][INFO] - Determined Task Type: classification
[2025-05-01 11:57:51,675][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-01 11:57:51,675][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:57:53,037][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:57:55,487][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:57:55,488][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:57:55,538][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,562][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,642][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-01 11:57:55,654][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:57:55,654][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-01 11:57:55,655][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:57:55,670][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,697][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,706][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-01 11:57:55,708][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:57:55,708][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-01 11:57:55,709][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:57:55,723][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,749][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:57:55,760][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-01 11:57:55,762][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:57:55,762][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-01 11:57:55,763][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-01 11:57:55,763][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:57:55,764][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-01 11:57:55,764][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 11:57:55,764][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:57:55,765][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 11:57:55,765][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 11:57:55,765][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 11:57:55,766][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 11:57:55,766][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 11:57:55,766][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-01 11:57:55,766][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 11:57:55,766][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-01 11:57:55,766][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:57:55,766][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:57:55,767][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 11:57:55,767][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:57:59,574][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:57:59,574][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:57:59,575][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:57:59,575][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:57:59,579][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:57:59,580][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:57:59,580][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:57:59,580][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-01 11:57:59,581][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:57:59,581][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7542Epoch 1/10: [                              ] 2/75 batches, loss: 0.7371Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7331Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7758Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7836Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7723Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7612Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7569Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7534Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7463Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7420Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7393Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7383Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7433Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7388Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7388Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7375Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7366Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7321Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7354Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7314Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7316Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7281Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7249Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7289Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7287Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7296Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7281Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7267Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7266Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7264Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7257Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7231Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7236Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7237Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7234Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7217Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7217Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7221Epoch 1/10: [================              ] 40/75 batches, loss: 0.7203Epoch 1/10: [================              ] 41/75 batches, loss: 0.7178Epoch 1/10: [================              ] 42/75 batches, loss: 0.7182Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7185Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7175Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7188Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7206Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7193Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7199Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7209Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7214Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7201Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7179Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7178Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7163Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7150Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7140Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7127Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7124Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7102Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7081Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7072Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7058Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7048Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7035Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7018Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7008Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7012Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7007Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6989Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6977Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6966Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6962Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6950Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6937Epoch 1/10: [==============================] 75/75 batches, loss: 0.6923
[2025-05-01 11:58:09,776][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6923
[2025-05-01 11:58:10,014][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6089, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.8985507246376812, 'precision': 0.9393939393939394, 'recall': 0.8611111111111112}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6398Epoch 2/10: [                              ] 2/75 batches, loss: 0.6829Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6393Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6115Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6012Epoch 2/10: [==                            ] 6/75 batches, loss: 0.5981Epoch 2/10: [==                            ] 7/75 batches, loss: 0.5960Epoch 2/10: [===                           ] 8/75 batches, loss: 0.5934Epoch 2/10: [===                           ] 9/75 batches, loss: 0.5955Epoch 2/10: [====                          ] 10/75 batches, loss: 0.5898Epoch 2/10: [====                          ] 11/75 batches, loss: 0.5883Epoch 2/10: [====                          ] 12/75 batches, loss: 0.5868Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5894Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5855Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5812Epoch 2/10: [======                        ] 16/75 batches, loss: 0.5817Epoch 2/10: [======                        ] 17/75 batches, loss: 0.5816Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5776Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5769Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5746Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5748Epoch 2/10: [========                      ] 22/75 batches, loss: 0.5786Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5805Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5783Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5741Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5732Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5688Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5678Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.5690Epoch 2/10: [============                  ] 30/75 batches, loss: 0.5689Epoch 2/10: [============                  ] 31/75 batches, loss: 0.5697Epoch 2/10: [============                  ] 32/75 batches, loss: 0.5667Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.5658Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5671Epoch 2/10: [==============                ] 35/75 batches, loss: 0.5671Epoch 2/10: [==============                ] 36/75 batches, loss: 0.5648Epoch 2/10: [==============                ] 37/75 batches, loss: 0.5644Epoch 2/10: [===============               ] 38/75 batches, loss: 0.5660Epoch 2/10: [===============               ] 39/75 batches, loss: 0.5641Epoch 2/10: [================              ] 40/75 batches, loss: 0.5640Epoch 2/10: [================              ] 41/75 batches, loss: 0.5639Epoch 2/10: [================              ] 42/75 batches, loss: 0.5637Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5611Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5662Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5679Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5668Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5646Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5662Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5653Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5656Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5650Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5635Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5617Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5604Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5590Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5598Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5589Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.5571Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.5555Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5551Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5555Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5548Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5526Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5521Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5510Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5497Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5480Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5471Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5475Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5469Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5471Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5465Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5456Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5457Epoch 2/10: [==============================] 75/75 batches, loss: 0.5452
[2025-05-01 11:58:18,018][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5452
[2025-05-01 11:58:18,262][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5568, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666, 'precision': 0.9166666666666666, 'recall': 0.9166666666666666}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6233Epoch 3/10: [                              ] 2/75 batches, loss: 0.5846Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5672Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5320Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5232Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5166Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5224Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5350Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5266Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5319Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5274Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5349Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5278Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5214Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5245Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5233Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5265Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5266Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5269Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5295Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5296Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5274Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5285Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5314Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5304Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5268Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5274Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5258Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5235Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5256Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5257Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5251Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5230Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5232Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5234Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5236Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5271Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5269Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5264Epoch 3/10: [================              ] 40/75 batches, loss: 0.5253Epoch 3/10: [================              ] 41/75 batches, loss: 0.5263Epoch 3/10: [================              ] 42/75 batches, loss: 0.5267Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5273Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5271Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5261Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5246Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5255Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5260Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5264Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5260Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5258Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5240Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5232Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5236Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5226Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5232Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5226Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5231Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5228Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5225Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5232Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5225Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5217Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5211Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5212Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5208Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5221Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5224Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5236Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5230Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5230Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5218Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5219Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5223Epoch 3/10: [==============================] 75/75 batches, loss: 0.5211
[2025-05-01 11:58:26,280][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5211
[2025-05-01 11:58:26,539][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5611, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919, 'precision': 0.868421052631579, 'recall': 0.9166666666666666}
[2025-05-01 11:58:26,539][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5664Epoch 4/10: [                              ] 2/75 batches, loss: 0.5354Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5664Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5449Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5367Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5195Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5207Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5176Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5134Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5054Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5031Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4974Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4943Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4918Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4879Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4956Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4933Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4900Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4907Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4890Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4886Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4893Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4947Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5001Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5031Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5046Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5046Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5037Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5048Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5085Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5114Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5154Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5157Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5161Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5151Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5135Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5113Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5143Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5116Epoch 4/10: [================              ] 40/75 batches, loss: 0.5132Epoch 4/10: [================              ] 41/75 batches, loss: 0.5158Epoch 4/10: [================              ] 42/75 batches, loss: 0.5156Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5170Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5198Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5238Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5237Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5231Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5208Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5222Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5223Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5224Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5207Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5209Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5201Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5216Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5213Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5210Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5212Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5230Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5231Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5240Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5261Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5253Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5251Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5243Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5261Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5264Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5270Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5263Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5250Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5240Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5234Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5233Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5215Epoch 4/10: [==============================] 75/75 batches, loss: 0.5212
[2025-05-01 11:58:34,163][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5212
[2025-05-01 11:58:34,423][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5456, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9295774647887324, 'precision': 0.9428571428571428, 'recall': 0.9166666666666666}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.5526Epoch 5/10: [                              ] 2/75 batches, loss: 0.5518Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5437Epoch 5/10: [=                             ] 4/75 batches, loss: 0.5040Epoch 5/10: [==                            ] 5/75 batches, loss: 0.5293Epoch 5/10: [==                            ] 6/75 batches, loss: 0.5171Epoch 5/10: [==                            ] 7/75 batches, loss: 0.5152Epoch 5/10: [===                           ] 8/75 batches, loss: 0.5198Epoch 5/10: [===                           ] 9/75 batches, loss: 0.5234Epoch 5/10: [====                          ] 10/75 batches, loss: 0.5167Epoch 5/10: [====                          ] 11/75 batches, loss: 0.5133Epoch 5/10: [====                          ] 12/75 batches, loss: 0.5146Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.5203Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.5175Epoch 5/10: [======                        ] 15/75 batches, loss: 0.5118Epoch 5/10: [======                        ] 16/75 batches, loss: 0.5084Epoch 5/10: [======                        ] 17/75 batches, loss: 0.5119Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.5128Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.5111Epoch 5/10: [========                      ] 20/75 batches, loss: 0.5107Epoch 5/10: [========                      ] 21/75 batches, loss: 0.5081Epoch 5/10: [========                      ] 22/75 batches, loss: 0.5095Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.5124Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.5115Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.5137Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.5107Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.5122Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.5136Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.5129Epoch 5/10: [============                  ] 30/75 batches, loss: 0.5111Epoch 5/10: [============                  ] 31/75 batches, loss: 0.5126Epoch 5/10: [============                  ] 32/75 batches, loss: 0.5130Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.5128Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.5137Epoch 5/10: [==============                ] 35/75 batches, loss: 0.5173Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5186Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5157Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5148Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5164Epoch 5/10: [================              ] 40/75 batches, loss: 0.5155Epoch 5/10: [================              ] 41/75 batches, loss: 0.5164Epoch 5/10: [================              ] 42/75 batches, loss: 0.5184Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5169Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5156Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5158Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5166Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5171Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5186Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5179Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5177Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5172Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5165Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5159Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5155Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5161Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5155Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5157Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5155Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5157Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5173Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5167Epoch 5/10: [========================      ] 62/75 batches, loss: 0.5183Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.5183Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.5177Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.5176Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.5163Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.5161Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.5170Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.5171Epoch 5/10: [============================  ] 70/75 batches, loss: 0.5169Epoch 5/10: [============================  ] 71/75 batches, loss: 0.5173Epoch 5/10: [============================  ] 72/75 batches, loss: 0.5196Epoch 5/10: [============================= ] 73/75 batches, loss: 0.5194Epoch 5/10: [============================= ] 74/75 batches, loss: 0.5198Epoch 5/10: [==============================] 75/75 batches, loss: 0.5191
[2025-05-01 11:58:42,377][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5191
[2025-05-01 11:58:42,640][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5449, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9295774647887324, 'precision': 0.9428571428571428, 'recall': 0.9166666666666666}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.6136Epoch 6/10: [                              ] 2/75 batches, loss: 0.5585Epoch 6/10: [=                             ] 3/75 batches, loss: 0.5378Epoch 6/10: [=                             ] 4/75 batches, loss: 0.5471Epoch 6/10: [==                            ] 5/75 batches, loss: 0.5556Epoch 6/10: [==                            ] 6/75 batches, loss: 0.5469Epoch 6/10: [==                            ] 7/75 batches, loss: 0.5306Epoch 6/10: [===                           ] 8/75 batches, loss: 0.5201Epoch 6/10: [===                           ] 9/75 batches, loss: 0.5226Epoch 6/10: [====                          ] 10/75 batches, loss: 0.5231Epoch 6/10: [====                          ] 11/75 batches, loss: 0.5149Epoch 6/10: [====                          ] 12/75 batches, loss: 0.5139Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.5204Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.5203Epoch 6/10: [======                        ] 15/75 batches, loss: 0.5129Epoch 6/10: [======                        ] 16/75 batches, loss: 0.5094Epoch 6/10: [======                        ] 17/75 batches, loss: 0.5113Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.5179Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.5204Epoch 6/10: [========                      ] 20/75 batches, loss: 0.5207Epoch 6/10: [========                      ] 21/75 batches, loss: 0.5199Epoch 6/10: [========                      ] 22/75 batches, loss: 0.5192Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.5154Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.5139Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.5116Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.5132Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.5128Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.5164Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.5127Epoch 6/10: [============                  ] 30/75 batches, loss: 0.5148Epoch 6/10: [============                  ] 31/75 batches, loss: 0.5144Epoch 6/10: [============                  ] 32/75 batches, loss: 0.5156Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.5159Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.5163Epoch 6/10: [==============                ] 35/75 batches, loss: 0.5173Epoch 6/10: [==============                ] 36/75 batches, loss: 0.5173Epoch 6/10: [==============                ] 37/75 batches, loss: 0.5150Epoch 6/10: [===============               ] 38/75 batches, loss: 0.5142Epoch 6/10: [===============               ] 39/75 batches, loss: 0.5127Epoch 6/10: [================              ] 40/75 batches, loss: 0.5146Epoch 6/10: [================              ] 41/75 batches, loss: 0.5138Epoch 6/10: [================              ] 42/75 batches, loss: 0.5139Epoch 6/10: [=================             ] 43/75 batches, loss: 0.5159Epoch 6/10: [=================             ] 44/75 batches, loss: 0.5161Epoch 6/10: [==================            ] 45/75 batches, loss: 0.5158Epoch 6/10: [==================            ] 46/75 batches, loss: 0.5188Epoch 6/10: [==================            ] 47/75 batches, loss: 0.5180Epoch 6/10: [===================           ] 48/75 batches, loss: 0.5185Epoch 6/10: [===================           ] 49/75 batches, loss: 0.5195Epoch 6/10: [====================          ] 50/75 batches, loss: 0.5182Epoch 6/10: [====================          ] 51/75 batches, loss: 0.5170Epoch 6/10: [====================          ] 52/75 batches, loss: 0.5176Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.5165Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.5184Epoch 6/10: [======================        ] 55/75 batches, loss: 0.5205Epoch 6/10: [======================        ] 56/75 batches, loss: 0.5206Epoch 6/10: [======================        ] 57/75 batches, loss: 0.5195Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.5192Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.5189Epoch 6/10: [========================      ] 60/75 batches, loss: 0.5191Epoch 6/10: [========================      ] 61/75 batches, loss: 0.5194Epoch 6/10: [========================      ] 62/75 batches, loss: 0.5188Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.5192Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.5206Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.5206Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.5211Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.5215Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.5216Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.5213Epoch 6/10: [============================  ] 70/75 batches, loss: 0.5204Epoch 6/10: [============================  ] 71/75 batches, loss: 0.5198Epoch 6/10: [============================  ] 72/75 batches, loss: 0.5208Epoch 6/10: [============================= ] 73/75 batches, loss: 0.5212Epoch 6/10: [============================= ] 74/75 batches, loss: 0.5210Epoch 6/10: [==============================] 75/75 batches, loss: 0.5192
[2025-05-01 11:58:50,634][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5192
[2025-05-01 11:58:50,884][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5663, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919, 'precision': 0.868421052631579, 'recall': 0.9166666666666666}
[2025-05-01 11:58:50,884][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.5509Epoch 7/10: [                              ] 2/75 batches, loss: 0.5271Epoch 7/10: [=                             ] 3/75 batches, loss: 0.5114Epoch 7/10: [=                             ] 4/75 batches, loss: 0.5154Epoch 7/10: [==                            ] 5/75 batches, loss: 0.5178Epoch 7/10: [==                            ] 6/75 batches, loss: 0.5218Epoch 7/10: [==                            ] 7/75 batches, loss: 0.5315Epoch 7/10: [===                           ] 8/75 batches, loss: 0.5309Epoch 7/10: [===                           ] 9/75 batches, loss: 0.5322Epoch 7/10: [====                          ] 10/75 batches, loss: 0.5269Epoch 7/10: [====                          ] 11/75 batches, loss: 0.5262Epoch 7/10: [====                          ] 12/75 batches, loss: 0.5295Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.5286Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.5251Epoch 7/10: [======                        ] 15/75 batches, loss: 0.5231Epoch 7/10: [======                        ] 16/75 batches, loss: 0.5174Epoch 7/10: [======                        ] 17/75 batches, loss: 0.5180Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.5225Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.5227Epoch 7/10: [========                      ] 20/75 batches, loss: 0.5230Epoch 7/10: [========                      ] 21/75 batches, loss: 0.5232Epoch 7/10: [========                      ] 22/75 batches, loss: 0.5233Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.5204Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.5187Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.5197Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.5187Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.5155Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.5160Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.5131Epoch 7/10: [============                  ] 30/75 batches, loss: 0.5148Epoch 7/10: [============                  ] 31/75 batches, loss: 0.5129Epoch 7/10: [============                  ] 32/75 batches, loss: 0.5148Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.5153Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.5136Epoch 7/10: [==============                ] 35/75 batches, loss: 0.5154Epoch 7/10: [==============                ] 36/75 batches, loss: 0.5164Epoch 7/10: [==============                ] 37/75 batches, loss: 0.5181Epoch 7/10: [===============               ] 38/75 batches, loss: 0.5184Epoch 7/10: [===============               ] 39/75 batches, loss: 0.5174Epoch 7/10: [================              ] 40/75 batches, loss: 0.5192Epoch 7/10: [================              ] 41/75 batches, loss: 0.5194Epoch 7/10: [================              ] 42/75 batches, loss: 0.5190Epoch 7/10: [=================             ] 43/75 batches, loss: 0.5197Epoch 7/10: [=================             ] 44/75 batches, loss: 0.5194Epoch 7/10: [==================            ] 45/75 batches, loss: 0.5195Epoch 7/10: [==================            ] 46/75 batches, loss: 0.5197Epoch 7/10: [==================            ] 47/75 batches, loss: 0.5203Epoch 7/10: [===================           ] 48/75 batches, loss: 0.5209Epoch 7/10: [===================           ] 49/75 batches, loss: 0.5201Epoch 7/10: [====================          ] 50/75 batches, loss: 0.5192Epoch 7/10: [====================          ] 51/75 batches, loss: 0.5201Epoch 7/10: [====================          ] 52/75 batches, loss: 0.5215Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.5208Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.5220Epoch 7/10: [======================        ] 55/75 batches, loss: 0.5221Epoch 7/10: [======================        ] 56/75 batches, loss: 0.5235Epoch 7/10: [======================        ] 57/75 batches, loss: 0.5223Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.5228Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.5244Epoch 7/10: [========================      ] 60/75 batches, loss: 0.5245Epoch 7/10: [========================      ] 61/75 batches, loss: 0.5246Epoch 7/10: [========================      ] 62/75 batches, loss: 0.5242Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.5243Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.5236Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.5237Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.5242Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.5242Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.5232Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.5228Epoch 7/10: [============================  ] 70/75 batches, loss: 0.5215Epoch 7/10: [============================  ] 71/75 batches, loss: 0.5206Epoch 7/10: [============================  ] 72/75 batches, loss: 0.5210Epoch 7/10: [============================= ] 73/75 batches, loss: 0.5201Epoch 7/10: [============================= ] 74/75 batches, loss: 0.5208Epoch 7/10: [==============================] 75/75 batches, loss: 0.5200
[2025-05-01 11:58:58,474][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5200
[2025-05-01 11:58:58,726][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5456, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9295774647887324, 'precision': 0.9428571428571428, 'recall': 0.9166666666666666}
[2025-05-01 11:58:58,727][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.5034Epoch 8/10: [                              ] 2/75 batches, loss: 0.5034Epoch 8/10: [=                             ] 3/75 batches, loss: 0.4796Epoch 8/10: [=                             ] 4/75 batches, loss: 0.4797Epoch 8/10: [==                            ] 5/75 batches, loss: 0.4892Epoch 8/10: [==                            ] 6/75 batches, loss: 0.4836Epoch 8/10: [==                            ] 7/75 batches, loss: 0.4920Epoch 8/10: [===                           ] 8/75 batches, loss: 0.4890Epoch 8/10: [===                           ] 9/75 batches, loss: 0.4880Epoch 8/10: [====                          ] 10/75 batches, loss: 0.4942Epoch 8/10: [====                          ] 11/75 batches, loss: 0.4865Epoch 8/10: [====                          ] 12/75 batches, loss: 0.4931Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.4958Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.5008Epoch 8/10: [======                        ] 15/75 batches, loss: 0.4946Epoch 8/10: [======                        ] 16/75 batches, loss: 0.5049Epoch 8/10: [======                        ] 17/75 batches, loss: 0.5035Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.5049Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.5043Epoch 8/10: [========                      ] 20/75 batches, loss: 0.5031Epoch 8/10: [========                      ] 21/75 batches, loss: 0.5062Epoch 8/10: [========                      ] 22/75 batches, loss: 0.5078Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.5088Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.5141Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.5147Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.5152Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.5136Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.5116Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.5097Epoch 8/10: [============                  ] 30/75 batches, loss: 0.5119Epoch 8/10: [============                  ] 31/75 batches, loss: 0.5110Epoch 8/10: [============                  ] 32/75 batches, loss: 0.5115Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.5124Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.5140Epoch 8/10: [==============                ] 35/75 batches, loss: 0.5130Epoch 8/10: [==============                ] 36/75 batches, loss: 0.5128Epoch 8/10: [==============                ] 37/75 batches, loss: 0.5119Epoch 8/10: [===============               ] 38/75 batches, loss: 0.5120Epoch 8/10: [===============               ] 39/75 batches, loss: 0.5149Epoch 8/10: [================              ] 40/75 batches, loss: 0.5138Epoch 8/10: [================              ] 41/75 batches, loss: 0.5150Epoch 8/10: [================              ] 42/75 batches, loss: 0.5148Epoch 8/10: [=================             ] 43/75 batches, loss: 0.5162Epoch 8/10: [=================             ] 44/75 batches, loss: 0.5167Epoch 8/10: [==================            ] 45/75 batches, loss: 0.5163Epoch 8/10: [==================            ] 46/75 batches, loss: 0.5155Epoch 8/10: [==================            ] 47/75 batches, loss: 0.5168Epoch 8/10: [===================           ] 48/75 batches, loss: 0.5175Epoch 8/10: [===================           ] 49/75 batches, loss: 0.5163Epoch 8/10: [====================          ] 50/75 batches, loss: 0.5163Epoch 8/10: [====================          ] 51/75 batches, loss: 0.5178Epoch 8/10: [====================          ] 52/75 batches, loss: 0.5200Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.5193Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.5197Epoch 8/10: [======================        ] 55/75 batches, loss: 0.5194Epoch 8/10: [======================        ] 56/75 batches, loss: 0.5178Epoch 8/10: [======================        ] 57/75 batches, loss: 0.5176Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.5169Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.5183Epoch 8/10: [========================      ] 60/75 batches, loss: 0.5177Epoch 8/10: [========================      ] 61/75 batches, loss: 0.5189Epoch 8/10: [========================      ] 62/75 batches, loss: 0.5194Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.5187Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.5185Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.5183Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.5191Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.5193Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.5197Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.5210Epoch 8/10: [============================  ] 70/75 batches, loss: 0.5200Epoch 8/10: [============================  ] 71/75 batches, loss: 0.5204Epoch 8/10: [============================  ] 72/75 batches, loss: 0.5218Epoch 8/10: [============================= ] 73/75 batches, loss: 0.5225Epoch 8/10: [============================= ] 74/75 batches, loss: 0.5229Epoch 8/10: [==============================] 75/75 batches, loss: 0.5221
[2025-05-01 11:59:06,325][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5221
[2025-05-01 11:59:06,586][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5417, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9117647058823529, 'precision': 0.96875, 'recall': 0.8611111111111112}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.4947Epoch 9/10: [                              ] 2/75 batches, loss: 0.5109Epoch 9/10: [=                             ] 3/75 batches, loss: 0.5242Epoch 9/10: [=                             ] 4/75 batches, loss: 0.5228Epoch 9/10: [==                            ] 5/75 batches, loss: 0.5237Epoch 9/10: [==                            ] 6/75 batches, loss: 0.5283Epoch 9/10: [==                            ] 7/75 batches, loss: 0.5303Epoch 9/10: [===                           ] 8/75 batches, loss: 0.5240Epoch 9/10: [===                           ] 9/75 batches, loss: 0.5111Epoch 9/10: [====                          ] 10/75 batches, loss: 0.5080Epoch 9/10: [====                          ] 11/75 batches, loss: 0.5190Epoch 9/10: [====                          ] 12/75 batches, loss: 0.5137Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.5111Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.5116Epoch 9/10: [======                        ] 15/75 batches, loss: 0.5146Epoch 9/10: [======                        ] 16/75 batches, loss: 0.5124Epoch 9/10: [======                        ] 17/75 batches, loss: 0.5148Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.5136Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.5172Epoch 9/10: [========                      ] 20/75 batches, loss: 0.5165Epoch 9/10: [========                      ] 21/75 batches, loss: 0.5193Epoch 9/10: [========                      ] 22/75 batches, loss: 0.5218Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.5220Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.5222Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.5230Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.5228Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.5195Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.5194Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.5197Epoch 9/10: [============                  ] 30/75 batches, loss: 0.5215Epoch 9/10: [============                  ] 31/75 batches, loss: 0.5232Epoch 9/10: [============                  ] 32/75 batches, loss: 0.5219Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.5220Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.5201Epoch 9/10: [==============                ] 35/75 batches, loss: 0.5182Epoch 9/10: [==============                ] 36/75 batches, loss: 0.5189Epoch 9/10: [==============                ] 37/75 batches, loss: 0.5180Epoch 9/10: [===============               ] 38/75 batches, loss: 0.5193Epoch 9/10: [===============               ] 39/75 batches, loss: 0.5171Epoch 9/10: [================              ] 40/75 batches, loss: 0.5179Epoch 9/10: [================              ] 41/75 batches, loss: 0.5187Epoch 9/10: [================              ] 42/75 batches, loss: 0.5186Epoch 9/10: [=================             ] 43/75 batches, loss: 0.5183Epoch 9/10: [=================             ] 44/75 batches, loss: 0.5180Epoch 9/10: [==================            ] 45/75 batches, loss: 0.5192Epoch 9/10: [==================            ] 46/75 batches, loss: 0.5200Epoch 9/10: [==================            ] 47/75 batches, loss: 0.5209Epoch 9/10: [===================           ] 48/75 batches, loss: 0.5201Epoch 9/10: [===================           ] 49/75 batches, loss: 0.5202Epoch 9/10: [====================          ] 50/75 batches, loss: 0.5194Epoch 9/10: [====================          ] 51/75 batches, loss: 0.5200Epoch 9/10: [====================          ] 52/75 batches, loss: 0.5206Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.5212Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.5204Epoch 9/10: [======================        ] 55/75 batches, loss: 0.5197Epoch 9/10: [======================        ] 56/75 batches, loss: 0.5202Epoch 9/10: [======================        ] 57/75 batches, loss: 0.5195Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.5180Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.5188Epoch 9/10: [========================      ] 60/75 batches, loss: 0.5184Epoch 9/10: [========================      ] 61/75 batches, loss: 0.5204Epoch 9/10: [========================      ] 62/75 batches, loss: 0.5201Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.5206Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.5203Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.5212Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.5215Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.5206Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.5221Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.5220Epoch 9/10: [============================  ] 70/75 batches, loss: 0.5233Epoch 9/10: [============================  ] 71/75 batches, loss: 0.5232Epoch 9/10: [============================  ] 72/75 batches, loss: 0.5233Epoch 9/10: [============================= ] 73/75 batches, loss: 0.5217Epoch 9/10: [============================= ] 74/75 batches, loss: 0.5218Epoch 9/10: [==============================] 75/75 batches, loss: 0.5205
[2025-05-01 11:59:14,642][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5205
[2025-05-01 11:59:14,904][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5563, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9014084507042254, 'precision': 0.9142857142857143, 'recall': 0.8888888888888888}
[2025-05-01 11:59:14,905][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.5111Epoch 10/10: [                              ] 2/75 batches, loss: 0.5191Epoch 10/10: [=                             ] 3/75 batches, loss: 0.5217Epoch 10/10: [=                             ] 4/75 batches, loss: 0.5112Epoch 10/10: [==                            ] 5/75 batches, loss: 0.4954Epoch 10/10: [==                            ] 6/75 batches, loss: 0.4967Epoch 10/10: [==                            ] 7/75 batches, loss: 0.4998Epoch 10/10: [===                           ] 8/75 batches, loss: 0.5080Epoch 10/10: [===                           ] 9/75 batches, loss: 0.5145Epoch 10/10: [====                          ] 10/75 batches, loss: 0.5062Epoch 10/10: [====                          ] 11/75 batches, loss: 0.5039Epoch 10/10: [====                          ] 12/75 batches, loss: 0.5058Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.5021Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.5005Epoch 10/10: [======                        ] 15/75 batches, loss: 0.5023Epoch 10/10: [======                        ] 16/75 batches, loss: 0.5009Epoch 10/10: [======                        ] 17/75 batches, loss: 0.4977Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.5002Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.5016Epoch 10/10: [========                      ] 20/75 batches, loss: 0.5017Epoch 10/10: [========                      ] 21/75 batches, loss: 0.5048Epoch 10/10: [========                      ] 22/75 batches, loss: 0.5058Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.5046Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.5075Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.5061Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.5084Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.5064Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.5091Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.5097Epoch 10/10: [============                  ] 30/75 batches, loss: 0.5095Epoch 10/10: [============                  ] 31/75 batches, loss: 0.5116Epoch 10/10: [============                  ] 32/75 batches, loss: 0.5143Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.5159Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.5173Epoch 10/10: [==============                ] 35/75 batches, loss: 0.5169Epoch 10/10: [==============                ] 36/75 batches, loss: 0.5159Epoch 10/10: [==============                ] 37/75 batches, loss: 0.5136Epoch 10/10: [===============               ] 38/75 batches, loss: 0.5140Epoch 10/10: [===============               ] 39/75 batches, loss: 0.5143Epoch 10/10: [================              ] 40/75 batches, loss: 0.5156Epoch 10/10: [================              ] 41/75 batches, loss: 0.5141Epoch 10/10: [================              ] 42/75 batches, loss: 0.5150Epoch 10/10: [=================             ] 43/75 batches, loss: 0.5142Epoch 10/10: [=================             ] 44/75 batches, loss: 0.5150Epoch 10/10: [==================            ] 45/75 batches, loss: 0.5121Epoch 10/10: [==================            ] 46/75 batches, loss: 0.5128Epoch 10/10: [==================            ] 47/75 batches, loss: 0.5151Epoch 10/10: [===================           ] 48/75 batches, loss: 0.5168Epoch 10/10: [===================           ] 49/75 batches, loss: 0.5170Epoch 10/10: [====================          ] 50/75 batches, loss: 0.5163Epoch 10/10: [====================          ] 51/75 batches, loss: 0.5151Epoch 10/10: [====================          ] 52/75 batches, loss: 0.5144Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.5151Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.5149Epoch 10/10: [======================        ] 55/75 batches, loss: 0.5158Epoch 10/10: [======================        ] 56/75 batches, loss: 0.5156Epoch 10/10: [======================        ] 57/75 batches, loss: 0.5154Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.5160Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.5158Epoch 10/10: [========================      ] 60/75 batches, loss: 0.5166Epoch 10/10: [========================      ] 61/75 batches, loss: 0.5164Epoch 10/10: [========================      ] 62/75 batches, loss: 0.5170Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.5187Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.5189Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.5190Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.5184Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.5185Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.5176Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.5171Epoch 10/10: [============================  ] 70/75 batches, loss: 0.5179Epoch 10/10: [============================  ] 71/75 batches, loss: 0.5179Epoch 10/10: [============================  ] 72/75 batches, loss: 0.5172Epoch 10/10: [============================= ] 73/75 batches, loss: 0.5177Epoch 10/10: [============================= ] 74/75 batches, loss: 0.5169Epoch 10/10: [==============================] 75/75 batches, loss: 0.5190
[2025-05-01 11:59:22,533][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5190
[2025-05-01 11:59:22,812][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.5437, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9295774647887324, 'precision': 0.9428571428571428, 'recall': 0.9166666666666666}
[2025-05-01 11:59:22,813][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-05-01 11:59:22,813][src.training.lm_trainer][INFO] - Training completed in 81.61 seconds
[2025-05-01 11:59:22,813][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 11:59:25,747][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9748743718592965, 'f1': 0.9750830564784053, 'precision': 0.9670510708401977, 'recall': 0.983249581239531}
[2025-05-01 11:59:25,747][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9117647058823529, 'precision': 0.96875, 'recall': 0.8611111111111112}
[2025-05-01 11:59:25,747][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9636363636363636, 'f1': 0.9642857142857143, 'precision': 0.9473684210526315, 'recall': 0.9818181818181818}
[2025-05-01 11:59:27,578][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ru/ru/model.pt
[2025-05-01 11:59:27,583][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅██▅
wandb:           best_val_f1 ▁▅██▄
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▄▁▅▅█
wandb:       best_val_recall ▁███▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▂▂▂▂▂▂
wandb:            train_loss █▂▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▃▆▁██▁█▆▃█
wandb:                val_f1 ▂▆▁██▁█▅▃█
wandb:              val_loss █▃▃▁▁▄▁▁▃▁
wandb:         val_precision ▆▄▁▆▆▁▆█▄▆
wandb:            val_recall ▁██████▁▅█
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.91667
wandb:           best_val_f1 0.91176
wandb:         best_val_loss 0.54173
wandb:    best_val_precision 0.96875
wandb:       best_val_recall 0.86111
wandb:                 epoch 10
wandb:   final_test_accuracy 0.96364
wandb:         final_test_f1 0.96429
wandb:  final_test_precision 0.94737
wandb:     final_test_recall 0.98182
wandb:  final_train_accuracy 0.97487
wandb:        final_train_f1 0.97508
wandb: final_train_precision 0.96705
wandb:    final_train_recall 0.98325
wandb:    final_val_accuracy 0.91667
wandb:          final_val_f1 0.91176
wandb:   final_val_precision 0.96875
wandb:      final_val_recall 0.86111
wandb:         learning_rate 2e-05
wandb:            train_loss 0.51903
wandb:            train_time 81.6057
wandb:          val_accuracy 0.93056
wandb:                val_f1 0.92958
wandb:              val_loss 0.54374
wandb:         val_precision 0.94286
wandb:            val_recall 0.91667
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115751-ylvgspj3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115751-ylvgspj3/logs
Experiment finetune_question_type_ru completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ru/results.json
Running experiment: finetune_complexity_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ru"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 11:59:39,268][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ru
experiment_name: finetune_complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 11:59:39,269][__main__][INFO] - Normalized task: complexity
[2025-05-01 11:59:39,269][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 11:59:39,269][__main__][INFO] - Determined Task Type: regression
[2025-05-01 11:59:39,273][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-01 11:59:39,273][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 11:59:40,632][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 11:59:42,904][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 11:59:42,904][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:59:43,045][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,071][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,202][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-01 11:59:43,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:59:43,211][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-01 11:59:43,212][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:59:43,236][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,266][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,279][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-01 11:59:43,280][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:59:43,280][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-01 11:59:43,281][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 11:59:43,299][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,329][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 11:59:43,342][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-01 11:59:43,343][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 11:59:43,343][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-01 11:59:43,344][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-01 11:59:43,345][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:59:43,345][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:59:43,345][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:59:43,345][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:59:43,345][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:59:43,345][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-01 11:59:43,345][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:59:43,346][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:59:43,346][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-01 11:59:43,346][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 11:59:43,347][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 11:59:43,347][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-01 11:59:43,347][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 11:59:43,348][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 11:59:43,348][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 11:59:43,348][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 11:59:47,270][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 11:59:47,271][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 11:59:47,271][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 11:59:47,271][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 11:59:47,275][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 11:59:47,276][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 11:59:47,276][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 11:59:47,276][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-01 11:59:47,277][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 11:59:47,277][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1589Epoch 1/10: [                              ] 2/75 batches, loss: 0.1612Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1642Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1554Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1467Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1472Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1509Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1514Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1552Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1510Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1535Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1552Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1534Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1505Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1544Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1542Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1567Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1539Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1570Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1550Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1545Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1554Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1575Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1586Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1572Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1558Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1541Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1546Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1532Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1513Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1501Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1493Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1486Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1463Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1445Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1439Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1431Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1417Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1391Epoch 1/10: [================              ] 40/75 batches, loss: 0.1377Epoch 1/10: [================              ] 41/75 batches, loss: 0.1355Epoch 1/10: [================              ] 42/75 batches, loss: 0.1338Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1324Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1322Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1306Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1291Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1275Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1261Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1246Epoch 1/10: [====================          ] 50/75 batches, loss: 0.1235Epoch 1/10: [====================          ] 51/75 batches, loss: 0.1217Epoch 1/10: [====================          ] 52/75 batches, loss: 0.1206Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.1191Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.1176Epoch 1/10: [======================        ] 55/75 batches, loss: 0.1157Epoch 1/10: [======================        ] 56/75 batches, loss: 0.1143Epoch 1/10: [======================        ] 57/75 batches, loss: 0.1127Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.1112Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.1097Epoch 1/10: [========================      ] 60/75 batches, loss: 0.1083Epoch 1/10: [========================      ] 61/75 batches, loss: 0.1067Epoch 1/10: [========================      ] 62/75 batches, loss: 0.1058Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.1044Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.1033Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.1021Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.1012Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.1001Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0989Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0978Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0967Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0957Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0946Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0937Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0927Epoch 1/10: [==============================] 75/75 batches, loss: 0.0915
[2025-05-01 11:59:57,276][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0915
[2025-05-01 11:59:57,508][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0441, Metrics: {'mse': 0.04510840028524399, 'rmse': 0.21238738259426804, 'r2': 0.030326247215270996}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0109Epoch 2/10: [                              ] 2/75 batches, loss: 0.0117Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0105Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0143Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0142Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0153Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0151Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0155Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0151Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0151Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0154Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0157Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0155Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0161Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0158Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0164Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0160Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0160Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0161Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0161Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0159Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0160Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0165Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0165Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0165Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0163Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0168Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0173Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0173Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0171Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0172Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0171Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0169Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0167Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0165Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0169Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0171Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0170Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0170Epoch 2/10: [================              ] 40/75 batches, loss: 0.0167Epoch 2/10: [================              ] 41/75 batches, loss: 0.0165Epoch 2/10: [================              ] 42/75 batches, loss: 0.0162Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0161Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0160Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0158Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0161Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0162Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0160Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0160Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0160Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0160Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0160Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0160Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0161Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0161Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0163Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0161Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0164Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0162Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0163Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0162Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0162Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0162Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0162Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0161Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0161Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0163Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0163Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0162Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0162Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0161Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0162Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0161Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0162Epoch 2/10: [==============================] 75/75 batches, loss: 0.0161
[2025-05-01 12:00:05,507][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0161
[2025-05-01 12:00:05,794][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0250, Metrics: {'mse': 0.022830255329608917, 'rmse': 0.1510968408988385, 'r2': 0.5092289447784424}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0112Epoch 3/10: [                              ] 2/75 batches, loss: 0.0160Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0130Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0117Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0108Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0130Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0126Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0117Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0113Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0110Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0105Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0107Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0105Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0120Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0117Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0114Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0112Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0108Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0112Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0115Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0116Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0116Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0120Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0121Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0120Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0121Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0122Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0122Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0120Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0118Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0118Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0116Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0117Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0115Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0114Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0116Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0116Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0118Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0116Epoch 3/10: [================              ] 40/75 batches, loss: 0.0115Epoch 3/10: [================              ] 41/75 batches, loss: 0.0113Epoch 3/10: [================              ] 42/75 batches, loss: 0.0112Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0112Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0111Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0112Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0112Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0114Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0115Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0115Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0115Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0117Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0119Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0119Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0121Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0121Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0121Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0122Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0123Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0123Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0123Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0122Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0121Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0121Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0121Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0121Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0121Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0121Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0122Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0122Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0121Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0120Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0120Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0120Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0121Epoch 3/10: [==============================] 75/75 batches, loss: 0.0121
[2025-05-01 12:00:13,820][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0121
[2025-05-01 12:00:14,068][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0204, Metrics: {'mse': 0.019132094457745552, 'rmse': 0.1383188145472103, 'r2': 0.5887265205383301}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0146Epoch 4/10: [                              ] 2/75 batches, loss: 0.0109Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0103Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0117Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0111Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0116Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0123Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0121Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0122Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0131Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0130Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0137Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0137Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0131Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0128Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0125Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0121Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0118Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0118Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0118Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0115Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0114Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0115Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0115Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0116Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0116Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0116Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0115Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0115Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0113Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0112Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0112Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0114Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0114Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0112Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0111Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0119Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0118Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0118Epoch 4/10: [================              ] 40/75 batches, loss: 0.0118Epoch 4/10: [================              ] 41/75 batches, loss: 0.0118Epoch 4/10: [================              ] 42/75 batches, loss: 0.0117Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0117Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0116Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0115Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0115Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0114Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0114Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0115Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0114Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0113Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0112Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0113Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0114Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0115Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0114Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0114Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0116Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0120Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0122Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0122Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0121Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0120Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0120Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0120Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0120Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0121Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0121Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0121Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0122Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0122Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0121Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0121Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0121Epoch 4/10: [==============================] 75/75 batches, loss: 0.0121
[2025-05-01 12:00:22,033][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0121
[2025-05-01 12:00:22,298][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.02889196388423443, 'rmse': 0.1699763627220986, 'r2': 0.378923237323761}
[2025-05-01 12:00:22,299][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0052Epoch 5/10: [                              ] 2/75 batches, loss: 0.0077Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0082Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0076Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0096Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0094Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0088Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0089Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0087Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0085Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0091Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0092Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0091Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0094Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0107Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0102Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0105Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0105Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0104Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0105Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0107Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0106Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0104Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0103Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0103Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0101Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0101Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0099Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0099Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0099Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0101Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0101Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0102Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0101Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0102Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0101Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0100Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0100Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0100Epoch 5/10: [================              ] 40/75 batches, loss: 0.0100Epoch 5/10: [================              ] 41/75 batches, loss: 0.0099Epoch 5/10: [================              ] 42/75 batches, loss: 0.0098Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0098Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0097Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0096Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0096Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0096Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0096Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0096Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0096Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0096Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0096Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0097Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0099Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0103Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0114Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0115Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0117Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0123Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0125Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0132Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0139Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0159Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0177Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0198Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0220Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0232Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0239Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0242Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0248Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0254Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0259Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0265Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0268Epoch 5/10: [==============================] 75/75 batches, loss: 0.0275
[2025-05-01 12:00:29,897][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0275
[2025-05-01 12:00:30,151][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1839, Metrics: {'mse': 0.1942393183708191, 'rmse': 0.4407258993646948, 'r2': -3.175469398498535}
[2025-05-01 12:00:30,152][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0554Epoch 6/10: [                              ] 2/75 batches, loss: 0.0717Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0898Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0842Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0778Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0796Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0748Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0710Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0657Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0629Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0646Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0619Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0599Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0580Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0572Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0563Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0542Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0523Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0514Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0501Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0496Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0490Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0476Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0480Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0470Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0462Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0459Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0446Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0439Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0435Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0435Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0437Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0429Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0430Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0423Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0415Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0413Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0412Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0409Epoch 6/10: [================              ] 40/75 batches, loss: 0.0404Epoch 6/10: [================              ] 41/75 batches, loss: 0.0408Epoch 6/10: [================              ] 42/75 batches, loss: 0.0410Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0406Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0402Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0398Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0393Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0387Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0383Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0380Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0377Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0375Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0373Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0369Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0367Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0365Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0364Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0362Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0359Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0360Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0360Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0361Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0361Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0358Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0356Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0353Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0350Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0348Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0346Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0342Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0341Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0343Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0341Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0340Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0338Epoch 6/10: [==============================] 75/75 batches, loss: 0.0339
[2025-05-01 12:00:37,778][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0339
[2025-05-01 12:00:38,029][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0581, Metrics: {'mse': 0.05555982142686844, 'rmse': 0.23571130950140776, 'r2': -0.19434285163879395}
[2025-05-01 12:00:38,030][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:00:38,030][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-01 12:00:38,030][src.training.lm_trainer][INFO] - Training completed in 49.18 seconds
[2025-05-01 12:00:38,030][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:00:40,903][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.006560520268976688, 'rmse': 0.08099703864325343, 'r2': 0.670940637588501}
[2025-05-01 12:00:40,904][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.019132094457745552, 'rmse': 0.1383188145472103, 'r2': 0.5887265205383301}
[2025-05-01 12:00:40,904][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.021094538271427155, 'rmse': 0.145239589201523, 'r2': 0.4664335250854492}
[2025-05-01 12:00:42,587][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ru/ru/model.pt
[2025-05-01 12:00:42,593][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █████▁
wandb:       train_loss █▁▁▁▂▃
wandb:       train_time ▁
wandb:         val_loss ▂▁▁▁█▃
wandb:          val_mse ▂▁▁▁█▂
wandb:           val_r2 ▇███▁▇
wandb:         val_rmse ▃▁▁▂█▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02041
wandb:     best_val_mse 0.01913
wandb:      best_val_r2 0.58873
wandb:    best_val_rmse 0.13832
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.02109
wandb:    final_test_r2 0.46643
wandb:  final_test_rmse 0.14524
wandb:  final_train_mse 0.00656
wandb:   final_train_r2 0.67094
wandb: final_train_rmse 0.081
wandb:    final_val_mse 0.01913
wandb:     final_val_r2 0.58873
wandb:   final_val_rmse 0.13832
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03389
wandb:       train_time 49.18462
wandb:         val_loss 0.0581
wandb:          val_mse 0.05556
wandb:           val_r2 -0.19434
wandb:         val_rmse 0.23571
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115939-ay7hlswq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_115939-ay7hlswq/logs
Experiment finetune_complexity_ru completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ru/results.json
Running control finetuning experiments...
Running experiment: finetune_question_type_control1_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:00:54,527][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1
experiment_name: finetune_question_type_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:00:54,527][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:00:54,527][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:00:54,527][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:00:54,536][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 12:00:54,536][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:00:55,929][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:00:58,300][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:00:58,301][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:00:58,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-01 12:00:58,381][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-01 12:00:58,468][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:00:58,475][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:00:58,476][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:00:58,477][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:00:58,496][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:00:58,526][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:00:58,540][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:00:58,541][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:00:58,541][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:00:58,542][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:00:58,560][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:00:58,590][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:00:58,603][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:00:58,604][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:00:58,604][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:00:58,605][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:00:58,606][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:00:58,607][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 12:00:58,607][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:00:58,607][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:00:58,608][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 12:00:58,608][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:00:58,608][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:00:58,608][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 12:00:58,609][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 12:00:58,609][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:00:58,609][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:00:58,609][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:00:58,609][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:00:58,609][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:00:58,609][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:00:58,610][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:01:02,474][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:01:02,475][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:01:02,475][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:01:02,475][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:01:02,479][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:01:02,480][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:01:02,480][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:01:02,480][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:01:02,481][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:01:02,481][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.7237Epoch 1/10: [                              ] 2/63 batches, loss: 0.7070Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7028Epoch 1/10: [=                             ] 4/63 batches, loss: 0.6917Epoch 1/10: [==                            ] 5/63 batches, loss: 0.6973Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7020Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7008Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7079Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7023Epoch 1/10: [====                          ] 10/63 batches, loss: 0.6983Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.6957Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.6978Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7087Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7035Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7106Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7170Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7111Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7135Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7153Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7200Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7200Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7219Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7193Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7208Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7196Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7214Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7181Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7158Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7152Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7185Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7166Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7149Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7161Epoch 1/10: [================              ] 34/63 batches, loss: 0.7187Epoch 1/10: [================              ] 35/63 batches, loss: 0.7154Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7141Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7152Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7184Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7178Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7170Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7195Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7194Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7181Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7182Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7196Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7196Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7210Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7214Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7226Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7208Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7196Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7208Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7213Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7215Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7210Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7215Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7200Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7186Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7176Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7186Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7202Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7216Epoch 1/10: [==============================] 63/63 batches, loss: 0.7254
[2025-05-01 12:01:11,368][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7254
[2025-05-01 12:01:11,563][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7449, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7417Epoch 2/10: [                              ] 2/63 batches, loss: 0.7168Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7200Epoch 2/10: [=                             ] 4/63 batches, loss: 0.6959Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7002Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7087Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7133Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7140Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7121Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7113Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7068Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7031Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7050Epoch 2/10: [======                        ] 14/63 batches, loss: 0.7039Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.7059Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.7042Epoch 2/10: [========                      ] 17/63 batches, loss: 0.7056Epoch 2/10: [========                      ] 18/63 batches, loss: 0.7097Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.7067Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.7074Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.7103Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.7122Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.7146Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.7167Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.7141Epoch 2/10: [============                  ] 26/63 batches, loss: 0.7144Epoch 2/10: [============                  ] 27/63 batches, loss: 0.7179Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.7162Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.7148Epoch 2/10: [==============                ] 30/63 batches, loss: 0.7152Epoch 2/10: [==============                ] 31/63 batches, loss: 0.7137Epoch 2/10: [===============               ] 32/63 batches, loss: 0.7111Epoch 2/10: [===============               ] 33/63 batches, loss: 0.7136Epoch 2/10: [================              ] 34/63 batches, loss: 0.7138Epoch 2/10: [================              ] 35/63 batches, loss: 0.7149Epoch 2/10: [=================             ] 36/63 batches, loss: 0.7145Epoch 2/10: [=================             ] 37/63 batches, loss: 0.7140Epoch 2/10: [==================            ] 38/63 batches, loss: 0.7132Epoch 2/10: [==================            ] 39/63 batches, loss: 0.7125Epoch 2/10: [===================           ] 40/63 batches, loss: 0.7107Epoch 2/10: [===================           ] 41/63 batches, loss: 0.7099Epoch 2/10: [====================          ] 42/63 batches, loss: 0.7102Epoch 2/10: [====================          ] 43/63 batches, loss: 0.7109Epoch 2/10: [====================          ] 44/63 batches, loss: 0.7116Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.7107Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.7118Epoch 2/10: [======================        ] 47/63 batches, loss: 0.7132Epoch 2/10: [======================        ] 48/63 batches, loss: 0.7133Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.7122Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.7109Epoch 2/10: [========================      ] 51/63 batches, loss: 0.7103Epoch 2/10: [========================      ] 52/63 batches, loss: 0.7103Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.7115Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.7132Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.7139Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.7140Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.7135Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.7128Epoch 2/10: [============================  ] 59/63 batches, loss: 0.7130Epoch 2/10: [============================  ] 60/63 batches, loss: 0.7127Epoch 2/10: [============================= ] 61/63 batches, loss: 0.7123Epoch 2/10: [============================= ] 62/63 batches, loss: 0.7137Epoch 2/10: [==============================] 63/63 batches, loss: 0.7127
[2025-05-01 12:01:18,354][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.7127
[2025-05-01 12:01:18,564][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.7380, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6889Epoch 3/10: [                              ] 2/63 batches, loss: 0.6673Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6747Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6877Epoch 3/10: [==                            ] 5/63 batches, loss: 0.7041Epoch 3/10: [==                            ] 6/63 batches, loss: 0.7086Epoch 3/10: [===                           ] 7/63 batches, loss: 0.7136Epoch 3/10: [===                           ] 8/63 batches, loss: 0.7102Epoch 3/10: [====                          ] 9/63 batches, loss: 0.7123Epoch 3/10: [====                          ] 10/63 batches, loss: 0.7160Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.7161Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.7134Epoch 3/10: [======                        ] 13/63 batches, loss: 0.7114Epoch 3/10: [======                        ] 14/63 batches, loss: 0.7061Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.7015Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.7060Epoch 3/10: [========                      ] 17/63 batches, loss: 0.7054Epoch 3/10: [========                      ] 18/63 batches, loss: 0.7030Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.7007Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.7030Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.7037Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.7051Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.7069Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.7054Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.7059Epoch 3/10: [============                  ] 26/63 batches, loss: 0.7040Epoch 3/10: [============                  ] 27/63 batches, loss: 0.7059Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.7099Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.7067Epoch 3/10: [==============                ] 30/63 batches, loss: 0.7097Epoch 3/10: [==============                ] 31/63 batches, loss: 0.7108Epoch 3/10: [===============               ] 32/63 batches, loss: 0.7106Epoch 3/10: [===============               ] 33/63 batches, loss: 0.7093Epoch 3/10: [================              ] 34/63 batches, loss: 0.7088Epoch 3/10: [================              ] 35/63 batches, loss: 0.7098Epoch 3/10: [=================             ] 36/63 batches, loss: 0.7099Epoch 3/10: [=================             ] 37/63 batches, loss: 0.7104Epoch 3/10: [==================            ] 38/63 batches, loss: 0.7113Epoch 3/10: [==================            ] 39/63 batches, loss: 0.7110Epoch 3/10: [===================           ] 40/63 batches, loss: 0.7113Epoch 3/10: [===================           ] 41/63 batches, loss: 0.7124Epoch 3/10: [====================          ] 42/63 batches, loss: 0.7132Epoch 3/10: [====================          ] 43/63 batches, loss: 0.7136Epoch 3/10: [====================          ] 44/63 batches, loss: 0.7127Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.7123Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.7108Epoch 3/10: [======================        ] 47/63 batches, loss: 0.7112Epoch 3/10: [======================        ] 48/63 batches, loss: 0.7137Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.7127Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.7122Epoch 3/10: [========================      ] 51/63 batches, loss: 0.7117Epoch 3/10: [========================      ] 52/63 batches, loss: 0.7126Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.7142Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.7140Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.7151Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.7165Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.7172Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.7179Epoch 3/10: [============================  ] 59/63 batches, loss: 0.7166Epoch 3/10: [============================  ] 60/63 batches, loss: 0.7166Epoch 3/10: [============================= ] 61/63 batches, loss: 0.7164Epoch 3/10: [============================= ] 62/63 batches, loss: 0.7163Epoch 3/10: [==============================] 63/63 batches, loss: 0.7162
[2025-05-01 12:01:25,354][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.7162
[2025-05-01 12:01:25,578][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.7353, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.7210Epoch 4/10: [                              ] 2/63 batches, loss: 0.7226Epoch 4/10: [=                             ] 3/63 batches, loss: 0.7142Epoch 4/10: [=                             ] 4/63 batches, loss: 0.7151Epoch 4/10: [==                            ] 5/63 batches, loss: 0.7175Epoch 4/10: [==                            ] 6/63 batches, loss: 0.7100Epoch 4/10: [===                           ] 7/63 batches, loss: 0.7176Epoch 4/10: [===                           ] 8/63 batches, loss: 0.7123Epoch 4/10: [====                          ] 9/63 batches, loss: 0.7089Epoch 4/10: [====                          ] 10/63 batches, loss: 0.7099Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.7182Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.7133Epoch 4/10: [======                        ] 13/63 batches, loss: 0.7125Epoch 4/10: [======                        ] 14/63 batches, loss: 0.7127Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.7158Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.7144Epoch 4/10: [========                      ] 17/63 batches, loss: 0.7141Epoch 4/10: [========                      ] 18/63 batches, loss: 0.7161Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.7148Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.7149Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.7142Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.7121Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.7097Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.7081Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.7093Epoch 4/10: [============                  ] 26/63 batches, loss: 0.7100Epoch 4/10: [============                  ] 27/63 batches, loss: 0.7090Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.7101Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.7074Epoch 4/10: [==============                ] 30/63 batches, loss: 0.7084Epoch 4/10: [==============                ] 31/63 batches, loss: 0.7075Epoch 4/10: [===============               ] 32/63 batches, loss: 0.7065Epoch 4/10: [===============               ] 33/63 batches, loss: 0.7062Epoch 4/10: [================              ] 34/63 batches, loss: 0.7104Epoch 4/10: [================              ] 35/63 batches, loss: 0.7093Epoch 4/10: [=================             ] 36/63 batches, loss: 0.7079Epoch 4/10: [=================             ] 37/63 batches, loss: 0.7123Epoch 4/10: [==================            ] 38/63 batches, loss: 0.7127Epoch 4/10: [==================            ] 39/63 batches, loss: 0.7102Epoch 4/10: [===================           ] 40/63 batches, loss: 0.7097Epoch 4/10: [===================           ] 41/63 batches, loss: 0.7088Epoch 4/10: [====================          ] 42/63 batches, loss: 0.7109Epoch 4/10: [====================          ] 43/63 batches, loss: 0.7107Epoch 4/10: [====================          ] 44/63 batches, loss: 0.7110Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.7123Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.7124Epoch 4/10: [======================        ] 47/63 batches, loss: 0.7116Epoch 4/10: [======================        ] 48/63 batches, loss: 0.7139Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.7142Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.7140Epoch 4/10: [========================      ] 51/63 batches, loss: 0.7144Epoch 4/10: [========================      ] 52/63 batches, loss: 0.7145Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.7148Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.7135Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.7133Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.7129Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.7139Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.7136Epoch 4/10: [============================  ] 59/63 batches, loss: 0.7134Epoch 4/10: [============================  ] 60/63 batches, loss: 0.7134Epoch 4/10: [============================= ] 61/63 batches, loss: 0.7142Epoch 4/10: [============================= ] 62/63 batches, loss: 0.7140Epoch 4/10: [==============================] 63/63 batches, loss: 0.7130
[2025-05-01 12:01:32,300][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.7130
[2025-05-01 12:01:32,515][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.7382, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:01:32,515][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6852Epoch 5/10: [                              ] 2/63 batches, loss: 0.7013Epoch 5/10: [=                             ] 3/63 batches, loss: 0.6919Epoch 5/10: [=                             ] 4/63 batches, loss: 0.7123Epoch 5/10: [==                            ] 5/63 batches, loss: 0.6979Epoch 5/10: [==                            ] 6/63 batches, loss: 0.7003Epoch 5/10: [===                           ] 7/63 batches, loss: 0.6932Epoch 5/10: [===                           ] 8/63 batches, loss: 0.6916Epoch 5/10: [====                          ] 9/63 batches, loss: 0.6914Epoch 5/10: [====                          ] 10/63 batches, loss: 0.6921Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.6997Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.7030Epoch 5/10: [======                        ] 13/63 batches, loss: 0.7024Epoch 5/10: [======                        ] 14/63 batches, loss: 0.7006Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.7011Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.6995Epoch 5/10: [========                      ] 17/63 batches, loss: 0.7032Epoch 5/10: [========                      ] 18/63 batches, loss: 0.7054Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.7080Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.7056Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.7081Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.7111Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.7102Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.7114Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.7118Epoch 5/10: [============                  ] 26/63 batches, loss: 0.7122Epoch 5/10: [============                  ] 27/63 batches, loss: 0.7090Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.7088Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.7085Epoch 5/10: [==============                ] 30/63 batches, loss: 0.7081Epoch 5/10: [==============                ] 31/63 batches, loss: 0.7075Epoch 5/10: [===============               ] 32/63 batches, loss: 0.7066Epoch 5/10: [===============               ] 33/63 batches, loss: 0.7070Epoch 5/10: [================              ] 34/63 batches, loss: 0.7075Epoch 5/10: [================              ] 35/63 batches, loss: 0.7073Epoch 5/10: [=================             ] 36/63 batches, loss: 0.7066Epoch 5/10: [=================             ] 37/63 batches, loss: 0.7066Epoch 5/10: [==================            ] 38/63 batches, loss: 0.7079Epoch 5/10: [==================            ] 39/63 batches, loss: 0.7075Epoch 5/10: [===================           ] 40/63 batches, loss: 0.7078Epoch 5/10: [===================           ] 41/63 batches, loss: 0.7089Epoch 5/10: [====================          ] 42/63 batches, loss: 0.7080Epoch 5/10: [====================          ] 43/63 batches, loss: 0.7104Epoch 5/10: [====================          ] 44/63 batches, loss: 0.7111Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.7128Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.7132Epoch 5/10: [======================        ] 47/63 batches, loss: 0.7124Epoch 5/10: [======================        ] 48/63 batches, loss: 0.7132Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.7135Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.7122Epoch 5/10: [========================      ] 51/63 batches, loss: 0.7101Epoch 5/10: [========================      ] 52/63 batches, loss: 0.7088Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.7107Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.7110Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.7115Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.7112Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.7115Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.7124Epoch 5/10: [============================  ] 59/63 batches, loss: 0.7134Epoch 5/10: [============================  ] 60/63 batches, loss: 0.7146Epoch 5/10: [============================= ] 61/63 batches, loss: 0.7153Epoch 5/10: [============================= ] 62/63 batches, loss: 0.7151Epoch 5/10: [==============================] 63/63 batches, loss: 0.7148
[2025-05-01 12:01:38,924][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.7148
[2025-05-01 12:01:39,138][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.7381, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:01:39,138][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.7013Epoch 6/10: [                              ] 2/63 batches, loss: 0.7352Epoch 6/10: [=                             ] 3/63 batches, loss: 0.7144Epoch 6/10: [=                             ] 4/63 batches, loss: 0.7346Epoch 6/10: [==                            ] 5/63 batches, loss: 0.7303Epoch 6/10: [==                            ] 6/63 batches, loss: 0.7231Epoch 6/10: [===                           ] 7/63 batches, loss: 0.7239Epoch 6/10: [===                           ] 8/63 batches, loss: 0.7264Epoch 6/10: [====                          ] 9/63 batches, loss: 0.7322Epoch 6/10: [====                          ] 10/63 batches, loss: 0.7233Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.7155Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.7077Epoch 6/10: [======                        ] 13/63 batches, loss: 0.7074Epoch 6/10: [======                        ] 14/63 batches, loss: 0.7088Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.7038Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.7066Epoch 6/10: [========                      ] 17/63 batches, loss: 0.7069Epoch 6/10: [========                      ] 18/63 batches, loss: 0.7015Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.7043Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.7090Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.7078Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.7103Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.7089Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.7085Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.7087Epoch 6/10: [============                  ] 26/63 batches, loss: 0.7088Epoch 6/10: [============                  ] 27/63 batches, loss: 0.7080Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.7067Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.7061Epoch 6/10: [==============                ] 30/63 batches, loss: 0.7071Epoch 6/10: [==============                ] 31/63 batches, loss: 0.7069Epoch 6/10: [===============               ] 32/63 batches, loss: 0.7070Epoch 6/10: [===============               ] 33/63 batches, loss: 0.7049Epoch 6/10: [================              ] 34/63 batches, loss: 0.7044Epoch 6/10: [================              ] 35/63 batches, loss: 0.7056Epoch 6/10: [=================             ] 36/63 batches, loss: 0.7080Epoch 6/10: [=================             ] 37/63 batches, loss: 0.7089Epoch 6/10: [==================            ] 38/63 batches, loss: 0.7102Epoch 6/10: [==================            ] 39/63 batches, loss: 0.7092Epoch 6/10: [===================           ] 40/63 batches, loss: 0.7080Epoch 6/10: [===================           ] 41/63 batches, loss: 0.7076Epoch 6/10: [====================          ] 42/63 batches, loss: 0.7082Epoch 6/10: [====================          ] 43/63 batches, loss: 0.7108Epoch 6/10: [====================          ] 44/63 batches, loss: 0.7098Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.7110Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.7131Epoch 6/10: [======================        ] 47/63 batches, loss: 0.7128Epoch 6/10: [======================        ] 48/63 batches, loss: 0.7116Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.7113Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.7093Epoch 6/10: [========================      ] 51/63 batches, loss: 0.7099Epoch 6/10: [========================      ] 52/63 batches, loss: 0.7079Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.7080Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.7091Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.7104Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.7107Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.7113Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.7106Epoch 6/10: [============================  ] 59/63 batches, loss: 0.7115Epoch 6/10: [============================  ] 60/63 batches, loss: 0.7121Epoch 6/10: [============================= ] 61/63 batches, loss: 0.7140Epoch 6/10: [============================= ] 62/63 batches, loss: 0.7147Epoch 6/10: [==============================] 63/63 batches, loss: 0.7180
[2025-05-01 12:01:45,512][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.7180
[2025-05-01 12:01:45,723][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.7344, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.7543Epoch 7/10: [                              ] 2/63 batches, loss: 0.7065Epoch 7/10: [=                             ] 3/63 batches, loss: 0.6790Epoch 7/10: [=                             ] 4/63 batches, loss: 0.6962Epoch 7/10: [==                            ] 5/63 batches, loss: 0.6981Epoch 7/10: [==                            ] 6/63 batches, loss: 0.7014Epoch 7/10: [===                           ] 7/63 batches, loss: 0.7061Epoch 7/10: [===                           ] 8/63 batches, loss: 0.7077Epoch 7/10: [====                          ] 9/63 batches, loss: 0.6996Epoch 7/10: [====                          ] 10/63 batches, loss: 0.7072Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.7105Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.7043Epoch 7/10: [======                        ] 13/63 batches, loss: 0.6994Epoch 7/10: [======                        ] 14/63 batches, loss: 0.7030Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.7058Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.7052Epoch 7/10: [========                      ] 17/63 batches, loss: 0.7026Epoch 7/10: [========                      ] 18/63 batches, loss: 0.7015Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.6993Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.7008Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.6977Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.6984Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.6974Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.6960Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.6969Epoch 7/10: [============                  ] 26/63 batches, loss: 0.6948Epoch 7/10: [============                  ] 27/63 batches, loss: 0.6956Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.6948Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.6963Epoch 7/10: [==============                ] 30/63 batches, loss: 0.6979Epoch 7/10: [==============                ] 31/63 batches, loss: 0.6989Epoch 7/10: [===============               ] 32/63 batches, loss: 0.6998Epoch 7/10: [===============               ] 33/63 batches, loss: 0.7018Epoch 7/10: [================              ] 34/63 batches, loss: 0.7016Epoch 7/10: [================              ] 35/63 batches, loss: 0.7043Epoch 7/10: [=================             ] 36/63 batches, loss: 0.7040Epoch 7/10: [=================             ] 37/63 batches, loss: 0.7029Epoch 7/10: [==================            ] 38/63 batches, loss: 0.7039Epoch 7/10: [==================            ] 39/63 batches, loss: 0.7024Epoch 7/10: [===================           ] 40/63 batches, loss: 0.7022Epoch 7/10: [===================           ] 41/63 batches, loss: 0.7022Epoch 7/10: [====================          ] 42/63 batches, loss: 0.7021Epoch 7/10: [====================          ] 43/63 batches, loss: 0.7015Epoch 7/10: [====================          ] 44/63 batches, loss: 0.7008Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.7007Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.7011Epoch 7/10: [======================        ] 47/63 batches, loss: 0.7020Epoch 7/10: [======================        ] 48/63 batches, loss: 0.7021Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.7021Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.7017Epoch 7/10: [========================      ] 51/63 batches, loss: 0.7028Epoch 7/10: [========================      ] 52/63 batches, loss: 0.7026Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.7029Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.7034Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.7034Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.7036Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.7042Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.7045Epoch 7/10: [============================  ] 59/63 batches, loss: 0.7046Epoch 7/10: [============================  ] 60/63 batches, loss: 0.7049Epoch 7/10: [============================= ] 61/63 batches, loss: 0.7045Epoch 7/10: [============================= ] 62/63 batches, loss: 0.7049Epoch 7/10: [==============================] 63/63 batches, loss: 0.7040
[2025-05-01 12:01:52,468][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.7040
[2025-05-01 12:01:52,664][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.7212, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.7678Epoch 8/10: [                              ] 2/63 batches, loss: 0.7340Epoch 8/10: [=                             ] 3/63 batches, loss: 0.7229Epoch 8/10: [=                             ] 4/63 batches, loss: 0.7263Epoch 8/10: [==                            ] 5/63 batches, loss: 0.7258Epoch 8/10: [==                            ] 6/63 batches, loss: 0.7177Epoch 8/10: [===                           ] 7/63 batches, loss: 0.7146Epoch 8/10: [===                           ] 8/63 batches, loss: 0.7109Epoch 8/10: [====                          ] 9/63 batches, loss: 0.7128Epoch 8/10: [====                          ] 10/63 batches, loss: 0.7133Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.7144Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.7136Epoch 8/10: [======                        ] 13/63 batches, loss: 0.7122Epoch 8/10: [======                        ] 14/63 batches, loss: 0.7133Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.7136Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.7121Epoch 8/10: [========                      ] 17/63 batches, loss: 0.7106Epoch 8/10: [========                      ] 18/63 batches, loss: 0.7103Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.7089Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.7080Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.7071Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.7059Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.7047Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.7048Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.7049Epoch 8/10: [============                  ] 26/63 batches, loss: 0.7052Epoch 8/10: [============                  ] 27/63 batches, loss: 0.7050Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.7049Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.7048Epoch 8/10: [==============                ] 30/63 batches, loss: 0.7051Epoch 8/10: [==============                ] 31/63 batches, loss: 0.7054Epoch 8/10: [===============               ] 32/63 batches, loss: 0.7048Epoch 8/10: [===============               ] 33/63 batches, loss: 0.7046Epoch 8/10: [================              ] 34/63 batches, loss: 0.7044Epoch 8/10: [================              ] 35/63 batches, loss: 0.7041Epoch 8/10: [=================             ] 36/63 batches, loss: 0.7037Epoch 8/10: [=================             ] 37/63 batches, loss: 0.7038Epoch 8/10: [==================            ] 38/63 batches, loss: 0.7034Epoch 8/10: [==================            ] 39/63 batches, loss: 0.7035Epoch 8/10: [===================           ] 40/63 batches, loss: 0.7034Epoch 8/10: [===================           ] 41/63 batches, loss: 0.7030Epoch 8/10: [====================          ] 42/63 batches, loss: 0.7026Epoch 8/10: [====================          ] 43/63 batches, loss: 0.7023Epoch 8/10: [====================          ] 44/63 batches, loss: 0.7021Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.7020Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.7018Epoch 8/10: [======================        ] 47/63 batches, loss: 0.7016Epoch 8/10: [======================        ] 48/63 batches, loss: 0.7015Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.7012Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.7010Epoch 8/10: [========================      ] 51/63 batches, loss: 0.7007Epoch 8/10: [========================      ] 52/63 batches, loss: 0.7006Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.7006Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.7005Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.7004Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.7001Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.7000Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.6998Epoch 8/10: [============================  ] 59/63 batches, loss: 0.6995Epoch 8/10: [============================  ] 60/63 batches, loss: 0.6994Epoch 8/10: [============================= ] 61/63 batches, loss: 0.6992Epoch 8/10: [============================= ] 62/63 batches, loss: 0.6992Epoch 8/10: [==============================] 63/63 batches, loss: 0.6991
[2025-05-01 12:01:59,479][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6991
[2025-05-01 12:01:59,698][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.7077, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.6985Epoch 9/10: [                              ] 2/63 batches, loss: 0.6861Epoch 9/10: [=                             ] 3/63 batches, loss: 0.6998Epoch 9/10: [=                             ] 4/63 batches, loss: 0.7021Epoch 9/10: [==                            ] 5/63 batches, loss: 0.7046Epoch 9/10: [==                            ] 6/63 batches, loss: 0.7018Epoch 9/10: [===                           ] 7/63 batches, loss: 0.7069Epoch 9/10: [===                           ] 8/63 batches, loss: 0.7066Epoch 9/10: [====                          ] 9/63 batches, loss: 0.7025Epoch 9/10: [====                          ] 10/63 batches, loss: 0.7000Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.7011Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.7020Epoch 9/10: [======                        ] 13/63 batches, loss: 0.7009Epoch 9/10: [======                        ] 14/63 batches, loss: 0.7019Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.7021Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.7017Epoch 9/10: [========                      ] 17/63 batches, loss: 0.7010Epoch 9/10: [========                      ] 18/63 batches, loss: 0.7003Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.6998Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.6995Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.6992Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.6989Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.6986Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.6984Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.6982Epoch 9/10: [============                  ] 26/63 batches, loss: 0.6980Epoch 9/10: [============                  ] 27/63 batches, loss: 0.6978Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.6977Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.6975Epoch 9/10: [==============                ] 30/63 batches, loss: 0.6973Epoch 9/10: [==============                ] 31/63 batches, loss: 0.6972Epoch 9/10: [===============               ] 32/63 batches, loss: 0.6971Epoch 9/10: [===============               ] 33/63 batches, loss: 0.6970Epoch 9/10: [================              ] 34/63 batches, loss: 0.6968Epoch 9/10: [================              ] 35/63 batches, loss: 0.6967Epoch 9/10: [=================             ] 36/63 batches, loss: 0.6966Epoch 9/10: [=================             ] 37/63 batches, loss: 0.6965Epoch 9/10: [==================            ] 38/63 batches, loss: 0.6964Epoch 9/10: [==================            ] 39/63 batches, loss: 0.6963Epoch 9/10: [===================           ] 40/63 batches, loss: 0.6962Epoch 9/10: [===================           ] 41/63 batches, loss: 0.6962Epoch 9/10: [====================          ] 42/63 batches, loss: 0.6961Epoch 9/10: [====================          ] 43/63 batches, loss: 0.6961Epoch 9/10: [====================          ] 44/63 batches, loss: 0.6960Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.6959Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.6958Epoch 9/10: [======================        ] 47/63 batches, loss: 0.6958Epoch 9/10: [======================        ] 48/63 batches, loss: 0.6958Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.6957Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.6957Epoch 9/10: [========================      ] 51/63 batches, loss: 0.6956Epoch 9/10: [========================      ] 52/63 batches, loss: 0.6956Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.6956Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.6955Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.6955Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.6955Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.6954Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.6954Epoch 9/10: [============================  ] 59/63 batches, loss: 0.6953Epoch 9/10: [============================  ] 60/63 batches, loss: 0.6953Epoch 9/10: [============================= ] 61/63 batches, loss: 0.6952Epoch 9/10: [============================= ] 62/63 batches, loss: 0.6952Epoch 9/10: [==============================] 63/63 batches, loss: 0.6952
[2025-05-01 12:02:06,454][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6952
[2025-05-01 12:02:06,680][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.6940Epoch 10/10: [                              ] 2/63 batches, loss: 0.6934Epoch 10/10: [=                             ] 3/63 batches, loss: 0.6932Epoch 10/10: [=                             ] 4/63 batches, loss: 0.6937Epoch 10/10: [==                            ] 5/63 batches, loss: 0.6937Epoch 10/10: [==                            ] 6/63 batches, loss: 0.6936Epoch 10/10: [===                           ] 7/63 batches, loss: 0.6935Epoch 10/10: [===                           ] 8/63 batches, loss: 0.6934Epoch 10/10: [====                          ] 9/63 batches, loss: 0.6934Epoch 10/10: [====                          ] 10/63 batches, loss: 0.6935Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.6934Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.6935Epoch 10/10: [======                        ] 13/63 batches, loss: 0.6934Epoch 10/10: [======                        ] 14/63 batches, loss: 0.6934Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.6934Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.6933Epoch 10/10: [========                      ] 17/63 batches, loss: 0.6933Epoch 10/10: [========                      ] 18/63 batches, loss: 0.6934Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.6934Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.6934Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.6934Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.6934Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.6934Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.6934Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.6934Epoch 10/10: [============                  ] 26/63 batches, loss: 0.6935Epoch 10/10: [============                  ] 27/63 batches, loss: 0.6935Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.6935Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.6935Epoch 10/10: [==============                ] 30/63 batches, loss: 0.6935Epoch 10/10: [==============                ] 31/63 batches, loss: 0.6935Epoch 10/10: [===============               ] 32/63 batches, loss: 0.6934Epoch 10/10: [===============               ] 33/63 batches, loss: 0.6934Epoch 10/10: [================              ] 34/63 batches, loss: 0.6935Epoch 10/10: [================              ] 35/63 batches, loss: 0.6934Epoch 10/10: [=================             ] 36/63 batches, loss: 0.6934Epoch 10/10: [=================             ] 37/63 batches, loss: 0.6934Epoch 10/10: [==================            ] 38/63 batches, loss: 0.6934Epoch 10/10: [==================            ] 39/63 batches, loss: 0.6935Epoch 10/10: [===================           ] 40/63 batches, loss: 0.6935Epoch 10/10: [===================           ] 41/63 batches, loss: 0.6936Epoch 10/10: [====================          ] 42/63 batches, loss: 0.6935Epoch 10/10: [====================          ] 43/63 batches, loss: 0.6936Epoch 10/10: [====================          ] 44/63 batches, loss: 0.6935Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.6935Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.6935Epoch 10/10: [======================        ] 47/63 batches, loss: 0.6935Epoch 10/10: [======================        ] 48/63 batches, loss: 0.6935Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.6935Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.6935Epoch 10/10: [========================      ] 51/63 batches, loss: 0.6934Epoch 10/10: [========================      ] 52/63 batches, loss: 0.6934Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.6934Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.6933Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.6933Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.6932Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.6934Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.6934Epoch 10/10: [============================  ] 59/63 batches, loss: 0.6934Epoch 10/10: [============================  ] 60/63 batches, loss: 0.6933Epoch 10/10: [============================= ] 61/63 batches, loss: 0.6933Epoch 10/10: [============================= ] 62/63 batches, loss: 0.6933Epoch 10/10: [==============================] 63/63 batches, loss: 0.6929
[2025-05-01 12:02:13,431][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6929
[2025-05-01 12:02:13,654][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6939, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:02:13,655][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-01 12:02:13,655][src.training.lm_trainer][INFO] - Training completed in 69.65 seconds
[2025-05-01 12:02:13,655][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:02:16,211][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:02:16,212][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:02:16,213][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:02:17,906][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1/ar/model.pt
[2025-05-01 12:02:17,912][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁▁▁
wandb:         best_val_loss █▇▇▇▅▃▁
wandb:    best_val_precision ▁▁▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁▁▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▂▂
wandb:            train_loss █▅▆▅▆▆▃▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:              val_loss █▇▇▇▇▇▅▃▁▁
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69353
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:                 epoch 10
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 2e-05
wandb:            train_loss 0.69286
wandb:            train_time 69.64762
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69391
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120054-bpk0fk2j
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120054-bpk0fk2j/logs
Experiment finetune_question_type_control1_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control1/results.json
Running experiment: finetune_question_type_control2_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_control2_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:02:29,920][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2
experiment_name: finetune_question_type_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:02:29,920][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:02:29,920][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:02:29,920][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:02:29,924][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 12:02:29,925][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:02:31,360][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:02:33,567][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:02:33,567][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:02:33,613][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-01 12:02:33,642][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-01 12:02:33,748][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:02:33,755][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:02:33,756][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:02:33,757][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:02:33,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:02:33,810][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:02:33,823][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:02:33,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:02:33,824][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:02:33,825][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:02:33,849][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:02:33,883][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:02:33,896][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:02:33,897][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:02:33,897][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:02:33,898][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:02:33,898][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:02:33,898][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:02:33,898][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:02:33,899][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 12:02:33,899][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:02:33,899][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:02:33,900][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 12:02:33,900][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:02:33,900][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:02:33,900][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 12:02:33,900][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 12:02:33,901][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:02:33,901][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:02:33,901][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:02:33,901][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:02:33,901][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:02:33,901][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:02:33,901][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:02:37,936][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:02:37,937][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:02:37,937][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:02:37,937][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:02:37,941][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:02:37,942][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:02:37,942][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:02:37,942][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:02:37,943][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:02:37,943][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.7235Epoch 1/10: [                              ] 2/63 batches, loss: 0.7223Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7416Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7223Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7276Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7168Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7172Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7324Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7272Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7217Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7216Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7241Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7256Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7292Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7245Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7280Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7337Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7333Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7322Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7294Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7256Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7237Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7270Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7302Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7308Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7310Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7290Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7253Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7233Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7267Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7256Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7265Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7276Epoch 1/10: [================              ] 34/63 batches, loss: 0.7299Epoch 1/10: [================              ] 35/63 batches, loss: 0.7291Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7271Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7278Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7287Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7289Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7289Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7262Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7272Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7266Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7281Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7299Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7295Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7274Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7275Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7268Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7267Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7253Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7250Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7248Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7233Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7222Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7215Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7200Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7183Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7188Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7192Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7198Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7203Epoch 1/10: [==============================] 63/63 batches, loss: 0.7191
[2025-05-01 12:02:46,750][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7191
[2025-05-01 12:02:46,940][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7392, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.6199Epoch 2/10: [                              ] 2/63 batches, loss: 0.6682Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7030Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7014Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7018Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6996Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7090Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7173Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7242Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7208Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7189Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7182Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7199Epoch 2/10: [======                        ] 14/63 batches, loss: 0.7208Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.7211Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.7185Epoch 2/10: [========                      ] 17/63 batches, loss: 0.7205Epoch 2/10: [========                      ] 18/63 batches, loss: 0.7218Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.7249Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.7204Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.7207Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.7233Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.7238Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.7222Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.7224Epoch 2/10: [============                  ] 26/63 batches, loss: 0.7256Epoch 2/10: [============                  ] 27/63 batches, loss: 0.7243Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.7226Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.7202Epoch 2/10: [==============                ] 30/63 batches, loss: 0.7212Epoch 2/10: [==============                ] 31/63 batches, loss: 0.7205Epoch 2/10: [===============               ] 32/63 batches, loss: 0.7203Epoch 2/10: [===============               ] 33/63 batches, loss: 0.7201Epoch 2/10: [================              ] 34/63 batches, loss: 0.7200Epoch 2/10: [================              ] 35/63 batches, loss: 0.7209Epoch 2/10: [=================             ] 36/63 batches, loss: 0.7204Epoch 2/10: [=================             ] 37/63 batches, loss: 0.7179Epoch 2/10: [==================            ] 38/63 batches, loss: 0.7178Epoch 2/10: [==================            ] 39/63 batches, loss: 0.7191Epoch 2/10: [===================           ] 40/63 batches, loss: 0.7196Epoch 2/10: [===================           ] 41/63 batches, loss: 0.7180Epoch 2/10: [====================          ] 42/63 batches, loss: 0.7178Epoch 2/10: [====================          ] 43/63 batches, loss: 0.7148Epoch 2/10: [====================          ] 44/63 batches, loss: 0.7137Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.7135Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.7136Epoch 2/10: [======================        ] 47/63 batches, loss: 0.7124Epoch 2/10: [======================        ] 48/63 batches, loss: 0.7125Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.7138Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.7131Epoch 2/10: [========================      ] 51/63 batches, loss: 0.7119Epoch 2/10: [========================      ] 52/63 batches, loss: 0.7129Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.7143Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.7144Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.7149Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.7152Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.7147Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.7148Epoch 2/10: [============================  ] 59/63 batches, loss: 0.7150Epoch 2/10: [============================  ] 60/63 batches, loss: 0.7149Epoch 2/10: [============================= ] 61/63 batches, loss: 0.7142Epoch 2/10: [============================= ] 62/63 batches, loss: 0.7141Epoch 2/10: [==============================] 63/63 batches, loss: 0.7147
[2025-05-01 12:02:53,687][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.7147
[2025-05-01 12:02:53,894][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6917, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6680Epoch 3/10: [                              ] 2/63 batches, loss: 0.6799Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6879Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6827Epoch 3/10: [==                            ] 5/63 batches, loss: 0.6902Epoch 3/10: [==                            ] 6/63 batches, loss: 0.6925Epoch 3/10: [===                           ] 7/63 batches, loss: 0.6920Epoch 3/10: [===                           ] 8/63 batches, loss: 0.6916Epoch 3/10: [====                          ] 9/63 batches, loss: 0.6925Epoch 3/10: [====                          ] 10/63 batches, loss: 0.6972Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.6956Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.6969Epoch 3/10: [======                        ] 13/63 batches, loss: 0.6998Epoch 3/10: [======                        ] 14/63 batches, loss: 0.7027Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.7044Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.7048Epoch 3/10: [========                      ] 17/63 batches, loss: 0.7048Epoch 3/10: [========                      ] 18/63 batches, loss: 0.7024Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.7057Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.7105Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.7094Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.7103Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.7094Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.7065Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.7047Epoch 3/10: [============                  ] 26/63 batches, loss: 0.7063Epoch 3/10: [============                  ] 27/63 batches, loss: 0.7062Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.7051Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.7041Epoch 3/10: [==============                ] 30/63 batches, loss: 0.7024Epoch 3/10: [==============                ] 31/63 batches, loss: 0.7046Epoch 3/10: [===============               ] 32/63 batches, loss: 0.7063Epoch 3/10: [===============               ] 33/63 batches, loss: 0.7069Epoch 3/10: [================              ] 34/63 batches, loss: 0.7064Epoch 3/10: [================              ] 35/63 batches, loss: 0.7064Epoch 3/10: [=================             ] 36/63 batches, loss: 0.7074Epoch 3/10: [=================             ] 37/63 batches, loss: 0.7096Epoch 3/10: [==================            ] 38/63 batches, loss: 0.7091Epoch 3/10: [==================            ] 39/63 batches, loss: 0.7096Epoch 3/10: [===================           ] 40/63 batches, loss: 0.7084Epoch 3/10: [===================           ] 41/63 batches, loss: 0.7088Epoch 3/10: [====================          ] 42/63 batches, loss: 0.7111Epoch 3/10: [====================          ] 43/63 batches, loss: 0.7103Epoch 3/10: [====================          ] 44/63 batches, loss: 0.7101Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.7090Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.7089Epoch 3/10: [======================        ] 47/63 batches, loss: 0.7081Epoch 3/10: [======================        ] 48/63 batches, loss: 0.7077Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.7079Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.7081Epoch 3/10: [========================      ] 51/63 batches, loss: 0.7077Epoch 3/10: [========================      ] 52/63 batches, loss: 0.7076Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.7087Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.7097Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.7103Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.7100Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.7113Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.7112Epoch 3/10: [============================  ] 59/63 batches, loss: 0.7133Epoch 3/10: [============================  ] 60/63 batches, loss: 0.7138Epoch 3/10: [============================= ] 61/63 batches, loss: 0.7132Epoch 3/10: [============================= ] 62/63 batches, loss: 0.7128Epoch 3/10: [==============================] 63/63 batches, loss: 0.7150
[2025-05-01 12:03:00,680][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.7150
[2025-05-01 12:03:00,894][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.7378, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:00,894][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.7252Epoch 4/10: [                              ] 2/63 batches, loss: 0.6871Epoch 4/10: [=                             ] 3/63 batches, loss: 0.7101Epoch 4/10: [=                             ] 4/63 batches, loss: 0.7152Epoch 4/10: [==                            ] 5/63 batches, loss: 0.7369Epoch 4/10: [==                            ] 6/63 batches, loss: 0.7264Epoch 4/10: [===                           ] 7/63 batches, loss: 0.7212Epoch 4/10: [===                           ] 8/63 batches, loss: 0.7271Epoch 4/10: [====                          ] 9/63 batches, loss: 0.7267Epoch 4/10: [====                          ] 10/63 batches, loss: 0.7243Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.7236Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.7260Epoch 4/10: [======                        ] 13/63 batches, loss: 0.7215Epoch 4/10: [======                        ] 14/63 batches, loss: 0.7241Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.7225Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.7254Epoch 4/10: [========                      ] 17/63 batches, loss: 0.7223Epoch 4/10: [========                      ] 18/63 batches, loss: 0.7193Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.7198Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.7154Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.7166Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.7183Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.7169Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.7161Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.7150Epoch 4/10: [============                  ] 26/63 batches, loss: 0.7165Epoch 4/10: [============                  ] 27/63 batches, loss: 0.7159Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.7133Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.7143Epoch 4/10: [==============                ] 30/63 batches, loss: 0.7143Epoch 4/10: [==============                ] 31/63 batches, loss: 0.7130Epoch 4/10: [===============               ] 32/63 batches, loss: 0.7133Epoch 4/10: [===============               ] 33/63 batches, loss: 0.7136Epoch 4/10: [================              ] 34/63 batches, loss: 0.7141Epoch 4/10: [================              ] 35/63 batches, loss: 0.7155Epoch 4/10: [=================             ] 36/63 batches, loss: 0.7154Epoch 4/10: [=================             ] 37/63 batches, loss: 0.7150Epoch 4/10: [==================            ] 38/63 batches, loss: 0.7146Epoch 4/10: [==================            ] 39/63 batches, loss: 0.7143Epoch 4/10: [===================           ] 40/63 batches, loss: 0.7143Epoch 4/10: [===================           ] 41/63 batches, loss: 0.7127Epoch 4/10: [====================          ] 42/63 batches, loss: 0.7125Epoch 4/10: [====================          ] 43/63 batches, loss: 0.7139Epoch 4/10: [====================          ] 44/63 batches, loss: 0.7133Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.7119Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.7107Epoch 4/10: [======================        ] 47/63 batches, loss: 0.7099Epoch 4/10: [======================        ] 48/63 batches, loss: 0.7107Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.7126Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.7123Epoch 4/10: [========================      ] 51/63 batches, loss: 0.7126Epoch 4/10: [========================      ] 52/63 batches, loss: 0.7136Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.7119Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.7115Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.7113Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.7128Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.7119Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.7128Epoch 4/10: [============================  ] 59/63 batches, loss: 0.7144Epoch 4/10: [============================  ] 60/63 batches, loss: 0.7143Epoch 4/10: [============================= ] 61/63 batches, loss: 0.7145Epoch 4/10: [============================= ] 62/63 batches, loss: 0.7147Epoch 4/10: [==============================] 63/63 batches, loss: 0.7135
[2025-05-01 12:03:07,279][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.7135
[2025-05-01 12:03:07,486][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.7305, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:07,487][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6844Epoch 5/10: [                              ] 2/63 batches, loss: 0.7229Epoch 5/10: [=                             ] 3/63 batches, loss: 0.7524Epoch 5/10: [=                             ] 4/63 batches, loss: 0.7434Epoch 5/10: [==                            ] 5/63 batches, loss: 0.7269Epoch 5/10: [==                            ] 6/63 batches, loss: 0.7287Epoch 5/10: [===                           ] 7/63 batches, loss: 0.7245Epoch 5/10: [===                           ] 8/63 batches, loss: 0.7165Epoch 5/10: [====                          ] 9/63 batches, loss: 0.7134Epoch 5/10: [====                          ] 10/63 batches, loss: 0.7178Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.7238Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.7223Epoch 5/10: [======                        ] 13/63 batches, loss: 0.7167Epoch 5/10: [======                        ] 14/63 batches, loss: 0.7139Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.7134Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.7152Epoch 5/10: [========                      ] 17/63 batches, loss: 0.7159Epoch 5/10: [========                      ] 18/63 batches, loss: 0.7145Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.7175Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.7171Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.7142Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.7148Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.7154Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.7145Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.7148Epoch 5/10: [============                  ] 26/63 batches, loss: 0.7172Epoch 5/10: [============                  ] 27/63 batches, loss: 0.7132Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.7132Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.7125Epoch 5/10: [==============                ] 30/63 batches, loss: 0.7105Epoch 5/10: [==============                ] 31/63 batches, loss: 0.7090Epoch 5/10: [===============               ] 32/63 batches, loss: 0.7090Epoch 5/10: [===============               ] 33/63 batches, loss: 0.7082Epoch 5/10: [================              ] 34/63 batches, loss: 0.7070Epoch 5/10: [================              ] 35/63 batches, loss: 0.7050Epoch 5/10: [=================             ] 36/63 batches, loss: 0.7063Epoch 5/10: [=================             ] 37/63 batches, loss: 0.7057Epoch 5/10: [==================            ] 38/63 batches, loss: 0.7075Epoch 5/10: [==================            ] 39/63 batches, loss: 0.7066Epoch 5/10: [===================           ] 40/63 batches, loss: 0.7067Epoch 5/10: [===================           ] 41/63 batches, loss: 0.7081Epoch 5/10: [====================          ] 42/63 batches, loss: 0.7057Epoch 5/10: [====================          ] 43/63 batches, loss: 0.7061Epoch 5/10: [====================          ] 44/63 batches, loss: 0.7062Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.7068Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.7061Epoch 5/10: [======================        ] 47/63 batches, loss: 0.7066Epoch 5/10: [======================        ] 48/63 batches, loss: 0.7067Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.7058Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.7057Epoch 5/10: [========================      ] 51/63 batches, loss: 0.7057Epoch 5/10: [========================      ] 52/63 batches, loss: 0.7051Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.7057Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.7049Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.7052Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.7056Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.7059Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.7050Epoch 5/10: [============================  ] 59/63 batches, loss: 0.7052Epoch 5/10: [============================  ] 60/63 batches, loss: 0.7057Epoch 5/10: [============================= ] 61/63 batches, loss: 0.7054Epoch 5/10: [============================= ] 62/63 batches, loss: 0.7053Epoch 5/10: [==============================] 63/63 batches, loss: 0.7054
[2025-05-01 12:03:13,846][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.7054
[2025-05-01 12:03:14,060][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6966, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:14,061][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:03:14,061][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-01 12:03:14,061][src.training.lm_trainer][INFO] - Training completed in 34.47 seconds
[2025-05-01 12:03:14,061][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:03:16,527][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:16,527][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:16,527][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:03:18,170][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2/ar/model.pt
[2025-05-01 12:03:18,175][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁
wandb:           best_val_f1 ▁▁
wandb:         best_val_loss █▁
wandb:    best_val_precision ▁▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▁▁
wandb:            train_loss █▆▆▅▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁
wandb:              val_loss █▁█▇▂
wandb:         val_precision ▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69171
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 2e-05
wandb:            train_loss 0.70537
wandb:            train_time 34.46528
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69662
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120229-gijrmp9s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120229-gijrmp9s/logs
Experiment finetune_question_type_control2_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control2/results.json
Running experiment: finetune_question_type_control3_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_control3_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control3"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:03:31,003][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control3
experiment_name: finetune_question_type_control3_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:03:31,003][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:03:31,003][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:03:31,003][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:03:31,008][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 12:03:31,008][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:03:32,655][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:03:35,043][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:03:35,043][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:03:35,117][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-01 12:03:35,146][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-01 12:03:35,399][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:03:35,407][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:03:35,408][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:03:35,409][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:03:35,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:03:35,459][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:03:35,472][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:03:35,474][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:03:35,474][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:03:35,475][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:03:35,495][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:03:35,523][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:03:35,537][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:03:35,538][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:03:35,538][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:03:35,539][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:03:35,540][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:03:35,540][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:03:35,540][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:03:35,541][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 12:03:35,541][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:03:35,541][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:03:35,542][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 12:03:35,542][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:03:35,542][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 12:03:35,542][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 12:03:35,542][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:03:35,543][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:03:35,543][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:03:35,543][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:03:35,543][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:03:35,543][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:03:35,543][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:03:39,789][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:03:39,790][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:03:39,790][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:03:39,790][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:03:39,794][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:03:39,795][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:03:39,795][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:03:39,795][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:03:39,796][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:03:39,796][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.6918Epoch 1/10: [                              ] 2/63 batches, loss: 0.7214Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7334Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7241Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7008Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7034Epoch 1/10: [===                           ] 7/63 batches, loss: 0.6969Epoch 1/10: [===                           ] 8/63 batches, loss: 0.6956Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7025Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7067Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7051Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7040Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7105Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7073Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7143Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7116Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7073Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7060Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7071Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7136Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7181Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7177Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7190Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7226Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7216Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7206Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7192Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7197Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7165Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7153Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7145Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7147Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7170Epoch 1/10: [================              ] 34/63 batches, loss: 0.7218Epoch 1/10: [================              ] 35/63 batches, loss: 0.7215Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7202Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7194Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7216Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7197Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7200Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7173Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7169Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7193Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7208Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7215Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7227Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7252Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7261Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7250Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7250Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7231Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7231Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7244Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7230Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7236Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7235Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7236Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7222Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7217Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7210Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7202Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7196Epoch 1/10: [==============================] 63/63 batches, loss: 0.7210
[2025-05-01 12:03:48,610][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7210
[2025-05-01 12:03:48,796][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7422, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7607Epoch 2/10: [                              ] 2/63 batches, loss: 0.7578Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7472Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7203Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7137Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7127Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7147Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6997Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7038Epoch 2/10: [====                          ] 10/63 batches, loss: 0.6987Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7057Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7019Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7015Epoch 2/10: [======                        ] 14/63 batches, loss: 0.7021Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.7050Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.7063Epoch 2/10: [========                      ] 17/63 batches, loss: 0.7081Epoch 2/10: [========                      ] 18/63 batches, loss: 0.7104Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.7134Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.7137Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.7095Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.7087Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.7099Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.7157Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.7177Epoch 2/10: [============                  ] 26/63 batches, loss: 0.7167Epoch 2/10: [============                  ] 27/63 batches, loss: 0.7200Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.7203Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.7189Epoch 2/10: [==============                ] 30/63 batches, loss: 0.7213Epoch 2/10: [==============                ] 31/63 batches, loss: 0.7216Epoch 2/10: [===============               ] 32/63 batches, loss: 0.7225Epoch 2/10: [===============               ] 33/63 batches, loss: 0.7230Epoch 2/10: [================              ] 34/63 batches, loss: 0.7198Epoch 2/10: [================              ] 35/63 batches, loss: 0.7173Epoch 2/10: [=================             ] 36/63 batches, loss: 0.7183Epoch 2/10: [=================             ] 37/63 batches, loss: 0.7188Epoch 2/10: [==================            ] 38/63 batches, loss: 0.7183Epoch 2/10: [==================            ] 39/63 batches, loss: 0.7185Epoch 2/10: [===================           ] 40/63 batches, loss: 0.7186Epoch 2/10: [===================           ] 41/63 batches, loss: 0.7173Epoch 2/10: [====================          ] 42/63 batches, loss: 0.7168Epoch 2/10: [====================          ] 43/63 batches, loss: 0.7146Epoch 2/10: [====================          ] 44/63 batches, loss: 0.7144Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.7136Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.7142Epoch 2/10: [======================        ] 47/63 batches, loss: 0.7157Epoch 2/10: [======================        ] 48/63 batches, loss: 0.7163Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.7170Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.7179Epoch 2/10: [========================      ] 51/63 batches, loss: 0.7182Epoch 2/10: [========================      ] 52/63 batches, loss: 0.7175Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.7185Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.7180Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.7184Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.7174Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.7183Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.7194Epoch 2/10: [============================  ] 59/63 batches, loss: 0.7186Epoch 2/10: [============================  ] 60/63 batches, loss: 0.7184Epoch 2/10: [============================= ] 61/63 batches, loss: 0.7173Epoch 2/10: [============================= ] 62/63 batches, loss: 0.7186Epoch 2/10: [==============================] 63/63 batches, loss: 0.7191
[2025-05-01 12:03:55,585][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.7191
[2025-05-01 12:03:55,784][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.7419, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6898Epoch 3/10: [                              ] 2/63 batches, loss: 0.7536Epoch 3/10: [=                             ] 3/63 batches, loss: 0.7236Epoch 3/10: [=                             ] 4/63 batches, loss: 0.7482Epoch 3/10: [==                            ] 5/63 batches, loss: 0.7489Epoch 3/10: [==                            ] 6/63 batches, loss: 0.7275Epoch 3/10: [===                           ] 7/63 batches, loss: 0.7305Epoch 3/10: [===                           ] 8/63 batches, loss: 0.7223Epoch 3/10: [====                          ] 9/63 batches, loss: 0.7336Epoch 3/10: [====                          ] 10/63 batches, loss: 0.7360Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.7375Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.7384Epoch 3/10: [======                        ] 13/63 batches, loss: 0.7350Epoch 3/10: [======                        ] 14/63 batches, loss: 0.7309Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.7346Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.7318Epoch 3/10: [========                      ] 17/63 batches, loss: 0.7265Epoch 3/10: [========                      ] 18/63 batches, loss: 0.7261Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.7258Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.7298Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.7316Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.7317Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.7288Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.7225Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.7235Epoch 3/10: [============                  ] 26/63 batches, loss: 0.7256Epoch 3/10: [============                  ] 27/63 batches, loss: 0.7261Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.7221Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.7213Epoch 3/10: [==============                ] 30/63 batches, loss: 0.7211Epoch 3/10: [==============                ] 31/63 batches, loss: 0.7180Epoch 3/10: [===============               ] 32/63 batches, loss: 0.7182Epoch 3/10: [===============               ] 33/63 batches, loss: 0.7174Epoch 3/10: [================              ] 34/63 batches, loss: 0.7158Epoch 3/10: [================              ] 35/63 batches, loss: 0.7123Epoch 3/10: [=================             ] 36/63 batches, loss: 0.7139Epoch 3/10: [=================             ] 37/63 batches, loss: 0.7129Epoch 3/10: [==================            ] 38/63 batches, loss: 0.7122Epoch 3/10: [==================            ] 39/63 batches, loss: 0.7150Epoch 3/10: [===================           ] 40/63 batches, loss: 0.7165Epoch 3/10: [===================           ] 41/63 batches, loss: 0.7145Epoch 3/10: [====================          ] 42/63 batches, loss: 0.7139Epoch 3/10: [====================          ] 43/63 batches, loss: 0.7137Epoch 3/10: [====================          ] 44/63 batches, loss: 0.7127Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.7137Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.7148Epoch 3/10: [======================        ] 47/63 batches, loss: 0.7133Epoch 3/10: [======================        ] 48/63 batches, loss: 0.7128Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.7135Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.7136Epoch 3/10: [========================      ] 51/63 batches, loss: 0.7135Epoch 3/10: [========================      ] 52/63 batches, loss: 0.7128Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.7128Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.7131Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.7142Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.7157Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.7164Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.7176Epoch 3/10: [============================  ] 59/63 batches, loss: 0.7178Epoch 3/10: [============================  ] 60/63 batches, loss: 0.7188Epoch 3/10: [============================= ] 61/63 batches, loss: 0.7189Epoch 3/10: [============================= ] 62/63 batches, loss: 0.7187Epoch 3/10: [==============================] 63/63 batches, loss: 0.7183
[2025-05-01 12:04:03,572][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.7183
[2025-05-01 12:04:03,780][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.7391, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.6970Epoch 4/10: [                              ] 2/63 batches, loss: 0.6867Epoch 4/10: [=                             ] 3/63 batches, loss: 0.6843Epoch 4/10: [=                             ] 4/63 batches, loss: 0.6825Epoch 4/10: [==                            ] 5/63 batches, loss: 0.6940Epoch 4/10: [==                            ] 6/63 batches, loss: 0.7005Epoch 4/10: [===                           ] 7/63 batches, loss: 0.7108Epoch 4/10: [===                           ] 8/63 batches, loss: 0.7113Epoch 4/10: [====                          ] 9/63 batches, loss: 0.7065Epoch 4/10: [====                          ] 10/63 batches, loss: 0.7008Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.7051Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.7053Epoch 4/10: [======                        ] 13/63 batches, loss: 0.7007Epoch 4/10: [======                        ] 14/63 batches, loss: 0.7027Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.7045Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.7020Epoch 4/10: [========                      ] 17/63 batches, loss: 0.7038Epoch 4/10: [========                      ] 18/63 batches, loss: 0.7031Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.7046Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.7051Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.7039Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.7027Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.7030Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.7037Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.7008Epoch 4/10: [============                  ] 26/63 batches, loss: 0.7029Epoch 4/10: [============                  ] 27/63 batches, loss: 0.7038Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.7014Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.7028Epoch 4/10: [==============                ] 30/63 batches, loss: 0.6987Epoch 4/10: [==============                ] 31/63 batches, loss: 0.6996Epoch 4/10: [===============               ] 32/63 batches, loss: 0.7015Epoch 4/10: [===============               ] 33/63 batches, loss: 0.7005Epoch 4/10: [================              ] 34/63 batches, loss: 0.7038Epoch 4/10: [================              ] 35/63 batches, loss: 0.7056Epoch 4/10: [=================             ] 36/63 batches, loss: 0.7057Epoch 4/10: [=================             ] 37/63 batches, loss: 0.7035Epoch 4/10: [==================            ] 38/63 batches, loss: 0.7054Epoch 4/10: [==================            ] 39/63 batches, loss: 0.7061Epoch 4/10: [===================           ] 40/63 batches, loss: 0.7063Epoch 4/10: [===================           ] 41/63 batches, loss: 0.7050Epoch 4/10: [====================          ] 42/63 batches, loss: 0.7055Epoch 4/10: [====================          ] 43/63 batches, loss: 0.7047Epoch 4/10: [====================          ] 44/63 batches, loss: 0.7055Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.7072Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.7073Epoch 4/10: [======================        ] 47/63 batches, loss: 0.7056Epoch 4/10: [======================        ] 48/63 batches, loss: 0.7042Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.7051Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.7076Epoch 4/10: [========================      ] 51/63 batches, loss: 0.7074Epoch 4/10: [========================      ] 52/63 batches, loss: 0.7076Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.7064Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.7064Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.7078Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.7093Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.7098Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.7116Epoch 4/10: [============================  ] 59/63 batches, loss: 0.7122Epoch 4/10: [============================  ] 60/63 batches, loss: 0.7122Epoch 4/10: [============================= ] 61/63 batches, loss: 0.7123Epoch 4/10: [============================= ] 62/63 batches, loss: 0.7124Epoch 4/10: [==============================] 63/63 batches, loss: 0.7113
[2025-05-01 12:04:10,516][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.7113
[2025-05-01 12:04:10,721][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.7307, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6642Epoch 5/10: [                              ] 2/63 batches, loss: 0.6750Epoch 5/10: [=                             ] 3/63 batches, loss: 0.6882Epoch 5/10: [=                             ] 4/63 batches, loss: 0.7064Epoch 5/10: [==                            ] 5/63 batches, loss: 0.7099Epoch 5/10: [==                            ] 6/63 batches, loss: 0.7096Epoch 5/10: [===                           ] 7/63 batches, loss: 0.7036Epoch 5/10: [===                           ] 8/63 batches, loss: 0.7002Epoch 5/10: [====                          ] 9/63 batches, loss: 0.7055Epoch 5/10: [====                          ] 10/63 batches, loss: 0.7043Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.7056Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.7038Epoch 5/10: [======                        ] 13/63 batches, loss: 0.7017Epoch 5/10: [======                        ] 14/63 batches, loss: 0.7010Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.6992Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.6969Epoch 5/10: [========                      ] 17/63 batches, loss: 0.6960Epoch 5/10: [========                      ] 18/63 batches, loss: 0.6976Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.6979Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.6985Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.6956Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.6966Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.6986Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.6993Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.6996Epoch 5/10: [============                  ] 26/63 batches, loss: 0.7006Epoch 5/10: [============                  ] 27/63 batches, loss: 0.6981Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.7005Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.6990Epoch 5/10: [==============                ] 30/63 batches, loss: 0.6982Epoch 5/10: [==============                ] 31/63 batches, loss: 0.6988Epoch 5/10: [===============               ] 32/63 batches, loss: 0.7000Epoch 5/10: [===============               ] 33/63 batches, loss: 0.7013Epoch 5/10: [================              ] 34/63 batches, loss: 0.7036Epoch 5/10: [================              ] 35/63 batches, loss: 0.7054Epoch 5/10: [=================             ] 36/63 batches, loss: 0.7068Epoch 5/10: [=================             ] 37/63 batches, loss: 0.7077Epoch 5/10: [==================            ] 38/63 batches, loss: 0.7071Epoch 5/10: [==================            ] 39/63 batches, loss: 0.7063Epoch 5/10: [===================           ] 40/63 batches, loss: 0.7072Epoch 5/10: [===================           ] 41/63 batches, loss: 0.7071Epoch 5/10: [====================          ] 42/63 batches, loss: 0.7064Epoch 5/10: [====================          ] 43/63 batches, loss: 0.7068Epoch 5/10: [====================          ] 44/63 batches, loss: 0.7079Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.7081Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.7073Epoch 5/10: [======================        ] 47/63 batches, loss: 0.7095Epoch 5/10: [======================        ] 48/63 batches, loss: 0.7099Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.7131Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.7134Epoch 5/10: [========================      ] 51/63 batches, loss: 0.7139Epoch 5/10: [========================      ] 52/63 batches, loss: 0.7131Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.7134Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.7136Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.7130Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.7138Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.7131Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.7126Epoch 5/10: [============================  ] 59/63 batches, loss: 0.7136Epoch 5/10: [============================  ] 60/63 batches, loss: 0.7138Epoch 5/10: [============================= ] 61/63 batches, loss: 0.7132Epoch 5/10: [============================= ] 62/63 batches, loss: 0.7125Epoch 5/10: [==============================] 63/63 batches, loss: 0.7125
[2025-05-01 12:04:17,502][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.7125
[2025-05-01 12:04:17,715][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.7362, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:17,716][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.7021Epoch 6/10: [                              ] 2/63 batches, loss: 0.6957Epoch 6/10: [=                             ] 3/63 batches, loss: 0.7055Epoch 6/10: [=                             ] 4/63 batches, loss: 0.6933Epoch 6/10: [==                            ] 5/63 batches, loss: 0.6910Epoch 6/10: [==                            ] 6/63 batches, loss: 0.6858Epoch 6/10: [===                           ] 7/63 batches, loss: 0.6805Epoch 6/10: [===                           ] 8/63 batches, loss: 0.6749Epoch 6/10: [====                          ] 9/63 batches, loss: 0.6775Epoch 6/10: [====                          ] 10/63 batches, loss: 0.6851Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.6903Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.6891Epoch 6/10: [======                        ] 13/63 batches, loss: 0.6855Epoch 6/10: [======                        ] 14/63 batches, loss: 0.6847Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.6882Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.6886Epoch 6/10: [========                      ] 17/63 batches, loss: 0.6869Epoch 6/10: [========                      ] 18/63 batches, loss: 0.6929Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.6920Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.6946Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.6955Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.6949Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.6990Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.7035Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.7039Epoch 6/10: [============                  ] 26/63 batches, loss: 0.7062Epoch 6/10: [============                  ] 27/63 batches, loss: 0.7075Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.7081Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.7055Epoch 6/10: [==============                ] 30/63 batches, loss: 0.7057Epoch 6/10: [==============                ] 31/63 batches, loss: 0.7046Epoch 6/10: [===============               ] 32/63 batches, loss: 0.7040Epoch 6/10: [===============               ] 33/63 batches, loss: 0.7058Epoch 6/10: [================              ] 34/63 batches, loss: 0.7060Epoch 6/10: [================              ] 35/63 batches, loss: 0.7081Epoch 6/10: [=================             ] 36/63 batches, loss: 0.7083Epoch 6/10: [=================             ] 37/63 batches, loss: 0.7063Epoch 6/10: [==================            ] 38/63 batches, loss: 0.7075Epoch 6/10: [==================            ] 39/63 batches, loss: 0.7092Epoch 6/10: [===================           ] 40/63 batches, loss: 0.7080Epoch 6/10: [===================           ] 41/63 batches, loss: 0.7087Epoch 6/10: [====================          ] 42/63 batches, loss: 0.7106Epoch 6/10: [====================          ] 43/63 batches, loss: 0.7125Epoch 6/10: [====================          ] 44/63 batches, loss: 0.7113Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.7112Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.7096Epoch 6/10: [======================        ] 47/63 batches, loss: 0.7105Epoch 6/10: [======================        ] 48/63 batches, loss: 0.7108Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.7090Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.7080Epoch 6/10: [========================      ] 51/63 batches, loss: 0.7098Epoch 6/10: [========================      ] 52/63 batches, loss: 0.7113Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.7113Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.7118Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.7126Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.7130Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.7122Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.7115Epoch 6/10: [============================  ] 59/63 batches, loss: 0.7107Epoch 6/10: [============================  ] 60/63 batches, loss: 0.7119Epoch 6/10: [============================= ] 61/63 batches, loss: 0.7109Epoch 6/10: [============================= ] 62/63 batches, loss: 0.7108Epoch 6/10: [==============================] 63/63 batches, loss: 0.7119
[2025-05-01 12:04:24,092][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.7119
[2025-05-01 12:04:24,300][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.7327, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:24,301][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.7253Epoch 7/10: [                              ] 2/63 batches, loss: 0.7185Epoch 7/10: [=                             ] 3/63 batches, loss: 0.7032Epoch 7/10: [=                             ] 4/63 batches, loss: 0.7088Epoch 7/10: [==                            ] 5/63 batches, loss: 0.7214Epoch 7/10: [==                            ] 6/63 batches, loss: 0.7241Epoch 7/10: [===                           ] 7/63 batches, loss: 0.7241Epoch 7/10: [===                           ] 8/63 batches, loss: 0.7227Epoch 7/10: [====                          ] 9/63 batches, loss: 0.7213Epoch 7/10: [====                          ] 10/63 batches, loss: 0.7192Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.7196Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.7230Epoch 7/10: [======                        ] 13/63 batches, loss: 0.7174Epoch 7/10: [======                        ] 14/63 batches, loss: 0.7142Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.7214Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.7177Epoch 7/10: [========                      ] 17/63 batches, loss: 0.7202Epoch 7/10: [========                      ] 18/63 batches, loss: 0.7232Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.7220Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.7183Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.7186Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.7205Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.7221Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.7252Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.7218Epoch 7/10: [============                  ] 26/63 batches, loss: 0.7198Epoch 7/10: [============                  ] 27/63 batches, loss: 0.7199Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.7165Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.7159Epoch 7/10: [==============                ] 30/63 batches, loss: 0.7160Epoch 7/10: [==============                ] 31/63 batches, loss: 0.7149Epoch 7/10: [===============               ] 32/63 batches, loss: 0.7144Epoch 7/10: [===============               ] 33/63 batches, loss: 0.7148Epoch 7/10: [================              ] 34/63 batches, loss: 0.7128Epoch 7/10: [================              ] 35/63 batches, loss: 0.7140Epoch 7/10: [=================             ] 36/63 batches, loss: 0.7142Epoch 7/10: [=================             ] 37/63 batches, loss: 0.7147Epoch 7/10: [==================            ] 38/63 batches, loss: 0.7154Epoch 7/10: [==================            ] 39/63 batches, loss: 0.7148Epoch 7/10: [===================           ] 40/63 batches, loss: 0.7148Epoch 7/10: [===================           ] 41/63 batches, loss: 0.7143Epoch 7/10: [====================          ] 42/63 batches, loss: 0.7128Epoch 7/10: [====================          ] 43/63 batches, loss: 0.7142Epoch 7/10: [====================          ] 44/63 batches, loss: 0.7154Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.7136Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.7139Epoch 7/10: [======================        ] 47/63 batches, loss: 0.7139Epoch 7/10: [======================        ] 48/63 batches, loss: 0.7138Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.7140Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.7129Epoch 7/10: [========================      ] 51/63 batches, loss: 0.7130Epoch 7/10: [========================      ] 52/63 batches, loss: 0.7121Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.7120Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.7123Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.7120Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.7128Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.7119Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.7105Epoch 7/10: [============================  ] 59/63 batches, loss: 0.7110Epoch 7/10: [============================  ] 60/63 batches, loss: 0.7108Epoch 7/10: [============================= ] 61/63 batches, loss: 0.7126Epoch 7/10: [============================= ] 62/63 batches, loss: 0.7113Epoch 7/10: [==============================] 63/63 batches, loss: 0.7083
[2025-05-01 12:04:30,679][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.7083
[2025-05-01 12:04:30,881][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.7248, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.7617Epoch 8/10: [                              ] 2/63 batches, loss: 0.7196Epoch 8/10: [=                             ] 3/63 batches, loss: 0.7051Epoch 8/10: [=                             ] 4/63 batches, loss: 0.7101Epoch 8/10: [==                            ] 5/63 batches, loss: 0.7227Epoch 8/10: [==                            ] 6/63 batches, loss: 0.7239Epoch 8/10: [===                           ] 7/63 batches, loss: 0.7208Epoch 8/10: [===                           ] 8/63 batches, loss: 0.7243Epoch 8/10: [====                          ] 9/63 batches, loss: 0.7295Epoch 8/10: [====                          ] 10/63 batches, loss: 0.7205Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.7148Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.7121Epoch 8/10: [======                        ] 13/63 batches, loss: 0.7149Epoch 8/10: [======                        ] 14/63 batches, loss: 0.7158Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.7184Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.7160Epoch 8/10: [========                      ] 17/63 batches, loss: 0.7133Epoch 8/10: [========                      ] 18/63 batches, loss: 0.7112Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.7065Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.7059Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.7051Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.7057Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.7056Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.7054Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.7051Epoch 8/10: [============                  ] 26/63 batches, loss: 0.7042Epoch 8/10: [============                  ] 27/63 batches, loss: 0.7035Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.7039Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.7033Epoch 8/10: [==============                ] 30/63 batches, loss: 0.7033Epoch 8/10: [==============                ] 31/63 batches, loss: 0.7032Epoch 8/10: [===============               ] 32/63 batches, loss: 0.7028Epoch 8/10: [===============               ] 33/63 batches, loss: 0.7025Epoch 8/10: [================              ] 34/63 batches, loss: 0.7030Epoch 8/10: [================              ] 35/63 batches, loss: 0.7027Epoch 8/10: [=================             ] 36/63 batches, loss: 0.7031Epoch 8/10: [=================             ] 37/63 batches, loss: 0.7028Epoch 8/10: [==================            ] 38/63 batches, loss: 0.7033Epoch 8/10: [==================            ] 39/63 batches, loss: 0.7026Epoch 8/10: [===================           ] 40/63 batches, loss: 0.7009Epoch 8/10: [===================           ] 41/63 batches, loss: 0.7011Epoch 8/10: [====================          ] 42/63 batches, loss: 0.7018Epoch 8/10: [====================          ] 43/63 batches, loss: 0.7023Epoch 8/10: [====================          ] 44/63 batches, loss: 0.7031Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.7037Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.7022Epoch 8/10: [======================        ] 47/63 batches, loss: 0.7020Epoch 8/10: [======================        ] 48/63 batches, loss: 0.7009Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.7030Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.7030Epoch 8/10: [========================      ] 51/63 batches, loss: 0.7037Epoch 8/10: [========================      ] 52/63 batches, loss: 0.7032Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.7031Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.7037Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.7041Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.7030Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.7028Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.7029Epoch 8/10: [============================  ] 59/63 batches, loss: 0.7020Epoch 8/10: [============================  ] 60/63 batches, loss: 0.7018Epoch 8/10: [============================= ] 61/63 batches, loss: 0.7021Epoch 8/10: [============================= ] 62/63 batches, loss: 0.7019Epoch 8/10: [==============================] 63/63 batches, loss: 0.7014
[2025-05-01 12:04:37,696][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.7014
[2025-05-01 12:04:37,921][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6948, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.6987Epoch 9/10: [                              ] 2/63 batches, loss: 0.6810Epoch 9/10: [=                             ] 3/63 batches, loss: 0.6690Epoch 9/10: [=                             ] 4/63 batches, loss: 0.6740Epoch 9/10: [==                            ] 5/63 batches, loss: 0.6743Epoch 9/10: [==                            ] 6/63 batches, loss: 0.6781Epoch 9/10: [===                           ] 7/63 batches, loss: 0.6831Epoch 9/10: [===                           ] 8/63 batches, loss: 0.6876Epoch 9/10: [====                          ] 9/63 batches, loss: 0.6842Epoch 9/10: [====                          ] 10/63 batches, loss: 0.6856Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.6888Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.6912Epoch 9/10: [======                        ] 13/63 batches, loss: 0.6967Epoch 9/10: [======                        ] 14/63 batches, loss: 0.6971Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.6963Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.6980Epoch 9/10: [========                      ] 17/63 batches, loss: 0.6961Epoch 9/10: [========                      ] 18/63 batches, loss: 0.6956Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.6960Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.6975Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.6988Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.6984Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.6983Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.6997Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.6995Epoch 9/10: [============                  ] 26/63 batches, loss: 0.7009Epoch 9/10: [============                  ] 27/63 batches, loss: 0.7015Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.7002Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.6991Epoch 9/10: [==============                ] 30/63 batches, loss: 0.6983Epoch 9/10: [==============                ] 31/63 batches, loss: 0.6981Epoch 9/10: [===============               ] 32/63 batches, loss: 0.6979Epoch 9/10: [===============               ] 33/63 batches, loss: 0.6977Epoch 9/10: [================              ] 34/63 batches, loss: 0.6974Epoch 9/10: [================              ] 35/63 batches, loss: 0.6970Epoch 9/10: [=================             ] 36/63 batches, loss: 0.6963Epoch 9/10: [=================             ] 37/63 batches, loss: 0.6962Epoch 9/10: [==================            ] 38/63 batches, loss: 0.6968Epoch 9/10: [==================            ] 39/63 batches, loss: 0.6978Epoch 9/10: [===================           ] 40/63 batches, loss: 0.6983Epoch 9/10: [===================           ] 41/63 batches, loss: 0.6982Epoch 9/10: [====================          ] 42/63 batches, loss: 0.6981Epoch 9/10: [====================          ] 43/63 batches, loss: 0.6980Epoch 9/10: [====================          ] 44/63 batches, loss: 0.6978Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.6979Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.6982Epoch 9/10: [======================        ] 47/63 batches, loss: 0.6981Epoch 9/10: [======================        ] 48/63 batches, loss: 0.6982Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.6981Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.6981Epoch 9/10: [========================      ] 51/63 batches, loss: 0.6979Epoch 9/10: [========================      ] 52/63 batches, loss: 0.6978Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.6975Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.6973Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.6971Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.6970Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.6969Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.6968Epoch 9/10: [============================  ] 59/63 batches, loss: 0.6968Epoch 9/10: [============================  ] 60/63 batches, loss: 0.6968Epoch 9/10: [============================= ] 61/63 batches, loss: 0.6965Epoch 9/10: [============================= ] 62/63 batches, loss: 0.6967Epoch 9/10: [==============================] 63/63 batches, loss: 0.6970
[2025-05-01 12:04:44,697][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6970
[2025-05-01 12:04:44,910][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6951, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:44,911][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.6833Epoch 10/10: [                              ] 2/63 batches, loss: 0.6851Epoch 10/10: [=                             ] 3/63 batches, loss: 0.6871Epoch 10/10: [=                             ] 4/63 batches, loss: 0.6898Epoch 10/10: [==                            ] 5/63 batches, loss: 0.6883Epoch 10/10: [==                            ] 6/63 batches, loss: 0.6949Epoch 10/10: [===                           ] 7/63 batches, loss: 0.6960Epoch 10/10: [===                           ] 8/63 batches, loss: 0.6994Epoch 10/10: [====                          ] 9/63 batches, loss: 0.6986Epoch 10/10: [====                          ] 10/63 batches, loss: 0.6979Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.6980Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.6960Epoch 10/10: [======                        ] 13/63 batches, loss: 0.6958Epoch 10/10: [======                        ] 14/63 batches, loss: 0.6951Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.6943Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.6943Epoch 10/10: [========                      ] 17/63 batches, loss: 0.6940Epoch 10/10: [========                      ] 18/63 batches, loss: 0.6947Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.6949Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.6945Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.6947Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.6944Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.6940Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.6933Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.6920Epoch 10/10: [============                  ] 26/63 batches, loss: 0.6923Epoch 10/10: [============                  ] 27/63 batches, loss: 0.6936Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.6930Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.6926Epoch 10/10: [==============                ] 30/63 batches, loss: 0.6933Epoch 10/10: [==============                ] 31/63 batches, loss: 0.6941Epoch 10/10: [===============               ] 32/63 batches, loss: 0.6957Epoch 10/10: [===============               ] 33/63 batches, loss: 0.6957Epoch 10/10: [================              ] 34/63 batches, loss: 0.6956Epoch 10/10: [================              ] 35/63 batches, loss: 0.6958Epoch 10/10: [=================             ] 36/63 batches, loss: 0.6965Epoch 10/10: [=================             ] 37/63 batches, loss: 0.6967Epoch 10/10: [==================            ] 38/63 batches, loss: 0.6960Epoch 10/10: [==================            ] 39/63 batches, loss: 0.6955Epoch 10/10: [===================           ] 40/63 batches, loss: 0.6956Epoch 10/10: [===================           ] 41/63 batches, loss: 0.6947Epoch 10/10: [====================          ] 42/63 batches, loss: 0.6942Epoch 10/10: [====================          ] 43/63 batches, loss: 0.6949Epoch 10/10: [====================          ] 44/63 batches, loss: 0.6950Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.6957Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.6950Epoch 10/10: [======================        ] 47/63 batches, loss: 0.6957Epoch 10/10: [======================        ] 48/63 batches, loss: 0.6957Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.6957Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.6957Epoch 10/10: [========================      ] 51/63 batches, loss: 0.6957Epoch 10/10: [========================      ] 52/63 batches, loss: 0.6956Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.6957Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.6959Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.6960Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.6960Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.6960Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.6959Epoch 10/10: [============================  ] 59/63 batches, loss: 0.6959Epoch 10/10: [============================  ] 60/63 batches, loss: 0.6958Epoch 10/10: [============================= ] 61/63 batches, loss: 0.6958Epoch 10/10: [============================= ] 62/63 batches, loss: 0.6957Epoch 10/10: [==============================] 63/63 batches, loss: 0.6958
[2025-05-01 12:04:51,284][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6958
[2025-05-01 12:04:51,512][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6906, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:51,916][src.training.lm_trainer][INFO] - Training completed in 70.62 seconds
[2025-05-01 12:04:51,916][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:04:54,394][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:54,394][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:54,394][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-01 12:04:56,092][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control3/ar/model.pt
[2025-05-01 12:04:56,097][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁▁▁
wandb:         best_val_loss ███▆▆▂▁
wandb:    best_val_precision ▁▁▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁▁▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▂▂
wandb:            train_loss █▇▇▅▆▅▄▃▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:              val_loss ███▆▇▇▆▂▂▁
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69056
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:                 epoch 10
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 2e-05
wandb:            train_loss 0.69576
wandb:            train_time 70.61607
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69056
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120331-ai7mlp15
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120331-ai7mlp15/logs
Experiment finetune_question_type_control3_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/control3/results.json
Running experiment: finetune_complexity_control1_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:05:10,920][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1
experiment_name: finetune_complexity_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:05:10,921][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:05:10,921][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:05:10,921][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:05:10,925][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 12:05:10,926][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:05:12,427][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:05:14,685][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:05:14,686][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:05:14,753][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-01 12:05:14,783][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-01 12:05:14,957][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:05:14,965][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:05:14,966][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:05:14,967][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:05:14,987][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:05:15,023][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:05:15,037][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:05:15,039][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:05:15,039][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:05:15,040][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:05:15,061][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:05:15,092][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:05:15,107][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:05:15,109][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:05:15,109][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:05:15,110][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:05:15,111][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:05:15,112][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:05:15,112][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:05:15,112][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:05:15,112][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:05:15,112][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 12:05:15,112][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Sample label: 0.20462249219417572
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:05:15,113][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:05:15,113][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:05:15,113][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:05:15,114][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:05:15,114][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:05:15,114][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:05:15,115][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:05:15,115][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:05:15,115][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:05:19,234][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:05:19,235][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:05:19,235][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:05:19,235][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:05:19,240][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:05:19,240][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:05:19,241][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:05:19,241][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:05:19,241][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:05:19,242][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.2748Epoch 1/10: [                              ] 2/63 batches, loss: 0.2816Epoch 1/10: [=                             ] 3/63 batches, loss: 0.2904Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2705Epoch 1/10: [==                            ] 5/63 batches, loss: 0.2510Epoch 1/10: [==                            ] 6/63 batches, loss: 0.2497Epoch 1/10: [===                           ] 7/63 batches, loss: 0.2475Epoch 1/10: [===                           ] 8/63 batches, loss: 0.2426Epoch 1/10: [====                          ] 9/63 batches, loss: 0.2387Epoch 1/10: [====                          ] 10/63 batches, loss: 0.2310Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.2268Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.2195Epoch 1/10: [======                        ] 13/63 batches, loss: 0.2133Epoch 1/10: [======                        ] 14/63 batches, loss: 0.2133Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.2040Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1976Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1911Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1845Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1802Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1760Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1705Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1673Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1651Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1611Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1597Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1574Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1554Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1535Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1509Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1475Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1451Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1435Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1418Epoch 1/10: [================              ] 34/63 batches, loss: 0.1417Epoch 1/10: [================              ] 35/63 batches, loss: 0.1399Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1379Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1366Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1365Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1353Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1359Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1346Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1340Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1323Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1309Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1301Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1289Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1273Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1264Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1257Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1252Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1239Epoch 1/10: [========================      ] 52/63 batches, loss: 0.1236Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.1224Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.1210Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.1206Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.1195Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.1190Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.1183Epoch 1/10: [============================  ] 59/63 batches, loss: 0.1174Epoch 1/10: [============================  ] 60/63 batches, loss: 0.1163Epoch 1/10: [============================= ] 61/63 batches, loss: 0.1151Epoch 1/10: [============================= ] 62/63 batches, loss: 0.1140Epoch 1/10: [==============================] 63/63 batches, loss: 0.1128
[2025-05-01 12:05:27,876][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1128
[2025-05-01 12:05:28,054][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0656, Metrics: {'mse': 0.06612181663513184, 'rmse': 0.25714162758124526, 'r2': -0.019157052040100098}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0179Epoch 2/10: [                              ] 2/63 batches, loss: 0.0298Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0501Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0513Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0521Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0525Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0483Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0481Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0467Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0462Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0458Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0482Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0508Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0518Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0503Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0485Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0469Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0463Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0453Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0449Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0449Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0446Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0433Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0428Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0441Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0450Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0447Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0443Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0436Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0434Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0438Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0437Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0441Epoch 2/10: [================              ] 34/63 batches, loss: 0.0434Epoch 2/10: [================              ] 35/63 batches, loss: 0.0435Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0430Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0435Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0433Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0436Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0432Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0430Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0426Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0431Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0427Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0426Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0424Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0421Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0419Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0419Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0416Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0416Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0415Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0411Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0415Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0412Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0417Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0418Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0417Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0415Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0413Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0415Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0412Epoch 2/10: [==============================] 63/63 batches, loss: 0.0408
[2025-05-01 12:05:34,792][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0408
[2025-05-01 12:05:34,990][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0661, Metrics: {'mse': 0.06591123342514038, 'rmse': 0.2567318317333096, 'r2': -0.015911340713500977}
[2025-05-01 12:05:34,991][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0268Epoch 3/10: [                              ] 2/63 batches, loss: 0.0314Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0285Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0307Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0320Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0315Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0319Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0315Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0327Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0343Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0354Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0367Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0356Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0363Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0363Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0366Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0371Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0364Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0370Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0365Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0377Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0379Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0374Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0369Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0374Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0369Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0367Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0368Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0364Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0362Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0355Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0360Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0358Epoch 3/10: [================              ] 34/63 batches, loss: 0.0357Epoch 3/10: [================              ] 35/63 batches, loss: 0.0359Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0363Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0370Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0371Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0371Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0367Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0367Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0365Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0364Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0366Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0367Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0367Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0374Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0373Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0377Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0374Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0373Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0375Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0378Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0377Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0378Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0381Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0383Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0382Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0380Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0385Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0384Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0384Epoch 3/10: [==============================] 63/63 batches, loss: 0.0387
[2025-05-01 12:05:41,354][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0387
[2025-05-01 12:05:41,564][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0603, Metrics: {'mse': 0.06197192519903183, 'rmse': 0.24894161001936144, 'r2': 0.04480654001235962}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0520Epoch 4/10: [                              ] 2/63 batches, loss: 0.0459Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0396Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0406Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0397Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0394Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0368Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0347Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0404Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0408Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0394Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0372Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0385Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0389Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0390Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0386Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0390Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0378Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0384Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0374Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0380Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0386Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0388Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0381Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0382Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0382Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0382Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0378Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0376Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0377Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0376Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0372Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0376Epoch 4/10: [================              ] 34/63 batches, loss: 0.0377Epoch 4/10: [================              ] 35/63 batches, loss: 0.0377Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0374Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0372Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0371Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0370Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0365Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0363Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0362Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0360Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0356Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0361Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0359Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0362Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0363Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0364Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0363Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0359Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0359Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0361Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0363Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0360Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0359Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0359Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0358Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0360Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0357Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0359Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0356Epoch 4/10: [==============================] 63/63 batches, loss: 0.0359
[2025-05-01 12:05:48,298][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0359
[2025-05-01 12:05:48,517][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0646, Metrics: {'mse': 0.06396260857582092, 'rmse': 0.25290830072542286, 'r2': 0.014123499393463135}
[2025-05-01 12:05:48,518][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0668Epoch 5/10: [                              ] 2/63 batches, loss: 0.0477Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0508Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0454Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0533Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0508Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0517Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0513Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0488Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0488Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0484Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0477Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0458Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0457Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0449Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0432Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0429Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0434Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0430Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0422Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0415Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0424Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0415Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0410Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0417Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0408Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0401Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0400Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0397Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0397Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0411Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0405Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0403Epoch 5/10: [================              ] 34/63 batches, loss: 0.0407Epoch 5/10: [================              ] 35/63 batches, loss: 0.0404Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0401Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0399Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0399Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0395Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0389Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0386Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0382Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0379Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0376Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0374Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0373Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0373Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0376Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0374Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0370Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0369Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0367Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0365Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0368Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0368Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0366Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0367Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0366Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0363Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0365Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0362Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0361Epoch 5/10: [==============================] 63/63 batches, loss: 0.0362
[2025-05-01 12:05:54,894][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0362
[2025-05-01 12:05:55,098][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0818, Metrics: {'mse': 0.08199434727430344, 'rmse': 0.28634655100822054, 'r2': -0.2638055086135864}
[2025-05-01 12:05:55,099][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0408Epoch 6/10: [                              ] 2/63 batches, loss: 0.0359Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0309Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0379Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0360Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0358Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0382Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0375Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0367Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0369Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0354Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0347Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0361Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0349Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0341Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0328Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0339Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0332Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0345Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0352Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0352Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0354Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0357Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0352Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0346Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0344Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0336Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0337Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0340Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0346Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0344Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0345Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0346Epoch 6/10: [================              ] 34/63 batches, loss: 0.0345Epoch 6/10: [================              ] 35/63 batches, loss: 0.0345Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0341Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0337Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0338Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0337Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0335Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0336Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0343Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0347Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0346Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0352Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0359Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0355Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0355Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0353Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0353Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0357Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0355Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0354Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0356Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0357Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0356Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0356Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0354Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0350Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0350Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0351Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0349Epoch 6/10: [==============================] 63/63 batches, loss: 0.0348
[2025-05-01 12:06:01,477][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0348
[2025-05-01 12:06:01,675][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0725, Metrics: {'mse': 0.07242807745933533, 'rmse': 0.26912465041191475, 'r2': -0.11635756492614746}
[2025-05-01 12:06:01,676][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:06:01,676][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-01 12:06:01,676][src.training.lm_trainer][INFO] - Training completed in 40.91 seconds
[2025-05-01 12:06:01,676][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:06:04,139][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0401795394718647, 'rmse': 0.20044834614399965, 'r2': -0.3088921308517456}
[2025-05-01 12:06:04,140][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06197192519903183, 'rmse': 0.24894161001936144, 'r2': 0.04480654001235962}
[2025-05-01 12:06:04,140][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05801210179924965, 'rmse': 0.24085701525853392, 'r2': -0.00010275840759277344}
[2025-05-01 12:06:05,792][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1/ar/model.pt
[2025-05-01 12:06:05,798][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▃▄▄▁
wandb:       train_loss █▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▃▃▁▂█▅
wandb:          val_mse ▂▂▁▂█▅
wandb:           val_r2 ▇▇█▇▁▄
wandb:         val_rmse ▃▂▁▂█▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0603
wandb:     best_val_mse 0.06197
wandb:      best_val_r2 0.04481
wandb:    best_val_rmse 0.24894
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.05801
wandb:    final_test_r2 -0.0001
wandb:  final_test_rmse 0.24086
wandb:  final_train_mse 0.04018
wandb:   final_train_r2 -0.30889
wandb: final_train_rmse 0.20045
wandb:    final_val_mse 0.06197
wandb:     final_val_r2 0.04481
wandb:   final_val_rmse 0.24894
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03483
wandb:       train_time 40.9098
wandb:         val_loss 0.0725
wandb:          val_mse 0.07243
wandb:           val_r2 -0.11636
wandb:         val_rmse 0.26912
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120510-k3yscr9b
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120510-k3yscr9b/logs
Experiment finetune_complexity_control1_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control1/results.json
Running experiment: finetune_complexity_control2_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_control2_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control2"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:06:17,906][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control2
experiment_name: finetune_complexity_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:06:17,906][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:06:17,906][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:06:17,906][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:06:17,910][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 12:06:17,911][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:06:19,407][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:06:21,627][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:06:21,628][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:06:21,669][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-01 12:06:21,693][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-01 12:06:21,863][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:06:21,870][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:06:21,870][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:06:21,872][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:06:21,889][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:06:21,918][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:06:21,928][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:06:21,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:06:21,929][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:06:21,930][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:06:21,947][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:06:21,970][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:06:21,980][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:06:21,981][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:06:21,981][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:06:21,982][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:06:21,983][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:06:21,983][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:06:21,983][src.data.datasets][INFO] - Sample label: 0.443451464176178
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:06:21,984][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:06:21,984][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:06:21,984][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:06:21,985][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:06:21,985][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:06:21,985][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:06:21,986][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:06:21,986][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:06:25,894][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:06:25,895][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:06:25,895][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:06:25,895][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:06:25,900][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:06:25,900][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:06:25,900][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:06:25,901][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:06:25,901][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:06:25,901][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.1567Epoch 1/10: [                              ] 2/63 batches, loss: 0.1596Epoch 1/10: [=                             ] 3/63 batches, loss: 0.1777Epoch 1/10: [=                             ] 4/63 batches, loss: 0.1805Epoch 1/10: [==                            ] 5/63 batches, loss: 0.1696Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1677Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1775Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1789Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1774Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1859Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1825Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1805Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1765Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1696Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1718Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1673Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1619Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1593Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1572Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1567Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1569Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1556Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1534Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1520Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1497Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1512Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1525Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1553Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1535Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1520Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1519Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1497Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1466Epoch 1/10: [================              ] 34/63 batches, loss: 0.1445Epoch 1/10: [================              ] 35/63 batches, loss: 0.1421Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1407Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1387Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1368Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1364Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1350Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1340Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1317Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1304Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1281Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1268Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1255Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1252Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1249Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1237Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1226Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1211Epoch 1/10: [========================      ] 52/63 batches, loss: 0.1199Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.1186Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.1173Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.1164Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.1154Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.1139Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.1127Epoch 1/10: [============================  ] 59/63 batches, loss: 0.1114Epoch 1/10: [============================  ] 60/63 batches, loss: 0.1101Epoch 1/10: [============================= ] 61/63 batches, loss: 0.1090Epoch 1/10: [============================= ] 62/63 batches, loss: 0.1075Epoch 1/10: [==============================] 63/63 batches, loss: 0.1082
[2025-05-01 12:06:34,593][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1082
[2025-05-01 12:06:34,766][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1132, Metrics: {'mse': 0.11369471997022629, 'rmse': 0.3371864765529992, 'r2': -0.7524137496948242}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0226Epoch 2/10: [                              ] 2/63 batches, loss: 0.0255Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0324Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0289Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0293Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0297Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0331Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0331Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0324Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0339Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0380Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0404Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0419Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0427Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0450Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0441Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0443Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0444Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0445Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0442Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0433Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0433Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0425Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0425Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0431Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0427Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0422Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0417Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0414Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0410Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0420Epoch 2/10: [===============               ] 32/63 batches, loss: 0.0416Epoch 2/10: [===============               ] 33/63 batches, loss: 0.0417Epoch 2/10: [================              ] 34/63 batches, loss: 0.0414Epoch 2/10: [================              ] 35/63 batches, loss: 0.0414Epoch 2/10: [=================             ] 36/63 batches, loss: 0.0415Epoch 2/10: [=================             ] 37/63 batches, loss: 0.0412Epoch 2/10: [==================            ] 38/63 batches, loss: 0.0415Epoch 2/10: [==================            ] 39/63 batches, loss: 0.0421Epoch 2/10: [===================           ] 40/63 batches, loss: 0.0419Epoch 2/10: [===================           ] 41/63 batches, loss: 0.0425Epoch 2/10: [====================          ] 42/63 batches, loss: 0.0428Epoch 2/10: [====================          ] 43/63 batches, loss: 0.0436Epoch 2/10: [====================          ] 44/63 batches, loss: 0.0438Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.0448Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.0444Epoch 2/10: [======================        ] 47/63 batches, loss: 0.0437Epoch 2/10: [======================        ] 48/63 batches, loss: 0.0436Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.0441Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.0441Epoch 2/10: [========================      ] 51/63 batches, loss: 0.0437Epoch 2/10: [========================      ] 52/63 batches, loss: 0.0434Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.0431Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.0429Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.0429Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.0427Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.0427Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.0426Epoch 2/10: [============================  ] 59/63 batches, loss: 0.0423Epoch 2/10: [============================  ] 60/63 batches, loss: 0.0421Epoch 2/10: [============================= ] 61/63 batches, loss: 0.0419Epoch 2/10: [============================= ] 62/63 batches, loss: 0.0424Epoch 2/10: [==============================] 63/63 batches, loss: 0.0426
[2025-05-01 12:06:41,532][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0426
[2025-05-01 12:06:41,724][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0848, Metrics: {'mse': 0.0832289457321167, 'rmse': 0.28849427330904975, 'r2': -0.28283488750457764}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.0347Epoch 3/10: [                              ] 2/63 batches, loss: 0.0346Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0350Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0332Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0341Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0326Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0370Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0353Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0352Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0383Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0422Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0407Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0408Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0417Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0409Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0410Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0412Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0404Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0399Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0388Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0406Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0400Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0402Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0405Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0409Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0401Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0402Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0404Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0397Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0398Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0391Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0391Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0392Epoch 3/10: [================              ] 34/63 batches, loss: 0.0392Epoch 3/10: [================              ] 35/63 batches, loss: 0.0393Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0397Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0392Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0387Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0387Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0390Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0388Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0388Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0388Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0396Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0395Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0398Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0399Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0395Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0394Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0389Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0388Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0383Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0381Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0381Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0382Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0380Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0379Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0377Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0373Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0374Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0376Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0383Epoch 3/10: [==============================] 63/63 batches, loss: 0.0382
[2025-05-01 12:06:48,487][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0382
[2025-05-01 12:06:48,691][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0841, Metrics: {'mse': 0.08285412937402725, 'rmse': 0.2878439323210188, 'r2': -0.2770576477050781}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.0253Epoch 4/10: [                              ] 2/63 batches, loss: 0.0433Epoch 4/10: [=                             ] 3/63 batches, loss: 0.0397Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0405Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0410Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0373Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0355Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0361Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0385Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0378Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.0371Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0364Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0368Epoch 4/10: [======                        ] 14/63 batches, loss: 0.0359Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.0361Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0360Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0350Epoch 4/10: [========                      ] 18/63 batches, loss: 0.0346Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.0354Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.0349Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.0362Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0355Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0362Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.0364Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0363Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0366Epoch 4/10: [============                  ] 27/63 batches, loss: 0.0367Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.0362Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.0365Epoch 4/10: [==============                ] 30/63 batches, loss: 0.0369Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0377Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0375Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0374Epoch 4/10: [================              ] 34/63 batches, loss: 0.0373Epoch 4/10: [================              ] 35/63 batches, loss: 0.0373Epoch 4/10: [=================             ] 36/63 batches, loss: 0.0373Epoch 4/10: [=================             ] 37/63 batches, loss: 0.0372Epoch 4/10: [==================            ] 38/63 batches, loss: 0.0375Epoch 4/10: [==================            ] 39/63 batches, loss: 0.0375Epoch 4/10: [===================           ] 40/63 batches, loss: 0.0369Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0369Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0363Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0360Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0356Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0357Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0358Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0355Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0352Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0356Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0362Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0367Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0366Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0364Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0363Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0363Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0360Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0358Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0361Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0362Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0362Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0364Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0364Epoch 4/10: [==============================] 63/63 batches, loss: 0.0359
[2025-05-01 12:06:55,424][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0359
[2025-05-01 12:06:55,634][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0834, Metrics: {'mse': 0.08247587084770203, 'rmse': 0.2871861257924937, 'r2': -0.27122747898101807}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0539Epoch 5/10: [                              ] 2/63 batches, loss: 0.0303Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0309Epoch 5/10: [=                             ] 4/63 batches, loss: 0.0314Epoch 5/10: [==                            ] 5/63 batches, loss: 0.0326Epoch 5/10: [==                            ] 6/63 batches, loss: 0.0336Epoch 5/10: [===                           ] 7/63 batches, loss: 0.0343Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0352Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0347Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0370Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0372Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0368Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0362Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0361Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0353Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0352Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0366Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0360Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0354Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0355Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0360Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0359Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0360Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0370Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0363Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0361Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0356Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0352Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0356Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0354Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0354Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0349Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0361Epoch 5/10: [================              ] 34/63 batches, loss: 0.0367Epoch 5/10: [================              ] 35/63 batches, loss: 0.0367Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0364Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0359Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0359Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0366Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0367Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0368Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0363Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0358Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0357Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0355Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0354Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0350Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0354Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0356Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0353Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0349Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0348Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0347Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0347Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0346Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0345Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0347Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0345Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0344Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0347Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0347Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0347Epoch 5/10: [==============================] 63/63 batches, loss: 0.0343
[2025-05-01 12:07:02,330][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0343
[2025-05-01 12:07:02,537][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0608, Metrics: {'mse': 0.06018301472067833, 'rmse': 0.2453222670706398, 'r2': 0.07237958908081055}
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0270Epoch 6/10: [                              ] 2/63 batches, loss: 0.0266Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0246Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0286Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0319Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0346Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0338Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0344Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0333Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0360Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0338Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0339Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0333Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0324Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0312Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0298Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0299Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0302Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0311Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0313Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0324Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0320Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0320Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0336Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0337Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0335Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0336Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0335Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0329Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0332Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0336Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0343Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0349Epoch 6/10: [================              ] 34/63 batches, loss: 0.0360Epoch 6/10: [================              ] 35/63 batches, loss: 0.0363Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0357Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0354Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0349Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0351Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0352Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0354Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0350Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0346Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0352Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0354Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0349Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0349Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0350Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0347Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0348Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0346Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0348Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0347Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0347Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0349Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0350Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0348Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0348Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0345Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0342Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0342Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0344Epoch 6/10: [==============================] 63/63 batches, loss: 0.0340
[2025-05-01 12:07:09,279][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0340
[2025-05-01 12:07:09,493][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0724, Metrics: {'mse': 0.07218261808156967, 'rmse': 0.2686682305029191, 'r2': -0.11257421970367432}
[2025-05-01 12:07:09,493][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.0384Epoch 7/10: [                              ] 2/63 batches, loss: 0.0436Epoch 7/10: [=                             ] 3/63 batches, loss: 0.0417Epoch 7/10: [=                             ] 4/63 batches, loss: 0.0426Epoch 7/10: [==                            ] 5/63 batches, loss: 0.0420Epoch 7/10: [==                            ] 6/63 batches, loss: 0.0385Epoch 7/10: [===                           ] 7/63 batches, loss: 0.0372Epoch 7/10: [===                           ] 8/63 batches, loss: 0.0354Epoch 7/10: [====                          ] 9/63 batches, loss: 0.0353Epoch 7/10: [====                          ] 10/63 batches, loss: 0.0373Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.0380Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.0366Epoch 7/10: [======                        ] 13/63 batches, loss: 0.0384Epoch 7/10: [======                        ] 14/63 batches, loss: 0.0383Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.0378Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.0374Epoch 7/10: [========                      ] 17/63 batches, loss: 0.0362Epoch 7/10: [========                      ] 18/63 batches, loss: 0.0354Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.0350Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.0359Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.0353Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.0353Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.0360Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.0377Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.0382Epoch 7/10: [============                  ] 26/63 batches, loss: 0.0383Epoch 7/10: [============                  ] 27/63 batches, loss: 0.0384Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.0383Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.0379Epoch 7/10: [==============                ] 30/63 batches, loss: 0.0371Epoch 7/10: [==============                ] 31/63 batches, loss: 0.0373Epoch 7/10: [===============               ] 32/63 batches, loss: 0.0378Epoch 7/10: [===============               ] 33/63 batches, loss: 0.0380Epoch 7/10: [================              ] 34/63 batches, loss: 0.0377Epoch 7/10: [================              ] 35/63 batches, loss: 0.0383Epoch 7/10: [=================             ] 36/63 batches, loss: 0.0377Epoch 7/10: [=================             ] 37/63 batches, loss: 0.0374Epoch 7/10: [==================            ] 38/63 batches, loss: 0.0375Epoch 7/10: [==================            ] 39/63 batches, loss: 0.0371Epoch 7/10: [===================           ] 40/63 batches, loss: 0.0374Epoch 7/10: [===================           ] 41/63 batches, loss: 0.0369Epoch 7/10: [====================          ] 42/63 batches, loss: 0.0365Epoch 7/10: [====================          ] 43/63 batches, loss: 0.0362Epoch 7/10: [====================          ] 44/63 batches, loss: 0.0360Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.0356Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.0356Epoch 7/10: [======================        ] 47/63 batches, loss: 0.0355Epoch 7/10: [======================        ] 48/63 batches, loss: 0.0355Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.0352Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.0350Epoch 7/10: [========================      ] 51/63 batches, loss: 0.0348Epoch 7/10: [========================      ] 52/63 batches, loss: 0.0348Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.0352Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.0349Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.0352Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.0352Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.0352Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.0350Epoch 7/10: [============================  ] 59/63 batches, loss: 0.0347Epoch 7/10: [============================  ] 60/63 batches, loss: 0.0345Epoch 7/10: [============================= ] 61/63 batches, loss: 0.0343Epoch 7/10: [============================= ] 62/63 batches, loss: 0.0343Epoch 7/10: [==============================] 63/63 batches, loss: 0.0346
[2025-05-01 12:07:15,886][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-05-01 12:07:16,076][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0723, Metrics: {'mse': 0.07152602076530457, 'rmse': 0.2674434907888105, 'r2': -0.10245394706726074}
[2025-05-01 12:07:16,077][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.0259Epoch 8/10: [                              ] 2/63 batches, loss: 0.0317Epoch 8/10: [=                             ] 3/63 batches, loss: 0.0269Epoch 8/10: [=                             ] 4/63 batches, loss: 0.0359Epoch 8/10: [==                            ] 5/63 batches, loss: 0.0417Epoch 8/10: [==                            ] 6/63 batches, loss: 0.0385Epoch 8/10: [===                           ] 7/63 batches, loss: 0.0363Epoch 8/10: [===                           ] 8/63 batches, loss: 0.0345Epoch 8/10: [====                          ] 9/63 batches, loss: 0.0355Epoch 8/10: [====                          ] 10/63 batches, loss: 0.0371Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.0368Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.0371Epoch 8/10: [======                        ] 13/63 batches, loss: 0.0363Epoch 8/10: [======                        ] 14/63 batches, loss: 0.0364Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.0375Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.0382Epoch 8/10: [========                      ] 17/63 batches, loss: 0.0395Epoch 8/10: [========                      ] 18/63 batches, loss: 0.0389Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.0385Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.0386Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.0387Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.0390Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.0384Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.0382Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.0380Epoch 8/10: [============                  ] 26/63 batches, loss: 0.0379Epoch 8/10: [============                  ] 27/63 batches, loss: 0.0384Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.0391Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.0394Epoch 8/10: [==============                ] 30/63 batches, loss: 0.0396Epoch 8/10: [==============                ] 31/63 batches, loss: 0.0400Epoch 8/10: [===============               ] 32/63 batches, loss: 0.0395Epoch 8/10: [===============               ] 33/63 batches, loss: 0.0392Epoch 8/10: [================              ] 34/63 batches, loss: 0.0392Epoch 8/10: [================              ] 35/63 batches, loss: 0.0402Epoch 8/10: [=================             ] 36/63 batches, loss: 0.0398Epoch 8/10: [=================             ] 37/63 batches, loss: 0.0398Epoch 8/10: [==================            ] 38/63 batches, loss: 0.0397Epoch 8/10: [==================            ] 39/63 batches, loss: 0.0396Epoch 8/10: [===================           ] 40/63 batches, loss: 0.0393Epoch 8/10: [===================           ] 41/63 batches, loss: 0.0390Epoch 8/10: [====================          ] 42/63 batches, loss: 0.0388Epoch 8/10: [====================          ] 43/63 batches, loss: 0.0385Epoch 8/10: [====================          ] 44/63 batches, loss: 0.0385Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.0384Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.0383Epoch 8/10: [======================        ] 47/63 batches, loss: 0.0378Epoch 8/10: [======================        ] 48/63 batches, loss: 0.0382Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.0383Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.0380Epoch 8/10: [========================      ] 51/63 batches, loss: 0.0377Epoch 8/10: [========================      ] 52/63 batches, loss: 0.0375Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.0373Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.0371Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.0370Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.0368Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.0369Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.0375Epoch 8/10: [============================  ] 59/63 batches, loss: 0.0375Epoch 8/10: [============================  ] 60/63 batches, loss: 0.0373Epoch 8/10: [============================= ] 61/63 batches, loss: 0.0375Epoch 8/10: [============================= ] 62/63 batches, loss: 0.0375Epoch 8/10: [==============================] 63/63 batches, loss: 0.0377
[2025-05-01 12:07:22,431][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0377
[2025-05-01 12:07:22,632][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0670, Metrics: {'mse': 0.06586781144142151, 'rmse': 0.25664725099135877, 'r2': -0.01524209976196289}
[2025-05-01 12:07:22,632][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:07:22,632][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-01 12:07:22,632][src.training.lm_trainer][INFO] - Training completed in 55.09 seconds
[2025-05-01 12:07:22,633][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:07:25,088][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.030799033120274544, 'rmse': 0.17549653307195143, 'r2': -0.0033119916915893555}
[2025-05-01 12:07:25,088][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06018301472067833, 'rmse': 0.2453222670706398, 'r2': 0.07237958908081055}
[2025-05-01 12:07:25,088][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06335078179836273, 'rmse': 0.251695812039777, 'r2': -0.09213924407958984}
[2025-05-01 12:07:26,764][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control2/ar/model.pt
[2025-05-01 12:07:26,769][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▄▁
wandb:     best_val_mse █▄▄▄▁
wandb:      best_val_r2 ▁▅▅▅█
wandb:    best_val_rmse █▄▄▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▄▄▆▅▅
wandb:       train_loss █▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▄▁▃▃▂
wandb:          val_mse █▄▄▄▁▃▂▂
wandb:           val_r2 ▁▅▅▅█▆▇▇
wandb:         val_rmse █▄▄▄▁▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06081
wandb:     best_val_mse 0.06018
wandb:      best_val_r2 0.07238
wandb:    best_val_rmse 0.24532
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.06335
wandb:    final_test_r2 -0.09214
wandb:  final_test_rmse 0.2517
wandb:  final_train_mse 0.0308
wandb:   final_train_r2 -0.00331
wandb: final_train_rmse 0.1755
wandb:    final_val_mse 0.06018
wandb:     final_val_r2 0.07238
wandb:   final_val_rmse 0.24532
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03769
wandb:       train_time 55.08779
wandb:         val_loss 0.06698
wandb:          val_mse 0.06587
wandb:           val_r2 -0.01524
wandb:         val_rmse 0.25665
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120617-nj19xr82
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120617-nj19xr82/logs
Experiment finetune_complexity_control2_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control2/results.json
Running experiment: finetune_complexity_control3_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_control3_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control3"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:07:39,461][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/control3
experiment_name: finetune_complexity_control3_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:07:39,461][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:07:39,461][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:07:39,461][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:07:39,465][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 12:07:39,466][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
slurmstepd: error: *** JOB 64435680 ON k28i22 CANCELLED AT 2025-05-01T12:07:39 ***
