SLURM_JOB_ID: 58114519
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r24g41
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed Apr 30 00:20:23 CEST 2025
Walltime: 01-12:00:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 00:21:00,737][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 00:21:00,738][__main__][INFO] - Normalized task: question_type
[2025-04-30 00:21:00,738][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 00:21:00,738][__main__][INFO] - Determined Task Type: classification
[2025-04-30 00:21:00,742][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-30 00:21:00,743][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 00:21:03,027][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 00:21:06,311][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 00:21:06,311][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:21:06,470][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,543][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,698][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-30 00:21:06,709][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:21:06,710][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-30 00:21:06,711][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:21:06,727][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,749][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,797][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-30 00:21:06,799][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:21:06,799][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-30 00:21:06,801][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:21:06,816][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,839][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:21:06,872][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-30 00:21:06,874][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:21:06,874][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-30 00:21:06,875][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-30 00:21:06,876][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:21:06,876][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:21:06,876][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:21:06,876][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:21:06,876][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-30 00:21:06,877][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:21:06,877][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:21:06,877][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-30 00:21:06,878][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:21:06,878][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:21:06,878][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-30 00:21:06,879][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-30 00:21:06,879][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-30 00:21:06,879][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 00:21:06,879][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-30 00:21:06,879][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 00:21:06,880][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 00:21:06,880][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 00:21:16,116][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 00:21:16,116][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,117][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,118][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,119][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,120][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,121][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,122][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,123][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,124][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,125][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,126][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,127][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,129][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,130][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,131][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,132][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,133][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,133][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,133][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,133][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:21:16,307][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-30 00:21:16,308][src.models.model_factory][INFO] - Model has 394,200,289 trainable parameters out of 394,200,289 total parameters
[2025-04-30 00:21:16,308][__main__][INFO] - Successfully created model for ar
[2025-04-30 00:21:16,309][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.7162Epoch 1/10: [                              ] 2/63 batches, loss: 0.7154Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7333Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7309Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7338Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7354Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7484Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7457Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7408Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7414Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7338Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7317Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7333Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7351Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7328Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7292Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7262Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7243Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7264Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7245Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7262Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7237Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7210Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7211Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7190Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7175Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7146Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7160Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7143Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7174Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7166Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7160Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7161Epoch 1/10: [================              ] 34/63 batches, loss: 0.7150Epoch 1/10: [================              ] 35/63 batches, loss: 0.7159Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7141Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7106Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7124Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7120Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7099Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7064Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7075Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7082Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7050Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7044Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7038Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7018Epoch 1/10: [======================        ] 48/63 batches, loss: 0.6997Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.6986Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.6982Epoch 1/10: [========================      ] 51/63 batches, loss: 0.6986Epoch 1/10: [========================      ] 52/63 batches, loss: 0.6975Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.6985Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.6975Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.6973Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.6959Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.6931Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.6934Epoch 1/10: [============================  ] 59/63 batches, loss: 0.6920Epoch 1/10: [============================  ] 60/63 batches, loss: 0.6918Epoch 1/10: [============================= ] 61/63 batches, loss: 0.6899Epoch 1/10: [============================= ] 62/63 batches, loss: 0.6895Epoch 1/10: [==============================] 63/63 batches, loss: 0.6867
[2025-04-30 00:21:32,266][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6867
[2025-04-30 00:21:32,556][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6788, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7259Epoch 2/10: [                              ] 2/63 batches, loss: 0.6750Epoch 2/10: [=                             ] 3/63 batches, loss: 0.6579Epoch 2/10: [=                             ] 4/63 batches, loss: 0.6303Epoch 2/10: [==                            ] 5/63 batches, loss: 0.6431Epoch 2/10: [==                            ] 6/63 batches, loss: 0.6319Epoch 2/10: [===                           ] 7/63 batches, loss: 0.6414Epoch 2/10: [===                           ] 8/63 batches, loss: 0.6402Epoch 2/10: [====                          ] 9/63 batches, loss: 0.6510Epoch 2/10: [====                          ] 10/63 batches, loss: 0.6466Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.6446Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.6289Epoch 2/10: [======                        ] 13/63 batches, loss: 0.6292Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6311Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6285Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6301Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6300Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6310Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6246Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6224Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6209Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6186Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6164Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6193Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6192Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6209Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6204Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6191Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6196Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6182Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6149Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6155Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6155Epoch 2/10: [================              ] 34/63 batches, loss: 0.6160Epoch 2/10: [================              ] 35/63 batches, loss: 0.6154Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6143Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6104Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6114Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6112Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6111Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6114Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6124Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6121Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6116Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6074Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6034Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6000Epoch 2/10: [======================        ] 48/63 batches, loss: 0.5982Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.5972Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.5973Epoch 2/10: [========================      ] 51/63 batches, loss: 0.5957Epoch 2/10: [========================      ] 52/63 batches, loss: 0.5955Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.5959Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.5952Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.5948Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.5948Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.5953Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.5950Epoch 2/10: [============================  ] 59/63 batches, loss: 0.5929Epoch 2/10: [============================  ] 60/63 batches, loss: 0.5908Epoch 2/10: [============================= ] 61/63 batches, loss: 0.5879Epoch 2/10: [============================= ] 62/63 batches, loss: 0.5878Epoch 2/10: [==============================] 63/63 batches, loss: 0.5845
[2025-04-30 00:21:45,795][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5845
[2025-04-30 00:21:46,111][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6566, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.5975Epoch 3/10: [                              ] 2/63 batches, loss: 0.6323Epoch 3/10: [=                             ] 3/63 batches, loss: 0.6204Epoch 3/10: [=                             ] 4/63 batches, loss: 0.6132Epoch 3/10: [==                            ] 5/63 batches, loss: 0.5974Epoch 3/10: [==                            ] 6/63 batches, loss: 0.5938Epoch 3/10: [===                           ] 7/63 batches, loss: 0.5874Epoch 3/10: [===                           ] 8/63 batches, loss: 0.5920Epoch 3/10: [====                          ] 9/63 batches, loss: 0.5912Epoch 3/10: [====                          ] 10/63 batches, loss: 0.5978Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.5930Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.5909Epoch 3/10: [======                        ] 13/63 batches, loss: 0.5885Epoch 3/10: [======                        ] 14/63 batches, loss: 0.5910Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.5926Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.5785Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5778Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5772Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5761Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5745Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5730Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5692Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5682Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5609Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5599Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5572Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5582Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5558Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5532Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5513Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5514Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5512Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5488Epoch 3/10: [================              ] 34/63 batches, loss: 0.5434Epoch 3/10: [================              ] 35/63 batches, loss: 0.5449Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5484Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5473Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5442Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5446Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5462Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5424Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5418Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5381Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5386Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5366Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5366Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5330Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5330Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5336Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5338Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5339Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5325Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5322Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5313Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5312Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5310Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5289Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5294Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5288Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5290Epoch 3/10: [============================= ] 61/63 batches, loss: 0.5288Epoch 3/10: [============================= ] 62/63 batches, loss: 0.5274Epoch 3/10: [==============================] 63/63 batches, loss: 0.5247
[2025-04-30 00:21:59,411][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5247
[2025-04-30 00:21:59,741][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5989, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.95}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.5286Epoch 4/10: [                              ] 2/63 batches, loss: 0.5106Epoch 4/10: [=                             ] 3/63 batches, loss: 0.5189Epoch 4/10: [=                             ] 4/63 batches, loss: 0.5128Epoch 4/10: [==                            ] 5/63 batches, loss: 0.5213Epoch 4/10: [==                            ] 6/63 batches, loss: 0.5226Epoch 4/10: [===                           ] 7/63 batches, loss: 0.5122Epoch 4/10: [===                           ] 8/63 batches, loss: 0.5146Epoch 4/10: [====                          ] 9/63 batches, loss: 0.5082Epoch 4/10: [====                          ] 10/63 batches, loss: 0.5108Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.5103Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.5214Epoch 4/10: [======                        ] 13/63 batches, loss: 0.5177Epoch 4/10: [======                        ] 14/63 batches, loss: 0.5169Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.5168Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.5185Epoch 4/10: [========                      ] 17/63 batches, loss: 0.5177Epoch 4/10: [========                      ] 18/63 batches, loss: 0.5172Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.5135Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.5049Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.5066Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.5033Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.5014Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.5005Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.5011Epoch 4/10: [============                  ] 26/63 batches, loss: 0.5002Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4965Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.4942Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4889Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4905Epoch 4/10: [==============                ] 31/63 batches, loss: 0.4915Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4917Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4884Epoch 4/10: [================              ] 34/63 batches, loss: 0.4887Epoch 4/10: [================              ] 35/63 batches, loss: 0.4894Epoch 4/10: [=================             ] 36/63 batches, loss: 0.4896Epoch 4/10: [=================             ] 37/63 batches, loss: 0.4905Epoch 4/10: [==================            ] 38/63 batches, loss: 0.4901Epoch 4/10: [==================            ] 39/63 batches, loss: 0.4906Epoch 4/10: [===================           ] 40/63 batches, loss: 0.4897Epoch 4/10: [===================           ] 41/63 batches, loss: 0.4889Epoch 4/10: [====================          ] 42/63 batches, loss: 0.4872Epoch 4/10: [====================          ] 43/63 batches, loss: 0.4859Epoch 4/10: [====================          ] 44/63 batches, loss: 0.4848Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.4860Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.4826Epoch 4/10: [======================        ] 47/63 batches, loss: 0.4832Epoch 4/10: [======================        ] 48/63 batches, loss: 0.4830Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.4806Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.4790Epoch 4/10: [========================      ] 51/63 batches, loss: 0.4788Epoch 4/10: [========================      ] 52/63 batches, loss: 0.4773Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.4760Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.4749Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.4730Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.4716Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.4702Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.4689Epoch 4/10: [============================  ] 59/63 batches, loss: 0.4675Epoch 4/10: [============================  ] 60/63 batches, loss: 0.4663Epoch 4/10: [============================= ] 61/63 batches, loss: 0.4647Epoch 4/10: [============================= ] 62/63 batches, loss: 0.4654Epoch 4/10: [==============================] 63/63 batches, loss: 0.4662
[2025-04-30 00:22:12,975][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.4662
[2025-04-30 00:22:13,292][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3822, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9230769230769231}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.5177Epoch 5/10: [                              ] 2/63 batches, loss: 0.4654Epoch 5/10: [=                             ] 3/63 batches, loss: 0.4610Epoch 5/10: [=                             ] 4/63 batches, loss: 0.4444Epoch 5/10: [==                            ] 5/63 batches, loss: 0.4451Epoch 5/10: [==                            ] 6/63 batches, loss: 0.4357Epoch 5/10: [===                           ] 7/63 batches, loss: 0.4484Epoch 5/10: [===                           ] 8/63 batches, loss: 0.4466Epoch 5/10: [====                          ] 9/63 batches, loss: 0.4353Epoch 5/10: [====                          ] 10/63 batches, loss: 0.4251Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.4183Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.4124Epoch 5/10: [======                        ] 13/63 batches, loss: 0.4083Epoch 5/10: [======                        ] 14/63 batches, loss: 0.4102Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.4051Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.3991Epoch 5/10: [========                      ] 17/63 batches, loss: 0.3944Epoch 5/10: [========                      ] 18/63 batches, loss: 0.3922Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.3912Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.3883Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.3898Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.3872Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.3850Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.3853Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.3841Epoch 5/10: [============                  ] 26/63 batches, loss: 0.3821Epoch 5/10: [============                  ] 27/63 batches, loss: 0.3811Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.3788Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.3788Epoch 5/10: [==============                ] 30/63 batches, loss: 0.3814Epoch 5/10: [==============                ] 31/63 batches, loss: 0.3788Epoch 5/10: [===============               ] 32/63 batches, loss: 0.3788Epoch 5/10: [===============               ] 33/63 batches, loss: 0.3785Epoch 5/10: [================              ] 34/63 batches, loss: 0.3775Epoch 5/10: [================              ] 35/63 batches, loss: 0.3774Epoch 5/10: [=================             ] 36/63 batches, loss: 0.3767Epoch 5/10: [=================             ] 37/63 batches, loss: 0.3745Epoch 5/10: [==================            ] 38/63 batches, loss: 0.3725Epoch 5/10: [==================            ] 39/63 batches, loss: 0.3716Epoch 5/10: [===================           ] 40/63 batches, loss: 0.3709Epoch 5/10: [===================           ] 41/63 batches, loss: 0.3695Epoch 5/10: [====================          ] 42/63 batches, loss: 0.3699Epoch 5/10: [====================          ] 43/63 batches, loss: 0.3688Epoch 5/10: [====================          ] 44/63 batches, loss: 0.3677Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.3670Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.3668Epoch 5/10: [======================        ] 47/63 batches, loss: 0.3660Epoch 5/10: [======================        ] 48/63 batches, loss: 0.3664Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.3659Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.3647Epoch 5/10: [========================      ] 51/63 batches, loss: 0.3637Epoch 5/10: [========================      ] 52/63 batches, loss: 0.3627Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.3625Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.3616Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.3609Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.3614Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.3604Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.3595Epoch 5/10: [============================  ] 59/63 batches, loss: 0.3594Epoch 5/10: [============================  ] 60/63 batches, loss: 0.3581Epoch 5/10: [============================= ] 61/63 batches, loss: 0.3574Epoch 5/10: [============================= ] 62/63 batches, loss: 0.3589Epoch 5/10: [==============================] 63/63 batches, loss: 0.3584
[2025-04-30 00:22:26,695][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.3584
[2025-04-30 00:22:27,009][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.4283, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.926829268292683}
[2025-04-30 00:22:27,010][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.3611Epoch 6/10: [                              ] 2/63 batches, loss: 0.4079Epoch 6/10: [=                             ] 3/63 batches, loss: 0.3725Epoch 6/10: [=                             ] 4/63 batches, loss: 0.3860Epoch 6/10: [==                            ] 5/63 batches, loss: 0.3819Epoch 6/10: [==                            ] 6/63 batches, loss: 0.3768Epoch 6/10: [===                           ] 7/63 batches, loss: 0.3834Epoch 6/10: [===                           ] 8/63 batches, loss: 0.3711Epoch 6/10: [====                          ] 9/63 batches, loss: 0.3624Epoch 6/10: [====                          ] 10/63 batches, loss: 0.3650Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.3594Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.3577Epoch 6/10: [======                        ] 13/63 batches, loss: 0.3549Epoch 6/10: [======                        ] 14/63 batches, loss: 0.3514Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.3506Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.3484Epoch 6/10: [========                      ] 17/63 batches, loss: 0.3489Epoch 6/10: [========                      ] 18/63 batches, loss: 0.3475Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.3463Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.3447Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.3436Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.3418Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.3397Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.3408Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.3384Epoch 6/10: [============                  ] 26/63 batches, loss: 0.3365Epoch 6/10: [============                  ] 27/63 batches, loss: 0.3346Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.3322Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.3321Epoch 6/10: [==============                ] 30/63 batches, loss: 0.3317Epoch 6/10: [==============                ] 31/63 batches, loss: 0.3299Epoch 6/10: [===============               ] 32/63 batches, loss: 0.3289Epoch 6/10: [===============               ] 33/63 batches, loss: 0.3277Epoch 6/10: [================              ] 34/63 batches, loss: 0.3268Epoch 6/10: [================              ] 35/63 batches, loss: 0.3255Epoch 6/10: [=================             ] 36/63 batches, loss: 0.3241Epoch 6/10: [=================             ] 37/63 batches, loss: 0.3230Epoch 6/10: [==================            ] 38/63 batches, loss: 0.3242Epoch 6/10: [==================            ] 39/63 batches, loss: 0.3237Epoch 6/10: [===================           ] 40/63 batches, loss: 0.3236Epoch 6/10: [===================           ] 41/63 batches, loss: 0.3226Epoch 6/10: [====================          ] 42/63 batches, loss: 0.3218Epoch 6/10: [====================          ] 43/63 batches, loss: 0.3212Epoch 6/10: [====================          ] 44/63 batches, loss: 0.3207Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.3199Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.3193Epoch 6/10: [======================        ] 47/63 batches, loss: 0.3186Epoch 6/10: [======================        ] 48/63 batches, loss: 0.3178Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.3196Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.3184Epoch 6/10: [========================      ] 51/63 batches, loss: 0.3180Epoch 6/10: [========================      ] 52/63 batches, loss: 0.3172Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.3166Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.3157Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.3147Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.3140Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.3136Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.3141Epoch 6/10: [============================  ] 59/63 batches, loss: 0.3133Epoch 6/10: [============================  ] 60/63 batches, loss: 0.3123Epoch 6/10: [============================= ] 61/63 batches, loss: 0.3124Epoch 6/10: [============================= ] 62/63 batches, loss: 0.3119Epoch 6/10: [==============================] 63/63 batches, loss: 0.3103
[2025-04-30 00:22:39,698][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.3103
[2025-04-30 00:22:40,015][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3266, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.3302Epoch 7/10: [                              ] 2/63 batches, loss: 0.3106Epoch 7/10: [=                             ] 3/63 batches, loss: 0.3232Epoch 7/10: [=                             ] 4/63 batches, loss: 0.3209Epoch 7/10: [==                            ] 5/63 batches, loss: 0.3088Epoch 7/10: [==                            ] 6/63 batches, loss: 0.3061Epoch 7/10: [===                           ] 7/63 batches, loss: 0.3012Epoch 7/10: [===                           ] 8/63 batches, loss: 0.3023Epoch 7/10: [====                          ] 9/63 batches, loss: 0.2998Epoch 7/10: [====                          ] 10/63 batches, loss: 0.3003Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.3021Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.3045Epoch 7/10: [======                        ] 13/63 batches, loss: 0.3054Epoch 7/10: [======                        ] 14/63 batches, loss: 0.3055Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.3059Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.3066Epoch 7/10: [========                      ] 17/63 batches, loss: 0.3054Epoch 7/10: [========                      ] 18/63 batches, loss: 0.3043Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.3051Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.3072Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.3118Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.3146Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.3131Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.3118Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.3163Epoch 7/10: [============                  ] 26/63 batches, loss: 0.3196Epoch 7/10: [============                  ] 27/63 batches, loss: 0.3215Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.3198Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.3180Epoch 7/10: [==============                ] 30/63 batches, loss: 0.3163Epoch 7/10: [==============                ] 31/63 batches, loss: 0.3145Epoch 7/10: [===============               ] 32/63 batches, loss: 0.3138Epoch 7/10: [===============               ] 33/63 batches, loss: 0.3129Epoch 7/10: [================              ] 34/63 batches, loss: 0.3153Epoch 7/10: [================              ] 35/63 batches, loss: 0.3137Epoch 7/10: [=================             ] 36/63 batches, loss: 0.3126Epoch 7/10: [=================             ] 37/63 batches, loss: 0.3130Epoch 7/10: [==================            ] 38/63 batches, loss: 0.3115Epoch 7/10: [==================            ] 39/63 batches, loss: 0.3108Epoch 7/10: [===================           ] 40/63 batches, loss: 0.3091Epoch 7/10: [===================           ] 41/63 batches, loss: 0.3079Epoch 7/10: [====================          ] 42/63 batches, loss: 0.3070Epoch 7/10: [====================          ] 43/63 batches, loss: 0.3064Epoch 7/10: [====================          ] 44/63 batches, loss: 0.3077Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.3069Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.3076Epoch 7/10: [======================        ] 47/63 batches, loss: 0.3066Epoch 7/10: [======================        ] 48/63 batches, loss: 0.3059Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.3049Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.3038Epoch 7/10: [========================      ] 51/63 batches, loss: 0.3031Epoch 7/10: [========================      ] 52/63 batches, loss: 0.3022Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.3015Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.3006Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.2995Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.2991Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.2986Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.2980Epoch 7/10: [============================  ] 59/63 batches, loss: 0.2999Epoch 7/10: [============================  ] 60/63 batches, loss: 0.2992Epoch 7/10: [============================= ] 61/63 batches, loss: 0.2998Epoch 7/10: [============================= ] 62/63 batches, loss: 0.2991Epoch 7/10: [==============================] 63/63 batches, loss: 0.2980
[2025-04-30 00:22:53,355][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.2980
[2025-04-30 00:22:53,678][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2619, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9230769230769231}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.2625Epoch 8/10: [                              ] 2/63 batches, loss: 0.2531Epoch 8/10: [=                             ] 3/63 batches, loss: 0.2547Epoch 8/10: [=                             ] 4/63 batches, loss: 0.2481Epoch 8/10: [==                            ] 5/63 batches, loss: 0.2486Epoch 8/10: [==                            ] 6/63 batches, loss: 0.2536Epoch 8/10: [===                           ] 7/63 batches, loss: 0.2529Epoch 8/10: [===                           ] 8/63 batches, loss: 0.2537Epoch 8/10: [====                          ] 9/63 batches, loss: 0.2543Epoch 8/10: [====                          ] 10/63 batches, loss: 0.2630Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.2711Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.2713Epoch 8/10: [======                        ] 13/63 batches, loss: 0.2685Epoch 8/10: [======                        ] 14/63 batches, loss: 0.2668Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.2662Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.2717Epoch 8/10: [========                      ] 17/63 batches, loss: 0.2704Epoch 8/10: [========                      ] 18/63 batches, loss: 0.2736Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.2741Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.2734Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.2740Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.2741Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.2735Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.2780Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.2782Epoch 8/10: [============                  ] 26/63 batches, loss: 0.2766Epoch 8/10: [============                  ] 27/63 batches, loss: 0.2757Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.2753Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.2749Epoch 8/10: [==============                ] 30/63 batches, loss: 0.2738Epoch 8/10: [==============                ] 31/63 batches, loss: 0.2730Epoch 8/10: [===============               ] 32/63 batches, loss: 0.2724Epoch 8/10: [===============               ] 33/63 batches, loss: 0.2718Epoch 8/10: [================              ] 34/63 batches, loss: 0.2711Epoch 8/10: [================              ] 35/63 batches, loss: 0.2708Epoch 8/10: [=================             ] 36/63 batches, loss: 0.2700Epoch 8/10: [=================             ] 37/63 batches, loss: 0.2696Epoch 8/10: [==================            ] 38/63 batches, loss: 0.2690Epoch 8/10: [==================            ] 39/63 batches, loss: 0.2697Epoch 8/10: [===================           ] 40/63 batches, loss: 0.2701Epoch 8/10: [===================           ] 41/63 batches, loss: 0.2696Epoch 8/10: [====================          ] 42/63 batches, loss: 0.2685Epoch 8/10: [====================          ] 43/63 batches, loss: 0.2699Epoch 8/10: [====================          ] 44/63 batches, loss: 0.2692Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.2689Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.2688Epoch 8/10: [======================        ] 47/63 batches, loss: 0.2703Epoch 8/10: [======================        ] 48/63 batches, loss: 0.2698Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.2693Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.2682Epoch 8/10: [========================      ] 51/63 batches, loss: 0.2672Epoch 8/10: [========================      ] 52/63 batches, loss: 0.2668Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.2665Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.2671Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.2665Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.2660Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.2666Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.2676Epoch 8/10: [============================  ] 59/63 batches, loss: 0.2676Epoch 8/10: [============================  ] 60/63 batches, loss: 0.2673Epoch 8/10: [============================= ] 61/63 batches, loss: 0.2685Epoch 8/10: [============================= ] 62/63 batches, loss: 0.2684Epoch 8/10: [==============================] 63/63 batches, loss: 0.2760
[2025-04-30 00:23:06,953][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.2760
[2025-04-30 00:23:07,281][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.2634, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.95}
[2025-04-30 00:23:07,282][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.2434Epoch 9/10: [                              ] 2/63 batches, loss: 0.2416Epoch 9/10: [=                             ] 3/63 batches, loss: 0.2420Epoch 9/10: [=                             ] 4/63 batches, loss: 0.2382Epoch 9/10: [==                            ] 5/63 batches, loss: 0.2386Epoch 9/10: [==                            ] 6/63 batches, loss: 0.2428Epoch 9/10: [===                           ] 7/63 batches, loss: 0.2425Epoch 9/10: [===                           ] 8/63 batches, loss: 0.2477Epoch 9/10: [====                          ] 9/63 batches, loss: 0.2457Epoch 9/10: [====                          ] 10/63 batches, loss: 0.2438Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.2467Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.2473Epoch 9/10: [======                        ] 13/63 batches, loss: 0.2471Epoch 9/10: [======                        ] 14/63 batches, loss: 0.2491Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.2483Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.2501Epoch 9/10: [========                      ] 17/63 batches, loss: 0.2499Epoch 9/10: [========                      ] 18/63 batches, loss: 0.2506Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.2512Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.2510Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.2629Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.2618Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.2692Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.2695Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.2734Epoch 9/10: [============                  ] 26/63 batches, loss: 0.2724Epoch 9/10: [============                  ] 27/63 batches, loss: 0.2717Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.2703Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.2697Epoch 9/10: [==============                ] 30/63 batches, loss: 0.2703Epoch 9/10: [==============                ] 31/63 batches, loss: 0.2693Epoch 9/10: [===============               ] 32/63 batches, loss: 0.2681Epoch 9/10: [===============               ] 33/63 batches, loss: 0.2669Epoch 9/10: [================              ] 34/63 batches, loss: 0.2658Epoch 9/10: [================              ] 35/63 batches, loss: 0.2658Epoch 9/10: [=================             ] 36/63 batches, loss: 0.2649Epoch 9/10: [=================             ] 37/63 batches, loss: 0.2642Epoch 9/10: [==================            ] 38/63 batches, loss: 0.2629Epoch 9/10: [==================            ] 39/63 batches, loss: 0.2620Epoch 9/10: [===================           ] 40/63 batches, loss: 0.2608Epoch 9/10: [===================           ] 41/63 batches, loss: 0.2596Epoch 9/10: [====================          ] 42/63 batches, loss: 0.2590Epoch 9/10: [====================          ] 43/63 batches, loss: 0.2584Epoch 9/10: [====================          ] 44/63 batches, loss: 0.2585Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.2579Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.2578Epoch 9/10: [======================        ] 47/63 batches, loss: 0.2575Epoch 9/10: [======================        ] 48/63 batches, loss: 0.2569Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.2568Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.2562Epoch 9/10: [========================      ] 51/63 batches, loss: 0.2557Epoch 9/10: [========================      ] 52/63 batches, loss: 0.2550Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.2565Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.2560Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.2555Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.2550Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.2569Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.2563Epoch 9/10: [============================  ] 59/63 batches, loss: 0.2563Epoch 9/10: [============================  ] 60/63 batches, loss: 0.2557Epoch 9/10: [============================= ] 61/63 batches, loss: 0.2555Epoch 9/10: [============================= ] 62/63 batches, loss: 0.2551Epoch 9/10: [==============================] 63/63 batches, loss: 0.2543
[2025-04-30 00:23:20,006][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.2543
[2025-04-30 00:23:20,353][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.2687, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-30 00:23:20,354][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.2484Epoch 10/10: [                              ] 2/63 batches, loss: 0.2526Epoch 10/10: [=                             ] 3/63 batches, loss: 0.2491Epoch 10/10: [=                             ] 4/63 batches, loss: 0.2519Epoch 10/10: [==                            ] 5/63 batches, loss: 0.2592Epoch 10/10: [==                            ] 6/63 batches, loss: 0.2588Epoch 10/10: [===                           ] 7/63 batches, loss: 0.2551Epoch 10/10: [===                           ] 8/63 batches, loss: 0.2521Epoch 10/10: [====                          ] 9/63 batches, loss: 0.2496Epoch 10/10: [====                          ] 10/63 batches, loss: 0.2471Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.2439Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.2526Epoch 10/10: [======                        ] 13/63 batches, loss: 0.2513Epoch 10/10: [======                        ] 14/63 batches, loss: 0.2489Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.2521Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.2543Epoch 10/10: [========                      ] 17/63 batches, loss: 0.2586Epoch 10/10: [========                      ] 18/63 batches, loss: 0.2558Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.2563Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.2547Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.2533Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.2539Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.2519Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.2508Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.2496Epoch 10/10: [============                  ] 26/63 batches, loss: 0.2484Epoch 10/10: [============                  ] 27/63 batches, loss: 0.2478Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.2472Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.2487Epoch 10/10: [==============                ] 30/63 batches, loss: 0.2477Epoch 10/10: [==============                ] 31/63 batches, loss: 0.2469Epoch 10/10: [===============               ] 32/63 batches, loss: 0.2455Epoch 10/10: [===============               ] 33/63 batches, loss: 0.2455Epoch 10/10: [================              ] 34/63 batches, loss: 0.2443Epoch 10/10: [================              ] 35/63 batches, loss: 0.2440Epoch 10/10: [=================             ] 36/63 batches, loss: 0.2453Epoch 10/10: [=================             ] 37/63 batches, loss: 0.2451Epoch 10/10: [==================            ] 38/63 batches, loss: 0.2442Epoch 10/10: [==================            ] 39/63 batches, loss: 0.2441Epoch 10/10: [===================           ] 40/63 batches, loss: 0.2436Epoch 10/10: [===================           ] 41/63 batches, loss: 0.2433Epoch 10/10: [====================          ] 42/63 batches, loss: 0.2429Epoch 10/10: [====================          ] 43/63 batches, loss: 0.2422Epoch 10/10: [====================          ] 44/63 batches, loss: 0.2420Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.2415Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.2416Epoch 10/10: [======================        ] 47/63 batches, loss: 0.2436Epoch 10/10: [======================        ] 48/63 batches, loss: 0.2428Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.2425Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.2431Epoch 10/10: [========================      ] 51/63 batches, loss: 0.2428Epoch 10/10: [========================      ] 52/63 batches, loss: 0.2420Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.2414Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.2409Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.2407Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.2402Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.2396Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.2391Epoch 10/10: [============================  ] 59/63 batches, loss: 0.2391Epoch 10/10: [============================  ] 60/63 batches, loss: 0.2388Epoch 10/10: [============================= ] 61/63 batches, loss: 0.2381Epoch 10/10: [============================= ] 62/63 batches, loss: 0.2382Epoch 10/10: [==============================] 63/63 batches, loss: 0.2386
[2025-04-30 00:23:33,080][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.2386
[2025-04-30 00:23:33,420][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.2656, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
[2025-04-30 00:23:33,421][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-30 00:23:33,421][src.training.lm_trainer][INFO] - Early stopping at epoch 10
[2025-04-30 00:23:33,421][src.training.lm_trainer][INFO] - Training completed in 134.84 seconds
[2025-04-30 00:23:33,421][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 00:23:38,129][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0}
[2025-04-30 00:23:38,129][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9230769230769231}
[2025-04-30 00:23:38,130][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7402597402597403, 'f1': 0.6428571428571429}
[2025-04-30 00:23:40,480][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁████
wandb:          best_val_f1 ▁▁████
wandb:        best_val_loss ██▇▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▅▅▃▂▂▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁█▇▇█▇███
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▇▃▄▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.93182
wandb:          best_val_f1 0.92308
wandb:        best_val_loss 0.2619
wandb:                epoch 10
wandb:  final_test_accuracy 0.74026
wandb:        final_test_f1 0.64286
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.93182
wandb:         final_val_f1 0.92308
wandb:        learning_rate 2e-05
wandb:           train_loss 0.23856
wandb:           train_time 134.8449
wandb:         val_accuracy 0.97727
wandb:               val_f1 0.97561
wandb:             val_loss 0.26563
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002100-qka0ecor
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002100-qka0ecor/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 00:23:52,859][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-30 00:23:52,859][__main__][INFO] - Normalized task: complexity
[2025-04-30 00:23:52,859][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-30 00:23:52,859][__main__][INFO] - Determined Task Type: regression
[2025-04-30 00:23:52,865][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-30 00:23:52,865][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 00:23:53,972][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 00:23:56,806][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 00:23:56,806][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:23:56,899][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:56,929][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:57,003][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-30 00:23:57,013][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:23:57,014][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-30 00:23:57,015][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:23:57,031][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:57,050][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:57,060][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-30 00:23:57,062][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:23:57,062][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-30 00:23:57,063][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:23:57,078][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:57,097][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:23:57,106][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-30 00:23:57,108][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:23:57,108][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-30 00:23:57,109][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-30 00:23:57,109][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:23:57,109][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:23:57,110][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:23:57,110][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:23:57,110][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:23:57,110][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-30 00:23:57,110][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-30 00:23:57,110][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-30 00:23:57,111][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:23:57,111][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:23:57,111][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:23:57,111][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:23:57,111][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:23:57,111][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-30 00:23:57,111][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-30 00:23:57,112][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-30 00:23:57,112][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:23:57,112][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:23:57,112][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:23:57,112][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:23:57,112][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:23:57,113][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-30 00:23:57,113][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-30 00:23:57,113][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-30 00:23:57,113][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-30 00:23:57,113][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 00:23:57,113][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 00:23:57,114][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 00:24:01,464][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 00:24:01,464][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,465][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,466][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,467][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,468][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,469][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,470][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,471][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,472][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,473][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,474][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,475][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,476][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,477][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,478][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,479][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,480][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,481][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,482][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:24:01,652][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-30 00:24:01,654][src.models.model_factory][INFO] - Model has 394,288,321 trainable parameters out of 394,288,321 total parameters
[2025-04-30 00:24:01,654][__main__][INFO] - Successfully created model for ar
[2025-04-30 00:24:01,654][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.1421Epoch 1/10: [                              ] 2/63 batches, loss: 0.1112Epoch 1/10: [=                             ] 3/63 batches, loss: 0.1798Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2045Epoch 1/10: [==                            ] 5/63 batches, loss: 0.1725Epoch 1/10: [==                            ] 6/63 batches, loss: 0.2358Epoch 1/10: [===                           ] 7/63 batches, loss: 0.2213Epoch 1/10: [===                           ] 8/63 batches, loss: 0.2104Epoch 1/10: [====                          ] 9/63 batches, loss: 0.2008Epoch 1/10: [====                          ] 10/63 batches, loss: 0.2207Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.2927Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.2965Epoch 1/10: [======                        ] 13/63 batches, loss: 0.3126Epoch 1/10: [======                        ] 14/63 batches, loss: 0.3123Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.3012Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.2892Epoch 1/10: [========                      ] 17/63 batches, loss: 0.2846Epoch 1/10: [========                      ] 18/63 batches, loss: 0.2818Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.2748Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.2715Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.2767Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.2680Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.2615Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.2553Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.2515Epoch 1/10: [============                  ] 26/63 batches, loss: 0.2460Epoch 1/10: [============                  ] 27/63 batches, loss: 0.2445Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.2415Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.2373Epoch 1/10: [==============                ] 30/63 batches, loss: 0.2322Epoch 1/10: [==============                ] 31/63 batches, loss: 0.2429Epoch 1/10: [===============               ] 32/63 batches, loss: 0.2408Epoch 1/10: [===============               ] 33/63 batches, loss: 0.2365Epoch 1/10: [================              ] 34/63 batches, loss: 0.2373Epoch 1/10: [================              ] 35/63 batches, loss: 0.2333Epoch 1/10: [=================             ] 36/63 batches, loss: 0.2284Epoch 1/10: [=================             ] 37/63 batches, loss: 0.2311Epoch 1/10: [==================            ] 38/63 batches, loss: 0.2264Epoch 1/10: [==================            ] 39/63 batches, loss: 0.2230Epoch 1/10: [===================           ] 40/63 batches, loss: 0.2184Epoch 1/10: [===================           ] 41/63 batches, loss: 0.2148Epoch 1/10: [====================          ] 42/63 batches, loss: 0.2135Epoch 1/10: [====================          ] 43/63 batches, loss: 0.2104Epoch 1/10: [====================          ] 44/63 batches, loss: 0.2086Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.2048Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.2035Epoch 1/10: [======================        ] 47/63 batches, loss: 0.2010Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1985Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1959Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1960Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1940Epoch 1/10: [========================      ] 52/63 batches, loss: 0.1928Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.1912Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.1894Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.1875Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.1859Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.1847Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.1825Epoch 1/10: [============================  ] 59/63 batches, loss: 0.1809Epoch 1/10: [============================  ] 60/63 batches, loss: 0.1789Epoch 1/10: [============================= ] 61/63 batches, loss: 0.1784Epoch 1/10: [============================= ] 62/63 batches, loss: 0.1788Epoch 1/10: [==============================] 63/63 batches, loss: 0.1780
[2025-04-30 00:24:16,825][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1780
[2025-04-30 00:24:17,128][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0646, Metrics: {'mse': 0.06465756893157959, 'rmse': 0.2542785262887521, 'r2': 0.0034118294715881348}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0654Epoch 2/10: [                              ] 2/63 batches, loss: 0.0901Epoch 2/10: [=                             ] 3/63 batches, loss: 0.1096Epoch 2/10: [=                             ] 4/63 batches, loss: 0.1230Epoch 2/10: [==                            ] 5/63 batches, loss: 0.1126Epoch 2/10: [==                            ] 6/63 batches, loss: 0.1215Epoch 2/10: [===                           ] 7/63 batches, loss: 0.1307Epoch 2/10: [===                           ] 8/63 batches, loss: 0.1284Epoch 2/10: [====                          ] 9/63 batches, loss: 0.1289Epoch 2/10: [====                          ] 10/63 batches, loss: 0.1262Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.1280Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.1298Epoch 2/10: [======                        ] 13/63 batches, loss: 0.1298Epoch 2/10: [======                        ] 14/63 batches, loss: 0.1254Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.1265Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.1255Epoch 2/10: [========                      ] 17/63 batches, loss: 0.1272Epoch 2/10: [========                      ] 18/63 batches, loss: 0.1229Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.1201Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.1163Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.1139Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.1129Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.1097Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.1103Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.1089Epoch 2/10: [============                  ] 26/63 batches, loss: 0.1082Epoch 2/10: [============                  ] 27/63 batches, loss: 0.1098Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.1075Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.1063Epoch 2/10: [==============                ] 30/63 batches, loss: 0.1065Epoch 2/10: [==============                ] 31/63 batches, loss: 0.1065Epoch 2/10: [===============               ] 32/63 batches, loss: 0.1080Epoch 2/10: [===============               ] 33/63 batches, loss: 0.1063Epoch 2/10: [================              ] 34/63 batches, loss: 0.1048Epoch 2/10: [================              ] 35/63 batches, loss: 0.1051Epoch 2/10: [=================             ] 36/63 batches, loss: 0.1035Epoch 2/10: [=================             ] 37/63 batches, loss: 0.1055Epoch 2/10: [==================            ] 38/63 batches, loss: 0.1040Epoch 2/10: [==================            ] 39/63 batches, loss: 0.1033Epoch 2/10: [===================           ] 40/63 batches, loss: 0.1033Epoch 2/10: [===================           ] 41/63 batches, loss: 0.1033Epoch 2/10: [====================          ] 42/63 batches, loss: 0.1021Epoch 2/10: [====================          ] 43/63 batches, loss: 0.1005Epoch 2/10: [====================          ] 44/63 batches, loss: 0.1017Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.1031Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.1024Epoch 2/10: [======================        ] 47/63 batches, loss: 0.1036Epoch 2/10: [======================        ] 48/63 batches, loss: 0.1048Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.1043Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.1037Epoch 2/10: [========================      ] 51/63 batches, loss: 0.1029Epoch 2/10: [========================      ] 52/63 batches, loss: 0.1040Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.1033Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.1040Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.1036Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.1032Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.1026Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.1017Epoch 2/10: [============================  ] 59/63 batches, loss: 0.1029Epoch 2/10: [============================  ] 60/63 batches, loss: 0.1066Epoch 2/10: [============================= ] 61/63 batches, loss: 0.1062Epoch 2/10: [============================= ] 62/63 batches, loss: 0.1057Epoch 2/10: [==============================] 63/63 batches, loss: 0.1123
[2025-04-30 00:24:30,429][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1123
[2025-04-30 00:24:30,754][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0687, Metrics: {'mse': 0.06839065998792648, 'rmse': 0.2615160797884644, 'r2': -0.05412745475769043}
[2025-04-30 00:24:30,755][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.1480Epoch 3/10: [                              ] 2/63 batches, loss: 0.1048Epoch 3/10: [=                             ] 3/63 batches, loss: 0.0859Epoch 3/10: [=                             ] 4/63 batches, loss: 0.0846Epoch 3/10: [==                            ] 5/63 batches, loss: 0.0848Epoch 3/10: [==                            ] 6/63 batches, loss: 0.0953Epoch 3/10: [===                           ] 7/63 batches, loss: 0.0960Epoch 3/10: [===                           ] 8/63 batches, loss: 0.0991Epoch 3/10: [====                          ] 9/63 batches, loss: 0.0981Epoch 3/10: [====                          ] 10/63 batches, loss: 0.0944Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.0980Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.0971Epoch 3/10: [======                        ] 13/63 batches, loss: 0.0959Epoch 3/10: [======                        ] 14/63 batches, loss: 0.0944Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.0941Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.0989Epoch 3/10: [========                      ] 17/63 batches, loss: 0.0945Epoch 3/10: [========                      ] 18/63 batches, loss: 0.0914Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.0878Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.0849Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.0911Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.0896Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.0917Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.0936Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.0910Epoch 3/10: [============                  ] 26/63 batches, loss: 0.0898Epoch 3/10: [============                  ] 27/63 batches, loss: 0.0891Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.0904Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.0889Epoch 3/10: [==============                ] 30/63 batches, loss: 0.0895Epoch 3/10: [==============                ] 31/63 batches, loss: 0.0881Epoch 3/10: [===============               ] 32/63 batches, loss: 0.0869Epoch 3/10: [===============               ] 33/63 batches, loss: 0.0872Epoch 3/10: [================              ] 34/63 batches, loss: 0.0890Epoch 3/10: [================              ] 35/63 batches, loss: 0.0902Epoch 3/10: [=================             ] 36/63 batches, loss: 0.0907Epoch 3/10: [=================             ] 37/63 batches, loss: 0.0904Epoch 3/10: [==================            ] 38/63 batches, loss: 0.0901Epoch 3/10: [==================            ] 39/63 batches, loss: 0.0895Epoch 3/10: [===================           ] 40/63 batches, loss: 0.0909Epoch 3/10: [===================           ] 41/63 batches, loss: 0.0919Epoch 3/10: [====================          ] 42/63 batches, loss: 0.0906Epoch 3/10: [====================          ] 43/63 batches, loss: 0.0925Epoch 3/10: [====================          ] 44/63 batches, loss: 0.0926Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.0957Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.0963Epoch 3/10: [======================        ] 47/63 batches, loss: 0.0964Epoch 3/10: [======================        ] 48/63 batches, loss: 0.0955Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.0948Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.0944Epoch 3/10: [========================      ] 51/63 batches, loss: 0.0945Epoch 3/10: [========================      ] 52/63 batches, loss: 0.0938Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.0952Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.0955Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.0956Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.0949Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.0950Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.0976Epoch 3/10: [============================  ] 59/63 batches, loss: 0.0978Epoch 3/10: [============================  ] 60/63 batches, loss: 0.0978Epoch 3/10: [============================= ] 61/63 batches, loss: 0.0977Epoch 3/10: [============================= ] 62/63 batches, loss: 0.0973Epoch 3/10: [==============================] 63/63 batches, loss: 0.0974
[2025-04-30 00:24:43,447][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0974
[2025-04-30 00:24:43,754][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0631, Metrics: {'mse': 0.06326517462730408, 'rmse': 0.2515256937716385, 'r2': 0.024873197078704834}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.1373Epoch 4/10: [                              ] 2/63 batches, loss: 0.1079Epoch 4/10: [=                             ] 3/63 batches, loss: 0.1149Epoch 4/10: [=                             ] 4/63 batches, loss: 0.0959Epoch 4/10: [==                            ] 5/63 batches, loss: 0.0859Epoch 4/10: [==                            ] 6/63 batches, loss: 0.0823Epoch 4/10: [===                           ] 7/63 batches, loss: 0.0890Epoch 4/10: [===                           ] 8/63 batches, loss: 0.0956Epoch 4/10: [====                          ] 9/63 batches, loss: 0.0947Epoch 4/10: [====                          ] 10/63 batches, loss: 0.0955Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.1046Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.0992Epoch 4/10: [======                        ] 13/63 batches, loss: 0.0968Epoch 4/10: [======                        ] 14/63 batches, loss: 0.1028Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.1006Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.0991Epoch 4/10: [========                      ] 17/63 batches, loss: 0.0969Epoch 4/10: [========                      ] 18/63 batches, loss: 0.1048Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.1041Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.1039Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.1015Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.0993Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.0981Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.1011Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.0981Epoch 4/10: [============                  ] 26/63 batches, loss: 0.0978Epoch 4/10: [============                  ] 27/63 batches, loss: 0.1003Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.1019Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.1017Epoch 4/10: [==============                ] 30/63 batches, loss: 0.1006Epoch 4/10: [==============                ] 31/63 batches, loss: 0.0983Epoch 4/10: [===============               ] 32/63 batches, loss: 0.0978Epoch 4/10: [===============               ] 33/63 batches, loss: 0.0998Epoch 4/10: [================              ] 34/63 batches, loss: 0.0999Epoch 4/10: [================              ] 35/63 batches, loss: 0.0988Epoch 4/10: [=================             ] 36/63 batches, loss: 0.1023Epoch 4/10: [=================             ] 37/63 batches, loss: 0.1032Epoch 4/10: [==================            ] 38/63 batches, loss: 0.1026Epoch 4/10: [==================            ] 39/63 batches, loss: 0.1014Epoch 4/10: [===================           ] 40/63 batches, loss: 0.1009Epoch 4/10: [===================           ] 41/63 batches, loss: 0.0998Epoch 4/10: [====================          ] 42/63 batches, loss: 0.0981Epoch 4/10: [====================          ] 43/63 batches, loss: 0.0997Epoch 4/10: [====================          ] 44/63 batches, loss: 0.0985Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.0979Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.0981Epoch 4/10: [======================        ] 47/63 batches, loss: 0.0972Epoch 4/10: [======================        ] 48/63 batches, loss: 0.0967Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.0960Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.0963Epoch 4/10: [========================      ] 51/63 batches, loss: 0.0954Epoch 4/10: [========================      ] 52/63 batches, loss: 0.0977Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.0966Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.0964Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.0958Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.0966Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.0969Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.0963Epoch 4/10: [============================  ] 59/63 batches, loss: 0.0978Epoch 4/10: [============================  ] 60/63 batches, loss: 0.0976Epoch 4/10: [============================= ] 61/63 batches, loss: 0.0991Epoch 4/10: [============================= ] 62/63 batches, loss: 0.0995Epoch 4/10: [==============================] 63/63 batches, loss: 0.1037
[2025-04-30 00:24:57,081][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1037
[2025-04-30 00:24:57,412][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0730, Metrics: {'mse': 0.07312691956758499, 'rmse': 0.2704198949182271, 'r2': -0.1271289587020874}
[2025-04-30 00:24:57,413][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.0906Epoch 5/10: [                              ] 2/63 batches, loss: 0.0862Epoch 5/10: [=                             ] 3/63 batches, loss: 0.0862Epoch 5/10: [=                             ] 4/63 batches, loss: 0.1221Epoch 5/10: [==                            ] 5/63 batches, loss: 0.1159Epoch 5/10: [==                            ] 6/63 batches, loss: 0.1102Epoch 5/10: [===                           ] 7/63 batches, loss: 0.1020Epoch 5/10: [===                           ] 8/63 batches, loss: 0.0960Epoch 5/10: [====                          ] 9/63 batches, loss: 0.0984Epoch 5/10: [====                          ] 10/63 batches, loss: 0.0952Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.0918Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.0884Epoch 5/10: [======                        ] 13/63 batches, loss: 0.0896Epoch 5/10: [======                        ] 14/63 batches, loss: 0.0871Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.0837Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.0910Epoch 5/10: [========                      ] 17/63 batches, loss: 0.0901Epoch 5/10: [========                      ] 18/63 batches, loss: 0.0924Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.0898Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.0881Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.0869Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.0876Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.0862Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.0888Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.0894Epoch 5/10: [============                  ] 26/63 batches, loss: 0.0884Epoch 5/10: [============                  ] 27/63 batches, loss: 0.0887Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.0877Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.0865Epoch 5/10: [==============                ] 30/63 batches, loss: 0.0858Epoch 5/10: [==============                ] 31/63 batches, loss: 0.0854Epoch 5/10: [===============               ] 32/63 batches, loss: 0.0864Epoch 5/10: [===============               ] 33/63 batches, loss: 0.0851Epoch 5/10: [================              ] 34/63 batches, loss: 0.0843Epoch 5/10: [================              ] 35/63 batches, loss: 0.0859Epoch 5/10: [=================             ] 36/63 batches, loss: 0.0848Epoch 5/10: [=================             ] 37/63 batches, loss: 0.0852Epoch 5/10: [==================            ] 38/63 batches, loss: 0.0868Epoch 5/10: [==================            ] 39/63 batches, loss: 0.0864Epoch 5/10: [===================           ] 40/63 batches, loss: 0.0872Epoch 5/10: [===================           ] 41/63 batches, loss: 0.0869Epoch 5/10: [====================          ] 42/63 batches, loss: 0.0883Epoch 5/10: [====================          ] 43/63 batches, loss: 0.0877Epoch 5/10: [====================          ] 44/63 batches, loss: 0.0878Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.0923Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.0920Epoch 5/10: [======================        ] 47/63 batches, loss: 0.0915Epoch 5/10: [======================        ] 48/63 batches, loss: 0.0923Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.0938Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.0945Epoch 5/10: [========================      ] 51/63 batches, loss: 0.0952Epoch 5/10: [========================      ] 52/63 batches, loss: 0.0949Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.0948Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.0952Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.0964Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.0957Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.0954Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.0953Epoch 5/10: [============================  ] 59/63 batches, loss: 0.0953Epoch 5/10: [============================  ] 60/63 batches, loss: 0.0950Epoch 5/10: [============================= ] 61/63 batches, loss: 0.0946Epoch 5/10: [============================= ] 62/63 batches, loss: 0.0945Epoch 5/10: [==============================] 63/63 batches, loss: 0.0974
[2025-04-30 00:25:10,120][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0974
[2025-04-30 00:25:10,443][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0647, Metrics: {'mse': 0.0645885318517685, 'rmse': 0.25414273912856233, 'r2': 0.004475951194763184}
[2025-04-30 00:25:10,444][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.0542Epoch 6/10: [                              ] 2/63 batches, loss: 0.0431Epoch 6/10: [=                             ] 3/63 batches, loss: 0.0515Epoch 6/10: [=                             ] 4/63 batches, loss: 0.0503Epoch 6/10: [==                            ] 5/63 batches, loss: 0.0492Epoch 6/10: [==                            ] 6/63 batches, loss: 0.0514Epoch 6/10: [===                           ] 7/63 batches, loss: 0.0603Epoch 6/10: [===                           ] 8/63 batches, loss: 0.0599Epoch 6/10: [====                          ] 9/63 batches, loss: 0.0563Epoch 6/10: [====                          ] 10/63 batches, loss: 0.0565Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.0561Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.0609Epoch 6/10: [======                        ] 13/63 batches, loss: 0.0617Epoch 6/10: [======                        ] 14/63 batches, loss: 0.0599Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.0593Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.0617Epoch 6/10: [========                      ] 17/63 batches, loss: 0.0642Epoch 6/10: [========                      ] 18/63 batches, loss: 0.0681Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.0722Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.0724Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.0719Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.0710Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.0755Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.0775Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.0785Epoch 6/10: [============                  ] 26/63 batches, loss: 0.0813Epoch 6/10: [============                  ] 27/63 batches, loss: 0.0797Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.0799Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.0789Epoch 6/10: [==============                ] 30/63 batches, loss: 0.0791Epoch 6/10: [==============                ] 31/63 batches, loss: 0.0792Epoch 6/10: [===============               ] 32/63 batches, loss: 0.0808Epoch 6/10: [===============               ] 33/63 batches, loss: 0.0818Epoch 6/10: [================              ] 34/63 batches, loss: 0.0816Epoch 6/10: [================              ] 35/63 batches, loss: 0.0807Epoch 6/10: [=================             ] 36/63 batches, loss: 0.0809Epoch 6/10: [=================             ] 37/63 batches, loss: 0.0808Epoch 6/10: [==================            ] 38/63 batches, loss: 0.0807Epoch 6/10: [==================            ] 39/63 batches, loss: 0.0804Epoch 6/10: [===================           ] 40/63 batches, loss: 0.0795Epoch 6/10: [===================           ] 41/63 batches, loss: 0.0812Epoch 6/10: [====================          ] 42/63 batches, loss: 0.0802Epoch 6/10: [====================          ] 43/63 batches, loss: 0.0804Epoch 6/10: [====================          ] 44/63 batches, loss: 0.0797Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.0790Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.0790Epoch 6/10: [======================        ] 47/63 batches, loss: 0.0780Epoch 6/10: [======================        ] 48/63 batches, loss: 0.0797Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.0789Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.0787Epoch 6/10: [========================      ] 51/63 batches, loss: 0.0784Epoch 6/10: [========================      ] 52/63 batches, loss: 0.0798Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.0806Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.0806Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.0796Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.0793Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.0790Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.0795Epoch 6/10: [============================  ] 59/63 batches, loss: 0.0797Epoch 6/10: [============================  ] 60/63 batches, loss: 0.0795Epoch 6/10: [============================= ] 61/63 batches, loss: 0.0795Epoch 6/10: [============================= ] 62/63 batches, loss: 0.0793Epoch 6/10: [==============================] 63/63 batches, loss: 0.0786
[2025-04-30 00:25:23,133][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0786
[2025-04-30 00:25:23,456][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0822, Metrics: {'mse': 0.08277822285890579, 'rmse': 0.2877120485118859, 'r2': -0.27588772773742676}
[2025-04-30 00:25:23,457][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-30 00:25:23,457][src.training.lm_trainer][INFO] - Early stopping at epoch 6
[2025-04-30 00:25:23,457][src.training.lm_trainer][INFO] - Training completed in 80.25 seconds
[2025-04-30 00:25:23,457][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 00:25:28,114][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.036721572279930115, 'rmse': 0.19162873552766066, 'r2': -0.1962451934814453}
[2025-04-30 00:25:28,115][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06326517462730408, 'rmse': 0.2515256937716385, 'r2': 0.024873197078704834}
[2025-04-30 00:25:28,115][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.058801013976335526, 'rmse': 0.24248920383459452, 'r2': -0.013703346252441406}
[2025-04-30 00:25:30,466][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▃▂▁
wandb:       train_time ▁
wandb:         val_loss ▂▃▁▅▂█
wandb:          val_mse ▁▃▁▅▁█
wandb:           val_r2 █▆█▄█▁
wandb:         val_rmse ▂▃▁▅▂█
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06306
wandb:     best_val_mse 0.06327
wandb:      best_val_r2 0.02487
wandb:    best_val_rmse 0.25153
wandb:            epoch 6
wandb:   final_test_mse 0.0588
wandb:    final_test_r2 -0.0137
wandb:  final_test_rmse 0.24249
wandb:  final_train_mse 0.03672
wandb:   final_train_r2 -0.19625
wandb: final_train_rmse 0.19163
wandb:    final_val_mse 0.06327
wandb:     final_val_r2 0.02487
wandb:   final_val_rmse 0.25153
wandb:    learning_rate 2e-05
wandb:       train_loss 0.07861
wandb:       train_time 80.25419
wandb:         val_loss 0.08217
wandb:          val_mse 0.08278
wandb:           val_r2 -0.27589
wandb:         val_rmse 0.28771
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002352-4jj1stj3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002352-4jj1stj3/logs
Experiment finetune_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar/results.json
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 00:25:40,904][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/en
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 00:25:40,904][__main__][INFO] - Normalized task: question_type
[2025-04-30 00:25:40,904][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 00:25:40,904][__main__][INFO] - Determined Task Type: classification
[2025-04-30 00:25:40,909][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-30 00:25:40,910][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 00:25:42,239][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 00:25:45,129][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 00:25:45,130][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:25:45,200][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,226][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,315][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-30 00:25:45,327][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:25:45,328][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-30 00:25:45,329][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:25:45,343][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,365][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,375][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-30 00:25:45,377][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:25:45,377][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-30 00:25:45,379][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:25:45,393][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,413][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:25:45,422][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-30 00:25:45,424][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:25:45,424][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-30 00:25:45,425][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-30 00:25:45,425][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:25:45,426][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:25:45,426][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:25:45,426][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:25:45,426][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-30 00:25:45,426][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-30 00:25:45,426][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-30 00:25:45,426][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 00:25:45,427][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:25:45,427][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:25:45,427][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:25:45,427][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:25:45,427][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-30 00:25:45,427][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-30 00:25:45,427][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-30 00:25:45,428][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 00:25:45,428][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:25:45,428][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:25:45,428][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:25:45,428][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:25:45,428][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-30 00:25:45,428][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-30 00:25:45,429][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-30 00:25:45,429][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 00:25:45,429][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-30 00:25:45,429][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 00:25:45,429][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 00:25:45,430][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 00:25:49,881][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 00:25:49,881][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,881][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,882][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,883][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,884][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,885][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,886][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,887][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,888][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,889][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,890][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,891][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,892][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,893][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,894][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,895][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,896][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,897][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:49,898][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:25:50,086][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-30 00:25:50,088][src.models.model_factory][INFO] - Model has 394,200,289 trainable parameters out of 394,200,289 total parameters
[2025-04-30 00:25:50,088][__main__][INFO] - Successfully created model for en
[2025-04-30 00:25:50,088][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7741Epoch 1/10: [                              ] 2/75 batches, loss: 0.7269Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7243Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7066Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7248Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7175Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7211Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7274Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7273Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7351Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7335Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7273Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7199Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7178Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7148Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7164Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7125Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7151Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7074Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7039Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7045Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7066Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7029Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7005Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7001Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7035Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7019Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7034Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7008Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6993Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7006Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6984Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7002Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6993Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7014Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6994Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6996Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7013Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7018Epoch 1/10: [================              ] 40/75 batches, loss: 0.7023Epoch 1/10: [================              ] 41/75 batches, loss: 0.7036Epoch 1/10: [================              ] 42/75 batches, loss: 0.7038Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7028Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7016Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6994Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6987Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6986Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6988Epoch 1/10: [===================           ] 49/75 batches, loss: 0.6994Epoch 1/10: [====================          ] 50/75 batches, loss: 0.6987Epoch 1/10: [====================          ] 51/75 batches, loss: 0.6993Epoch 1/10: [====================          ] 52/75 batches, loss: 0.6985Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.6991Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.6979Epoch 1/10: [======================        ] 55/75 batches, loss: 0.6974Epoch 1/10: [======================        ] 56/75 batches, loss: 0.6969Epoch 1/10: [======================        ] 57/75 batches, loss: 0.6976Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.6985Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.6974Epoch 1/10: [========================      ] 60/75 batches, loss: 0.6973Epoch 1/10: [========================      ] 61/75 batches, loss: 0.6968Epoch 1/10: [========================      ] 62/75 batches, loss: 0.6957Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.6958Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.6960Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.6957Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.6940Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.6942Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.6939Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.6939Epoch 1/10: [============================  ] 70/75 batches, loss: 0.6931Epoch 1/10: [============================  ] 71/75 batches, loss: 0.6935Epoch 1/10: [============================  ] 72/75 batches, loss: 0.6935Epoch 1/10: [============================= ] 73/75 batches, loss: 0.6939Epoch 1/10: [============================= ] 74/75 batches, loss: 0.6930Epoch 1/10: [==============================] 75/75 batches, loss: 0.6927
[2025-04-30 00:26:07,378][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6927
[2025-04-30 00:26:07,807][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6830, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6576Epoch 2/10: [                              ] 2/75 batches, loss: 0.6647Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6960Epoch 2/10: [=                             ] 4/75 batches, loss: 0.7021Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6776Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6842Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6825Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6731Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6788Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6784Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6797Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6761Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6844Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6889Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6803Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6809Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6869Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6876Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6839Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6867Epoch 2/10: [========                      ] 21/75 batches, loss: 0.6868Epoch 2/10: [========                      ] 22/75 batches, loss: 0.6849Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.6839Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.6900Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.6883Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.6872Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.6892Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.6865Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.6842Epoch 2/10: [============                  ] 30/75 batches, loss: 0.6802Epoch 2/10: [============                  ] 31/75 batches, loss: 0.6806Epoch 2/10: [============                  ] 32/75 batches, loss: 0.6845Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.6828Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.6820Epoch 2/10: [==============                ] 35/75 batches, loss: 0.6836Epoch 2/10: [==============                ] 36/75 batches, loss: 0.6832Epoch 2/10: [==============                ] 37/75 batches, loss: 0.6816Epoch 2/10: [===============               ] 38/75 batches, loss: 0.6797Epoch 2/10: [===============               ] 39/75 batches, loss: 0.6774Epoch 2/10: [================              ] 40/75 batches, loss: 0.6783Epoch 2/10: [================              ] 41/75 batches, loss: 0.6774Epoch 2/10: [================              ] 42/75 batches, loss: 0.6773Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6747Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6723Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6715Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6736Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6749Epoch 2/10: [===================           ] 48/75 batches, loss: 0.6739Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6733Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6713Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6695Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6678Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6662Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6645Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6635Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6625Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6612Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6617Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6605Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6603Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6601Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6578Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6565Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6557Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6553Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6550Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6558Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6556Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6553Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6531Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6520Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6523Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6520Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6519Epoch 2/10: [==============================] 75/75 batches, loss: 0.6528
[2025-04-30 00:26:23,527][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6528
[2025-04-30 00:26:23,957][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6654, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.5980Epoch 3/10: [                              ] 2/75 batches, loss: 0.5542Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5671Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5638Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5918Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5876Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5886Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5892Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5914Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5910Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5930Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5995Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5992Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.6091Epoch 3/10: [======                        ] 15/75 batches, loss: 0.6125Epoch 3/10: [======                        ] 16/75 batches, loss: 0.6175Epoch 3/10: [======                        ] 17/75 batches, loss: 0.6163Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.6182Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.6131Epoch 3/10: [========                      ] 20/75 batches, loss: 0.6175Epoch 3/10: [========                      ] 21/75 batches, loss: 0.6174Epoch 3/10: [========                      ] 22/75 batches, loss: 0.6196Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.6201Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.6207Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.6223Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.6179Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.6166Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.6161Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.6165Epoch 3/10: [============                  ] 30/75 batches, loss: 0.6204Epoch 3/10: [============                  ] 31/75 batches, loss: 0.6206Epoch 3/10: [============                  ] 32/75 batches, loss: 0.6212Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.6245Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.6229Epoch 3/10: [==============                ] 35/75 batches, loss: 0.6210Epoch 3/10: [==============                ] 36/75 batches, loss: 0.6232Epoch 3/10: [==============                ] 37/75 batches, loss: 0.6195Epoch 3/10: [===============               ] 38/75 batches, loss: 0.6176Epoch 3/10: [===============               ] 39/75 batches, loss: 0.6188Epoch 3/10: [================              ] 40/75 batches, loss: 0.6192Epoch 3/10: [================              ] 41/75 batches, loss: 0.6185Epoch 3/10: [================              ] 42/75 batches, loss: 0.6227Epoch 3/10: [=================             ] 43/75 batches, loss: 0.6225Epoch 3/10: [=================             ] 44/75 batches, loss: 0.6232Epoch 3/10: [==================            ] 45/75 batches, loss: 0.6236Epoch 3/10: [==================            ] 46/75 batches, loss: 0.6238Epoch 3/10: [==================            ] 47/75 batches, loss: 0.6242Epoch 3/10: [===================           ] 48/75 batches, loss: 0.6228Epoch 3/10: [===================           ] 49/75 batches, loss: 0.6197Epoch 3/10: [====================          ] 50/75 batches, loss: 0.6204Epoch 3/10: [====================          ] 51/75 batches, loss: 0.6211Epoch 3/10: [====================          ] 52/75 batches, loss: 0.6196Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.6182Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.6175Epoch 3/10: [======================        ] 55/75 batches, loss: 0.6167Epoch 3/10: [======================        ] 56/75 batches, loss: 0.6182Epoch 3/10: [======================        ] 57/75 batches, loss: 0.6173Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.6174Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.6173Epoch 3/10: [========================      ] 60/75 batches, loss: 0.6177Epoch 3/10: [========================      ] 61/75 batches, loss: 0.6184Epoch 3/10: [========================      ] 62/75 batches, loss: 0.6173Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.6173Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.6181Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.6161Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.6157Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.6149Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.6151Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.6134Epoch 3/10: [============================  ] 70/75 batches, loss: 0.6141Epoch 3/10: [============================  ] 71/75 batches, loss: 0.6126Epoch 3/10: [============================  ] 72/75 batches, loss: 0.6126Epoch 3/10: [============================= ] 73/75 batches, loss: 0.6109Epoch 3/10: [============================= ] 74/75 batches, loss: 0.6101Epoch 3/10: [==============================] 75/75 batches, loss: 0.6128
[2025-04-30 00:26:39,699][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6128
[2025-04-30 00:26:40,129][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6453, Metrics: {'accuracy': 0.5138888888888888, 'f1': 0.05405405405405406}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5304Epoch 4/10: [                              ] 2/75 batches, loss: 0.5328Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5726Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5707Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5748Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5714Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5702Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5619Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5656Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5629Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5591Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5663Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5656Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5674Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5673Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5642Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5625Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5620Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5649Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5616Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5598Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5594Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5586Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5604Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5604Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5605Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5608Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5574Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5507Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5474Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5435Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5391Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5353Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5352Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5347Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5338Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5323Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5325Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5311Epoch 4/10: [================              ] 40/75 batches, loss: 0.5309Epoch 4/10: [================              ] 41/75 batches, loss: 0.5275Epoch 4/10: [================              ] 42/75 batches, loss: 0.5278Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5268Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5266Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5268Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5269Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5255Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5224Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5220Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5206Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5210Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5207Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5212Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5205Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5208Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5199Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5197Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5193Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5199Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5185Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5186Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5184Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5191Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5180Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5180Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5172Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5168Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5157Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5161Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5155Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5157Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5149Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5139Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5126Epoch 4/10: [==============================] 75/75 batches, loss: 0.5113
[2025-04-30 00:26:55,865][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5113
[2025-04-30 00:26:56,281][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5127, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9333333333333333}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.4770Epoch 5/10: [                              ] 2/75 batches, loss: 0.4482Epoch 5/10: [=                             ] 3/75 batches, loss: 0.4296Epoch 5/10: [=                             ] 4/75 batches, loss: 0.4594Epoch 5/10: [==                            ] 5/75 batches, loss: 0.4710Epoch 5/10: [==                            ] 6/75 batches, loss: 0.4676Epoch 5/10: [==                            ] 7/75 batches, loss: 0.4683Epoch 5/10: [===                           ] 8/75 batches, loss: 0.4697Epoch 5/10: [===                           ] 9/75 batches, loss: 0.4714Epoch 5/10: [====                          ] 10/75 batches, loss: 0.4712Epoch 5/10: [====                          ] 11/75 batches, loss: 0.4719Epoch 5/10: [====                          ] 12/75 batches, loss: 0.4703Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.4678Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.4643Epoch 5/10: [======                        ] 15/75 batches, loss: 0.4652Epoch 5/10: [======                        ] 16/75 batches, loss: 0.4644Epoch 5/10: [======                        ] 17/75 batches, loss: 0.4646Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.4616Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.4627Epoch 5/10: [========                      ] 20/75 batches, loss: 0.4623Epoch 5/10: [========                      ] 21/75 batches, loss: 0.4590Epoch 5/10: [========                      ] 22/75 batches, loss: 0.4610Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.4616Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.4603Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.4589Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.4572Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.4555Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.4564Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.4535Epoch 5/10: [============                  ] 30/75 batches, loss: 0.4555Epoch 5/10: [============                  ] 31/75 batches, loss: 0.4538Epoch 5/10: [============                  ] 32/75 batches, loss: 0.4522Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.4524Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.4521Epoch 5/10: [==============                ] 35/75 batches, loss: 0.4528Epoch 5/10: [==============                ] 36/75 batches, loss: 0.4529Epoch 5/10: [==============                ] 37/75 batches, loss: 0.4514Epoch 5/10: [===============               ] 38/75 batches, loss: 0.4500Epoch 5/10: [===============               ] 39/75 batches, loss: 0.4522Epoch 5/10: [================              ] 40/75 batches, loss: 0.4519Epoch 5/10: [================              ] 41/75 batches, loss: 0.4506Epoch 5/10: [================              ] 42/75 batches, loss: 0.4502Epoch 5/10: [=================             ] 43/75 batches, loss: 0.4502Epoch 5/10: [=================             ] 44/75 batches, loss: 0.4517Epoch 5/10: [==================            ] 45/75 batches, loss: 0.4519Epoch 5/10: [==================            ] 46/75 batches, loss: 0.4513Epoch 5/10: [==================            ] 47/75 batches, loss: 0.4504Epoch 5/10: [===================           ] 48/75 batches, loss: 0.4540Epoch 5/10: [===================           ] 49/75 batches, loss: 0.4525Epoch 5/10: [====================          ] 50/75 batches, loss: 0.4528Epoch 5/10: [====================          ] 51/75 batches, loss: 0.4524Epoch 5/10: [====================          ] 52/75 batches, loss: 0.4512Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.4502Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.4498Epoch 5/10: [======================        ] 55/75 batches, loss: 0.4487Epoch 5/10: [======================        ] 56/75 batches, loss: 0.4476Epoch 5/10: [======================        ] 57/75 batches, loss: 0.4469Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.4457Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.4462Epoch 5/10: [========================      ] 60/75 batches, loss: 0.4451Epoch 5/10: [========================      ] 61/75 batches, loss: 0.4446Epoch 5/10: [========================      ] 62/75 batches, loss: 0.4445Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.4441Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.4434Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.4428Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.4431Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.4414Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.4422Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.4412Epoch 5/10: [============================  ] 70/75 batches, loss: 0.4409Epoch 5/10: [============================  ] 71/75 batches, loss: 0.4403Epoch 5/10: [============================  ] 72/75 batches, loss: 0.4393Epoch 5/10: [============================= ] 73/75 batches, loss: 0.4382Epoch 5/10: [============================= ] 74/75 batches, loss: 0.4373Epoch 5/10: [==============================] 75/75 batches, loss: 0.4362
[2025-04-30 00:27:12,054][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.4362
[2025-04-30 00:27:12,498][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3534, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.3919Epoch 6/10: [                              ] 2/75 batches, loss: 0.3657Epoch 6/10: [=                             ] 3/75 batches, loss: 0.3811Epoch 6/10: [=                             ] 4/75 batches, loss: 0.3894Epoch 6/10: [==                            ] 5/75 batches, loss: 0.3797Epoch 6/10: [==                            ] 6/75 batches, loss: 0.3839Epoch 6/10: [==                            ] 7/75 batches, loss: 0.3789Epoch 6/10: [===                           ] 8/75 batches, loss: 0.3722Epoch 6/10: [===                           ] 9/75 batches, loss: 0.3679Epoch 6/10: [====                          ] 10/75 batches, loss: 0.3711Epoch 6/10: [====                          ] 11/75 batches, loss: 0.3697Epoch 6/10: [====                          ] 12/75 batches, loss: 0.3705Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.3725Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.3735Epoch 6/10: [======                        ] 15/75 batches, loss: 0.3725Epoch 6/10: [======                        ] 16/75 batches, loss: 0.3712Epoch 6/10: [======                        ] 17/75 batches, loss: 0.3677Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.3652Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.3685Epoch 6/10: [========                      ] 20/75 batches, loss: 0.3671Epoch 6/10: [========                      ] 21/75 batches, loss: 0.3649Epoch 6/10: [========                      ] 22/75 batches, loss: 0.3628Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.3631Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.3669Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.3703Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.3717Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.3699Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.3701Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.3681Epoch 6/10: [============                  ] 30/75 batches, loss: 0.3683Epoch 6/10: [============                  ] 31/75 batches, loss: 0.3678Epoch 6/10: [============                  ] 32/75 batches, loss: 0.3686Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.3709Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.3700Epoch 6/10: [==============                ] 35/75 batches, loss: 0.3708Epoch 6/10: [==============                ] 36/75 batches, loss: 0.3700Epoch 6/10: [==============                ] 37/75 batches, loss: 0.3686Epoch 6/10: [===============               ] 38/75 batches, loss: 0.3671Epoch 6/10: [===============               ] 39/75 batches, loss: 0.3668Epoch 6/10: [================              ] 40/75 batches, loss: 0.3660Epoch 6/10: [================              ] 41/75 batches, loss: 0.3657Epoch 6/10: [================              ] 42/75 batches, loss: 0.3650Epoch 6/10: [=================             ] 43/75 batches, loss: 0.3658Epoch 6/10: [=================             ] 44/75 batches, loss: 0.3644Epoch 6/10: [==================            ] 45/75 batches, loss: 0.3639Epoch 6/10: [==================            ] 46/75 batches, loss: 0.3636Epoch 6/10: [==================            ] 47/75 batches, loss: 0.3626Epoch 6/10: [===================           ] 48/75 batches, loss: 0.3621Epoch 6/10: [===================           ] 49/75 batches, loss: 0.3634Epoch 6/10: [====================          ] 50/75 batches, loss: 0.3629Epoch 6/10: [====================          ] 51/75 batches, loss: 0.3625Epoch 6/10: [====================          ] 52/75 batches, loss: 0.3620Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.3617Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.3609Epoch 6/10: [======================        ] 55/75 batches, loss: 0.3607Epoch 6/10: [======================        ] 56/75 batches, loss: 0.3602Epoch 6/10: [======================        ] 57/75 batches, loss: 0.3607Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.3602Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.3602Epoch 6/10: [========================      ] 60/75 batches, loss: 0.3612Epoch 6/10: [========================      ] 61/75 batches, loss: 0.3608Epoch 6/10: [========================      ] 62/75 batches, loss: 0.3608Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.3605Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.3602Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.3594Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.3606Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.3615Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.3607Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.3613Epoch 6/10: [============================  ] 70/75 batches, loss: 0.3610Epoch 6/10: [============================  ] 71/75 batches, loss: 0.3608Epoch 6/10: [============================  ] 72/75 batches, loss: 0.3601Epoch 6/10: [============================= ] 73/75 batches, loss: 0.3606Epoch 6/10: [============================= ] 74/75 batches, loss: 0.3602Epoch 6/10: [==============================] 75/75 batches, loss: 0.3590
[2025-04-30 00:27:28,238][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.3590
[2025-04-30 00:27:28,691][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3776, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-30 00:27:28,692][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.3124Epoch 7/10: [                              ] 2/75 batches, loss: 0.3352Epoch 7/10: [=                             ] 3/75 batches, loss: 0.3324Epoch 7/10: [=                             ] 4/75 batches, loss: 0.3570Epoch 7/10: [==                            ] 5/75 batches, loss: 0.3399Epoch 7/10: [==                            ] 6/75 batches, loss: 0.3340Epoch 7/10: [==                            ] 7/75 batches, loss: 0.3262Epoch 7/10: [===                           ] 8/75 batches, loss: 0.3239Epoch 7/10: [===                           ] 9/75 batches, loss: 0.3190Epoch 7/10: [====                          ] 10/75 batches, loss: 0.3183Epoch 7/10: [====                          ] 11/75 batches, loss: 0.3254Epoch 7/10: [====                          ] 12/75 batches, loss: 0.3263Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.3290Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.3259Epoch 7/10: [======                        ] 15/75 batches, loss: 0.3235Epoch 7/10: [======                        ] 16/75 batches, loss: 0.3261Epoch 7/10: [======                        ] 17/75 batches, loss: 0.3218Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.3187Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.3150Epoch 7/10: [========                      ] 20/75 batches, loss: 0.3126Epoch 7/10: [========                      ] 21/75 batches, loss: 0.3108Epoch 7/10: [========                      ] 22/75 batches, loss: 0.3087Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.3100Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.3126Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.3140Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.3192Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.3181Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.3173Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.3155Epoch 7/10: [============                  ] 30/75 batches, loss: 0.3141Epoch 7/10: [============                  ] 31/75 batches, loss: 0.3137Epoch 7/10: [============                  ] 32/75 batches, loss: 0.3130Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.3114Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.3106Epoch 7/10: [==============                ] 35/75 batches, loss: 0.3095Epoch 7/10: [==============                ] 36/75 batches, loss: 0.3088Epoch 7/10: [==============                ] 37/75 batches, loss: 0.3078Epoch 7/10: [===============               ] 38/75 batches, loss: 0.3071Epoch 7/10: [===============               ] 39/75 batches, loss: 0.3140Epoch 7/10: [================              ] 40/75 batches, loss: 0.3129Epoch 7/10: [================              ] 41/75 batches, loss: 0.3123Epoch 7/10: [================              ] 42/75 batches, loss: 0.3123Epoch 7/10: [=================             ] 43/75 batches, loss: 0.3119Epoch 7/10: [=================             ] 44/75 batches, loss: 0.3130Epoch 7/10: [==================            ] 45/75 batches, loss: 0.3130Epoch 7/10: [==================            ] 46/75 batches, loss: 0.3120Epoch 7/10: [==================            ] 47/75 batches, loss: 0.3107Epoch 7/10: [===================           ] 48/75 batches, loss: 0.3096Epoch 7/10: [===================           ] 49/75 batches, loss: 0.3092Epoch 7/10: [====================          ] 50/75 batches, loss: 0.3086Epoch 7/10: [====================          ] 51/75 batches, loss: 0.3076Epoch 7/10: [====================          ] 52/75 batches, loss: 0.3067Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.3072Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.3076Epoch 7/10: [======================        ] 55/75 batches, loss: 0.3105Epoch 7/10: [======================        ] 56/75 batches, loss: 0.3099Epoch 7/10: [======================        ] 57/75 batches, loss: 0.3096Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.3083Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.3087Epoch 7/10: [========================      ] 60/75 batches, loss: 0.3096Epoch 7/10: [========================      ] 61/75 batches, loss: 0.3088Epoch 7/10: [========================      ] 62/75 batches, loss: 0.3095Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.3086Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.3083Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.3076Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.3066Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.3073Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.3076Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.3068Epoch 7/10: [============================  ] 70/75 batches, loss: 0.3067Epoch 7/10: [============================  ] 71/75 batches, loss: 0.3067Epoch 7/10: [============================  ] 72/75 batches, loss: 0.3060Epoch 7/10: [============================= ] 73/75 batches, loss: 0.3062Epoch 7/10: [============================= ] 74/75 batches, loss: 0.3060Epoch 7/10: [==============================] 75/75 batches, loss: 0.3054
[2025-04-30 00:27:43,860][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.3054
[2025-04-30 00:27:44,316][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2666, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.2842Epoch 8/10: [                              ] 2/75 batches, loss: 0.2661Epoch 8/10: [=                             ] 3/75 batches, loss: 0.2817Epoch 8/10: [=                             ] 4/75 batches, loss: 0.2854Epoch 8/10: [==                            ] 5/75 batches, loss: 0.2910Epoch 8/10: [==                            ] 6/75 batches, loss: 0.2900Epoch 8/10: [==                            ] 7/75 batches, loss: 0.2846Epoch 8/10: [===                           ] 8/75 batches, loss: 0.2891Epoch 8/10: [===                           ] 9/75 batches, loss: 0.2885Epoch 8/10: [====                          ] 10/75 batches, loss: 0.2889Epoch 8/10: [====                          ] 11/75 batches, loss: 0.2852Epoch 8/10: [====                          ] 12/75 batches, loss: 0.2827Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.2844Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.2826Epoch 8/10: [======                        ] 15/75 batches, loss: 0.2866Epoch 8/10: [======                        ] 16/75 batches, loss: 0.2859Epoch 8/10: [======                        ] 17/75 batches, loss: 0.2843Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.2831Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.2859Epoch 8/10: [========                      ] 20/75 batches, loss: 0.2849Epoch 8/10: [========                      ] 21/75 batches, loss: 0.2864Epoch 8/10: [========                      ] 22/75 batches, loss: 0.2915Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.2897Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.2879Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.2892Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.2893Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.2881Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.2921Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.2918Epoch 8/10: [============                  ] 30/75 batches, loss: 0.2924Epoch 8/10: [============                  ] 31/75 batches, loss: 0.2920Epoch 8/10: [============                  ] 32/75 batches, loss: 0.2902Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.2891Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.2885Epoch 8/10: [==============                ] 35/75 batches, loss: 0.2871Epoch 8/10: [==============                ] 36/75 batches, loss: 0.2858Epoch 8/10: [==============                ] 37/75 batches, loss: 0.2846Epoch 8/10: [===============               ] 38/75 batches, loss: 0.2832Epoch 8/10: [===============               ] 39/75 batches, loss: 0.2828Epoch 8/10: [================              ] 40/75 batches, loss: 0.2817Epoch 8/10: [================              ] 41/75 batches, loss: 0.2823Epoch 8/10: [================              ] 42/75 batches, loss: 0.2817Epoch 8/10: [=================             ] 43/75 batches, loss: 0.2843Epoch 8/10: [=================             ] 44/75 batches, loss: 0.2845Epoch 8/10: [==================            ] 45/75 batches, loss: 0.2847Epoch 8/10: [==================            ] 46/75 batches, loss: 0.2838Epoch 8/10: [==================            ] 47/75 batches, loss: 0.2836Epoch 8/10: [===================           ] 48/75 batches, loss: 0.2833Epoch 8/10: [===================           ] 49/75 batches, loss: 0.2839Epoch 8/10: [====================          ] 50/75 batches, loss: 0.2845Epoch 8/10: [====================          ] 51/75 batches, loss: 0.2838Epoch 8/10: [====================          ] 52/75 batches, loss: 0.2831Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.2835Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.2829Epoch 8/10: [======================        ] 55/75 batches, loss: 0.2819Epoch 8/10: [======================        ] 56/75 batches, loss: 0.2845Epoch 8/10: [======================        ] 57/75 batches, loss: 0.2836Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.2830Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.2835Epoch 8/10: [========================      ] 60/75 batches, loss: 0.2828Epoch 8/10: [========================      ] 61/75 batches, loss: 0.2817Epoch 8/10: [========================      ] 62/75 batches, loss: 0.2822Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.2815Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.2809Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.2805Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.2805Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.2800Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.2798Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.2794Epoch 8/10: [============================  ] 70/75 batches, loss: 0.2795Epoch 8/10: [============================  ] 71/75 batches, loss: 0.2791Epoch 8/10: [============================  ] 72/75 batches, loss: 0.2789Epoch 8/10: [============================= ] 73/75 batches, loss: 0.2787Epoch 8/10: [============================= ] 74/75 batches, loss: 0.2780Epoch 8/10: [==============================] 75/75 batches, loss: 0.2781
[2025-04-30 00:28:00,064][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.2781
[2025-04-30 00:28:00,516][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.2727, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-30 00:28:00,517][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.2333Epoch 9/10: [                              ] 2/75 batches, loss: 0.2446Epoch 9/10: [=                             ] 3/75 batches, loss: 0.2461Epoch 9/10: [=                             ] 4/75 batches, loss: 0.2551Epoch 9/10: [==                            ] 5/75 batches, loss: 0.2485Epoch 9/10: [==                            ] 6/75 batches, loss: 0.2527Epoch 9/10: [==                            ] 7/75 batches, loss: 0.2560Epoch 9/10: [===                           ] 8/75 batches, loss: 0.2542Epoch 9/10: [===                           ] 9/75 batches, loss: 0.2596Epoch 9/10: [====                          ] 10/75 batches, loss: 0.2748Epoch 9/10: [====                          ] 11/75 batches, loss: 0.2705Epoch 9/10: [====                          ] 12/75 batches, loss: 0.2723Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.2720Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.2732Epoch 9/10: [======                        ] 15/75 batches, loss: 0.2709Epoch 9/10: [======                        ] 16/75 batches, loss: 0.2707Epoch 9/10: [======                        ] 17/75 batches, loss: 0.2686Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.2661Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.2658Epoch 9/10: [========                      ] 20/75 batches, loss: 0.2647Epoch 9/10: [========                      ] 21/75 batches, loss: 0.2641Epoch 9/10: [========                      ] 22/75 batches, loss: 0.2660Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.2660Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.2653Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.2647Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.2675Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.2781Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.2783Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.2781Epoch 9/10: [============                  ] 30/75 batches, loss: 0.2770Epoch 9/10: [============                  ] 31/75 batches, loss: 0.2774Epoch 9/10: [============                  ] 32/75 batches, loss: 0.2769Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.2774Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.2762Epoch 9/10: [==============                ] 35/75 batches, loss: 0.2779Epoch 9/10: [==============                ] 36/75 batches, loss: 0.2766Epoch 9/10: [==============                ] 37/75 batches, loss: 0.2749Epoch 9/10: [===============               ] 38/75 batches, loss: 0.2763Epoch 9/10: [===============               ] 39/75 batches, loss: 0.2748Epoch 9/10: [================              ] 40/75 batches, loss: 0.2737Epoch 9/10: [================              ] 41/75 batches, loss: 0.2725Epoch 9/10: [================              ] 42/75 batches, loss: 0.2743Epoch 9/10: [=================             ] 43/75 batches, loss: 0.2741Epoch 9/10: [=================             ] 44/75 batches, loss: 0.2733Epoch 9/10: [==================            ] 45/75 batches, loss: 0.2720Epoch 9/10: [==================            ] 46/75 batches, loss: 0.2741Epoch 9/10: [==================            ] 47/75 batches, loss: 0.2729Epoch 9/10: [===================           ] 48/75 batches, loss: 0.2732Epoch 9/10: [===================           ] 49/75 batches, loss: 0.2732Epoch 9/10: [====================          ] 50/75 batches, loss: 0.2725Epoch 9/10: [====================          ] 51/75 batches, loss: 0.2724Epoch 9/10: [====================          ] 52/75 batches, loss: 0.2714Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.2716Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.2709Epoch 9/10: [======================        ] 55/75 batches, loss: 0.2704Epoch 9/10: [======================        ] 56/75 batches, loss: 0.2698Epoch 9/10: [======================        ] 57/75 batches, loss: 0.2693Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.2691Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.2687Epoch 9/10: [========================      ] 60/75 batches, loss: 0.2681Epoch 9/10: [========================      ] 61/75 batches, loss: 0.2679Epoch 9/10: [========================      ] 62/75 batches, loss: 0.2676Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.2682Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.2693Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.2689Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.2682Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.2674Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.2673Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.2679Epoch 9/10: [============================  ] 70/75 batches, loss: 0.2681Epoch 9/10: [============================  ] 71/75 batches, loss: 0.2680Epoch 9/10: [============================  ] 72/75 batches, loss: 0.2676Epoch 9/10: [============================= ] 73/75 batches, loss: 0.2671Epoch 9/10: [============================= ] 74/75 batches, loss: 0.2664Epoch 9/10: [==============================] 75/75 batches, loss: 0.2658
[2025-04-30 00:28:15,675][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.2658
[2025-04-30 00:28:16,139][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.2989, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-30 00:28:16,140][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.2112Epoch 10/10: [                              ] 2/75 batches, loss: 0.2491Epoch 10/10: [=                             ] 3/75 batches, loss: 0.2728Epoch 10/10: [=                             ] 4/75 batches, loss: 0.2623Epoch 10/10: [==                            ] 5/75 batches, loss: 0.2601Epoch 10/10: [==                            ] 6/75 batches, loss: 0.2553Epoch 10/10: [==                            ] 7/75 batches, loss: 0.2510Epoch 10/10: [===                           ] 8/75 batches, loss: 0.2468Epoch 10/10: [===                           ] 9/75 batches, loss: 0.2428Epoch 10/10: [====                          ] 10/75 batches, loss: 0.2399Epoch 10/10: [====                          ] 11/75 batches, loss: 0.2372Epoch 10/10: [====                          ] 12/75 batches, loss: 0.2359Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.2348Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.2351Epoch 10/10: [======                        ] 15/75 batches, loss: 0.2333Epoch 10/10: [======                        ] 16/75 batches, loss: 0.2335Epoch 10/10: [======                        ] 17/75 batches, loss: 0.2321Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.2317Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.2336Epoch 10/10: [========                      ] 20/75 batches, loss: 0.2362Epoch 10/10: [========                      ] 21/75 batches, loss: 0.2361Epoch 10/10: [========                      ] 22/75 batches, loss: 0.2370Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.2390Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.2396Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.2394Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.2411Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.2417Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.2423Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.2431Epoch 10/10: [============                  ] 30/75 batches, loss: 0.2421Epoch 10/10: [============                  ] 31/75 batches, loss: 0.2436Epoch 10/10: [============                  ] 32/75 batches, loss: 0.2432Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.2466Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.2453Epoch 10/10: [==============                ] 35/75 batches, loss: 0.2445Epoch 10/10: [==============                ] 36/75 batches, loss: 0.2458Epoch 10/10: [==============                ] 37/75 batches, loss: 0.2490Epoch 10/10: [===============               ] 38/75 batches, loss: 0.2489Epoch 10/10: [===============               ] 39/75 batches, loss: 0.2478Epoch 10/10: [================              ] 40/75 batches, loss: 0.2478Epoch 10/10: [================              ] 41/75 batches, loss: 0.2477Epoch 10/10: [================              ] 42/75 batches, loss: 0.2477Epoch 10/10: [=================             ] 43/75 batches, loss: 0.2476Epoch 10/10: [=================             ] 44/75 batches, loss: 0.2469Epoch 10/10: [==================            ] 45/75 batches, loss: 0.2469Epoch 10/10: [==================            ] 46/75 batches, loss: 0.2464Epoch 10/10: [==================            ] 47/75 batches, loss: 0.2484Epoch 10/10: [===================           ] 48/75 batches, loss: 0.2487Epoch 10/10: [===================           ] 49/75 batches, loss: 0.2482Epoch 10/10: [====================          ] 50/75 batches, loss: 0.2475Epoch 10/10: [====================          ] 51/75 batches, loss: 0.2474Epoch 10/10: [====================          ] 52/75 batches, loss: 0.2471Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.2469Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.2473Epoch 10/10: [======================        ] 55/75 batches, loss: 0.2466Epoch 10/10: [======================        ] 56/75 batches, loss: 0.2479Epoch 10/10: [======================        ] 57/75 batches, loss: 0.2472Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.2479Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.2478Epoch 10/10: [========================      ] 60/75 batches, loss: 0.2474Epoch 10/10: [========================      ] 61/75 batches, loss: 0.2470Epoch 10/10: [========================      ] 62/75 batches, loss: 0.2484Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.2490Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.2484Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.2479Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.2478Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.2491Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.2491Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.2488Epoch 10/10: [============================  ] 70/75 batches, loss: 0.2487Epoch 10/10: [============================  ] 71/75 batches, loss: 0.2482Epoch 10/10: [============================  ] 72/75 batches, loss: 0.2477Epoch 10/10: [============================= ] 73/75 batches, loss: 0.2471Epoch 10/10: [============================= ] 74/75 batches, loss: 0.2474Epoch 10/10: [==============================] 75/75 batches, loss: 0.2468
[2025-04-30 00:28:31,324][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.2468
[2025-04-30 00:28:31,744][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.2687, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-30 00:28:31,745][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-30 00:28:31,745][src.training.lm_trainer][INFO] - Early stopping at epoch 10
[2025-04-30 00:28:31,745][src.training.lm_trainer][INFO] - Training completed in 160.14 seconds
[2025-04-30 00:28:31,745][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 00:28:37,357][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9958053691275168, 'f1': 0.9958228905597326}
[2025-04-30 00:28:37,358][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-30 00:28:37,358][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9818181818181818, 'f1': 0.9818181818181818}
[2025-04-30 00:28:39,697][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁███
wandb:          best_val_f1 ▁▁▁███
wandb:        best_val_loss ██▇▅▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▇▅▄▃▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁███████
wandb:               val_f1 ▁▁▁███████
wandb:             val_loss ██▇▅▂▃▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.26659
wandb:                epoch 10
wandb:  final_test_accuracy 0.98182
wandb:        final_test_f1 0.98182
wandb: final_train_accuracy 0.99581
wandb:       final_train_f1 0.99582
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 2e-05
wandb:           train_loss 0.24676
wandb:           train_time 160.1383
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.26873
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002540-gkohny26
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002540-gkohny26/logs
Experiment finetune_question_type_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/en"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 00:28:51,719][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/en
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-30 00:28:51,720][__main__][INFO] - Normalized task: complexity
[2025-04-30 00:28:51,720][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-30 00:28:51,720][__main__][INFO] - Determined Task Type: regression
[2025-04-30 00:28:51,726][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-30 00:28:51,726][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 00:28:53,501][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 00:28:56,392][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 00:28:56,392][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:28:56,450][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,481][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,562][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-30 00:28:56,573][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:28:56,574][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-30 00:28:56,575][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:28:56,590][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,622][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,633][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-30 00:28:56,635][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:28:56,635][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-30 00:28:56,636][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:28:56,652][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,674][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:28:56,696][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-30 00:28:56,698][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:28:56,698][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-30 00:28:56,699][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-30 00:28:56,699][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:28:56,699][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:28:56,699][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:28:56,700][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:28:56,700][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:28:56,700][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-30 00:28:56,700][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-30 00:28:56,700][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:28:56,701][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:28:56,701][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-30 00:28:56,701][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-30 00:28:56,702][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-30 00:28:56,702][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-30 00:28:56,702][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-30 00:28:56,702][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-30 00:28:56,702][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-30 00:28:56,702][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-30 00:28:56,702][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-30 00:28:56,703][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-30 00:28:56,703][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-30 00:28:56,703][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 00:28:56,703][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 00:28:56,703][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 00:29:01,428][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 00:29:01,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,428][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,429][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,430][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,431][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,432][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,433][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,434][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,435][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,436][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,437][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,438][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,439][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,440][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,441][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,442][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,443][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,444][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,445][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:29:01,656][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-30 00:29:01,658][src.models.model_factory][INFO] - Model has 394,288,321 trainable parameters out of 394,288,321 total parameters
[2025-04-30 00:29:01,658][__main__][INFO] - Successfully created model for en
[2025-04-30 00:29:01,658][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.1628Epoch 1/10: [                              ] 2/75 batches, loss: 0.2671Epoch 1/10: [=                             ] 3/75 batches, loss: 0.4808Epoch 1/10: [=                             ] 4/75 batches, loss: 0.4120Epoch 1/10: [==                            ] 5/75 batches, loss: 0.3600Epoch 1/10: [==                            ] 6/75 batches, loss: 0.4017Epoch 1/10: [==                            ] 7/75 batches, loss: 0.3711Epoch 1/10: [===                           ] 8/75 batches, loss: 0.4070Epoch 1/10: [===                           ] 9/75 batches, loss: 0.3892Epoch 1/10: [====                          ] 10/75 batches, loss: 0.3824Epoch 1/10: [====                          ] 11/75 batches, loss: 0.3667Epoch 1/10: [====                          ] 12/75 batches, loss: 0.3745Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.3678Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.3691Epoch 1/10: [======                        ] 15/75 batches, loss: 0.3566Epoch 1/10: [======                        ] 16/75 batches, loss: 0.3434Epoch 1/10: [======                        ] 17/75 batches, loss: 0.3396Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.3318Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.3193Epoch 1/10: [========                      ] 20/75 batches, loss: 0.3128Epoch 1/10: [========                      ] 21/75 batches, loss: 0.3068Epoch 1/10: [========                      ] 22/75 batches, loss: 0.2980Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.2912Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.2938Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.2863Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.2804Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.2761Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.2686Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.2622Epoch 1/10: [============                  ] 30/75 batches, loss: 0.2589Epoch 1/10: [============                  ] 31/75 batches, loss: 0.2537Epoch 1/10: [============                  ] 32/75 batches, loss: 0.2526Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.2523Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.2507Epoch 1/10: [==============                ] 35/75 batches, loss: 0.2477Epoch 1/10: [==============                ] 36/75 batches, loss: 0.2526Epoch 1/10: [==============                ] 37/75 batches, loss: 0.2499Epoch 1/10: [===============               ] 38/75 batches, loss: 0.2475Epoch 1/10: [===============               ] 39/75 batches, loss: 0.2494Epoch 1/10: [================              ] 40/75 batches, loss: 0.2482Epoch 1/10: [================              ] 41/75 batches, loss: 0.2434Epoch 1/10: [================              ] 42/75 batches, loss: 0.2412Epoch 1/10: [=================             ] 43/75 batches, loss: 0.2372Epoch 1/10: [=================             ] 44/75 batches, loss: 0.2336Epoch 1/10: [==================            ] 45/75 batches, loss: 0.2318Epoch 1/10: [==================            ] 46/75 batches, loss: 0.2284Epoch 1/10: [==================            ] 47/75 batches, loss: 0.2280Epoch 1/10: [===================           ] 48/75 batches, loss: 0.2256Epoch 1/10: [===================           ] 49/75 batches, loss: 0.2231Epoch 1/10: [====================          ] 50/75 batches, loss: 0.2208Epoch 1/10: [====================          ] 51/75 batches, loss: 0.2187Epoch 1/10: [====================          ] 52/75 batches, loss: 0.2161Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.2137Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.2126Epoch 1/10: [======================        ] 55/75 batches, loss: 0.2099Epoch 1/10: [======================        ] 56/75 batches, loss: 0.2074Epoch 1/10: [======================        ] 57/75 batches, loss: 0.2054Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.2029Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.2004Epoch 1/10: [========================      ] 60/75 batches, loss: 0.1980Epoch 1/10: [========================      ] 61/75 batches, loss: 0.1978Epoch 1/10: [========================      ] 62/75 batches, loss: 0.1973Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.1966Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.1944Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.1927Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.1914Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.1909Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.1893Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.1890Epoch 1/10: [============================  ] 70/75 batches, loss: 0.1874Epoch 1/10: [============================  ] 71/75 batches, loss: 0.1852Epoch 1/10: [============================  ] 72/75 batches, loss: 0.1835Epoch 1/10: [============================= ] 73/75 batches, loss: 0.1826Epoch 1/10: [============================= ] 74/75 batches, loss: 0.1813Epoch 1/10: [==============================] 75/75 batches, loss: 0.1807
[2025-04-30 00:29:20,730][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1807
[2025-04-30 00:29:21,128][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0443, Metrics: {'mse': 0.04608778655529022, 'rmse': 0.21468066181025766, 'r2': -0.10122954845428467}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0701Epoch 2/10: [                              ] 2/75 batches, loss: 0.0685Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0547Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0834Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0871Epoch 2/10: [==                            ] 6/75 batches, loss: 0.1473Epoch 2/10: [==                            ] 7/75 batches, loss: 0.1321Epoch 2/10: [===                           ] 8/75 batches, loss: 0.1265Epoch 2/10: [===                           ] 9/75 batches, loss: 0.1211Epoch 2/10: [====                          ] 10/75 batches, loss: 0.1169Epoch 2/10: [====                          ] 11/75 batches, loss: 0.1137Epoch 2/10: [====                          ] 12/75 batches, loss: 0.1110Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.1049Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.1034Epoch 2/10: [======                        ] 15/75 batches, loss: 0.1039Epoch 2/10: [======                        ] 16/75 batches, loss: 0.1004Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0985Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.1014Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.1037Epoch 2/10: [========                      ] 20/75 batches, loss: 0.1040Epoch 2/10: [========                      ] 21/75 batches, loss: 0.1066Epoch 2/10: [========                      ] 22/75 batches, loss: 0.1095Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.1059Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.1043Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.1077Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.1074Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.1088Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.1066Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.1060Epoch 2/10: [============                  ] 30/75 batches, loss: 0.1104Epoch 2/10: [============                  ] 31/75 batches, loss: 0.1082Epoch 2/10: [============                  ] 32/75 batches, loss: 0.1095Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.1137Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.1196Epoch 2/10: [==============                ] 35/75 batches, loss: 0.1189Epoch 2/10: [==============                ] 36/75 batches, loss: 0.1189Epoch 2/10: [==============                ] 37/75 batches, loss: 0.1197Epoch 2/10: [===============               ] 38/75 batches, loss: 0.1185Epoch 2/10: [===============               ] 39/75 batches, loss: 0.1208Epoch 2/10: [================              ] 40/75 batches, loss: 0.1228Epoch 2/10: [================              ] 41/75 batches, loss: 0.1229Epoch 2/10: [================              ] 42/75 batches, loss: 0.1254Epoch 2/10: [=================             ] 43/75 batches, loss: 0.1271Epoch 2/10: [=================             ] 44/75 batches, loss: 0.1257Epoch 2/10: [==================            ] 45/75 batches, loss: 0.1252Epoch 2/10: [==================            ] 46/75 batches, loss: 0.1244Epoch 2/10: [==================            ] 47/75 batches, loss: 0.1291Epoch 2/10: [===================           ] 48/75 batches, loss: 0.1299Epoch 2/10: [===================           ] 49/75 batches, loss: 0.1297Epoch 2/10: [====================          ] 50/75 batches, loss: 0.1277Epoch 2/10: [====================          ] 51/75 batches, loss: 0.1269Epoch 2/10: [====================          ] 52/75 batches, loss: 0.1277Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.1334Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.1347Epoch 2/10: [======================        ] 55/75 batches, loss: 0.1330Epoch 2/10: [======================        ] 56/75 batches, loss: 0.1353Epoch 2/10: [======================        ] 57/75 batches, loss: 0.1337Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.1334Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.1327Epoch 2/10: [========================      ] 60/75 batches, loss: 0.1352Epoch 2/10: [========================      ] 61/75 batches, loss: 0.1344Epoch 2/10: [========================      ] 62/75 batches, loss: 0.1338Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.1327Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.1317Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.1318Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.1310Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.1322Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.1320Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.1319Epoch 2/10: [============================  ] 70/75 batches, loss: 0.1313Epoch 2/10: [============================  ] 71/75 batches, loss: 0.1314Epoch 2/10: [============================  ] 72/75 batches, loss: 0.1307Epoch 2/10: [============================= ] 73/75 batches, loss: 0.1305Epoch 2/10: [============================= ] 74/75 batches, loss: 0.1294Epoch 2/10: [==============================] 75/75 batches, loss: 0.1282
[2025-04-30 00:29:36,857][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1282
[2025-04-30 00:29:37,269][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0445, Metrics: {'mse': 0.04572407528758049, 'rmse': 0.21383188557270988, 'r2': -0.09253895282745361}
[2025-04-30 00:29:37,270][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0698Epoch 3/10: [                              ] 2/75 batches, loss: 0.1244Epoch 3/10: [=                             ] 3/75 batches, loss: 0.1131Epoch 3/10: [=                             ] 4/75 batches, loss: 0.1156Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0997Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0897Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0858Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0922Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0873Epoch 3/10: [====                          ] 10/75 batches, loss: 0.1086Epoch 3/10: [====                          ] 11/75 batches, loss: 0.1016Epoch 3/10: [====                          ] 12/75 batches, loss: 0.1027Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.1040Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.1018Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0997Epoch 3/10: [======                        ] 16/75 batches, loss: 0.1023Epoch 3/10: [======                        ] 17/75 batches, loss: 0.1007Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.1005Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.1017Epoch 3/10: [========                      ] 20/75 batches, loss: 0.1002Epoch 3/10: [========                      ] 21/75 batches, loss: 0.1002Epoch 3/10: [========                      ] 22/75 batches, loss: 0.1038Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.1040Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.1022Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0992Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0985Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0979Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0963Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0959Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0946Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0935Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0924Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0926Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0919Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0938Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0931Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0953Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0951Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0948Epoch 3/10: [================              ] 40/75 batches, loss: 0.0942Epoch 3/10: [================              ] 41/75 batches, loss: 0.0935Epoch 3/10: [================              ] 42/75 batches, loss: 0.0930Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0944Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0942Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0926Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0922Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0918Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0904Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0921Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0917Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0924Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0928Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0921Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0918Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0905Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0897Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0889Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0887Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0883Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0874Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0871Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0877Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0874Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0869Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0881Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0881Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0898Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0892Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0903Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0911Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0911Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0908Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0911Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0914Epoch 3/10: [==============================] 75/75 batches, loss: 0.0924
[2025-04-30 00:29:52,407][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0924
[2025-04-30 00:29:52,826][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0409, Metrics: {'mse': 0.042368821799755096, 'rmse': 0.20583688153427485, 'r2': -0.012367963790893555}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.1845Epoch 4/10: [                              ] 2/75 batches, loss: 0.1340Epoch 4/10: [=                             ] 3/75 batches, loss: 0.1193Epoch 4/10: [=                             ] 4/75 batches, loss: 0.1150Epoch 4/10: [==                            ] 5/75 batches, loss: 0.1022Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0985Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0924Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0843Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0818Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0791Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0846Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0796Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0758Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0736Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0743Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0748Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0752Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0746Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0759Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0765Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0774Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0758Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0769Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0809Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0788Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0809Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0818Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0801Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0810Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0800Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0800Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0808Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0830Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0821Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0811Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0801Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0850Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0855Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0854Epoch 4/10: [================              ] 40/75 batches, loss: 0.0856Epoch 4/10: [================              ] 41/75 batches, loss: 0.0862Epoch 4/10: [================              ] 42/75 batches, loss: 0.0861Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0859Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0849Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0855Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0856Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0854Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0853Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0843Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0829Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0826Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0821Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0817Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0820Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0819Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0823Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0837Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0831Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0826Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0823Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0821Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0818Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0817Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0811Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0810Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0834Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0829Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0843Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0844Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0860Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0853Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0860Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0866Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0870Epoch 4/10: [==============================] 75/75 batches, loss: 0.0875
[2025-04-30 00:30:08,603][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0875
[2025-04-30 00:30:09,035][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0416, Metrics: {'mse': 0.04344554618000984, 'rmse': 0.2084359522251616, 'r2': -0.03809535503387451}
[2025-04-30 00:30:09,036][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0379Epoch 5/10: [                              ] 2/75 batches, loss: 0.0357Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0498Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0788Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0686Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0671Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0710Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0716Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0801Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0788Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0763Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0802Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0794Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0796Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0783Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0761Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0885Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0862Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0869Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0846Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0845Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0871Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0861Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0856Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0843Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0850Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0853Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0889Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0913Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0934Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0917Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0923Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0964Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0988Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0997Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0988Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0979Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0966Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0956Epoch 5/10: [================              ] 40/75 batches, loss: 0.0985Epoch 5/10: [================              ] 41/75 batches, loss: 0.1003Epoch 5/10: [================              ] 42/75 batches, loss: 0.1001Epoch 5/10: [=================             ] 43/75 batches, loss: 0.1010Epoch 5/10: [=================             ] 44/75 batches, loss: 0.1009Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0990Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0979Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0977Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0977Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0998Epoch 5/10: [====================          ] 50/75 batches, loss: 0.1010Epoch 5/10: [====================          ] 51/75 batches, loss: 0.1007Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0997Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0997Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0986Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0983Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0994Epoch 5/10: [======================        ] 57/75 batches, loss: 0.1006Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.1034Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.1024Epoch 5/10: [========================      ] 60/75 batches, loss: 0.1024Epoch 5/10: [========================      ] 61/75 batches, loss: 0.1034Epoch 5/10: [========================      ] 62/75 batches, loss: 0.1031Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.1024Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.1014Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.1016Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.1015Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.1020Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.1035Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.1029Epoch 5/10: [============================  ] 70/75 batches, loss: 0.1038Epoch 5/10: [============================  ] 71/75 batches, loss: 0.1050Epoch 5/10: [============================  ] 72/75 batches, loss: 0.1053Epoch 5/10: [============================= ] 73/75 batches, loss: 0.1051Epoch 5/10: [============================= ] 74/75 batches, loss: 0.1050Epoch 5/10: [==============================] 75/75 batches, loss: 0.1057
[2025-04-30 00:30:24,181][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1057
[2025-04-30 00:30:24,599][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0968, Metrics: {'mse': 0.09547504037618637, 'rmse': 0.3089903564452884, 'r2': -1.281296968460083}
[2025-04-30 00:30:24,600][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.1431Epoch 6/10: [                              ] 2/75 batches, loss: 0.1658Epoch 6/10: [=                             ] 3/75 batches, loss: 0.1380Epoch 6/10: [=                             ] 4/75 batches, loss: 0.1468Epoch 6/10: [==                            ] 5/75 batches, loss: 0.1266Epoch 6/10: [==                            ] 6/75 batches, loss: 0.1137Epoch 6/10: [==                            ] 7/75 batches, loss: 0.1079Epoch 6/10: [===                           ] 8/75 batches, loss: 0.1028Epoch 6/10: [===                           ] 9/75 batches, loss: 0.1041Epoch 6/10: [====                          ] 10/75 batches, loss: 0.1022Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0986Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0987Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0972Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0958Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0984Epoch 6/10: [======                        ] 16/75 batches, loss: 0.1008Epoch 6/10: [======                        ] 17/75 batches, loss: 0.1005Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.1006Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0996Epoch 6/10: [========                      ] 20/75 batches, loss: 0.1034Epoch 6/10: [========                      ] 21/75 batches, loss: 0.1010Epoch 6/10: [========                      ] 22/75 batches, loss: 0.1005Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0990Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0982Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0991Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0991Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0985Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0980Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0968Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0970Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0959Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0946Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0960Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0985Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0976Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0987Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0985Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0976Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0973Epoch 6/10: [================              ] 40/75 batches, loss: 0.0974Epoch 6/10: [================              ] 41/75 batches, loss: 0.0963Epoch 6/10: [================              ] 42/75 batches, loss: 0.0964Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0955Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0957Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0961Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0967Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0959Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0957Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0945Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0941Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0943Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0933Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0922Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0911Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0901Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0906Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0912Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0904Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0897Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0900Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0908Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0907Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0907Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0901Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0895Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0896Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0890Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0885Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0892Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0888Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0886Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0883Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0882Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0882Epoch 6/10: [==============================] 75/75 batches, loss: 0.0893
[2025-04-30 00:30:39,737][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0893
[2025-04-30 00:30:40,177][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0384, Metrics: {'mse': 0.04049577936530113, 'rmse': 0.20123563145054887, 'r2': 0.032386839389801025}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.1723Epoch 7/10: [                              ] 2/75 batches, loss: 0.0985Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0915Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0730Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0809Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0875Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0895Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0851Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0870Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0817Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0779Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0790Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0806Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0807Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0773Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0753Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0724Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0705Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0702Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0721Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0694Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0706Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0703Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0682Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0686Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0690Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0693Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0695Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0698Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0717Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0745Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0735Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0748Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0777Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0768Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0770Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0758Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0750Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0759Epoch 7/10: [================              ] 40/75 batches, loss: 0.0746Epoch 7/10: [================              ] 41/75 batches, loss: 0.0747Epoch 7/10: [================              ] 42/75 batches, loss: 0.0756Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0744Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0748Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0753Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0768Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0768Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0785Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0780Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0774Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0782Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0780Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0791Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0795Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0794Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0793Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0792Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0788Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0787Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0780Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0778Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0781Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0775Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0777Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0772Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0776Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0783Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0786Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0788Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0783Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0790Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0782Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0798Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0804Epoch 7/10: [==============================] 75/75 batches, loss: 0.0796
[2025-04-30 00:30:55,960][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0796
[2025-04-30 00:30:56,391][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0434, Metrics: {'mse': 0.043178483843803406, 'rmse': 0.20779433063441216, 'r2': -0.03171420097351074}
[2025-04-30 00:30:56,392][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0321Epoch 8/10: [                              ] 2/75 batches, loss: 0.0518Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0621Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0601Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0731Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0700Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0703Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0680Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0697Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0688Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0659Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0739Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0724Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0743Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0758Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0732Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0718Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0723Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0755Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0767Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0741Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0717Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0696Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0730Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0754Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0750Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0751Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0800Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0806Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0833Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0841Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0867Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0852Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0854Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0847Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0848Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0841Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0869Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0869Epoch 8/10: [================              ] 40/75 batches, loss: 0.0883Epoch 8/10: [================              ] 41/75 batches, loss: 0.0867Epoch 8/10: [================              ] 42/75 batches, loss: 0.0869Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0875Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0887Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0880Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0877Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0880Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0900Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0903Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0905Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0914Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0904Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0899Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0896Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0892Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0905Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0893Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0891Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0884Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0898Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0903Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0899Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0892Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0898Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0913Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0908Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0906Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0902Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0898Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0890Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0884Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0880Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0878Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0875Epoch 8/10: [==============================] 75/75 batches, loss: 0.0876
[2025-04-30 00:31:11,585][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0876
[2025-04-30 00:31:12,019][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0382, Metrics: {'mse': 0.03958554565906525, 'rmse': 0.19896116620854745, 'r2': 0.05413621664047241}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0286Epoch 9/10: [                              ] 2/75 batches, loss: 0.0618Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0540Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0551Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0671Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0619Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0583Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0615Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0673Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0657Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0642Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0679Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0711Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0678Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0680Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0653Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0664Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0645Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0668Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0668Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0697Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0678Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0683Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0674Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0674Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0670Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0671Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0676Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0670Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0672Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0671Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0672Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0676Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0672Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0657Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0654Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0687Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0686Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0695Epoch 9/10: [================              ] 40/75 batches, loss: 0.0682Epoch 9/10: [================              ] 41/75 batches, loss: 0.0681Epoch 9/10: [================              ] 42/75 batches, loss: 0.0688Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0694Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0705Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0711Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0719Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0721Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0754Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0750Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0754Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0773Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0779Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0781Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0775Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0769Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0778Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0775Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0769Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0783Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0773Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0780Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0775Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0783Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0781Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0778Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0777Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0777Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0772Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0775Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0771Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0770Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0769Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0774Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0769Epoch 9/10: [==============================] 75/75 batches, loss: 0.0775
[2025-04-30 00:31:27,748][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0775
[2025-04-30 00:31:28,181][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0390, Metrics: {'mse': 0.04005398973822594, 'rmse': 0.20013492883109119, 'r2': 0.04294306039810181}
[2025-04-30 00:31:28,182][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0745Epoch 10/10: [                              ] 2/75 batches, loss: 0.0548Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0784Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0981Epoch 10/10: [==                            ] 5/75 batches, loss: 0.1033Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0969Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0895Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0876Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0827Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0880Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0877Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0917Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0895Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0917Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0952Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0941Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0910Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0934Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0983Epoch 10/10: [========                      ] 20/75 batches, loss: 0.1027Epoch 10/10: [========                      ] 21/75 batches, loss: 0.1011Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0979Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0968Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.1017Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0996Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0976Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0983Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0979Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0967Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0952Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0928Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0906Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0891Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0877Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0872Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0859Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0855Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0844Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0860Epoch 10/10: [================              ] 40/75 batches, loss: 0.0852Epoch 10/10: [================              ] 41/75 batches, loss: 0.0876Epoch 10/10: [================              ] 42/75 batches, loss: 0.0879Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0880Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0877Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0864Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0860Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0851Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0861Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0854Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0843Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0840Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0833Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0824Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0840Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0844Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0839Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0844Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0836Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0831Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0828Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0822Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0818Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0826Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0829Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0818Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0809Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0807Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0803Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0802Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0803Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0821Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0820Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0815Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0811Epoch 10/10: [==============================] 75/75 batches, loss: 0.0813
[2025-04-30 00:31:43,360][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0813
[2025-04-30 00:31:43,795][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0391, Metrics: {'mse': 0.039922211319208145, 'rmse': 0.19980543365786663, 'r2': 0.04609179496765137}
[2025-04-30 00:31:43,796][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
[2025-04-30 00:31:43,796][src.training.lm_trainer][INFO] - Training completed in 158.82 seconds
[2025-04-30 00:31:43,796][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-30 00:31:49,386][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03167419135570526, 'rmse': 0.17797244549565885, 'r2': -0.18060994148254395}
[2025-04-30 00:31:49,387][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03958554565906525, 'rmse': 0.19896116620854745, 'r2': 0.05413621664047241}
[2025-04-30 00:31:49,387][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.044727545231580734, 'rmse': 0.21148887732356217, 'r2': -0.1605905294418335}
[2025-04-30 00:31:51,781][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁▁
wandb:     best_val_mse █▄▂▁
wandb:      best_val_r2 ▁▅▇█
wandb:    best_val_rmse █▄▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▃▂▁▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▂▂▁▁█▁▂▁▁▁
wandb:          val_mse ▂▂▁▁█▁▁▁▁▁
wandb:           val_r2 ▇▇██▁█████
wandb:         val_rmse ▂▂▁▂█▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03816
wandb:     best_val_mse 0.03959
wandb:      best_val_r2 0.05414
wandb:    best_val_rmse 0.19896
wandb:            epoch 10
wandb:   final_test_mse 0.04473
wandb:    final_test_r2 -0.16059
wandb:  final_test_rmse 0.21149
wandb:  final_train_mse 0.03167
wandb:   final_train_r2 -0.18061
wandb: final_train_rmse 0.17797
wandb:    final_val_mse 0.03959
wandb:     final_val_r2 0.05414
wandb:   final_val_rmse 0.19896
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0813
wandb:       train_time 158.82381
wandb:         val_loss 0.03907
wandb:          val_mse 0.03992
wandb:           val_r2 0.04609
wandb:         val_rmse 0.19981
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002851-tqpjf4wq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250430_002851-tqpjf4wq/logs
Experiment finetune_complexity_en completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/fi"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 00:32:08,138][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/fi
experiment_name: finetune_question_type_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 00:32:08,138][__main__][INFO] - Normalized task: question_type
[2025-04-30 00:32:08,138][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 00:32:08,138][__main__][INFO] - Determined Task Type: classification
[2025-04-30 00:32:08,143][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-04-30 00:32:08,144][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 00:32:09,923][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 00:32:12,905][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 00:32:12,906][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:32:13,003][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,040][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,179][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-30 00:32:13,192][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:32:13,193][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-30 00:32:13,194][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:32:13,233][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,268][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,290][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-30 00:32:13,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:32:13,292][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-30 00:32:13,293][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 00:32:13,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,390][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 00:32:13,400][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-30 00:32:13,402][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 00:32:13,402][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-30 00:32:13,403][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-30 00:32:13,404][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:32:13,404][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:32:13,404][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:32:13,404][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:32:13,405][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-30 00:32:13,405][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:32:13,405][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:32:13,406][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-30 00:32:13,406][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-30 00:32:13,406][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-30 00:32:13,406][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 00:32:13,406][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 00:32:13,406][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 00:32:13,406][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 00:32:13,407][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 00:32:13,407][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-30 00:32:13,407][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-30 00:32:13,407][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-30 00:32:13,407][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 00:32:13,407][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-30 00:32:13,407][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 00:32:13,408][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 00:32:13,408][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 00:32:18,341][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,342][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,343][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,344][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,345][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,346][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,347][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,348][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,349][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,350][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,351][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,352][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,353][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,354][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,355][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,356][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,357][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,358][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,358][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,358][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,358][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-30 00:32:18,525][src.models.model_factory][INFO] - Model configuration: layer-wise=False, layer_index=-1, freeze_model=False, finetune=True
[2025-04-30 00:32:18,526][src.models.model_factory][INFO] - Model has 394,200,289 trainable parameters out of 394,200,289 total parameters
[2025-04-30 00:32:18,527][__main__][INFO] - Successfully created model for fi
[2025-04-30 00:32:18,527][__main__][INFO] - finetuning with gradient accum steps: 2
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.7905Epoch 1/10: [                              ] 2/75 batches, loss: 0.7929Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7530Epoch 1/10: [=                             ] 4/75 batches, loss: 0.7323Epoch 1/10: [==                            ] 5/75 batches, loss: 0.7228Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7219Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7173Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7236Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7273Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7239Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7198Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7192Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7236Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7167Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7193Epoch 1/10: [======                        ] 16/75 batches, loss: 0.7180Epoch 1/10: [======                        ] 17/75 batches, loss: 0.7130Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.7116Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.7069Epoch 1/10: [========                      ] 20/75 batches, loss: 0.7083Epoch 1/10: [========                      ] 21/75 batches, loss: 0.7077Epoch 1/10: [========                      ] 22/75 batches, loss: 0.7091Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.7078Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.7081Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.7059Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.7111Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.7139Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.7154Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.7135Epoch 1/10: [============                  ] 30/75 batches, loss: 0.7113Epoch 1/10: [============                  ] 31/75 batches, loss: 0.7090Epoch 1/10: [============                  ] 32/75 batches, loss: 0.7056Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.7080Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.7107Epoch 1/10: [==============                ] 35/75 batches, loss: 0.7109Epoch 1/10: [==============                ] 36/75 batches, loss: 0.7118Epoch 1/10: [==============                ] 37/75 batches, loss: 0.7102Epoch 1/10: [===============               ] 38/75 batches, loss: 0.7112Epoch 1/10: [===============               ] 39/75 batches, loss: 0.7115Epoch 1/10: [================              ] 40/75 batches, loss: 0.7105Epoch 1/10: [================              ] 41/75 batches, loss: 0.7111Epoch 1/10: [================              ] 42/75 batches, loss: 0.7102Epoch 1/10: [=================             ] 43/75 batches, loss: 0.7091Epoch 1/10: [=================             ] 44/75 batches, loss: 0.7103Epoch 1/10: [==================            ] 45/75 batches, loss: 0.7106Epoch 1/10: [==================            ] 46/75 batches, loss: 0.7095Epoch 1/10: [==================            ] 47/75 batches, loss: 0.7103Epoch 1/10: [===================           ] 48/75 batches, loss: 0.7098Epoch 1/10: [===================           ] 49/75 batches, loss: 0.7103Epoch 1/10: [====================          ] 50/75 batches, loss: 0.7105Epoch 1/10: [====================          ] 51/75 batches, loss: 0.7095Epoch 1/10: [====================          ] 52/75 batches, loss: 0.7129Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.7133Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.7124Epoch 1/10: [======================        ] 55/75 batches, loss: 0.7114Epoch 1/10: [======================        ] 56/75 batches, loss: 0.7126Epoch 1/10: [======================        ] 57/75 batches, loss: 0.7117Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.7118Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.7116Epoch 1/10: [========================      ] 60/75 batches, loss: 0.7111Epoch 1/10: [========================      ] 61/75 batches, loss: 0.7104Epoch 1/10: [========================      ] 62/75 batches, loss: 0.7112Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.7115Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.7111Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.7110Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.7099Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.7108Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.7098Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.7080Epoch 1/10: [============================  ] 70/75 batches, loss: 0.7075Epoch 1/10: [============================  ] 71/75 batches, loss: 0.7079Epoch 1/10: [============================  ] 72/75 batches, loss: 0.7075Epoch 1/10: [============================= ] 73/75 batches, loss: 0.7075Epoch 1/10: [============================= ] 74/75 batches, loss: 0.7078Epoch 1/10: [==============================] 75/75 batches, loss: 0.7066
[2025-04-30 00:32:37,389][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7066
[2025-04-30 00:32:37,764][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6840, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.6535Epoch 2/10: [                              ] 2/75 batches, loss: 0.6563Epoch 2/10: [=                             ] 3/75 batches, loss: 0.6470Epoch 2/10: [=                             ] 4/75 batches, loss: 0.6459Epoch 2/10: [==                            ] 5/75 batches, loss: 0.6523Epoch 2/10: [==                            ] 6/75 batches, loss: 0.6637Epoch 2/10: [==                            ] 7/75 batches, loss: 0.6734Epoch 2/10: [===                           ] 8/75 batches, loss: 0.6689Epoch 2/10: [===                           ] 9/75 batches, loss: 0.6703Epoch 2/10: [====                          ] 10/75 batches, loss: 0.6769Epoch 2/10: [====                          ] 11/75 batches, loss: 0.6809Epoch 2/10: [====                          ] 12/75 batches, loss: 0.6764Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.6795Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.6761Epoch 2/10: [======                        ] 15/75 batches, loss: 0.6731Epoch 2/10: [======                        ] 16/75 batches, loss: 0.6750Epoch 2/10: [======                        ] 17/75 batches, loss: 0.6700Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.6725Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.6766Epoch 2/10: [========                      ] 20/75 batches, loss: 0.6746Epoch 2/10: [========                      ] 21/75 batches, loss: 0.6756Epoch 2/10: [========                      ] 22/75 batches, loss: 0.6744Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.6734Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.6733Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.6779Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.6763Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.6770Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.6778Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.6781Epoch 2/10: [============                  ] 30/75 batches, loss: 0.6788Epoch 2/10: [============                  ] 31/75 batches, loss: 0.6797Epoch 2/10: [============                  ] 32/75 batches, loss: 0.6794Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.6792Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.6799Epoch 2/10: [==============                ] 35/75 batches, loss: 0.6788Epoch 2/10: [==============                ] 36/75 batches, loss: 0.6795Epoch 2/10: [==============                ] 37/75 batches, loss: 0.6793Epoch 2/10: [===============               ] 38/75 batches, loss: 0.6780Epoch 2/10: [===============               ] 39/75 batches, loss: 0.6743Epoch 2/10: [================              ] 40/75 batches, loss: 0.6749Epoch 2/10: [================              ] 41/75 batches, loss: 0.6721Epoch 2/10: [================              ] 42/75 batches, loss: 0.6719Epoch 2/10: [=================             ] 43/75 batches, loss: 0.6717Epoch 2/10: [=================             ] 44/75 batches, loss: 0.6731Epoch 2/10: [==================            ] 45/75 batches, loss: 0.6717Epoch 2/10: [==================            ] 46/75 batches, loss: 0.6704Epoch 2/10: [==================            ] 47/75 batches, loss: 0.6717Epoch 2/10: [===================           ] 48/75 batches, loss: 0.6720Epoch 2/10: [===================           ] 49/75 batches, loss: 0.6730Epoch 2/10: [====================          ] 50/75 batches, loss: 0.6723Epoch 2/10: [====================          ] 51/75 batches, loss: 0.6724Epoch 2/10: [====================          ] 52/75 batches, loss: 0.6706Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.6704Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.6702Epoch 2/10: [======================        ] 55/75 batches, loss: 0.6700Epoch 2/10: [======================        ] 56/75 batches, loss: 0.6712Epoch 2/10: [======================        ] 57/75 batches, loss: 0.6708Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.6708Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.6704Epoch 2/10: [========================      ] 60/75 batches, loss: 0.6704Epoch 2/10: [========================      ] 61/75 batches, loss: 0.6701Epoch 2/10: [========================      ] 62/75 batches, loss: 0.6704Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.6703Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.6695Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.6686Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.6686Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.6685Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.6680Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.6677Epoch 2/10: [============================  ] 70/75 batches, loss: 0.6675Epoch 2/10: [============================  ] 71/75 batches, loss: 0.6675Epoch 2/10: [============================  ] 72/75 batches, loss: 0.6674Epoch 2/10: [============================= ] 73/75 batches, loss: 0.6664Epoch 2/10: [============================= ] 74/75 batches, loss: 0.6681Epoch 2/10: [==============================] 75/75 batches, loss: 0.6677
[2025-04-30 00:32:53,485][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6677
[2025-04-30 00:32:53,848][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6772, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.6370Epoch 3/10: [                              ] 2/75 batches, loss: 0.6611Epoch 3/10: [=                             ] 3/75 batches, loss: 0.6783Epoch 3/10: [=                             ] 4/75 batches, loss: 0.6774Epoch 3/10: [==                            ] 5/75 batches, loss: 0.6651Epoch 3/10: [==                            ] 6/75 batches, loss: 0.6578Epoch 3/10: [==                            ] 7/75 batches, loss: 0.6522Epoch 3/10: [===                           ] 8/75 batches, loss: 0.6500Epoch 3/10: [===                           ] 9/75 batches, loss: 0.6497Epoch 3/10: [====                          ] 10/75 batches, loss: 0.6531Epoch 3/10: [====                          ] 11/75 batches, loss: 0.6528Epoch 3/10: [====                          ] 12/75 batches, loss: 0.6514Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.6555Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.6563Epoch 3/10: [======                        ] 15/75 batches, loss: 0.6553Epoch 3/10: [======                        ] 16/75 batches, loss: 0.6535Epoch 3/10: [======                        ] 17/75 batches, loss: 0.6532Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.6443Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.6417Epoch 3/10: [========                      ] 20/75 batches, loss: 0.6405Epoch 3/10: [========                      ] 21/75 batches, loss: 0.6392Epoch 3/10: [========                      ] 22/75 batches, loss: 0.6414Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.6411Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.6404Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.6382Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.6384Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.6405Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.6406Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.6416Epoch 3/10: [============                  ] 30/75 batches, loss: 0.6428Epoch 3/10: [============                  ] 31/75 batches, loss: 0.6419Epoch 3/10: [============                  ] 32/75 batches, loss: 0.6429Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.6421Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.6406Epoch 3/10: [==============                ] 35/75 batches, loss: 0.6389Epoch 3/10: [==============                ] 36/75 batches, loss: 0.6353Epoch 3/10: [==============                ] 37/75 batches, loss: 0.6333Epoch 3/10: [===============               ] 38/75 batches, loss: 0.6293Epoch 3/10: [===============               ] 39/75 batches, loss: 0.6291Epoch 3/10: [================              ] 40/75 batches, loss: 0.6256Epoch 3/10: [================              ] 41/75 batches, loss: 0.6254Epoch 3/10: [================              ] 42/75 batches, loss: 0.6239Epoch 3/10: [=================             ] 43/75 batches, loss: 0.6234Epoch 3/10: [=================             ] 44/75 batches, loss: 0.6230Epoch 3/10: [==================            ] 45/75 batches, loss: 0.6222Epoch 3/10: [==================            ] 46/75 batches, loss: 0.6213Epoch 3/10: [==================            ] 47/75 batches, loss: 0.6213Epoch 3/10: [===================           ] 48/75 batches, loss: 0.6197Epoch 3/10: [===================           ] 49/75 batches, loss: 0.6194Epoch 3/10: [====================          ] 50/75 batches, loss: 0.6184Epoch 3/10: [====================          ] 51/75 batches, loss: 0.6205Epoch 3/10: [====================          ] 52/75 batches, loss: 0.6198Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.6188Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.6178Epoch 3/10: [======================        ] 55/75 batches, loss: 0.6178Epoch 3/10: [======================        ] 56/75 batches, loss: 0.6175Epoch 3/10: [======================        ] 57/75 batches, loss: 0.6168Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.6163Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.6155Epoch 3/10: [========================      ] 60/75 batches, loss: 0.6147Epoch 3/10: [========================      ] 61/75 batches, loss: 0.6136Epoch 3/10: [========================      ] 62/75 batches, loss: 0.6127Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.6120Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.6124Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.6121Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.6116Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.6117Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.6113Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.6111Epoch 3/10: [============================  ] 70/75 batches, loss: 0.6084Epoch 3/10: [============================  ] 71/75 batches, loss: 0.6060Epoch 3/10: [============================  ] 72/75 batches, loss: 0.6062Epoch 3/10: [============================= ] 73/75 batches, loss: 0.6052Epoch 3/10: [============================= ] 74/75 batches, loss: 0.6024Epoch 3/10: [==============================] 75/75 batches, loss: 0.5997
[2025-04-30 00:33:09,659][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5997
[2025-04-30 00:33:10,056][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6314, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.5189Epoch 4/10: [                              ] 2/75 batches, loss: 0.5413Epoch 4/10: [=                             ] 3/75 batches, loss: 0.5701Epoch 4/10: [=                             ] 4/75 batches, loss: 0.5768Epoch 4/10: [==                            ] 5/75 batches, loss: 0.5742Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5794Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5665Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5689Epoch 4/10: [===                           ] 9/75 batches, loss: 0.5717Epoch 4/10: [====                          ] 10/75 batches, loss: 0.5568Epoch 4/10: [====                          ] 11/75 batches, loss: 0.5440Epoch 4/10: [====                          ] 12/75 batches, loss: 0.5492Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.5473Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.5552Epoch 4/10: [======                        ] 15/75 batches, loss: 0.5537Epoch 4/10: [======                        ] 16/75 batches, loss: 0.5536Epoch 4/10: [======                        ] 17/75 batches, loss: 0.5528Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.5530Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.5557Epoch 4/10: [========                      ] 20/75 batches, loss: 0.5551Epoch 4/10: [========                      ] 21/75 batches, loss: 0.5571Epoch 4/10: [========                      ] 22/75 batches, loss: 0.5514Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.5521Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.5509Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.5445Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.5401Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.5346Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.5358Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.5338Epoch 4/10: [============                  ] 30/75 batches, loss: 0.5320Epoch 4/10: [============                  ] 31/75 batches, loss: 0.5305Epoch 4/10: [============                  ] 32/75 batches, loss: 0.5327Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.5311Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.5311Epoch 4/10: [==============                ] 35/75 batches, loss: 0.5300Epoch 4/10: [==============                ] 36/75 batches, loss: 0.5307Epoch 4/10: [==============                ] 37/75 batches, loss: 0.5312Epoch 4/10: [===============               ] 38/75 batches, loss: 0.5306Epoch 4/10: [===============               ] 39/75 batches, loss: 0.5285Epoch 4/10: [================              ] 40/75 batches, loss: 0.5273Epoch 4/10: [================              ] 41/75 batches, loss: 0.5275Epoch 4/10: [================              ] 42/75 batches, loss: 0.5248Epoch 4/10: [=================             ] 43/75 batches, loss: 0.5244Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5234Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5240Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5257Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5254Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5242Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5244Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5240Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5226Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5217Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5207Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5209Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5207Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5208Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5209Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5232Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5220Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5219Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5213Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5199Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5191Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5217Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5202Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5199Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5194Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5181Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5173Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5170Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5162Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5155Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5156Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5151Epoch 4/10: [==============================] 75/75 batches, loss: 0.5169
[2025-04-30 00:33:25,805][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5169
[2025-04-30 00:33:26,251][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5186, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.9090909090909091}
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.6013Epoch 5/10: [                              ] 2/75 batches, loss: 0.5695Epoch 5/10: [=                             ] 3/75 batches, loss: 0.5796Epoch 5/10: [=                             ] 4/75 batches, loss: 0.5774Epoch 5/10: [==                            ] 5/75 batches, loss: 0.5906Epoch 5/10: [==                            ] 6/75 batches, loss: 0.5747Epoch 5/10: [==                            ] 7/75 batches, loss: 0.5688Epoch 5/10: [===                           ] 8/75 batches, loss: 0.5489Epoch 5/10: [===                           ] 9/75 batches, loss: 0.5530Epoch 5/10: [====                          ] 10/75 batches, loss: 0.5482Epoch 5/10: [====                          ] 11/75 batches, loss: 0.5503Epoch 5/10: [====                          ] 12/75 batches, loss: 0.5457Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.5419Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.5391Epoch 5/10: [======                        ] 15/75 batches, loss: 0.5384Epoch 5/10: [======                        ] 16/75 batches, loss: 0.5351Epoch 5/10: [======                        ] 17/75 batches, loss: 0.5411Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.5368Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.5312Epoch 5/10: [========                      ] 20/75 batches, loss: 0.5304Epoch 5/10: [========                      ] 21/75 batches, loss: 0.5361Epoch 5/10: [========                      ] 22/75 batches, loss: 0.5349Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.5359Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.5301Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.5308Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.5276Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.5276Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.5288Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.5293Epoch 5/10: [============                  ] 30/75 batches, loss: 0.5281Epoch 5/10: [============                  ] 31/75 batches, loss: 0.5233Epoch 5/10: [============                  ] 32/75 batches, loss: 0.5206Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.5211Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.5197Epoch 5/10: [==============                ] 35/75 batches, loss: 0.5194Epoch 5/10: [==============                ] 36/75 batches, loss: 0.5190Epoch 5/10: [==============                ] 37/75 batches, loss: 0.5272Epoch 5/10: [===============               ] 38/75 batches, loss: 0.5254Epoch 5/10: [===============               ] 39/75 batches, loss: 0.5232Epoch 5/10: [================              ] 40/75 batches, loss: 0.5233Epoch 5/10: [================              ] 41/75 batches, loss: 0.5224Epoch 5/10: [================              ] 42/75 batches, loss: 0.5185Epoch 5/10: [=================             ] 43/75 batches, loss: 0.5179Epoch 5/10: [=================             ] 44/75 batches, loss: 0.5172Epoch 5/10: [==================            ] 45/75 batches, loss: 0.5173Epoch 5/10: [==================            ] 46/75 batches, loss: 0.5164Epoch 5/10: [==================            ] 47/75 batches, loss: 0.5157Epoch 5/10: [===================           ] 48/75 batches, loss: 0.5145Epoch 5/10: [===================           ] 49/75 batches, loss: 0.5146Epoch 5/10: [====================          ] 50/75 batches, loss: 0.5148Epoch 5/10: [====================          ] 51/75 batches, loss: 0.5125Epoch 5/10: [====================          ] 52/75 batches, loss: 0.5097Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.5087Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.5079Epoch 5/10: [======================        ] 55/75 batches, loss: 0.5077Epoch 5/10: [======================        ] 56/75 batches, loss: 0.5063Epoch 5/10: [======================        ] 57/75 batches, loss: 0.5048Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.5051Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.5027Epoch 5/10: [========================      ] 60/75 batches, loss: 0.5018Epoch 5/10: [========================      ] 61/75 batches, loss: 0.5003Epoch 5/10: [========================      ] 62/75 batches, loss: 0.4990Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.4991Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.4973Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.4947Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.4927Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.4913Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.4900Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.4899Epoch 5/10: [============================  ] 70/75 batches, loss: 0.4899Epoch 5/10: [============================  ] 71/75 batches, loss: 0.4885Epoch 5/10: [============================  ] 72/75 batches, loss: 0.4883Epoch 5/10: [============================= ] 73/75 batches, loss: 0.4866Epoch 5/10: [============================= ] 74/75 batches, loss: 0.4855Epoch 5/10: [==============================] 75/75 batches, loss: 0.4846
[2025-04-30 00:33:42,040][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.4846
[2025-04-30 00:33:42,448][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.4411, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9310344827586207}
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.4747Epoch 6/10: [                              ] 2/75 batches, loss: 0.4415Epoch 6/10: [=                             ] 3/75 batches, loss: 0.4392Epoch 6/10: [=                             ] 4/75 batches, loss: 0.4289Epoch 6/10: [==                            ] 5/75 batches, loss: 0.4399Epoch 6/10: [==                            ] 6/75 batches, loss: 0.4404Epoch 6/10: [==                            ] 7/75 batches, loss: 0.4257Epoch 6/10: [===                           ] 8/75 batches, loss: 0.4155Epoch 6/10: [===                           ] 9/75 batches, loss: 0.4280Epoch 6/10: [====                          ] 10/75 batches, loss: 0.4230Epoch 6/10: [====                          ] 11/75 batches, loss: 0.4225Epoch 6/10: [====                          ] 12/75 batches, loss: 0.4238Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.4205Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.4221Epoch 6/10: [======                        ] 15/75 batches, loss: 0.4213Epoch 6/10: [======                        ] 16/75 batches, loss: 0.4185Epoch 6/10: [======                        ] 17/75 batches, loss: 0.4237Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.4207Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.4199Epoch 6/10: [========                      ] 20/75 batches, loss: 0.4234Epoch 6/10: [========                      ] 21/75 batches, loss: 0.4230Epoch 6/10: [========                      ] 22/75 batches, loss: 0.4242Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.4221Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.4210Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.4179Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.4154Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.4121Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.4116Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.4105Epoch 6/10: [============                  ] 30/75 batches, loss: 0.4084Epoch 6/10: [============                  ] 31/75 batches, loss: 0.4057Epoch 6/10: [============                  ] 32/75 batches, loss: 0.4046Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.4057Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.4037Epoch 6/10: [==============                ] 35/75 batches, loss: 0.4023Epoch 6/10: [==============                ] 36/75 batches, loss: 0.4013Epoch 6/10: [==============                ] 37/75 batches, loss: 0.4032Epoch 6/10: [===============               ] 38/75 batches, loss: 0.4026Epoch 6/10: [===============               ] 39/75 batches, loss: 0.4004Epoch 6/10: [================              ] 40/75 batches, loss: 0.3985Epoch 6/10: [================              ] 41/75 batches, loss: 0.3977Epoch 6/10: [================              ] 42/75 batches, loss: 0.3978Epoch 6/10: [=================             ] 43/75 batches, loss: 0.3963Epoch 6/10: [=================             ] 44/75 batches, loss: 0.3948Epoch 6/10: [==================            ] 45/75 batches, loss: 0.3941Epoch 6/10: [==================            ] 46/75 batches, loss: 0.3939Epoch 6/10: [==================            ] 47/75 batches, loss: 0.3938Epoch 6/10: [===================           ] 48/75 batches, loss: 0.3924Epoch 6/10: [===================           ] 49/75 batches, loss: 0.3930Epoch 6/10: [====================          ] 50/75 batches, loss: 0.3934Epoch 6/10: [====================          ] 51/75 batches, loss: 0.3938Epoch 6/10: [====================          ] 52/75 batches, loss: 0.3924Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.3914Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.3897Epoch 6/10: [======================        ] 55/75 batches, loss: 0.3887Epoch 6/10: [======================        ] 56/75 batches, loss: 0.3870Epoch 6/10: [======================        ] 57/75 batches, loss: 0.3868Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.3858Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.3846Epoch 6/10: [========================      ] 60/75 batches, loss: 0.3857Epoch 6/10: [========================      ] 61/75 batches, loss: 0.3867Epoch 6/10: [========================      ] 62/75 batches, loss: 0.3875Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.3884Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.3885Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.3869Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.3860Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.3852Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.3846Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.3863Epoch 6/10: [============================  ] 70/75 batches, loss: 0.3866Epoch 6/10: [============================  ] 71/75 batches, loss: 0.3861Epoch 6/10: [============================  ] 72/75 batches, loss: 0.3861Epoch 6/10: [============================= ] 73/75 batches, loss: 0.3855Epoch 6/10: [============================= ] 74/75 batches, loss: 0.3850Epoch 6/10: [==============================] 75/75 batches, loss: 0.3841
[2025-04-30 00:33:58,198][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.3841
[2025-04-30 00:33:58,599][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4372, Metrics: {'accuracy': 0.9047619047619048, 'f1': 0.8928571428571429}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.4090Epoch 7/10: [                              ] 2/75 batches, loss: 0.3740Epoch 7/10: [=                             ] 3/75 batches, loss: 0.3665Epoch 7/10: [=                             ] 4/75 batches, loss: 0.3764Epoch 7/10: [==                            ] 5/75 batches, loss: 0.3762Epoch 7/10: [==                            ] 6/75 batches, loss: 0.3648Epoch 7/10: [==                            ] 7/75 batches, loss: 0.3614Epoch 7/10: [===                           ] 8/75 batches, loss: 0.3613Epoch 7/10: [===                           ] 9/75 batches, loss: 0.3619Epoch 7/10: [====                          ] 10/75 batches, loss: 0.3581Epoch 7/10: [====                          ] 11/75 batches, loss: 0.3533Epoch 7/10: [====                          ] 12/75 batches, loss: 0.3591Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.3528Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.3506Epoch 7/10: [======                        ] 15/75 batches, loss: 0.3468Epoch 7/10: [======                        ] 16/75 batches, loss: 0.3467Epoch 7/10: [======                        ] 17/75 batches, loss: 0.3439Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.3410Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.3441Epoch 7/10: [========                      ] 20/75 batches, loss: 0.3465Epoch 7/10: [========                      ] 21/75 batches, loss: 0.3448Epoch 7/10: [========                      ] 22/75 batches, loss: 0.3424Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.3425Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.3403Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.3384Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.3370Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.3352Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.3398Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.3373Epoch 7/10: [============                  ] 30/75 batches, loss: 0.3368Epoch 7/10: [============                  ] 31/75 batches, loss: 0.3367Epoch 7/10: [============                  ] 32/75 batches, loss: 0.3372Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.3391Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.3401Epoch 7/10: [==============                ] 35/75 batches, loss: 0.3384Epoch 7/10: [==============                ] 36/75 batches, loss: 0.3366Epoch 7/10: [==============                ] 37/75 batches, loss: 0.3355Epoch 7/10: [===============               ] 38/75 batches, loss: 0.3338Epoch 7/10: [===============               ] 39/75 batches, loss: 0.3329Epoch 7/10: [================              ] 40/75 batches, loss: 0.3325Epoch 7/10: [================              ] 41/75 batches, loss: 0.3325Epoch 7/10: [================              ] 42/75 batches, loss: 0.3313Epoch 7/10: [=================             ] 43/75 batches, loss: 0.3313Epoch 7/10: [=================             ] 44/75 batches, loss: 0.3311Epoch 7/10: [==================            ] 45/75 batches, loss: 0.3297Epoch 7/10: [==================            ] 46/75 batches, loss: 0.3291Epoch 7/10: [==================            ] 47/75 batches, loss: 0.3324Epoch 7/10: [===================           ] 48/75 batches, loss: 0.3321Epoch 7/10: [===================           ] 49/75 batches, loss: 0.3318Epoch 7/10: [====================          ] 50/75 batches, loss: 0.3308Epoch 7/10: [====================          ] 51/75 batches, loss: 0.3292Epoch 7/10: [====================          ] 52/75 batches, loss: 0.3286Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.3278Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.3266Epoch 7/10: [======================        ] 55/75 batches, loss: 0.3262Epoch 7/10: [======================        ] 56/75 batches, loss: 0.3250Epoch 7/10: [======================        ] 57/75 batches, loss: 0.3248Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.3236Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.3231Epoch 7/10: [========================      ] 60/75 batches, loss: 0.3221Epoch 7/10: [========================      ] 61/75 batches, loss: 0.3217Epoch 7/10: [========================      ] 62/75 batches, loss: 0.3209Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.3198Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.3196Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.3189Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.3185Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.3177Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.3173Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.3171Epoch 7/10: [============================  ] 70/75 batches, loss: 0.3166Epoch 7/10: [============================  ] 71/75 batches, loss: 0.3161Epoch 7/10: [============================  ] 72/75 batches, loss: 0.3157Epoch 7/10: [============================= ] 73/75 batches, loss: 0.3161Epoch 7/10: [============================= ] 74/75 batches, loss: 0.3158Epoch 7/10: [==============================] 75/75 batches, loss: 0.3155
[2025-04-30 00:34:14,368][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.3155
[2025-04-30 00:34:14,751][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.2814, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.2565Epoch 8/10: [                              ] 2/75 batches, loss: 0.2570Epoch 8/10: [=                             ] 3/75 batches, loss: 0.2986Epoch 8/10: [=                             ] 4/75 batches, loss: 0.3065Epoch 8/10: [==                            ] 5/75 batches, loss: 0.2975slurmstepd: error: *** JOB 58114519 ON r24g41 CANCELLED AT 2025-04-30T00:34:16 ***
