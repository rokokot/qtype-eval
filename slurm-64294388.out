SLURM_JOB_ID: 64294388
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: mini_test_debug
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Sat Apr  5 18:16:50 CEST 2025
Walltime: 00-00:30:00
========================================================================
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running mini experiment...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-05 18:17:34,971][__main__][INFO] - Configuration:
seed: 42
output_dir: ./mini_test_output
experiment_name: mini_test_glot500_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  layer_wise: false
  layer_index: -1
training:
  task_type: classification
  batch_size: 8
  num_epochs: 3
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks:
  - question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-05 18:17:35,077][__main__][INFO] - Using task: '['question_type']'
[2025-04-05 18:17:35,077][__main__][INFO] - Task type: classification
[2025-04-05 18:17:35,077][__main__][INFO] - Running LM probe experiment for ['question_type'] on languages: ['en']
[2025-04-05 18:17:35,084][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-05 18:17:37,484][src.data.datasets][INFO] - Creating dataloaders for task: question_type (original: ['question_type'])
[2025-04-05 18:17:37,498][src.data.datasets][INFO] - Found model snapshot: /data/leuven/371/vsc37132/qtype-eval/data/cache/models--cis-lmu--glot500-base/snapshots/d4d7c1ec01828fdf7452a4ccf7b55177aced175e
[2025-04-05 18:17:40,879][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-05 18:17:40,880][src.data.datasets][INFO] - Loading base dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-05 18:17:41,098][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,158][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,322][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-05 18:17:41,323][src.data.datasets][INFO] - Loading base dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-05 18:17:41,350][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,391][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,406][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-05 18:17:41,408][src.data.datasets][INFO] - Loading base dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-05 18:17:41,461][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,527][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Sat Apr  5 16:13:11 2025).
[2025-04-05 18:17:41,560][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-05 18:17:41,561][src.data.datasets][INFO] - Train data columns: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-05 18:17:41,562][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-05 18:17:41,563][src.data.datasets][INFO] - Sample question_type: 1
[2025-04-05 18:17:41,563][src.data.datasets][INFO] - Created train dataset with 1192 examples
[2025-04-05 18:17:41,563][src.data.datasets][INFO] - Created validation dataset with 72 examples
[2025-04-05 18:17:41,563][src.data.datasets][INFO] - Created test dataset with 110 examples
[2025-04-05 18:17:41,564][src.data.datasets][INFO] - Sample processed successfully with keys: ['input_ids', 'attention_mask', 'labels']
[2025-04-05 18:17:41,565][src.data.datasets][INFO] - Creating dataloaders with 0 workers
[2025-04-05 18:17:41,565][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-05 18:17:41,565][__main__][ERROR] - Failed to create model for en: create_model() got multiple values for argument 'model_type'
[2025-04-05 18:17:41,565][__main__][ERROR] - Traceback: Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/src/experiments/run_experiment.py", line 320, in run_lm_experiment
    model = create_model(task_type, **model_params)
TypeError: create_model() got multiple values for argument 'model_type'

[2025-04-05 18:17:41,577][__main__][INFO] - Results saved to ./mini_test_output/all_results.json
wandb:                                                                                
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval/outputs/mini_test_glot500_en/2025-04-05/18-17-34/wandb/offline-run-20250405_181735-9vv3y6uf
wandb: Find logs at: ./wandb/offline-run-20250405_181735-9vv3y6uf/logs
Output directory contents:
total 8
drwxr-x---  2 vsc37132 vsc37132 4096 Apr  5 18:16 .
drwxr-x--- 14 vsc37132 vsc37132 4096 Apr  5 18:16 ..
cat: mini_test_output/all_results.json: No such file or directory
No results file found
Error files if any:
No error files found
Mini test completed
GPU memory usage:
memory.used [MiB]
1 MiB
