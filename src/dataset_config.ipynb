{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting argilla\n",
      "  Downloading argilla-2.7.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting httpx>=0.26.0 (from argilla)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.6.0 (from argilla)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.22.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from argilla) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from argilla) (4.67.1)\n",
      "Collecting rich>=10.0.0 (from argilla)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from argilla) (3.3.0)\n",
      "Collecting pillow>=9.5.0 (from argilla)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting standardwebhooks>=1.0.0 (from argilla)\n",
      "  Downloading standardwebhooks-1.0.0.tar.gz (4.9 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.0.0->argilla) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (3.11.12)\n",
      "Requirement already satisfied: packaging in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from datasets>=2.0.0->argilla) (6.0.2)\n",
      "Collecting anyio (from httpx>=0.26.0->argilla)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from httpx>=0.26.0->argilla) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx>=0.26.0->argilla)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from httpx>=0.26.0->argilla) (3.10)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.26.0->argilla)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from huggingface_hub>=0.22.0->argilla) (4.12.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.6.0->argilla)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.6.0->argilla)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.0.0->argilla)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from rich>=10.0.0->argilla) (2.19.1)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from standardwebhooks>=1.0.0->argilla) (25.1.0)\n",
      "Requirement already satisfied: python-dateutil in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from standardwebhooks>=1.0.0->argilla) (2.9.0.post0)\n",
      "Collecting Deprecated (from standardwebhooks>=1.0.0->argilla)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting types-python-dateutil (from standardwebhooks>=1.0.0->argilla)\n",
      "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting types-Deprecated (from standardwebhooks>=1.0.0->argilla)\n",
      "  Downloading types_Deprecated-1.2.15.20241117-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->argilla) (1.18.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.0.0->argilla)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.0.0->argilla) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.0.0->argilla) (2.3.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.26.0->argilla)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wrapt<2,>=1.10 (from Deprecated->standardwebhooks>=1.0.0->argilla)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from pandas->datasets>=2.0.0->argilla) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from pandas->datasets>=2.0.0->argilla) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages (from python-dateutil->standardwebhooks>=1.0.0->argilla) (1.17.0)\n",
      "Downloading argilla-2.7.0-py3-none-any.whl (161 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading types_Deprecated-1.2.15.20241117-py3-none-any.whl (3.8 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Building wheels for collected packages: standardwebhooks\n",
      "  Building wheel for standardwebhooks (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for standardwebhooks: filename=standardwebhooks-1.0.0-py3-none-any.whl size=3571 sha256=488244dd78e08365445e20297b4e7c3988e976bef86b01bb81d86576afa78823\n",
      "  Stored in directory: /home/robin/.cache/pip/wheels/40/c9/01/255026ce5c725bee8d87fcf67c5c2149847f31b5df60bb83db\n",
      "Successfully built standardwebhooks\n",
      "Installing collected packages: wrapt, types-python-dateutil, types-Deprecated, sniffio, pydantic-core, pillow, mdurl, h11, annotated-types, pydantic, markdown-it-py, httpcore, Deprecated, anyio, rich, httpx, standardwebhooks, argilla\n",
      "Successfully installed Deprecated-1.2.18 annotated-types-0.7.0 anyio-4.8.0 argilla-2.7.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 markdown-it-py-3.0.0 mdurl-0.1.2 pillow-11.1.0 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 sniffio-1.3.1 standardwebhooks-1.0.0 types-Deprecated-1.2.15.20241117 types-python-dateutil-2.9.0.20241206 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install argilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import argilla as rg\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = rg.Settings(\n",
    "    guidelines=\"Annotated Universal Dependencies sentences with complexity metrics\",\n",
    "    allow_extra_metadata=True,\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\", required=True),\n",
    "        rg.TextField(name=\"question_id\", required=True),\n",
    "        rg.TextField(name=\"language\", required=True),\n",
    "        rg.TextField(name=\"n_tokens\", required=True),\n",
    "        rg.TextField(name=\"char_per_tok\", required=True),\n",
    "        rg.TextField(name=\"avg_max_depth\", required=True),\n",
    "        rg.TextField(name=\"lexical_density\", required=True),\n",
    "        rg.TextField(name=\"avg_links_len\", required=True),\n",
    "        rg.TextField(name=\"verbal_head_per_sent\", required=True),\n",
    "        rg.TextField(name=\"avg_prepositional_chain_len\", required=True)\n",
    "    ],\n",
    "    questions = [\n",
    "        rg.LabelQuestion(name=\"processed\", title=\"processing status\", description=\"shows if processed\", required=True, labels=[\"ok\",\"not ok\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = rg.Argilla(\n",
    "    api_url=\"https://rokii3-complexity-annotated-questions.hf.space\",\n",
    "    api_key=\"CJ83an24doRX_To1xKivqZ0H6KLGBxDftdG0iSWcWtuTiP33VxpZ0z-d2ODQgKYQoc0DQFj_SskZD4jlumdsAiRi18YZ6sdK16p_XOTDl6M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS = ['n_tokens','char_per_tok', 'lexical_density', 'avg_max_depth', 'avg_links_len', 'verbal_head_per_sent', 'avg_prepositional_chain_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argilla boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id=UUID('09ded185-06b6-4c1b-9ec4-402bc01d77da') inserted_at=datetime.datetime(2025, 2, 22, 22, 6, 3, 826365) updated_at=datetime.datetime(2025, 2, 22, 22, 6, 6, 419074) name='question-data-linguistic-features' status='ready' guidelines='Annotated Universal Dependencies sentences with complexity metrics' allow_extra_metadata=True distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('581b13ef-9395-456e-9754-401d3b9abf28') last_activity_at=datetime.datetime(2025, 2, 22, 22, 6, 6, 419074))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = rg.Dataset(\n",
    "    name=\"question-data-linguistic-features\",\n",
    "    settings=settings,\n",
    ")\n",
    "\n",
    "dataset.create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error creating dataset: cannot access local variable 'all_records' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "def create_argilla_dataset(sample_record):\n",
    " \n",
    "    fields = [\n",
    "        rg.TextField(name=\"text\", required=True),\n",
    "        rg.TextField(name=\"question_id\", required=True),\n",
    "        rg.TextField(name=\"language\", required=True)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Define settings\n",
    "    settings = rg.Settings(\n",
    "    guidelines=\"Annotated Universal Dependencies sentences with complexity metrics\",\n",
    "    allow_extra_metadata=True,\n",
    "    fields=[\n",
    "        rg.TextField(name=\"text\", required=True),\n",
    "        rg.TextField(name=\"question_id\", required=True),\n",
    "        rg.TextField(name=\"language\", required=True),\n",
    "        rg.TextField(name=\"n_tokens\", required=True),\n",
    "        rg.TextField(name=\"char_per_tok\", required=True),\n",
    "        rg.TextField(name=\"avg_max_depth\", required=True),\n",
    "        rg.TextField(name=\"lexical_density\", required=True),\n",
    "        rg.TextField(name=\"avg_links_len\", required=True),\n",
    "        rg.TextField(name=\"verbal_head_per_sent\", required=True),\n",
    "        rg.TextField(name=\"avg_prepositional_chain_len\", required=True)\n",
    "    ],\n",
    "    questions = [\n",
    "        rg.LabelQuestion(name=\"processed\", title=\"processing status\", description=\"shows if processed\", required=True, labels=[\"ok\",\"not ok\"])\n",
    "])\n",
    "\n",
    "\n",
    "    dataset = rg.Dataset(name=\"ud-syntactic-features\", settings=settings)\n",
    "    return dataset\n",
    "\n",
    "def process_file(file_path):\n",
    "    language = Path(file_path).stem.split('-')[1]\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get text column (first column)\n",
    "    text_col = df.columns[0]\n",
    "    print(f\"Text column name: {text_col}\")\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[text_col]):\n",
    "            continue\n",
    "\n",
    "    record = {\n",
    "            \"text\": str(row[text_col]),\n",
    "            \"question_id\": str(uuid.uuid4()),\n",
    "            \"language\": language,\n",
    "            \"verification\": \"needs_review\"\n",
    "        }\n",
    "    \n",
    "    record['metadata'] = {}\n",
    "    columns_to_skip = [text_col, 'Filename', 'base_number']\n",
    "    # Add all features except metadata columns\n",
    "    columns_to_skip = [text_col, 'Filename', 'base_number']\n",
    "    for col in df.columns:\n",
    "        if col not in columns_to_skip:\n",
    "            try:\n",
    "                value = row[col] if not pd.isna(row[col]) else 0.0\n",
    "                record[col] = float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Could not convert value for {col}: {row[col]}\")\n",
    "                record[col] = 0.0\n",
    "                    \n",
    "        records.append(record)\n",
    "    \n",
    "    print(f\"Created {len(records)} records from {len(df)} rows\")\n",
    "    return records\n",
    "  \n",
    "\n",
    "def main():\n",
    "   \n",
    "    \n",
    "    input_dir = Path('/home/robin/Research/qtype-eval/data/annotated_UD_questions')\n",
    "    files = list(input_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create and initialize dataset\n",
    "    try:\n",
    "        dataset = create_argilla_dataset(all_records[0])\n",
    "        dataset.create()  # Initialize the dataset\n",
    "        print(\"\\nSuccessfully created dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creating dataset: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    all_records = []\n",
    "    for file_path in files:\n",
    "        print(f\"\\nProcessing {file_path.name}...\")\n",
    "        records = process_file(file_path)\n",
    "        all_records.extend(records)\n",
    "        print(f\"Extracted {len(records)} records\")\n",
    "    \n",
    "    if not all_records:\n",
    "        print(\"No records found!\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    print(f\"\\nTotal records to upload: {len(all_records)}\")\n",
    "    print(all_records[0])\n",
    "    \n",
    "    # Upload in batches using records.log()\n",
    "    batch_size = 100\n",
    "    total_batches = (len(all_records) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(all_records), batch_size):\n",
    "        batch = all_records[i:i + batch_size]\n",
    "        try:\n",
    "            dataset.records.log(records=batch)  # Using records.log() instead of add_records()\n",
    "            print(f\"Uploaded batch {i//batch_size + 1}/{total_batches}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading batch: {str(e)}\")\n",
    "            print(\"First record in failed batch:\")\n",
    "            print(batch[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing UD-korean-polar-annotated.csv...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['text'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError uploading batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m   records \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m   all_records\u001b[38;5;241m.\u001b[39mextend(records)\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m FEATS \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m---> 10\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     12\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_filtered\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['text'] not in index\""
     ]
    }
   ],
   "source": [
    "def process_file(file_path):\n",
    "  language = Path(file_path).stem.split('-')[1]\n",
    "\n",
    "  df = pd.read_csv(file_path)\n",
    "\n",
    "  text = df.columns[0]\n",
    "\n",
    "  \n",
    "  features = ['text'] + [f for f in FEATS if f in df.columns]\n",
    "  df_filtered = df[features].copy()\n",
    "    \n",
    "  records = []\n",
    "  for _, row in df_filtered.iterrows():\n",
    "    if pd.isna(row[text]):\n",
    "      continue\n",
    "\n",
    "    record = {\n",
    "            \"text\": str(row['text']),\n",
    "            \"question_id\": str(uuid.uuid4()),\n",
    "            \"language\": language,\n",
    "            \"verification\": \"needs_review\" \n",
    "    }\n",
    "\n",
    "    columns_to_skip = [text, 'Filename', 'base_number']\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col not in columns_to_skip:\n",
    "            try:\n",
    "                value = row[col] if not pd.isna(row[col]) else 0.0\n",
    "                record[col] = float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Could not convert value for {col}: {row[col]}\")\n",
    "                record[col] = 0.0\n",
    "    \n",
    "    records.append(record)\n",
    "    \n",
    "    print(f\"Created {len(records)} records from {len(df)} rows\")\n",
    "    return records\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "  input_dir = Path('/home/robin/Research/qtype-eval/data/annotated_UD_questions')\n",
    "  files = list(input_dir.glob(\"*.csv\"))\n",
    "\n",
    "  all_records = []\n",
    "  for file_path in files:\n",
    "    print(f\"\\nProcessing {file_path.name}...\")\n",
    "    records = process_file(file_path)\n",
    "    all_records.extend(records)\n",
    "    print(f\"Extracted {len(records)} records\")\n",
    "    \n",
    "  print(f\"\\nTotal records to upload: {len(all_records)}\")\n",
    "\n",
    "  batch_size = 100\n",
    "  for i in range(0, len(all_records), batch_size):\n",
    "    batch = all_records[i:i + batch_size]\n",
    "    try:\n",
    "      dataset.add_records(batch)\n",
    "      print(f\"Uploaded batch {i//batch_size + 1}/{(len(all_records) + batch_size - 1)//batch_size}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error uploading batch: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtype-eval-pAepV5Z2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
