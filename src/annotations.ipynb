{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically annotating questions for type and complexity scores\n",
    "\n",
    "English, Finnish, Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from conllu import parse_incr\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from udapi.core.document import Document\n",
    "from udapi.block.read.conllu import Conllu\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_INFO = {\n",
    "  'en': {'qwords': ['what', 'who', 'where', 'when', 'why', 'how'], 'canonical_order': 'SVO'},\n",
    "  'ko': {'qwords': [], 'canonical_order': 'SOV'},\n",
    "  'fi': {'qwords': [], 'canonical_order': 'SVO'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset skeleton\n",
    "\n",
    "importing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [text, language, question_type, clause_count, dep_depth, core_args_count, dep_distances, question_word_movement, nonproj_deps_count]\n",
      "Index: []\n",
      "\n",
      "\n",
      "DataFrame with sample row:\n",
      "                text language question_type  clause_count  dep_depth  core_args_count dep_distances  question_word_movement  nonproj_deps_count\n",
      "0  What did you eat?       en       content             1          3                2         2,1,3                       3                   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', None)        \n",
    "pd.set_option('display.max_rows', None)     \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'text',                    # original question text\n",
    "    'language',                # language code (en, fi, ko, ja, id, ru)\n",
    "    'question_type',           # polar or content\n",
    "    'clause_count',            # number of clauses\n",
    "    'dep_depth',               # dependency depth\n",
    "    'core_args_count',         # count of core arguments\n",
    "    'dep_distances',           # dependency distances\n",
    "    'question_word_movement',  # distance of question word movement\n",
    "    'nonproj_deps_count'       # count of non-projective dependencies\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(\"Full DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "sample_row = {\n",
    "    'text': 'What did you eat?',\n",
    "    'language': 'en',\n",
    "    'question_type': 'content',\n",
    "    'clause_count': 1,\n",
    "    'dep_depth': 3,\n",
    "    'core_args_count': 2,\n",
    "    'dep_distances': '2,1,3',\n",
    "    'question_word_movement': 3,\n",
    "    'nonproj_deps_count': 0\n",
    "}\n",
    "\n",
    "df.loc[len(df)] = sample_row\n",
    "\n",
    "print(\"DataFrame with sample row:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Type Annotations\n",
    "\n",
    "The World Atlas of Language Structures (WALS) documents linguistic features of languages. These features are structural properties of language that describe aspects of cross-linguistic diversity. Most features come with their own chapters detailing their values and expected roles. For example, feature 81A represents the canonical word order syntax. The corresponding chapter catalogs the values, provides examples and oftentimes interesting discussions of relevance and distribution. \n",
    "\n",
    "We will be looking at word order and features related to question formation strategies:\n",
    "- 92A: Position of Polar Question Particles\n",
    "- 93A: Position of Interrogative Phrases in Content Questions\n",
    "- 116: Polar Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing questions_en_ewt-ud-train.txt...\n",
      "Processing questions_en_ewt-ud-dev.txt...\n",
      "Processing questions_en_ewt-ud-test.txt...\n",
      "Processing complete!\n",
      "Found 323 polar questions\n",
      "Found 240 content questions\n",
      "Results written to 'polar_questions.txt' and 'content_questions.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class QuestionClassifier:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.en_wh_words = r'\\b(what*|who*|where*|when*|why*|how*|which*)\\b'\n",
    "        self.en_polar_starters = r'^(is|are|do|does|did|have|has|can|could|will|would|should|may|might)'\n",
    "        self.embedded_verbs = r'\\b(know|tell|confirm|explain|understand|think|show|mean|see)\\b'\n",
    "\n",
    "        \n",
    "        self.fi_wh_words = r'\\b(mikä|mitä|missä|mistä|mihin|milloin|miksi|kuka|ketkä|kumpi|kuinka|miten)\\b'\n",
    "        \n",
    "        self.fi_special_content = r'\\bmontako\\b'\n",
    "        \n",
    "        self.ko_wh_words = r'(무엇|뭐|어디|언제|누구|왜|어떻게|무슨|어느|몇)'\n",
    "        \n",
    "    def classify_question(self, text, language='english'):\n",
    "        \"\"\"\n",
    "        Classify a question as either polar or content based on its structure and language.\n",
    "        Returns 'polar' or 'content' as classification.\n",
    "        \"\"\"\n",
    "        \n",
    "        text = text.strip().lower()\n",
    "        \n",
    "        if not text:\n",
    "            return None\n",
    "            \n",
    "        if language == 'english':\n",
    "            return self._classify_english(text)\n",
    "        elif language == 'finnish':\n",
    "            return self._classify_finnish(text)\n",
    "        elif language == 'korean':\n",
    "            return self._classify_korean(text)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "            \n",
    "    def _classify_english(self, text):\n",
    "        \n",
    "        if re.match(f'^{self.en_wh_words[2:]}', text, re.I):\n",
    "            return 'content'\n",
    "            \n",
    "        if re.match(f'{self.en_polar_starters}.*{self.embedded_verbs}.*{self.en_wh_words}', text, re.I):\n",
    "            return 'polar'\n",
    "            \n",
    "        if re.match(self.en_polar_starters, text, re.I):\n",
    "            return 'polar'\n",
    "\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    def _classify_finnish(self, text):\n",
    "        words = text.replace(',', ' ').replace('?', ' ').split()\n",
    "        for word in words:\n",
    "            if word.endswith('ko') or word.endswith('kö'):\n",
    "                return 'polar'\n",
    "        if 'vai' in words or 'montako' in words:\n",
    "            return 'content'\n",
    "        return 'content'\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "    def _classify_korean(self, text):\n",
    "        \n",
    "        if re.search(self.ko_wh_words, text):\n",
    "            return 'content'\n",
    "        return 'polar'\n",
    "\n",
    "\n",
    "\n",
    "def process_file(input_filename, language):\n",
    "    classifier = QuestionClassifier()\n",
    "    questions = {'polar': [], 'content': []}\n",
    "    \n",
    "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentence_text = line.strip()\n",
    "            if sentence_text:\n",
    "                question_type = classifier.classify_question(sentence_text, language)\n",
    "                if question_type:\n",
    "                    questions[question_type].append(sentence_text)\n",
    "                    \n",
    "    return questions\n",
    "\n",
    "\n",
    "def open_folder(directory, language):\n",
    "    all_questions = {'polar': [], 'content': []}\n",
    "    path = Path(directory)\n",
    "\n",
    "    files = list(path.glob('*.txt'))  # Now looking for text files\n",
    "    if not files:\n",
    "        print(f'No files found in {path}')\n",
    "        return all_questions\n",
    "    \n",
    "    for file in files:\n",
    "        print(f'Processing {file.name}...')\n",
    "        questions = process_file(file, language)\n",
    "        all_questions['polar'].extend(questions['polar'])\n",
    "        all_questions['content'].extend(questions['content'])\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "\n",
    "def write_output_files(questions, language):\n",
    "    with open(f\"{language}_polar_questions.txt\", 'w', encoding='utf-8') as f:\n",
    "        for sentence in questions['polar']:\n",
    "            f.write(f\"{sentence}\\n\")\n",
    "    \n",
    "    with open(f\"{language}_content_questions.txt\", 'w', encoding='utf-8') as f:\n",
    "        for sentence in questions['content']:\n",
    "            f.write(f\"{sentence}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "  \n",
    "    path = \"\"\n",
    "\n",
    "    language = \"\"\n",
    "    \n",
    "    try:\n",
    "        questions = open_folder(path, language)\n",
    "        \n",
    "        write_output_files(questions, language)\n",
    "        \n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Found {len(questions['polar'])} polar questions\")\n",
    "        print(f\"Found {len(questions['content'])} content questions\")\n",
    "        print(f\"Results written to 'polar_questions.txt' and 'content_questions.txt'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnish:\n",
    "\n",
    "\\bMikä |Millainen |Ketä |Mitähän |Mikähän |Mitä |Minä |Missä |Mistä |Mihin |Milloin |Miksi |Kuka |Ketkä |Kumpi |Kuinka |Miten |Minkä |Mitkä |Montako | |Millä |Keitä |Kenen |Minkälainen |Koska |Millaista \\b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Scoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = b605.17\n",
      "# text = Miksi aina silloin, kun oikeasti tarvitsee jotain, ei löydä mitään?\n",
      "─┮                                                                                                    \n",
      " │ ╭─╼       \u001b[32m1\u001b[0m  \u001b[33mMiksi\u001b[0m     \u001b[31mADV\u001b[0m   \u001b[34madvmod\u001b[0m _\u001b[0m                                                               _\u001b[0m\n",
      " │ │     ╭─╼ \u001b[32m2\u001b[0m  \u001b[33maina\u001b[0m      \u001b[31mADV\u001b[0m   \u001b[34madvmod\u001b[0m _\u001b[0m                                                               _\u001b[0m\n",
      " │ │   ╭─┶   \u001b[32m3\u001b[0m  \u001b[33msilloin\u001b[0m   \u001b[31mADV\u001b[0m   \u001b[34madvmod\u001b[0m _\u001b[0m                                                               SpaceAfter=No\u001b[0m\n",
      " │ │   ┢─╼   \u001b[32m4\u001b[0m  \u001b[33m,\u001b[0m         \u001b[31mPUNCT\u001b[0m \u001b[34mpunct\u001b[0m  _\u001b[0m                                                               _\u001b[0m\n",
      " │ │ ╭─┶     \u001b[32m5\u001b[0m  \u001b[33mkun\u001b[0m       \u001b[31mSCONJ\u001b[0m \u001b[34mmark\u001b[0m   _\u001b[0m                                                               _\u001b[0m\n",
      " │ │ ┢─╼     \u001b[32m6\u001b[0m  \u001b[33moikeasti\u001b[0m  \u001b[31mADV\u001b[0m   \u001b[34madvmod\u001b[0m Derivation=Sti\u001b[0m                                                  _\u001b[0m\n",
      " │ ┢─┾       \u001b[32m7\u001b[0m  \u001b[33mtarvitsee\u001b[0m \u001b[31mVERB\u001b[0m  \u001b[34madvcl\u001b[0m  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\u001b[0m _\u001b[0m\n",
      " │ │ ┡─╼     \u001b[32m8\u001b[0m  \u001b[33mjotain\u001b[0m    \u001b[31mPRON\u001b[0m  \u001b[34mobj\u001b[0m    Case=Par|Number=Sing|PronType=Ind\u001b[0m                               SpaceAfter=No\u001b[0m\n",
      " │ │ ╰─╼     \u001b[32m9\u001b[0m  \u001b[33m,\u001b[0m         \u001b[31mPUNCT\u001b[0m \u001b[34mpunct\u001b[0m  _\u001b[0m                                                               _\u001b[0m\n",
      " │ ┢─╼       \u001b[32m10\u001b[0m \u001b[33mei\u001b[0m        \u001b[31mAUX\u001b[0m   \u001b[34maux\u001b[0m    Number=Sing|Person=3|Polarity=Neg|VerbForm=Fin|Voice=Act\u001b[0m        _\u001b[0m\n",
      " ╰─┾         \u001b[32m11\u001b[0m \u001b[33mlöydä\u001b[0m     \u001b[31mVERB\u001b[0m  \u001b[34mroot\u001b[0m   Connegative=Yes|Mood=Ind|Tense=Pres|VerbForm=Fin\u001b[0m                _\u001b[0m\n",
      "   ┡─╼       \u001b[32m12\u001b[0m \u001b[33mmitään\u001b[0m    \u001b[31mPRON\u001b[0m  \u001b[34mobj\u001b[0m    Case=Par|Number=Sing|PronType=Ind\u001b[0m                               SpaceAfter=No\u001b[0m\n",
      "   ╰─╼       \u001b[32m13\u001b[0m \u001b[33m?\u001b[0m         \u001b[31mPUNCT\u001b[0m \u001b[34mpunct\u001b[0m  _\u001b[0m                                                               _\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import udapi\n",
    "from udapi.core.document import Document\n",
    "from udapi.block.read.conllu import Conllu\n",
    "from udapi.block.write.html import Html\n",
    "\n",
    "finnish_content = '/home/robin/Research/qtype-eval/src/UD-finnish-questions/content_questions_finnish_UD.conllu'\n",
    "finnish_polar = '/home/robin/Research/qtype-eval/src/UD-finnish-questions/polar_questions_finnish_UD.conllu'\n",
    "\n",
    "doc = udapi.Document(finnish_content)\n",
    "\n",
    "target_tree = next(tree for tree in doc.trees if tree.sent_id == \"b605.17\")\n",
    "\n",
    "target_tree.draw(layout=\"align\", attributes=\"ord,form,upos,deprel,feats,misc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtype-eval-pAepV5Z2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
