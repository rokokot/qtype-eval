{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using UDPipe for POS tagging and Dependency parsing\n",
    "\n",
    "This notebooks describes how we used UDPipe to annotate the TiDy QA data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robin/.pyenv/versions/3.8.20/envs/py38-trankit-env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import io\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-research-datasets/tydiqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df_train_split \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)  \n\u001b[1;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:2782\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:2767\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2766\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2767\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2769\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/formatting/formatting.py:658\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    656\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/formatting/formatting.py:415\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/formatting/formatting.py:491\u001b[0m, in \u001b[0;36mPandasFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 491\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(row)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/datasets/formatting/formatting.py:214\u001b[0m, in \u001b[0;36mPandasArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpandas_types_mapper\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pyarrow/array.pxi:889\u001b[0m, in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pyarrow/table.pxi:5132\u001b[0m, in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/qtype-eval-pAepV5Z2-py3.12/lib/python3.12/site-packages/pyarrow/pandas_compat.py:808\u001b[0m, in \u001b[0;36mtable_to_dataframe\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    805\u001b[0m columns \u001b[38;5;241m=\u001b[39m _deserialize_column_index(table, all_columns, column_indexes)\n\u001b[1;32m    807\u001b[0m column_names \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m--> 808\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mext_columns_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pandas_api\u001b[38;5;241m.\u001b[39mis_ge_v3():\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataframe_from_blocks\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"google-research-datasets/tydiqa\", \"primary_task\")\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "df_train_split = dataset['train'][:].copy()\n",
    "pd.set_option('display.max_colwidth', 100)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "display(df_train_split.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Можно излечиться от наркотической зависимости полностью?\n",
      "\n",
      "\n",
      "Is Creole a pidgin of French?\n",
      "\n",
      "\n",
      "هل كان خالد بكداش الأمين العالم السابق للحزب الشبوعي السوري ذو أصول كردية؟\n",
      "\n",
      "\n",
      "Были ли канонизированы русские князья Борис и Глеб?\n",
      "\n",
      "\n",
      "كم عدد مرات فوز الأوروغواي ببطولة كاس العالم لكرو القدم؟\n",
      "\n",
      "\n",
      "한국에서 일어난 가장 많은 연쇄살인사건은 무엇인가?\n",
      "\n",
      "\n",
      "2019년까지 월드컵은 몇개국에서 개최되었는가?\n",
      "\n",
      "\n",
      "노브고로드 공화국 입법은 베체 체제인가요?\n",
      "\n",
      "\n",
      "What is a way to increase your wound healing speed?\n",
      "\n",
      "\n",
      "Сколько машин в день проезжает по Крымскому мосту?\n",
      "\n",
      "\n",
      "When did the art deco movement begin?\n",
      "\n",
      "\n",
      "Возможно ли вылечить ВИЧ?\n",
      "\n",
      "\n",
      "Phormの創始者は誰\n",
      "\n",
      "\n",
      "Ovatko eläinrasvat kiellettyjä raaka-aineita luonnonkosmetiikassa?\n",
      "\n",
      "\n",
      "Siapakah yang menemuka benua Amerika ?\n",
      "\n",
      "\n",
      "Onko Kalervo Kummola toiminut kansanedustajana?\n",
      "\n",
      "\n",
      "berapakah jenis ras yang ada didunia?\n",
      "\n",
      "\n",
      "Milloin Charles Fort syntyi?\n",
      "\n",
      "\n",
      "DNAの中にアミノ酸は含まれていますか？\n",
      "\n",
      "\n",
      "ما هي المسألة الشرقية ؟\n",
      "\n",
      "\n",
      "Apakah Baladewa seorang manusia?\n",
      "\n",
      "\n",
      "로마의 면적은 서울시의 2배인가요?\n",
      "\n",
      "\n",
      "Milloin Kokemäki on perustettu?\n",
      "\n",
      "\n",
      "キリスト教はユダヤ人を迫害した？\n",
      "\n",
      "\n",
      "2018年アメリカで一番治安の悪い州はどこ\n",
      "\n",
      "\n",
      "Do zebra finches have stripes?\n",
      "\n",
      "\n",
      "Apakah Learning Management System  memerlukan jaringan internet?\n",
      "\n",
      "\n",
      "هل عدم القيام بجهد جسماني ممكن ان يسبب الأرق؟\n"
     ]
    }
   ],
   "source": [
    "txt_files = glob.glob(\"/home/robin/Research/qtype-eval/TyDi-questions/*.txt\", recursive=True)\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) >= 1:\n",
    "            print('\\n')\n",
    "            print(lines[0].strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://lindat.mff.cuni.cz/services/udpipe/api/process'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing tidy-questions-polar-train-english.txt ...\n",
      "saved parsed output to parsed-tydi-english/tidy-questions-polar-train-english.conllu\n",
      "processing tidy-questions-polar-valid-english.txt ...\n",
      "saved parsed output to parsed-tydi-english/tidy-questions-polar-valid-english.conllu\n",
      "processing tidy-questions-wh-train-english.txt ...\n",
      "saved parsed output to parsed-tydi-english/tidy-questions-wh-train-english.conllu\n",
      "processing tidy-questions-wh-valid-english.txt ...\n",
      "saved parsed output to parsed-tydi-english/tidy-questions-wh-valid-english.conllu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def udpipe(sentence):\n",
    "  \"\"\"\n",
    "  Function which sends a question from .txt file to udpipe for tagging/parsing.\n",
    "  Returns a conllu representation of the data.\n",
    "  \"\"\"\n",
    "\n",
    "  data = {'data': sentence,\n",
    "          'model': 'english',\n",
    "          'tokenizer': '',\n",
    "          'tagger': '',\n",
    "          'parser': ''\n",
    "  }\n",
    "  response = requests.post(API_URL, data=data)\n",
    "  response.raise_for_status()\n",
    "  return response.json()['result']\n",
    "\n",
    "\n",
    "def run_english(files, output):\n",
    "    \"\"\"\n",
    "    Reads .txt files from a list of file paths, processes them via Udpipe v2,\n",
    "    and writes .conllu outputs to the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(f'processing {os.path.basename(file)} ...')\n",
    "\n",
    "        try:\n",
    "            conllu = udpipe(text)\n",
    "            out_file = Path(output, f'{Path(file).stem}.conllu')\n",
    "            with open(out_file, 'w', encoding='utf-8') as out:\n",
    "                out.write(conllu)\n",
    "            print(f'saved parsed output to {out_file}')\n",
    "        except Exception as e:\n",
    "            print(f'error processing {file}: {e}')\n",
    "\n",
    "files = [\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-train-english.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-valid-english.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-train-english.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-valid-english.txt'\n",
    "\n",
    "]\n",
    "run_english(files, 'parsed-tydi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing tidy-questions-polar-train-russian.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-polar-train-russian.conllu\n",
      "processing tidy-questions-polar-valid-russian.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-polar-valid-russian.conllu\n",
      "processing tidy-questions-wh-train-russian.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-wh-train-russian.conllu\n",
      "processing tidy-questions-wh-valid-russian.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-wh-valid-russian.conllu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def udpipe(sentence):\n",
    "  \"\"\"\n",
    "  Function which sends a question from .txt file to udpipe for tagging/parsing.\n",
    "  Returns a conllu representation of the data.\n",
    "  \"\"\"\n",
    "\n",
    "  data = {'data': sentence,\n",
    "          'model': 'russian-gsd-ud-2.15-241121',\n",
    "          'tokenizer': '',\n",
    "          'tagger': '',\n",
    "          'parser': ''\n",
    "  }\n",
    "  response = requests.post(API_URL, data=data)\n",
    "  response.raise_for_status()\n",
    "  return response.json()['result']\n",
    "\n",
    "\n",
    "def run_english(files, output):\n",
    "    \"\"\"\n",
    "    Reads .txt files from a list of file paths, processes them via Udpipe v2,\n",
    "    and writes .conllu outputs to the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(f'processing {os.path.basename(file)} ...')\n",
    "\n",
    "        try:\n",
    "            conllu = udpipe(text)\n",
    "            out_file = Path(output, f'{Path(file).stem}.conllu')\n",
    "            with open(out_file, 'w', encoding='utf-8') as out:\n",
    "                out.write(conllu)\n",
    "            print(f'saved parsed output to {out_file}')\n",
    "        except Exception as e:\n",
    "            print(f'error processing {file}: {e}')\n",
    "\n",
    "files = [\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-train-russian.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-valid-russian.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-train-russian.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-valid-russian.txt'\n",
    "\n",
    "]\n",
    "run_english(files, 'parsed-tydi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing tidy-questions-polar-train-korean.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-polar-train-korean.conllu\n",
      "processing tidy-questions-polar-valid-korean.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-polar-valid-korean.conllu\n",
      "processing tidy-questions-wh-train-korean.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-wh-train-korean.conllu\n",
      "processing tidy-questions-wh-valid-korean.txt ...\n",
      "saved parsed output to parsed-tydi/tidy-questions-wh-valid-korean.conllu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def udpipe(sentence):\n",
    "  \"\"\"\n",
    "  Function which sends a question from .txt file to udpipe for tagging/parsing.\n",
    "  Returns a conllu representation of the data.\n",
    "  \"\"\"\n",
    "\n",
    "  data = {'data': sentence,\n",
    "          'model': 'korean-kaist-ud-2.15-241121',\n",
    "          'tokenizer': '',\n",
    "          'tagger': '',\n",
    "          'parser': ''\n",
    "  }\n",
    "  response = requests.post(API_URL, data=data)\n",
    "  response.raise_for_status()\n",
    "  return response.json()['result']\n",
    "\n",
    "\n",
    "def run_english(files, output):\n",
    "    \"\"\"\n",
    "    Reads .txt files from a list of file paths, processes them via Udpipe v2,\n",
    "    and writes .conllu outputs to the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(f'processing {os.path.basename(file)} ...')\n",
    "\n",
    "        try:\n",
    "            conllu = udpipe(text)\n",
    "            out_file = Path(output, f'{Path(file).stem}.conllu')\n",
    "            with open(out_file, 'w', encoding='utf-8') as out:\n",
    "                out.write(conllu)\n",
    "            print(f'saved parsed output to {out_file}')\n",
    "        except Exception as e:\n",
    "            print(f'error processing {file}: {e}')\n",
    "\n",
    "files = [\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-train-korean.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-polar-valid-korean.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-train-korean.txt',\n",
    "    '/home/robin/Research/qtype-eval/TyDi-questions/tidy-questions-wh-valid-korean.txt'\n",
    "\n",
    "]\n",
    "run_english(files, 'parsed-tydi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame.cache = {}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
