{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tidy-questions-polar-train-russian.txt: 994 lines\n",
      "tidy-questions-polar-train-english.txt: 444 lines\n",
      "tidy-questions-polar-valid-arabic.txt: 98 lines\n",
      "tidy-questions-polar-valid-russian.txt: 195 lines\n",
      "tidy-questions-wh-valid-arabic.txt: 1282 lines\n",
      "tidy-questions-wh-valid-korean.txt: 1633 lines\n",
      "tidy-questions-wh-train-korean.txt: 10685 lines\n",
      "tidy-questions-polar-valid-korean.txt: 65 lines\n",
      "tidy-questions-wh-valid-english.txt: 954 lines\n",
      "tidy-questions-wh-train-russian.txt: 11809 lines\n",
      "tidy-questions-wh-train-english.txt: 8767 lines\n",
      "tidy-questions-wh-valid-russian.txt: 1430 lines\n",
      "tidy-questions-wh-valid-japanese.txt: 1567 lines\n",
      "tidy-questions-polar-valid-finnish.txt: 94 lines\n",
      "tidy-questions-wh-valid-indonesian.txt: 1741 lines\n",
      "tidy-questions-polar-train-finnish.txt: 950 lines\n",
      "tidy-questions-wh-train-indonesian.txt: 14624 lines\n",
      "tidy-questions-wh-train-finnish.txt: 14335 lines\n",
      "tidy-questions-polar-valid-japanese.txt: 142 lines\n",
      "tidy-questions-wh-train-arabic.txt: 21913 lines\n",
      "tidy-questions-polar-train-indonesian.txt: 328 lines\n",
      "tidy-questions-polar-train-korean.txt: 296 lines\n",
      "tidy-questions-wh-valid-finnish.txt: 1988 lines\n",
      "tidy-questions-polar-train-japanese.txt: 1031 lines\n",
      "tidy-questions-wh-train-japanese.txt: 15257 lines\n",
      "tidy-questions-polar-valid-english.txt: 77 lines\n",
      "tidy-questions-polar-valid-indonesian.txt: 64 lines\n",
      "tidy-questions-polar-train-arabic.txt: 1179 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def count_lines_in_files(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    txt_files = folder.glob(\"*.txt\")\n",
    "    \n",
    "    file_line_counts = {}\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "            file_line_counts[txt_file.name] = line_count\n",
    "    \n",
    "    return file_line_counts\n",
    "\n",
    "folder_path = '/home/robin/Research/qtype-eval/data/TyDi-questions'\n",
    "line_counts = count_lines_in_files(folder_path)\n",
    "\n",
    "for file_name, line_count in line_counts.items():\n",
    "    print(f\"{file_name}: {line_count} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from conllu import parse\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Length Statistics for CoNLL-U Files\n",
      "==================================================\n",
      "\n",
      "File: questions_ko_kaist-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     29\n",
      "Average length:      9.9\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         22\n",
      "  10 tokens:         12\n",
      "  15 tokens:         5\n",
      "  20 tokens:         1\n",
      "\n",
      "File: questions_ar_padt-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     7\n",
      "Average length:      101.4\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         7\n",
      "  10 tokens:         6\n",
      "  15 tokens:         6\n",
      "  20 tokens:         6\n",
      "\n",
      "File: questions_ru_taiga-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     53\n",
      "Average length:      8.8\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         35\n",
      "  10 tokens:         12\n",
      "  15 tokens:         4\n",
      "  20 tokens:         2\n",
      "\n",
      "File: questions_id_gsd-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     478\n",
      "Average length:      13.8\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         424\n",
      "  10 tokens:         262\n",
      "  15 tokens:         140\n",
      "  20 tokens:         71\n",
      "\n",
      "File: questions_fi_tdt-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     59\n",
      "Average length:      11.3\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         49\n",
      "  10 tokens:         25\n",
      "  15 tokens:         11\n",
      "  20 tokens:         7\n",
      "\n",
      "File: questions_en_ewt-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     754\n",
      "Average length:      13.6\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         642\n",
      "  10 tokens:         410\n",
      "  15 tokens:         229\n",
      "  20 tokens:         131\n",
      "\n",
      "File: questions_ja_gsd-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     4\n",
      "Average length:      14.5\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         4\n",
      "  10 tokens:         4\n",
      "  15 tokens:         1\n",
      "  20 tokens:         1\n",
      "\n",
      "File: questions_ru_taiga-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     1287\n",
      "Average length:      12.7\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         1001\n",
      "  10 tokens:         511\n",
      "  15 tokens:         276\n",
      "  20 tokens:         162\n",
      "\n",
      "File: questions_ru_taiga-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     26\n",
      "Average length:      9.3\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         18\n",
      "  10 tokens:         7\n",
      "  15 tokens:         2\n",
      "  20 tokens:         2\n",
      "\n",
      "File: questions_ja_gsd-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     11\n",
      "Average length:      15.4\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         9\n",
      "  10 tokens:         6\n",
      "  15 tokens:         5\n",
      "  20 tokens:         3\n",
      "\n",
      "File: questions_ko_kaist-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     421\n",
      "Average length:      10.3\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         327\n",
      "  10 tokens:         181\n",
      "  15 tokens:         75\n",
      "  20 tokens:         16\n",
      "\n",
      "File: questions_en_ewt-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     163\n",
      "Average length:      11.9\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         128\n",
      "  10 tokens:         67\n",
      "  15 tokens:         38\n",
      "  20 tokens:         25\n",
      "\n",
      "File: questions_id_gsd-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     63\n",
      "Average length:      13.4\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         54\n",
      "  10 tokens:         33\n",
      "  15 tokens:         17\n",
      "  20 tokens:         9\n",
      "\n",
      "File: questions_ko_kaist-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     41\n",
      "Average length:      10.0\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         35\n",
      "  10 tokens:         15\n",
      "  15 tokens:         6\n",
      "  20 tokens:         0\n",
      "\n",
      "File: questions_en_ewt-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     166\n",
      "Average length:      11.9\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         131\n",
      "  10 tokens:         77\n",
      "  15 tokens:         37\n",
      "  20 tokens:         21\n",
      "\n",
      "File: questions_fi_tdt-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     40\n",
      "Average length:      11.1\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         32\n",
      "  10 tokens:         17\n",
      "  15 tokens:         4\n",
      "  20 tokens:         1\n",
      "\n",
      "File: questions_fi_tdt-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     353\n",
      "Average length:      9.5\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         234\n",
      "  10 tokens:         109\n",
      "  15 tokens:         41\n",
      "  20 tokens:         22\n",
      "\n",
      "File: questions_ar_padt-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     65\n",
      "Average length:      57.7\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         64\n",
      "  10 tokens:         61\n",
      "  15 tokens:         55\n",
      "  20 tokens:         48\n",
      "\n",
      "File: questions_ja_gsd-ud-train.conllu\n",
      "------------------------------\n",
      "Total sentences:     42\n",
      "Average length:      12.4\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         38\n",
      "  10 tokens:         19\n",
      "  15 tokens:         11\n",
      "  20 tokens:         5\n",
      "\n",
      "File: questions_ar_padt-ud-dev.conllu\n",
      "------------------------------\n",
      "Total sentences:     38\n",
      "Average length:      22.2\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         37\n",
      "  10 tokens:         23\n",
      "  15 tokens:         17\n",
      "  20 tokens:         9\n",
      "\n",
      "File: questions_id_gsd-ud-test.conllu\n",
      "------------------------------\n",
      "Total sentences:     65\n",
      "Average length:      13.4\n",
      "\n",
      "Sentences longer than:\n",
      "  5  tokens:         59\n",
      "  10 tokens:         43\n",
      "  15 tokens:         24\n",
      "  20 tokens:         9\n"
     ]
    }
   ],
   "source": [
    "def analyze_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sentences = list(parse(f.read()))\n",
    "        \n",
    "    lengths = [len(sent) for sent in sentences]\n",
    "    \n",
    "    stats = {\n",
    "        'total_sentences': len(sentences),\n",
    "        'avg_length': mean(lengths) if lengths else 0,\n",
    "        'over_5': sum(1 for x in lengths if x > 5),\n",
    "        'over_10': sum(1 for x in lengths if x > 10),\n",
    "        'over_15': sum(1 for x in lengths if x > 15),\n",
    "        'over_20': sum(1 for x in lengths if x > 20)\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    print(\"\\nSentence Length Statistics for CoNLL-U Files\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for file in Path('/home/robin/Research/qtype-eval/UD-questions').glob('*.conllu'):\n",
    "        try:\n",
    "            stats = analyze_file(file)\n",
    "            \n",
    "            print(f\"\\nFile: {file.name}\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Total sentences:     {stats['total_sentences']}\")\n",
    "            print(f\"Average length:      {stats['avg_length']:.1f}\")\n",
    "            print(\"\\nSentences longer than:\")\n",
    "            print(f\"  5  tokens:         {stats['over_5']}\")\n",
    "            print(f\"  10 tokens:         {stats['over_10']}\")\n",
    "            print(f\"  15 tokens:         {stats['over_15']}\")\n",
    "            print(f\"  20 tokens:         {stats['over_20']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file.name}: {str(e)}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out UDon2 for projectivity visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "      for sentence in parse_incr(f):\n",
    "        sent_id = sentence.metadata.get('sent_id', '')\n",
    "        text = sentence.metadata.get('text', '')\n",
    "\n",
    "        for file, function in functions.items():\n",
    "          score = function(sentence)\n",
    "          results[file].append({'sentence_id': sent_id, 'text': text, 'score': score})\n",
    "\n",
    "\n",
    "    for file, result in results.items():\n",
    "      df = pd.DataFrame(result)\n",
    "      out_path = self.output_dir / file\n",
    "      df.to_csv(out_path, index=False)\n",
    "      print(f'save {out_path} ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityScore:\n",
    "\n",
    "  def __init__(self, input_file, output_dir):\n",
    "    self.input_file = input_file\n",
    "    self.output_dir = Path(output_dir)\n",
    "    self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################\n",
    "  def graph(self, sentence):\n",
    "    pass\n",
    "\n",
    "  \n",
    "############################################\n",
    "  def clauses(self, sentence):\n",
    "    clausal_dependencies = ['acl:relcl', 'advcl', 'ccomp', 'xcomp', 'csubj', 'parataxis']\n",
    "    clauses = 1\n",
    "\n",
    "    for token in sentence:\n",
    "      if token['deprel'] in clausal_dependencies:\n",
    "        clauses += 1\n",
    "\n",
    "    return clauses\n",
    "\n",
    "############################################\n",
    "  def arguments(sentence):\n",
    "    pass\n",
    "############################################\n",
    "  def distance(self, sentence):\n",
    "    words = [token for token in sentence if token['upos'] != 'PUNCT' and isinstance(token['id'], int)]\n",
    "\n",
    "    roots = [token for token in words if token['deprel'] == 'root']\n",
    "    if len(roots) != 1:\n",
    "      return None\n",
    "  \n",
    "    distance = 0\n",
    "    for token in words:\n",
    "      if token['deprel'] != 'root':\n",
    "        if token['deprel'] in ['flat:name', 'fixed', 'goeswith']:\n",
    "          continue\n",
    "\n",
    "        token_id = int(token['id'])\n",
    "        head_id = int(token['head'])\n",
    "        length = abs(token_id - head_id)\n",
    "\n",
    "        distance += length\n",
    "  \n",
    "    return distance\n",
    "  \n",
    "############################################\n",
    "  def depth(self, sentence):\n",
    "    depths = {0: 0}  # Root has depth 0\n",
    "            \n",
    "    # Process all tokens\n",
    "    tokens = [token for token in sentence if isinstance(token['id'], int)]\n",
    "    while tokens:\n",
    "        # Find tokens whose head's depth is known\n",
    "        for token in tokens[:]:  # Copy list for iteration\n",
    "            head_id = token['head']\n",
    "            if head_id in depths:\n",
    "                # Skip tokens that are part of fixed expressions\n",
    "                if token['deprel'] in ['flat:name', 'fixed', 'goeswith']:\n",
    "                    depths[token['id']] = depths[head_id]\n",
    "                else:\n",
    "                    depths[token['id']] = depths[head_id] + 1\n",
    "                tokens.remove(token)\n",
    "                \n",
    "    # Return maximum depth\n",
    "    return max(depths.values()) if depths else 0\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "  def projectivity(sentence):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def run(self):\n",
    "    # 'clause_count.csv': self.clauses\n",
    "    # 'dep_distance.csv': self.distance\n",
    "    functions = {'tree_depth.csv': self.depth}\n",
    "    results = {name: [] for name in functions.keys()}\n",
    "\n",
    "    doc = Document()\n",
    "    reader = Conllu(files=[self.input_file])\n",
    "    reader.process_document(doc)\n",
    "\n",
    "    for tree in doc.trees:\n",
    "      sentence = []\n",
    "      for node in tree.descendants:\n",
    "        token = {\n",
    "                    'id': node.ord,\n",
    "                    'form': node.form,\n",
    "                    'lemma': node.lemma,\n",
    "                    'upos': node.upos,\n",
    "                    'xpos': node.xpos,\n",
    "                    'feats': node.feats,\n",
    "                    'head': node.parent.ord if node.parent else 0,\n",
    "                    'deprel': node.deprel,\n",
    "                    'deps': node.deps,\n",
    "                    'misc': node.misc\n",
    "                }\n",
    "        \n",
    "        sentence.append(token)\n",
    "\n",
    "      sent_id = tree.sent_id\n",
    "      text = tree.text\n",
    "\n",
    "      for file, function in functions.items():\n",
    "          score = function(sentence)\n",
    "          results[file].append({\n",
    "              'sentence_id': sent_id,\n",
    "              'text': text,\n",
    "              'score': score\n",
    "          })\n",
    "\n",
    "    for file, result in results.items():\n",
    "        df = pd.DataFrame(result)\n",
    "        out_path = self.output_dir / file\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f'save {out_path} ok')\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "  input_file = '/home/robin/Research/qtype-eval/src/UD-finnish-questions/content_questions_finnish_UD.conllu'\n",
    "  output_dir = '/home/robin/Research/qtype-eval/src/UD-finnish-questions/scores_content'\n",
    "  scorer = ComplexityScore(input_file, output_dir)\n",
    "  scorer.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-trankit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
