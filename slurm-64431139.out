SLURM_JOB_ID: 64431139
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed Apr 30 16:48:16 CEST 2025
Walltime: 00-00:30:00
========================================================================
========= Running main tasks experiments ==========
Processing language: ar
Processing layer: 2
===============================================================
Running question_type experiment for language ar, layer 2
===============================================================
Command: python -m src.experiments.run_experiment             "hydra.job.chdir=False"             "hydra.run.dir=."             "experiment=question_type"             "experiment.tasks=question_type"             "model=lm_probe"             "model.model_type=lm_probe"             "model.lm_name=cis-lmu/glot500-base"             "model.layer_wise=true"             "model.layer_index=2"             "model.freeze_model=true"             "model.probe_hidden_size=96"             "data.languages=[ar]"             "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"             "training.task_type=classification"             "training.num_epochs=15"             "training.lr=1e-4"             "training.batch_size=16"             "+training.gradient_accumulation_steps=2"             "experiment_name=layer_2_question_type_ar"             "output_dir=/scratch/leuven/371/vsc37132/probing_output/ar/layer_2/question_type"             "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-30 16:48:30,850][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probing_output/ar/layer_2/question_type
experiment_name: layer_2_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-30 16:48:30,851][__main__][INFO] - Normalized task: question_type
[2025-04-30 16:48:30,851][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-30 16:48:30,851][__main__][INFO] - Determined Task Type: classification
[2025-04-30 16:48:30,854][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-30 16:48:30,855][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-30 16:48:32,857][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-30 16:48:35,076][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-30 16:48:35,077][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 16:48:35,146][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,188][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,303][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-30 16:48:35,310][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 16:48:35,311][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-30 16:48:35,311][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 16:48:35,331][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,362][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,376][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-30 16:48:35,377][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 16:48:35,378][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-30 16:48:35,379][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-30 16:48:35,399][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,430][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-30 16:48:35,442][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-30 16:48:35,443][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-30 16:48:35,443][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-30 16:48:35,444][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-30 16:48:35,445][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 16:48:35,445][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 16:48:35,445][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 16:48:35,446][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-30 16:48:35,446][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Sample label: 1
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 16:48:35,446][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 16:48:35,447][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-30 16:48:35,447][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-30 16:48:35,447][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-30 16:48:35,447][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-30 16:48:35,447][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-30 16:48:35,448][src.data.datasets][INFO] - Sample label: 0
[2025-04-30 16:48:35,448][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-30 16:48:35,448][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-30 16:48:35,448][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-30 16:48:35,448][__main__][INFO] - Using model type: lm_probe
[2025-04-30 16:48:35,448][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-30 16:48:39,776][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-30 16:48:39,777][src.models.model_factory][INFO] - Language model parameters trainable
[2025-04-30 16:48:39,777][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=False
[2025-04-30 16:48:39,777][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-04-30 16:48:39,782][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-04-30 16:48:39,782][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-04-30 16:48:39,782][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.3
[2025-04-30 16:48:39,782][__main__][INFO] - Successfully created lm_probe model for ar
[2025-04-30 16:48:39,783][__main__][INFO] - Total parameters: 394,712,833
[2025-04-30 16:48:39,783][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.8195Epoch 1/15: [                              ] 2/63 batches, loss: 0.8474Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7759Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7541Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7527Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7570Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7435Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7401Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7283Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7373Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7325Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7307Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7240Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7271Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7255Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7256Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7268Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7284Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7260Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7229Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7208Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7196Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7174Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7162Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7152Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7144Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7141Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7126Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7124Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7114Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7098Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7095Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7088Epoch 1/15: [================              ] 34/63 batches, loss: 0.7085Epoch 1/15: [================              ] 35/63 batches, loss: 0.7076Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7078Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7065Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7038Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7022Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7003Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6979Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6965Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6940Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6920Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6895Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6875Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6846Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6800Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6767Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6724Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6688Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6675Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6641Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6611Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6592Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6560Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6546Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6508Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6483Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6451Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6420Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6390Epoch 1/15: [==============================] 63/63 batches, loss: 0.6379
[2025-04-30 16:48:46,144][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6379
[2025-04-30 16:48:46,334][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.5220, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.5510Epoch 2/15: [                              ] 2/63 batches, loss: 0.5510Epoch 2/15: [=                             ] 3/63 batches, loss: 0.5351Epoch 2/15: [=                             ] 4/63 batches, loss: 0.5153Epoch 2/15: [==                            ] 5/63 batches, loss: 0.5130Epoch 2/15: [==                            ] 6/63 batches, loss: 0.5114Epoch 2/15: [===                           ] 7/63 batches, loss: 0.5170Epoch 2/15: [===                           ] 8/63 batches, loss: 0.5183Epoch 2/15: [====                          ] 9/63 batches, loss: 0.5140Epoch 2/15: [====                          ] 10/63 batches, loss: 0.5106Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.5164Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.5113Epoch 2/15: [======                        ] 13/63 batches, loss: 0.5034Epoch 2/15: [======                        ] 14/63 batches, loss: 0.5000Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.4987Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.5034Epoch 2/15: [========                      ] 17/63 batches, loss: 0.5006Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4994Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.5009Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.5010Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.5000Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.5034Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.5034Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.5014Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.5043Epoch 2/15: [============                  ] 26/63 batches, loss: 0.5088Epoch 2/15: [============                  ] 27/63 batches, loss: 0.5095Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.5084Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.5066Epoch 2/15: [==============                ] 30/63 batches, loss: 0.5033Epoch 2/15: [==============                ] 31/63 batches, loss: 0.5033Epoch 2/15: [===============               ] 32/63 batches, loss: 0.5041Epoch 2/15: [===============               ] 33/63 batches, loss: 0.5048Epoch 2/15: [================              ] 34/63 batches, loss: 0.5054Epoch 2/15: [================              ] 35/63 batches, loss: 0.5061Epoch 2/15: [=================             ] 36/63 batches, loss: 0.5047Epoch 2/15: [=================             ] 37/63 batches, loss: 0.5040Epoch 2/15: [==================            ] 38/63 batches, loss: 0.5040Epoch 2/15: [==================            ] 39/63 batches, loss: 0.5033Epoch 2/15: [===================           ] 40/63 batches, loss: 0.5033Epoch 2/15: [===================           ] 41/63 batches, loss: 0.5033Epoch 2/15: [====================          ] 42/63 batches, loss: 0.5056Epoch 2/15: [====================          ] 43/63 batches, loss: 0.5055Epoch 2/15: [====================          ] 44/63 batches, loss: 0.5044Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.5033Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.5038Epoch 2/15: [======================        ] 47/63 batches, loss: 0.5023Epoch 2/15: [======================        ] 48/63 batches, loss: 0.5008Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.5024Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.5024Epoch 2/15: [========================      ] 51/63 batches, loss: 0.5024Epoch 2/15: [========================      ] 52/63 batches, loss: 0.5015Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.5029Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.5033Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.5037Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.5046Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.5050Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.5050Epoch 2/15: [============================  ] 59/63 batches, loss: 0.5045Epoch 2/15: [============================  ] 60/63 batches, loss: 0.5049Epoch 2/15: [============================= ] 61/63 batches, loss: 0.5049Epoch 2/15: [============================= ] 62/63 batches, loss: 0.5037Epoch 2/15: [==============================] 63/63 batches, loss: 0.5027
[2025-04-30 16:48:50,272][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5027
[2025-04-30 16:48:50,498][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5217, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5507Epoch 3/15: [                              ] 2/63 batches, loss: 0.5270Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5112Epoch 3/15: [=                             ] 4/63 batches, loss: 0.4914Epoch 3/15: [==                            ] 5/63 batches, loss: 0.4795Epoch 3/15: [==                            ] 6/63 batches, loss: 0.4676Epoch 3/15: [===                           ] 7/63 batches, loss: 0.4795Epoch 3/15: [===                           ] 8/63 batches, loss: 0.4884Epoch 3/15: [====                          ] 9/63 batches, loss: 0.4953Epoch 3/15: [====                          ] 10/63 batches, loss: 0.4985Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5033Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.4953Epoch 3/15: [======                        ] 13/63 batches, loss: 0.4941Epoch 3/15: [======                        ] 14/63 batches, loss: 0.4982Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.4953Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.4943Epoch 3/15: [========                      ] 17/63 batches, loss: 0.4949Epoch 3/15: [========                      ] 18/63 batches, loss: 0.4927Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.4920Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.4902Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.4931Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.4935Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.4950Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.4953Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.4976Epoch 3/15: [============                  ] 26/63 batches, loss: 0.4978Epoch 3/15: [============                  ] 27/63 batches, loss: 0.4997Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5033Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5041Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5017Epoch 3/15: [==============                ] 31/63 batches, loss: 0.4994Epoch 3/15: [===============               ] 32/63 batches, loss: 0.4995Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5004Epoch 3/15: [================              ] 34/63 batches, loss: 0.5019Epoch 3/15: [================              ] 35/63 batches, loss: 0.5039Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5033Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5033Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5033Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5014Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5015Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5021Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5021Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5010Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5032Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5022Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5027Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5022Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5032Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5032Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5028Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5014Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5014Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5024Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5024Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5041Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5037Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5032Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5032Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5037Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5032Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5040Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5029Epoch 3/15: [==============================] 63/63 batches, loss: 0.5059
[2025-04-30 16:48:54,476][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5059
[2025-04-30 16:48:54,706][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5217, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.4083Epoch 4/15: [                              ] 2/63 batches, loss: 0.4439Epoch 4/15: [=                             ] 3/63 batches, loss: 0.4478Epoch 4/15: [=                             ] 4/63 batches, loss: 0.4558Epoch 4/15: [==                            ] 5/63 batches, loss: 0.4605Epoch 4/15: [==                            ] 6/63 batches, loss: 0.4676Epoch 4/15: [===                           ] 7/63 batches, loss: 0.4761Epoch 4/15: [===                           ] 8/63 batches, loss: 0.4736Epoch 4/15: [====                          ] 9/63 batches, loss: 0.4769Epoch 4/15: [====                          ] 10/63 batches, loss: 0.4819Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.4773Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.4795Epoch 4/15: [======                        ] 13/63 batches, loss: 0.4831Epoch 4/15: [======                        ] 14/63 batches, loss: 0.4846Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.4858Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.4854Epoch 4/15: [========                      ] 17/63 batches, loss: 0.4851Epoch 4/15: [========                      ] 18/63 batches, loss: 0.4835Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.4857Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.4854Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.4874Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.4914Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.4950Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.4943Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.4966Epoch 4/15: [============                  ] 26/63 batches, loss: 0.4950Epoch 4/15: [============                  ] 27/63 batches, loss: 0.4944Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.4981Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.4959Epoch 4/15: [==============                ] 30/63 batches, loss: 0.4969Epoch 4/15: [==============                ] 31/63 batches, loss: 0.4979Epoch 4/15: [===============               ] 32/63 batches, loss: 0.4973Epoch 4/15: [===============               ] 33/63 batches, loss: 0.4968Epoch 4/15: [================              ] 34/63 batches, loss: 0.4983Epoch 4/15: [================              ] 35/63 batches, loss: 0.4985Epoch 4/15: [=================             ] 36/63 batches, loss: 0.4980Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5000Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5001Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5014Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5009Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5027Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5021Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5032Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5038Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5048Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5063Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5048Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5027Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5018Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5018Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5028Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5014Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5019Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5024Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5019Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5024Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5037Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5036Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5040Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5040Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5032Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5032Epoch 4/15: [==============================] 63/63 batches, loss: 0.5042
[2025-04-30 16:48:58,618][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5042
[2025-04-30 16:48:58,854][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5217, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6219Epoch 5/15: [                              ] 2/63 batches, loss: 0.6101Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5745Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5566Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5412Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5270Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5134Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5151Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5138Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5151Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5097Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5092Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5087Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5083Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5032Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5032Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5074Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5059Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5045Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.4985Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.4987Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.4968Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.4991Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.4993Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.4985Epoch 5/15: [============                  ] 26/63 batches, loss: 0.4996Epoch 5/15: [============                  ] 27/63 batches, loss: 0.4997Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.4990Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5016Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5040Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5063Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5069Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5061Epoch 5/15: [================              ] 34/63 batches, loss: 0.5060Epoch 5/15: [================              ] 35/63 batches, loss: 0.5073Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5039Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5032Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5026slurmstepd: error: *** JOB 64431139 ON k28i22 CANCELLED AT 2025-04-30T16:49:01 ***
