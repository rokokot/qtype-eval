SLURM_JOB_ID: 64423787
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Tue Apr 29 21:24:01 CEST 2025
Walltime: 00-00:30:00
========================================================================
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Tue Apr 29 21:24:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:17:00.0 Off |                    0 |
| N/A   34C    P0             45W /  300W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Phase 1: Running key validation experiments
Running question_type experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:24:23,984][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type
experiment_name: layer_6_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:24:23,984][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:24:23,984][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:24:23,984][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:24:23,988][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 21:24:23,989][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:24:26,805][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:24:29,093][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:24:29,093][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:24:29,207][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,262][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,395][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 21:24:29,404][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:24:29,404][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 21:24:29,406][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:24:29,436][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,480][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,498][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 21:24:29,499][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:24:29,499][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 21:24:29,501][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:24:29,530][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:24:29,590][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 21:24:29,591][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:24:29,592][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 21:24:29,593][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 21:24:29,593][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:24:29,593][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:24:29,593][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:24:29,593][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:24:29,593][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 21:24:29,594][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:24:29,594][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:24:29,594][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 21:24:29,594][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:24:29,595][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:24:29,595][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 21:24:29,595][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:24:29,596][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:24:29,596][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:24:34,748][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:24:34,749][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:24:34,750][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:24:34,750][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 21:24:34,751][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:24:34,751][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6717Epoch 1/15: [                              ] 2/75 batches, loss: 0.6933Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7115Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7180Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7115Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7107Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7149Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7131Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7094Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7081Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7112Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7113Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7105Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7101Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7083Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7094Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7084Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7095Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7091Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7087Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7073Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7061Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7058Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7044Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.7052Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.7052Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.7065Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.7059Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.7065Epoch 1/15: [============                  ] 30/75 batches, loss: 0.7055Epoch 1/15: [============                  ] 31/75 batches, loss: 0.7038Epoch 1/15: [============                  ] 32/75 batches, loss: 0.7043Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.7043Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.7038Epoch 1/15: [==============                ] 35/75 batches, loss: 0.7027Epoch 1/15: [==============                ] 36/75 batches, loss: 0.7019Epoch 1/15: [==============                ] 37/75 batches, loss: 0.7017Epoch 1/15: [===============               ] 38/75 batches, loss: 0.7013Epoch 1/15: [===============               ] 39/75 batches, loss: 0.7006Epoch 1/15: [================              ] 40/75 batches, loss: 0.7011Epoch 1/15: [================              ] 41/75 batches, loss: 0.7021Epoch 1/15: [================              ] 42/75 batches, loss: 0.7025Epoch 1/15: [=================             ] 43/75 batches, loss: 0.7014Epoch 1/15: [=================             ] 44/75 batches, loss: 0.7014Epoch 1/15: [==================            ] 45/75 batches, loss: 0.7010Epoch 1/15: [==================            ] 46/75 batches, loss: 0.7005Epoch 1/15: [==================            ] 47/75 batches, loss: 0.7007Epoch 1/15: [===================           ] 48/75 batches, loss: 0.7003Epoch 1/15: [===================           ] 49/75 batches, loss: 0.7000Epoch 1/15: [====================          ] 50/75 batches, loss: 0.7002Epoch 1/15: [====================          ] 51/75 batches, loss: 0.7004Epoch 1/15: [====================          ] 52/75 batches, loss: 0.7007Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.7003Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.7003Epoch 1/15: [======================        ] 55/75 batches, loss: 0.7001Epoch 1/15: [======================        ] 56/75 batches, loss: 0.7005Epoch 1/15: [======================        ] 57/75 batches, loss: 0.7005Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.7008Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.7007Epoch 1/15: [========================      ] 60/75 batches, loss: 0.7009Epoch 1/15: [========================      ] 61/75 batches, loss: 0.7008Epoch 1/15: [========================      ] 62/75 batches, loss: 0.7005Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6999Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6998Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6993Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6991Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6993Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6996Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6997Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6998Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6993Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6991Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6991Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6989Epoch 1/15: [==============================] 75/75 batches, loss: 0.7000
[2025-04-29 21:24:39,900][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7000
[2025-04-29 21:24:40,130][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6946Epoch 2/15: [                              ] 2/75 batches, loss: 0.6921Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6902Epoch 2/15: [=                             ] 4/75 batches, loss: 0.7040Epoch 2/15: [==                            ] 5/75 batches, loss: 0.7034Epoch 2/15: [==                            ] 6/75 batches, loss: 0.7036Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6988Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6964Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6928Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6955Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6967Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6939Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6953Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6966Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6970Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6963Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6981Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.7004Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.7004Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6993Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6998Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6979Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6975Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6981Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6961Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6963Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6957Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6960Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6964Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6971Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6957Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6953Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6952Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6945Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6948Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6957Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6956Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6954Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6953Epoch 2/15: [================              ] 40/75 batches, loss: 0.6947Epoch 2/15: [================              ] 41/75 batches, loss: 0.6958Epoch 2/15: [================              ] 42/75 batches, loss: 0.6957Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6955Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6951Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6951Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6946Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6951Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6950Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6950Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6954Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6947Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6948Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6947Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6941Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6947Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6948Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6946Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6951Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6960Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6964Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6964Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6960Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6962Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6965Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6966Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6968Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6968Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6972Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6968Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6967Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6965Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6962Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6963Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6965Epoch 2/15: [==============================] 75/75 batches, loss: 0.6966
[2025-04-29 21:24:42,775][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6966
[2025-04-29 21:24:43,006][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6671Epoch 3/15: [                              ] 2/75 batches, loss: 0.6853Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6892Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6919Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6974Epoch 3/15: [==                            ] 6/75 batches, loss: 0.7006Epoch 3/15: [==                            ] 7/75 batches, loss: 0.7022Epoch 3/15: [===                           ] 8/75 batches, loss: 0.7011Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6981Epoch 3/15: [====                          ] 10/75 batches, loss: 0.7009Epoch 3/15: [====                          ] 11/75 batches, loss: 0.7013Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6992Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6997Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6989Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6984Epoch 3/15: [======                        ] 16/75 batches, loss: 0.7000Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6991Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6976Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6980Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6976Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6979Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6975Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6982Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6987Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6983Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6978Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6974Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6973Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6976Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6978Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6983Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6981Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6983Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6986Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6987Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6988Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6984Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6980Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6976Epoch 3/15: [================              ] 40/75 batches, loss: 0.6976Epoch 3/15: [================              ] 41/75 batches, loss: 0.6975Epoch 3/15: [================              ] 42/75 batches, loss: 0.6973Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6967Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6962Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6961Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6962Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6970Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6972Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6972Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6973Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6970Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6969Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6972Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6976Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6978Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6976Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6975Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6971Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6967Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6966Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6965Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6969Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6969Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6969Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6968Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6969Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6969Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6969Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6969Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6968Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6968Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6967Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6967Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6966Epoch 3/15: [==============================] 75/75 batches, loss: 0.6966
[2025-04-29 21:24:45,666][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6966
[2025-04-29 21:24:45,902][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.7037Epoch 4/15: [                              ] 2/75 batches, loss: 0.6997Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6957Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6925Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6928Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6927Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6920Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6922Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6904Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6898Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6920Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6938Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6913Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6908Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6899Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6890Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6899Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6891Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6895Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6909Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6908Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6912Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6913Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6907Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6907Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6905Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6911Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6903Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6904Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6905Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6912Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6917Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6915Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6917Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6918Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6923Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6923Epoch 4/15: [================              ] 40/75 batches, loss: 0.6924Epoch 4/15: [================              ] 41/75 batches, loss: 0.6916Epoch 4/15: [================              ] 42/75 batches, loss: 0.6916Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6914Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6920Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6920Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6918Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6917Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6925Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6925Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6927Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6928Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6925Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6923Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6920Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6919Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6918Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6921Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6920Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6922Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6927Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6927Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6928Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6928Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6929Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6934Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6936Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6934Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 4/15: [==============================] 75/75 batches, loss: 0.6931
[2025-04-29 21:24:48,514][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6931
[2025-04-29 21:24:48,767][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.7009Epoch 5/15: [                              ] 2/75 batches, loss: 0.6937Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6975Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6915Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6944Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6912Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6941Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6925Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6958Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6945Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6920Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6911Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6914Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6920Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6914Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6899Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6912Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6917Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6915Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6921Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6925Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6922Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6926Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6919Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6916Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6919Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6918Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6909Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6914Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6919Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6916Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6918Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6914Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6916Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6915Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6916Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6915Epoch 5/15: [================              ] 40/75 batches, loss: 0.6913Epoch 5/15: [================              ] 41/75 batches, loss: 0.6918Epoch 5/15: [================              ] 42/75 batches, loss: 0.6916Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6916Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6920Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6921Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6919Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6920Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6917Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6915Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6917Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6919Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6920Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6922Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6920Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6918Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6918Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6916Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6916Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6915Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6916Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6916Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6918Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6916Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6917Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6922Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6925Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6927Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6924Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6926Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6923Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6923Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6923Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6926Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6926Epoch 5/15: [==============================] 75/75 batches, loss: 0.6921
[2025-04-29 21:24:51,371][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6921
[2025-04-29 21:24:51,622][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6730Epoch 6/15: [                              ] 2/75 batches, loss: 0.6721Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6806Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6851Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6892Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6895Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6920Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6939Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6915Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6923Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6939Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6926Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6926Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6922Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6936Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6923Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6933Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6946Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6940Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6940Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6944Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6940Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6942Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6933Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6937Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6937Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6941Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6950Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6952Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6951Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6949Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6948Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6957Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6957Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6958Epoch 6/15: [================              ] 40/75 batches, loss: 0.6958Epoch 6/15: [================              ] 41/75 batches, loss: 0.6958Epoch 6/15: [================              ] 42/75 batches, loss: 0.6957Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6958Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6967Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6965Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6962Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6962Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6964Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6965Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6962Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6962Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6960Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6958Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6962Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6957Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6959Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6956Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6952Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6948Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6948Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6947Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6944Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6946Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6945Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6945Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6943Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6943Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6941Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6942Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6940Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6937Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6934Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6934Epoch 6/15: [==============================] 75/75 batches, loss: 0.6935
[2025-04-29 21:24:54,229][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6935
[2025-04-29 21:24:54,468][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.7075Epoch 7/15: [                              ] 2/75 batches, loss: 0.6905Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6933Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6906Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6917Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6920Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6924Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6937Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6946Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6937Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6946Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6948Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6944Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6941Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6920Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6921Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6922Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6908Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6908Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6905Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6915Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6928Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6933Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6925Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6920Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6924Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6924Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6924Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6927Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6928Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6937Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6942Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6945Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6946Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6952Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6951Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6951Epoch 7/15: [================              ] 40/75 batches, loss: 0.6948Epoch 7/15: [================              ] 41/75 batches, loss: 0.6947Epoch 7/15: [================              ] 42/75 batches, loss: 0.6943Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6943Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6944Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6943Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6946Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6944Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6947Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6943Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6945Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6944Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6944Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6944Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6944Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6941Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6944Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6944Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6942Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6940Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6941Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6943Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6942Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6946Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6944Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6945Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6944Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6943Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6947Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6948Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6948Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6950Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6950Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6949Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6948Epoch 7/15: [==============================] 75/75 batches, loss: 0.6950
[2025-04-29 21:24:57,161][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6950
[2025-04-29 21:24:57,421][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6867Epoch 8/15: [                              ] 2/75 batches, loss: 0.6903Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6886Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6945Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6921Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6940Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6947Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6941Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6937Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6910Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6903Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6912Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6912Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6901Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6907Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6906Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6900Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6906Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6908Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6904Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6901Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6902Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6907Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6905Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6917Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6941Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6945Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6948Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6948Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6952Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6958Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6955Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6956Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6959Epoch 8/15: [================              ] 40/75 batches, loss: 0.6959Epoch 8/15: [================              ] 41/75 batches, loss: 0.6958Epoch 8/15: [================              ] 42/75 batches, loss: 0.6957Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6958Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6954Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6948Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6947Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6952Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6946Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6944Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6946Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6950Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6952Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6951Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6953Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6954Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6951Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6952Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6950Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6951Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6954Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6954Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6953Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6954Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6956Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6954Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6955Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6953Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6951Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6951Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6952Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6953Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6954Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6953Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6954Epoch 8/15: [==============================] 75/75 batches, loss: 0.6953
[2025-04-29 21:25:00,078][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6953
[2025-04-29 21:25:00,340][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:00,341][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6916Epoch 9/15: [                              ] 2/75 batches, loss: 0.7049Epoch 9/15: [=                             ] 3/75 batches, loss: 0.7008Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6961Epoch 9/15: [==                            ] 5/75 batches, loss: 0.7034Epoch 9/15: [==                            ] 6/75 batches, loss: 0.7024Epoch 9/15: [==                            ] 7/75 batches, loss: 0.7012Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6992Epoch 9/15: [===                           ] 9/75 batches, loss: 0.7011Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6998Epoch 9/15: [====                          ] 11/75 batches, loss: 0.7006Epoch 9/15: [====                          ] 12/75 batches, loss: 0.7013Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.7009Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.7020Epoch 9/15: [======                        ] 15/75 batches, loss: 0.7003Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6997Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6995Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6991Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6989Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6990Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6983Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6983Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6977Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6971Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6967Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6967Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6971Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6969Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6966Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6967Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6964Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6964Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6959Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6953Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6950Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6945Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6948Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6950Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6948Epoch 9/15: [================              ] 40/75 batches, loss: 0.6948Epoch 9/15: [================              ] 41/75 batches, loss: 0.6952Epoch 9/15: [================              ] 42/75 batches, loss: 0.6950Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6953Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6952Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6951Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6953Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6956Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6958Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6958Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6958Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6958Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6960Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6964Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6965Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6965Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6967Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6967Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6971Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6969Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6966Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6969Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6967Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6967Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6966Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6963Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6962Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6962Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6959Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6959Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6962Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6964Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6961Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6960Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6960Epoch 9/15: [==============================] 75/75 batches, loss: 0.6959
[2025-04-29 21:25:02,605][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6959
[2025-04-29 21:25:02,856][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:02,857][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.6953Epoch 10/15: [                              ] 2/75 batches, loss: 0.6970Epoch 10/15: [=                             ] 3/75 batches, loss: 0.6963Epoch 10/15: [=                             ] 4/75 batches, loss: 0.6955Epoch 10/15: [==                            ] 5/75 batches, loss: 0.6914Epoch 10/15: [==                            ] 6/75 batches, loss: 0.6922Epoch 10/15: [==                            ] 7/75 batches, loss: 0.6904Epoch 10/15: [===                           ] 8/75 batches, loss: 0.6918Epoch 10/15: [===                           ] 9/75 batches, loss: 0.6952Epoch 10/15: [====                          ] 10/75 batches, loss: 0.6941Epoch 10/15: [====                          ] 11/75 batches, loss: 0.6953Epoch 10/15: [====                          ] 12/75 batches, loss: 0.6948Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.6948Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.6959Epoch 10/15: [======                        ] 15/75 batches, loss: 0.6957Epoch 10/15: [======                        ] 16/75 batches, loss: 0.6967Epoch 10/15: [======                        ] 17/75 batches, loss: 0.6964Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.6960Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.6958Epoch 10/15: [========                      ] 20/75 batches, loss: 0.6960Epoch 10/15: [========                      ] 21/75 batches, loss: 0.6961Epoch 10/15: [========                      ] 22/75 batches, loss: 0.6954Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.6958Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.6950Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.6949Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.6947Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.6955Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.6954Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.6953Epoch 10/15: [============                  ] 30/75 batches, loss: 0.6944Epoch 10/15: [============                  ] 31/75 batches, loss: 0.6949Epoch 10/15: [============                  ] 32/75 batches, loss: 0.6952Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.6957Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.6957Epoch 10/15: [==============                ] 35/75 batches, loss: 0.6955Epoch 10/15: [==============                ] 36/75 batches, loss: 0.6957Epoch 10/15: [==============                ] 37/75 batches, loss: 0.6957Epoch 10/15: [===============               ] 38/75 batches, loss: 0.6955Epoch 10/15: [===============               ] 39/75 batches, loss: 0.6952Epoch 10/15: [================              ] 40/75 batches, loss: 0.6953Epoch 10/15: [================              ] 41/75 batches, loss: 0.6955Epoch 10/15: [================              ] 42/75 batches, loss: 0.6954Epoch 10/15: [=================             ] 43/75 batches, loss: 0.6958Epoch 10/15: [=================             ] 44/75 batches, loss: 0.6957Epoch 10/15: [==================            ] 45/75 batches, loss: 0.6955Epoch 10/15: [==================            ] 46/75 batches, loss: 0.6951Epoch 10/15: [==================            ] 47/75 batches, loss: 0.6952Epoch 10/15: [===================           ] 48/75 batches, loss: 0.6953Epoch 10/15: [===================           ] 49/75 batches, loss: 0.6957Epoch 10/15: [====================          ] 50/75 batches, loss: 0.6959Epoch 10/15: [====================          ] 51/75 batches, loss: 0.6957Epoch 10/15: [====================          ] 52/75 batches, loss: 0.6952Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.6956Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.6956Epoch 10/15: [======================        ] 55/75 batches, loss: 0.6952Epoch 10/15: [======================        ] 56/75 batches, loss: 0.6953Epoch 10/15: [======================        ] 57/75 batches, loss: 0.6951Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.6953Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.6950Epoch 10/15: [========================      ] 60/75 batches, loss: 0.6950Epoch 10/15: [========================      ] 61/75 batches, loss: 0.6953Epoch 10/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.6950Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.6948Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.6945Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.6945Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.6948Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.6952Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.6950Epoch 10/15: [============================  ] 70/75 batches, loss: 0.6948Epoch 10/15: [============================  ] 71/75 batches, loss: 0.6948Epoch 10/15: [============================  ] 72/75 batches, loss: 0.6950Epoch 10/15: [============================= ] 73/75 batches, loss: 0.6948Epoch 10/15: [============================= ] 74/75 batches, loss: 0.6949Epoch 10/15: [==============================] 75/75 batches, loss: 0.6948
[2025-04-29 21:25:05,108][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6948
[2025-04-29 21:25:05,359][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:05,360][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:25:05,360][src.training.lm_trainer][INFO] - Early stopping at epoch 10
[2025-04-29 21:25:05,360][src.training.lm_trainer][INFO] - Training completed in 28.63 seconds
[2025-04-29 21:25:05,360][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:25:08,262][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:08,263][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:08,263][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:25:09,893][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁▁
wandb:        best_val_loss █▄▂▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate █████████▁
wandb:           train_loss █▅▅▂▁▂▄▄▄▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69282
wandb:                epoch 10
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69478
wandb:           train_time 28.628
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69288
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212424-g61w5n3x
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212424-g61w5n3x/logs
Standard experiment completed successfully: layer_6_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/results.json
Running complexity experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:25:27,260][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity
experiment_name: layer_6_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:25:27,261][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:25:27,261][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:25:27,261][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:25:27,265][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 21:25:27,265][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:25:29,427][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:25:31,668][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:25:31,669][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:25:31,804][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:31,855][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:32,008][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 21:25:32,016][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:25:32,017][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 21:25:32,019][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:25:32,068][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:32,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:32,148][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 21:25:32,149][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:25:32,149][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 21:25:32,151][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:25:32,194][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:32,250][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:25:32,270][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 21:25:32,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:25:32,271][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 21:25:32,273][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 21:25:32,274][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:25:32,274][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:25:32,274][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:25:32,274][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:25:32,274][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:25:32,275][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:25:32,275][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:25:32,275][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:25:32,275][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:25:32,276][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:25:32,276][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 21:25:32,276][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 21:25:32,277][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:25:32,277][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:25:32,277][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:25:37,880][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:25:37,881][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:25:37,881][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:25:37,882][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 21:25:37,883][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:25:37,883][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2736Epoch 1/15: [                              ] 2/75 batches, loss: 0.3289Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3845Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3957Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3803Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3699Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3742Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3544Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3545Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3499Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3529Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3507Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3489Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3464Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3457Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3492Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3481Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3510Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3463Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3495Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3455Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3412Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3394Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3363Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3353Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3345Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3308Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3322Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3286Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3265Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3275Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3266Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3265Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3252Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3218Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3184Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3185Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3167Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3173Epoch 1/15: [================              ] 40/75 batches, loss: 0.3159Epoch 1/15: [================              ] 41/75 batches, loss: 0.3156Epoch 1/15: [================              ] 42/75 batches, loss: 0.3134Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3141Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3134Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3108Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3082Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3082Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3069Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3057Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3050Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3024Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3048Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3046Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3037Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3025Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3025Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3011Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3000Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2985Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2974Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2978Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2972Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2951Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2934Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2921Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2903Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2894Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2875Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2866Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2858Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2855Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2850Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2841Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2827Epoch 1/15: [==============================] 75/75 batches, loss: 0.2832
[2025-04-29 21:25:43,440][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2832
[2025-04-29 21:25:43,669][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1642, Metrics: {'mse': 0.17361438274383545, 'rmse': 0.41667059260744027, 'r2': -3.148372173309326}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1653Epoch 2/15: [                              ] 2/75 batches, loss: 0.1697Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1845Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2003Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2065Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2135Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2104Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2097Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2154Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2143Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2175Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2189Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2143Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2177Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2168Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2117Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2158Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2147Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2162Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2126Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2163Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2192Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2194Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2166Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2178Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2165Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2146Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2142Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2139Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2130Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2125Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2123Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2093Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2086Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2071Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2047Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2021Epoch 2/15: [===============               ] 38/75 batches, loss: 0.2008Epoch 2/15: [===============               ] 39/75 batches, loss: 0.2011Epoch 2/15: [================              ] 40/75 batches, loss: 0.1992Epoch 2/15: [================              ] 41/75 batches, loss: 0.1984Epoch 2/15: [================              ] 42/75 batches, loss: 0.1970Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1955Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1944Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1947Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1942Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1942Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1941Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1947Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1936Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1918Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1909Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1905Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1894Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1880Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1867Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1864Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1855Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1863Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1859Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1861Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1850Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1843Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1834Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1827Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1813Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1799Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1800Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1796Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1789Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1784Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1775Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1762Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1763Epoch 2/15: [==============================] 75/75 batches, loss: 0.1759
[2025-04-29 21:25:46,299][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1759
[2025-04-29 21:25:46,526][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0999, Metrics: {'mse': 0.10690303891897202, 'rmse': 0.3269603017477382, 'r2': -1.5543596744537354}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1466Epoch 3/15: [                              ] 2/75 batches, loss: 0.1341Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1397Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1382Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1287Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1273Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1378Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1400Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1378Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1435Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1412Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1386Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1359Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1321Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1294Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1330Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1295Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1303Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1286Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1299Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1292Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1269Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1259Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1256Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1265Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1294Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1296Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1274Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1270Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1267Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1259Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1227Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1234Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1229Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1233Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1231Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1223Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1218Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1216Epoch 3/15: [================              ] 40/75 batches, loss: 0.1206Epoch 3/15: [================              ] 41/75 batches, loss: 0.1191Epoch 3/15: [================              ] 42/75 batches, loss: 0.1177Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1172Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1171Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1172Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1172Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1177Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1188Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1187Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1186Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1179Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1180Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1166Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1167Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1164Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1155Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1146Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1145Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1144Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1141Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1147Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1146Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1142Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1147Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1143Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1147Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1143Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1133Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1136Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1131Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1132Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1134Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1134Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1129Epoch 3/15: [==============================] 75/75 batches, loss: 0.1124
[2025-04-29 21:25:49,181][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1124
[2025-04-29 21:25:49,416][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0654, Metrics: {'mse': 0.07047750055789948, 'rmse': 0.2654759886654525, 'r2': -0.6840014457702637}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0982Epoch 4/15: [                              ] 2/75 batches, loss: 0.1015Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0938Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0945Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0858Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0879Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0895Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0851Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0865Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0875Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0886Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0924Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0916Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0937Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0945Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0923Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0932Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0927Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0913Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0922Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0930Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0918Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0924Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0930Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0933Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0920Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0916Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0913Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0923Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0923Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0903Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0896Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0904Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0902Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0893Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0898Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0893Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0894Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0885Epoch 4/15: [================              ] 40/75 batches, loss: 0.0889Epoch 4/15: [================              ] 41/75 batches, loss: 0.0885Epoch 4/15: [================              ] 42/75 batches, loss: 0.0878Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0873Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0875Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0868Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0860Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0854Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0853Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0848Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0851Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0854Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0845Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0839Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0834Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0836Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0828Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0825Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0817Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0811Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0807Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0811Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0815Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0810Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0807Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0805Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0803Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0805Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0803Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0799Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0796Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0792Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0788Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0786Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0780Epoch 4/15: [==============================] 75/75 batches, loss: 0.0774
[2025-04-29 21:25:52,015][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0774
[2025-04-29 21:25:52,256][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0483, Metrics: {'mse': 0.05185195803642273, 'rmse': 0.2277102501786486, 'r2': -0.2389596700668335}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0776Epoch 5/15: [                              ] 2/75 batches, loss: 0.0756Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0793Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0845Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0781Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0765Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0774Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0721Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0706Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0741Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0738Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0719Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0745Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0730Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0707Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0692Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0673Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0655Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0664Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0666Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0655Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0648Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0645Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0637Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0649Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0650Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0648Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0650Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0642Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0655Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0650Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0640Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0643Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0637Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0631Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0630Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0628Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0632Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0632Epoch 5/15: [================              ] 40/75 batches, loss: 0.0628Epoch 5/15: [================              ] 41/75 batches, loss: 0.0626Epoch 5/15: [================              ] 42/75 batches, loss: 0.0625Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0624Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0617Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0609Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0608Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0608Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0609Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0617Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0611Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0616Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0616Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0611Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0608Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0602Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0597Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0597Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0594Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0592Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0594Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0593Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0589Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0591Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0596Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0595Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0594Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0594Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0591Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0586Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0583Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0582Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0580Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0577Epoch 5/15: [==============================] 75/75 batches, loss: 0.0570
[2025-04-29 21:25:54,838][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0570
[2025-04-29 21:25:55,084][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0418, Metrics: {'mse': 0.0440993458032608, 'rmse': 0.2099984423829396, 'r2': -0.05371737480163574}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0218Epoch 6/15: [                              ] 2/75 batches, loss: 0.0240Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0312Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0312Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0375Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0369Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0403Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0433Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0419Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0413Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0420Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0418Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0431Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0437Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0477Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0477Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0501Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0521Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0523Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0511Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0527Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0521Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0526Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0520Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0507Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0503Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0497Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0490Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0488Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0489Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0492Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0502Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0498Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0506Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0505Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0502Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0497Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0496Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0496Epoch 6/15: [================              ] 40/75 batches, loss: 0.0488Epoch 6/15: [================              ] 41/75 batches, loss: 0.0483Epoch 6/15: [================              ] 42/75 batches, loss: 0.0486Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0486Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0488Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0489Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0483Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0484Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0480Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0479Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0484Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0482Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0481Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0480Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0486Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0483Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0480Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0483Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0478Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0475Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0476Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0473Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0470Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0470Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0470Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0467Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0465Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0465Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0469Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0468Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0467Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0466Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0466Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0470Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0472Epoch 6/15: [==============================] 75/75 batches, loss: 0.0469
[2025-04-29 21:25:57,695][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0469
[2025-04-29 21:25:57,936][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0406, Metrics: {'mse': 0.04204241558909416, 'rmse': 0.2050424726467523, 'r2': -0.004568696022033691}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0177Epoch 7/15: [                              ] 2/75 batches, loss: 0.0199Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0247Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0231Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0252Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0285Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0282Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0304Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0311Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0337Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0338Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0347Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0341Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0344Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0336Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0332Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0328Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0356Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0352Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0360Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0364Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0391Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0390Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0384Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0388Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0390Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0385Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0383Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0383Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0390Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0388Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0382Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0380Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0387Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0391Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0389Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0401Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0397Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0403Epoch 7/15: [================              ] 40/75 batches, loss: 0.0401Epoch 7/15: [================              ] 41/75 batches, loss: 0.0402Epoch 7/15: [================              ] 42/75 batches, loss: 0.0399Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0399Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0395Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0396Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0395Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0395Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0393Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0390Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0388Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0395Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0398Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0396Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0405Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0405Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0401Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0399Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0400Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0400Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0397Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0395Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0395Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0396Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0393Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0395Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0394Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0392Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0393Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0390Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0387Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0385Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0385Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0382Epoch 7/15: [==============================] 75/75 batches, loss: 0.0381
[2025-04-29 21:26:00,601][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0381
[2025-04-29 21:26:00,846][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0415, Metrics: {'mse': 0.042350754141807556, 'rmse': 0.20579298856328307, 'r2': -0.011936306953430176}
[2025-04-29 21:26:00,847][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0213Epoch 8/15: [                              ] 2/75 batches, loss: 0.0428Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0399Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0385Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0417Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0388Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0403Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0393Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0372Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0358Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0364Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0357Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0349Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0349Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0363Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0350Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0341Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0341Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0336Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0349Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0347Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0340Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0334Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0333Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0339Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0335Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0345Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0345Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0345Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0348Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0350Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0353Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0348Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0353Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0363Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0367Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0369Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0384Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0382Epoch 8/15: [================              ] 40/75 batches, loss: 0.0380Epoch 8/15: [================              ] 41/75 batches, loss: 0.0377Epoch 8/15: [================              ] 42/75 batches, loss: 0.0373Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0371Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0367Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0365Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0366Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0368Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0372Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0380Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0382Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0381Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0384Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0380Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0380Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0376Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0377Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0379Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0376Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0373Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0372Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0370Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0372Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0373Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0375Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0373Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0378Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0376Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0377Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0377Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0375Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0373Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0372Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0371Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0370Epoch 8/15: [==============================] 75/75 batches, loss: 0.0369
[2025-04-29 21:26:03,097][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0369
[2025-04-29 21:26:03,355][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0431, Metrics: {'mse': 0.0434722900390625, 'rmse': 0.20850009601691435, 'r2': -0.03873443603515625}
[2025-04-29 21:26:03,356][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0357Epoch 9/15: [                              ] 2/75 batches, loss: 0.0356Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0397Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0447Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0495Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0455Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0436Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0411Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0394Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0402Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0384Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0371Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0377Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0395Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0381Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0385Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0371Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0366Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0366Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0357Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0349Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0345Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0345Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0342Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0340Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0334Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0334Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0331Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0329Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0326Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0328Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0330Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0339Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0337Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0332Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0327Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0320Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0322Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0322Epoch 9/15: [================              ] 40/75 batches, loss: 0.0328Epoch 9/15: [================              ] 41/75 batches, loss: 0.0339Epoch 9/15: [================              ] 42/75 batches, loss: 0.0336Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0338Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0338Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0338Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0335Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0341Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0345Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0349Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0349Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0351Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0351Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0357Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0354Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0350Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0350Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0349Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0348Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0348Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0349Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0352Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0356Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0358Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0357Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0356Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0359Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0357Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0357Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0355Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0358Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0356Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0355Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0355Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0354Epoch 9/15: [==============================] 75/75 batches, loss: 0.0353
[2025-04-29 21:26:05,622][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0353
[2025-04-29 21:26:05,872][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0444, Metrics: {'mse': 0.04451189562678337, 'rmse': 0.21097842455280438, 'r2': -0.0635749101638794}
[2025-04-29 21:26:05,873][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:26:05,873][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 21:26:05,873][src.training.lm_trainer][INFO] - Training completed in 25.59 seconds
[2025-04-29 21:26:05,873][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:26:08,737][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03336698189377785, 'rmse': 0.18266631296924413, 'r2': -0.24370622634887695}
[2025-04-29 21:26:08,737][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04204241558909416, 'rmse': 0.2050424726467523, 'r2': -0.004568696022033691}
[2025-04-29 21:26:08,738][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.046520452946424484, 'rmse': 0.21568600544871816, 'r2': -0.20711290836334229}
[2025-04-29 21:26:10,383][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▃▂▁▁
wandb:      best_val_r2 ▁▅▆▇██
wandb:    best_val_rmse █▅▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁
wandb:          val_mse █▄▃▂▁▁▁▁▁
wandb:           val_r2 ▁▅▆▇█████
wandb:         val_rmse █▅▃▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04058
wandb:     best_val_mse 0.04204
wandb:      best_val_r2 -0.00457
wandb:    best_val_rmse 0.20504
wandb:            epoch 9
wandb:   final_test_mse 0.04652
wandb:    final_test_r2 -0.20711
wandb:  final_test_rmse 0.21569
wandb:  final_train_mse 0.03337
wandb:   final_train_r2 -0.24371
wandb: final_train_rmse 0.18267
wandb:    final_val_mse 0.04204
wandb:     final_val_r2 -0.00457
wandb:   final_val_rmse 0.20504
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03527
wandb:       train_time 25.59293
wandb:         val_loss 0.04437
wandb:          val_mse 0.04451
wandb:           val_r2 -0.06357
wandb:         val_rmse 0.21098
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212527-kbg41l63
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212527-kbg41l63/logs
Standard experiment completed successfully: layer_6_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity/results.json
Running question_type control experiment for language en, layer 6, control 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:26:23,120][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1
experiment_name: layer_6_question_type_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:26:23,120][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:26:23,120][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:26:23,120][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:26:23,124][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 21:26:23,124][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:26:24,783][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:26:27,111][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:26:27,113][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:27,187][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 21:26:27,241][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 21:26:27,364][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 21:26:27,372][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:27,373][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 21:26:27,375][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:27,422][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:27,476][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:27,496][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 21:26:27,497][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:27,497][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 21:26:27,499][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:26:27,546][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:27,603][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:26:27,622][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 21:26:27,624][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:26:27,624][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 21:26:27,625][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 21:26:27,625][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:27,626][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 21:26:27,626][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:27,626][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:27,627][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 21:26:27,627][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:26:27,627][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:26:27,627][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 21:26:27,628][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 21:26:27,628][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 21:26:27,628][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:26:27,628][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 21:26:27,628][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:26:27,628][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:26:27,628][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:26:32,163][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:26:32,164][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:26:32,164][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:26:32,165][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 21:26:32,165][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:26:32,165][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6833Epoch 1/15: [                              ] 2/75 batches, loss: 0.7019Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7005Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7037Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7002Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7040Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7038Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7068Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7075Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7037Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7052Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7057Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7008Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7022Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7049Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7057Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7043Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7037Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7040Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7019Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7022Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7029Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7027Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7021Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.7028Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.7017Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.7025Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.7028Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.7019Epoch 1/15: [============                  ] 30/75 batches, loss: 0.7020Epoch 1/15: [============                  ] 31/75 batches, loss: 0.7015Epoch 1/15: [============                  ] 32/75 batches, loss: 0.7012Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.7012Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.7007Epoch 1/15: [==============                ] 35/75 batches, loss: 0.7002Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6994Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6998Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6998Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6996Epoch 1/15: [================              ] 40/75 batches, loss: 0.6994Epoch 1/15: [================              ] 41/75 batches, loss: 0.6985Epoch 1/15: [================              ] 42/75 batches, loss: 0.6987Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6982Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6988Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6990Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6996Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6988Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6984Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6982Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6990Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6989Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6981Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6975Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6975Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6977Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6968Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6961Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6958Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6957Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6953Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6959Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6954Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6957Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6952Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6957Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6956Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6961Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6963Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6965Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6967Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6969Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6971Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6969Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6967Epoch 1/15: [==============================] 75/75 batches, loss: 0.6963
[2025-04-29 21:26:41,161][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6963
[2025-04-29 21:26:41,402][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6779Epoch 2/15: [                              ] 2/75 batches, loss: 0.6972Epoch 2/15: [=                             ] 3/75 batches, loss: 0.7002Epoch 2/15: [=                             ] 4/75 batches, loss: 0.7080Epoch 2/15: [==                            ] 5/75 batches, loss: 0.7073Epoch 2/15: [==                            ] 6/75 batches, loss: 0.7024Epoch 2/15: [==                            ] 7/75 batches, loss: 0.7069Epoch 2/15: [===                           ] 8/75 batches, loss: 0.7088Epoch 2/15: [===                           ] 9/75 batches, loss: 0.7079Epoch 2/15: [====                          ] 10/75 batches, loss: 0.7049Epoch 2/15: [====                          ] 11/75 batches, loss: 0.7055Epoch 2/15: [====                          ] 12/75 batches, loss: 0.7017Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6999Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6990Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6987Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6976Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6956Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6958Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6964Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6957Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6959Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6957Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6964Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6973Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6976Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6968Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6969Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6969Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6963Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6967Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6963Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6958Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6961Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6957Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6965Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6961Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6958Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6959Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6964Epoch 2/15: [================              ] 40/75 batches, loss: 0.6961Epoch 2/15: [================              ] 41/75 batches, loss: 0.6956Epoch 2/15: [================              ] 42/75 batches, loss: 0.6960Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6958Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6958Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6960Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6954Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6951Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6955Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6949Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6957Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6955Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6949Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6948Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6945Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6945Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6942Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6947Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6948Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6948Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6947Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6947Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6942Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6937Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6940Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6943Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6942Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6939Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6937Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6942Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6939Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6936Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6938Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6934Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6929
[2025-04-29 21:26:44,046][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6929
[2025-04-29 21:26:44,303][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6968Epoch 3/15: [                              ] 2/75 batches, loss: 0.6875Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6842Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6850Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6866Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6857Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6857Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6830Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6813Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6798Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6802Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6822Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6821Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6823Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6845Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6866Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6871Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6866Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6867Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6862Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6864Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6860Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6850Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6852Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6848Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6855Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6869Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6869Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6879Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6875Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6877Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6878Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6883Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6887Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6891Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6895Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6897Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6894Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6908Epoch 3/15: [================              ] 40/75 batches, loss: 0.6912Epoch 3/15: [================              ] 41/75 batches, loss: 0.6915Epoch 3/15: [================              ] 42/75 batches, loss: 0.6917Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6916Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6918Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6914Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6916Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6924Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6925Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6922Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6921Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6919Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6922Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6923Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6922Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6924Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6925Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6935Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6938Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6939Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6933Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6934Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6934Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6936Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 3/15: [==============================] 75/75 batches, loss: 0.6929
[2025-04-29 21:26:46,970][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6929
[2025-04-29 21:26:47,214][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6657Epoch 4/15: [                              ] 2/75 batches, loss: 0.6672Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6719Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6809Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6818Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6807Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6830Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6863Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6891Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6907Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6944Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6919Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6937Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6944Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6938Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6924Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6935Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6939Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6936Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6922Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6910Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6919Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6927Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6936Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6936Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6945Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6948Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6950Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6952Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6950Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6955Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6951Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6952Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6939Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6937Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6935Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6929Epoch 4/15: [================              ] 42/75 batches, loss: 0.6930Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6937Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6940Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6943Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6941Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6937Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6941Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6937Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6942Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6948Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6945Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6945Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6950Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6953Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6953Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6957Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6959Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6956Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6954Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6956Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6956Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6953Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6955Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6960Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6964Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6959Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6958Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6959Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6957Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6959Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6958Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6959Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6957Epoch 4/15: [==============================] 75/75 batches, loss: 0.6961
[2025-04-29 21:26:49,828][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6961
[2025-04-29 21:26:50,075][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6597Epoch 5/15: [                              ] 2/75 batches, loss: 0.6786Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6768Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6768Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6782Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6805Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6844Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6910Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6935Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6952Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6941Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6944Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6959Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6960Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6965Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6971Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6973Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6960Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6953Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6951Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6963Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6961Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6960Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6962Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6964Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6965Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6981Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6973Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6974Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6974Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6974Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6971Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6976Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6974Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6969Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6967Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6968Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6965Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6966Epoch 5/15: [================              ] 40/75 batches, loss: 0.6960Epoch 5/15: [================              ] 41/75 batches, loss: 0.6958Epoch 5/15: [================              ] 42/75 batches, loss: 0.6958Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6959Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6963Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6959Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6960Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6958Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6956Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6953Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6958Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6962Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6956Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6960Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6957Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6954Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6956Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6957Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6958Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6957Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6954Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6954Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6952Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6957Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6957Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6960Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6959Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6961Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6964Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6960Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6957Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6956Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6955Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6954Epoch 5/15: [==============================] 75/75 batches, loss: 0.6947
[2025-04-29 21:26:52,690][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6947
[2025-04-29 21:26:52,946][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6808Epoch 6/15: [                              ] 2/75 batches, loss: 0.6831Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6920Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6939Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6924Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6916Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6922Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6906Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6891Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6857Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6882Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6880Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6874Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6872Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6872Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6871Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6863Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6870Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6866Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6865Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6873Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6870Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6872Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6873Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6872Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6874Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6881Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6887Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6892Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6897Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6896Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6902Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6898Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6890Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6895Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6898Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6900Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6900Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6902Epoch 6/15: [================              ] 40/75 batches, loss: 0.6903Epoch 6/15: [================              ] 41/75 batches, loss: 0.6907Epoch 6/15: [================              ] 42/75 batches, loss: 0.6910Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6912Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6908Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6912Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6912Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6915Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6914Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6915Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6914Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6914Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6913Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6913Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6913Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6913Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6911Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6911Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6912Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6912Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6917Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6918Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6916Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6920Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6924Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6926Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6924Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6924Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6925Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6926Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6926Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6926Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6924Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6927Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 6/15: [==============================] 75/75 batches, loss: 0.6927
[2025-04-29 21:26:55,584][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6927
[2025-04-29 21:26:55,833][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6892Epoch 7/15: [                              ] 2/75 batches, loss: 0.6749Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6846Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6861Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6892Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6889Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6896Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6891Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6874Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6889Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6887Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6906Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6887Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6896Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6905Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6914Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6917Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6920Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6922Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6936Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6941Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6939Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6940Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6945Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6947Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6950Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6944Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6935Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6939Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6933Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6934Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6927Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 7/15: [================              ] 40/75 batches, loss: 0.6935Epoch 7/15: [================              ] 41/75 batches, loss: 0.6937Epoch 7/15: [================              ] 42/75 batches, loss: 0.6936Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6939Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6940Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6938Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6938Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6941Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6940Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6945Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6950Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6948Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6947Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6948Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6948Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6949Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6949Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6946Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6946Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6949Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6947Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6947Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6948Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6945Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6944Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6943Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6941Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6942Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6942Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6940Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6939Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6939Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6937Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6938Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6941Epoch 7/15: [==============================] 75/75 batches, loss: 0.6940
[2025-04-29 21:26:58,545][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6940
[2025-04-29 21:26:58,802][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:26:58,803][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6979Epoch 8/15: [                              ] 2/75 batches, loss: 0.6931Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6918Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6936Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6912Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6910Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6926Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6910Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6909Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6904Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6910Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6913Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6918Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6911Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6903Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6904Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6925Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6922Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6925Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6923Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6924Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6923Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6927Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6919Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6926Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6924Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6926Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6921Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6917Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6916Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6916Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6920Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6923Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6926Epoch 8/15: [================              ] 40/75 batches, loss: 0.6924Epoch 8/15: [================              ] 41/75 batches, loss: 0.6917Epoch 8/15: [================              ] 42/75 batches, loss: 0.6914Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6921Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6921Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6923Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6928Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6923Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6924Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6924Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6926Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6924Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6925Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6925Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6927Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6934Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6939Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6936Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6940Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6940Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6940Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6937Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6937Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6938Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6938Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6937Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6938Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6938Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6942Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6944Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6944Epoch 8/15: [==============================] 75/75 batches, loss: 0.6947
[2025-04-29 21:27:01,085][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6947
[2025-04-29 21:27:01,334][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:27:01,335][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6886Epoch 9/15: [                              ] 2/75 batches, loss: 0.6858Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6842Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6879Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6941Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6952Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6945Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6943Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6939Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6945Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6954Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6966Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6961Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6942Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6959Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6954Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6967Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6974Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6970Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6978Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6978Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6977Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6983Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6980Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6987Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6989Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6987Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6983Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6991Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6988Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6991Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6990Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6996Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6991Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6989Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6986Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6987Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6983Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6979Epoch 9/15: [================              ] 40/75 batches, loss: 0.6981Epoch 9/15: [================              ] 41/75 batches, loss: 0.6983Epoch 9/15: [================              ] 42/75 batches, loss: 0.6984Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6981Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6979Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6983Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6983Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6985Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6982Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6983Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6982Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6980Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6980Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6977Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6979Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6976Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6972Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6966Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6968Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6969Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6965Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6963Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6964Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6965Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6964Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6963Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6961Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6961Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6959Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6960Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6959Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6959Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6955Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6952Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6953Epoch 9/15: [==============================] 75/75 batches, loss: 0.6955
[2025-04-29 21:27:03,587][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6955
[2025-04-29 21:27:03,829][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:27:03,829][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:27:03,830][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 21:27:03,830][src.training.lm_trainer][INFO] - Training completed in 25.78 seconds
[2025-04-29 21:27:03,830][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:27:06,704][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:27:06,705][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:27:06,705][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 21:27:08,352][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁
wandb:        best_val_loss █▄▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ████████▁
wandb:           train_loss █▁▁█▅▁▄▅▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▄▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69282
wandb:                epoch 9
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69545
wandb:           train_time 25.77689
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69286
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212623-xfqcmhnd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212623-xfqcmhnd/logs
Control experiment completed successfully: layer_6_question_type_control1_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1/results.json
Initial validation results:
experiment_type,language,layer,task,submetric,control_index,metric,value

Initial experiments completed. Please check the results above.
Press Enter to continue with full experiment suite, or Ctrl+C to abort.
Phase 2: Running main experiments
Running question_type experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:27:23,337][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type
experiment_name: layer_1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:27:23,337][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:27:23,337][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:27:23,337][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:27:23,342][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 21:27:23,342][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:27:25,524][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:27:27,768][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:27:27,769][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:27:27,882][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:27,923][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:28,023][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:27:28,030][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:27:28,031][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:27:28,035][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:27:28,067][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:28,110][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:28,126][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:27:28,127][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:27:28,127][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:27:28,129][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:27:28,160][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:28,199][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:27:28,214][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:27:28,216][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:27:28,216][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:27:28,217][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:27:28,218][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 21:27:28,218][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:27:28,218][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:27:28,219][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 21:27:28,219][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:27:28,219][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:27:28,220][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 21:27:28,220][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:27:28,220][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:27:28,221][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:27:32,678][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:27:32,679][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:27:32,680][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:27:32,680][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=1, freeze_model=True, finetune=False
[2025-04-29 21:27:32,681][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:27:32,681][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7423Epoch 1/15: [                              ] 2/63 batches, loss: 0.7457Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7165Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7205Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7024Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7035Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7054Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7029Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7001Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6988Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7035Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7073Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7053Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7035Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7045Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7050Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7051Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7040Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7045Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7049Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7039Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7020Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7034Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7050Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7045Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7045Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7034Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7027Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7033Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7013Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7025Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7025Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7030Epoch 1/15: [================              ] 34/63 batches, loss: 0.7045Epoch 1/15: [================              ] 35/63 batches, loss: 0.7046Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7030Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7031Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7033Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7040Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7041Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7051Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7060Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7056Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7057Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7054Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7049Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7054Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7053Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7042Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7043Epoch 1/15: [========================      ] 51/63 batches, loss: 0.7026Epoch 1/15: [========================      ] 52/63 batches, loss: 0.7021Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.7025Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.7029Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.7031Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.7024Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.7025Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.7016Epoch 1/15: [============================  ] 59/63 batches, loss: 0.7017Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7024Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7024Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7027Epoch 1/15: [==============================] 63/63 batches, loss: 0.7019
[2025-04-29 21:27:37,637][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7019
[2025-04-29 21:27:37,818][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6883, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6936Epoch 2/15: [                              ] 2/63 batches, loss: 0.6978Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6971Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6906Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6922Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6959Epoch 2/15: [===                           ] 7/63 batches, loss: 0.7063Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6973Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6958Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6942Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6901Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6926Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6928Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6906Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6901Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6934Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6940Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6942Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6921Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6916Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6904Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6908Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6921Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6916Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6924Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6937Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6941Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6960Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6953Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6940Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6950Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6963Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6952Epoch 2/15: [================              ] 34/63 batches, loss: 0.6951Epoch 2/15: [================              ] 35/63 batches, loss: 0.6944Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6944Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6939Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6942Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6945Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6947Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6956Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6962Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6959Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6954Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6951Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6950Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6945Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6944Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6946Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6945Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6947Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6945Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6948Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6950Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6955Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6955Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6954Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6953Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6955Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6953Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6958Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6961Epoch 2/15: [==============================] 63/63 batches, loss: 0.6970
[2025-04-29 21:27:40,115][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6970
[2025-04-29 21:27:40,316][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6889, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:27:40,316][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6635Epoch 3/15: [                              ] 2/63 batches, loss: 0.6895Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6784Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6765Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6843Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6835Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6828Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6819Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6850Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6835Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6848Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6865Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6855Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6854Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6882Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6898Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6907Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6896Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6899Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6903Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6909Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6907Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6922Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6910Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6924Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6928Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6926Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6927Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6929Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6932Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6929Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6929Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6925Epoch 3/15: [================              ] 34/63 batches, loss: 0.6927Epoch 3/15: [================              ] 35/63 batches, loss: 0.6935Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6933Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6930Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6933Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6935Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6939Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6937Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6946Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6944Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6945Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6947Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6948Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6944Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6943Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6945Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6948Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6948Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6946Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6946Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6945Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6943Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6943Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6944Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6944Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6941Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6941Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6944Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6946Epoch 3/15: [==============================] 63/63 batches, loss: 0.6949
[2025-04-29 21:27:42,219][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6949
[2025-04-29 21:27:42,423][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6898, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:27:42,425][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6867Epoch 4/15: [                              ] 2/63 batches, loss: 0.6855Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6887Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6967Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6994Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6999Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6953Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6948Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6930Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6901Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6917Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6925Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6949Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6974Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6979Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6968Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6972Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6969Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6965Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6972Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6962Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6957Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6963Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6960Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6966Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6963Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6951Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6955Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6949Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6950Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6944Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6943Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6943Epoch 4/15: [================              ] 34/63 batches, loss: 0.6942Epoch 4/15: [================              ] 35/63 batches, loss: 0.6947Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6943Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6950Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6952Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6951Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6951Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6949Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6955Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6953Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6953Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6951Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6952Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6953Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6947Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6943Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6946Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6946Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6946Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6942Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6938Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6938Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6942Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6939Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6937Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6937Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6938Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6939Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6940Epoch 4/15: [==============================] 63/63 batches, loss: 0.6937
[2025-04-29 21:27:44,341][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6937
[2025-04-29 21:27:44,544][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6904, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:27:44,545][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:27:44,545][src.training.lm_trainer][INFO] - Early stopping at epoch 4
[2025-04-29 21:27:44,545][src.training.lm_trainer][INFO] - Training completed in 9.57 seconds
[2025-04-29 21:27:44,545][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:27:47,271][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0}
[2025-04-29 21:27:47,272][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:27:47,272][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0}
[2025-04-29 21:27:48,922][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▄▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6883
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69372
wandb:           train_time 9.57317
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69037
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212723-gllfljte
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212723-gllfljte/logs
Standard experiment completed successfully: layer_1_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/results.json
Running complexity experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:28:03,374][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity
experiment_name: layer_1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:28:03,374][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:28:03,374][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:28:03,374][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:28:03,378][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 21:28:03,378][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:28:04,923][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:28:07,191][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:28:07,193][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:07,256][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,305][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,443][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:28:07,450][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:07,451][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:28:07,453][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:07,492][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,540][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,558][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:28:07,560][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:07,560][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:28:07,562][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:07,605][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,668][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:07,688][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:28:07,689][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:07,689][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:28:07,691][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:28:07,692][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:28:07,692][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:28:07,692][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:28:07,692][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:28:07,692][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:28:07,692][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:28:07,693][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:28:07,693][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 21:28:07,693][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:28:07,694][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:28:07,694][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 21:28:07,694][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:28:07,695][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:28:07,695][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:28:07,695][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:28:13,051][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:28:13,052][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:28:13,053][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:28:13,053][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=1, freeze_model=True, finetune=False
[2025-04-29 21:28:13,054][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:28:13,054][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.5189Epoch 1/15: [                              ] 2/63 batches, loss: 0.5069Epoch 1/15: [=                             ] 3/63 batches, loss: 0.5411Epoch 1/15: [=                             ] 4/63 batches, loss: 0.5150Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4938Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4921Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4865Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4668Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4646Epoch 1/15: [====                          ] 10/63 batches, loss: 0.4575Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.4589Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.4598Epoch 1/15: [======                        ] 13/63 batches, loss: 0.4531Epoch 1/15: [======                        ] 14/63 batches, loss: 0.4496Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.4401Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.4421Epoch 1/15: [========                      ] 17/63 batches, loss: 0.4451Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4428Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.4428Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.4453Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.4431Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.4370Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.4334Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.4322Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.4296Epoch 1/15: [============                  ] 26/63 batches, loss: 0.4235Epoch 1/15: [============                  ] 27/63 batches, loss: 0.4190Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.4123Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.4140Epoch 1/15: [==============                ] 30/63 batches, loss: 0.4120Epoch 1/15: [==============                ] 31/63 batches, loss: 0.4134Epoch 1/15: [===============               ] 32/63 batches, loss: 0.4112Epoch 1/15: [===============               ] 33/63 batches, loss: 0.4079Epoch 1/15: [================              ] 34/63 batches, loss: 0.4084Epoch 1/15: [================              ] 35/63 batches, loss: 0.4071Epoch 1/15: [=================             ] 36/63 batches, loss: 0.4033Epoch 1/15: [=================             ] 37/63 batches, loss: 0.3994Epoch 1/15: [==================            ] 38/63 batches, loss: 0.3983Epoch 1/15: [==================            ] 39/63 batches, loss: 0.3953Epoch 1/15: [===================           ] 40/63 batches, loss: 0.3938Epoch 1/15: [===================           ] 41/63 batches, loss: 0.3917Epoch 1/15: [====================          ] 42/63 batches, loss: 0.3899Epoch 1/15: [====================          ] 43/63 batches, loss: 0.3854Epoch 1/15: [====================          ] 44/63 batches, loss: 0.3844Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.3814Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.3788Epoch 1/15: [======================        ] 47/63 batches, loss: 0.3780Epoch 1/15: [======================        ] 48/63 batches, loss: 0.3746Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.3713Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.3700Epoch 1/15: [========================      ] 51/63 batches, loss: 0.3665Epoch 1/15: [========================      ] 52/63 batches, loss: 0.3648Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.3635Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3613Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3581Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.3554Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.3539Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.3521Epoch 1/15: [============================  ] 59/63 batches, loss: 0.3512Epoch 1/15: [============================  ] 60/63 batches, loss: 0.3504Epoch 1/15: [============================= ] 61/63 batches, loss: 0.3493Epoch 1/15: [============================= ] 62/63 batches, loss: 0.3478Epoch 1/15: [==============================] 63/63 batches, loss: 0.3463
[2025-04-29 21:28:17,962][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3463
[2025-04-29 21:28:18,138][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2290, Metrics: {'mse': 0.23127470910549164, 'rmse': 0.4809102921600781, 'r2': -2.5647125244140625}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2225Epoch 2/15: [                              ] 2/63 batches, loss: 0.2375Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2403Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2504Epoch 2/15: [==                            ] 5/63 batches, loss: 0.2466Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2323Epoch 2/15: [===                           ] 7/63 batches, loss: 0.2289Epoch 2/15: [===                           ] 8/63 batches, loss: 0.2259Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2237Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2185Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.2176Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.2147Epoch 2/15: [======                        ] 13/63 batches, loss: 0.2167Epoch 2/15: [======                        ] 14/63 batches, loss: 0.2129Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2101Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.2141Epoch 2/15: [========                      ] 17/63 batches, loss: 0.2115Epoch 2/15: [========                      ] 18/63 batches, loss: 0.2111Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.2092Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.2103Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.2074Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2044Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.2023Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.2004Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1998Epoch 2/15: [============                  ] 26/63 batches, loss: 0.2003Epoch 2/15: [============                  ] 27/63 batches, loss: 0.2013Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1998Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1959Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1949Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1934Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1916Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1900Epoch 2/15: [================              ] 34/63 batches, loss: 0.1896Epoch 2/15: [================              ] 35/63 batches, loss: 0.1871Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1866Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1849Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1838Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1826Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1814Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1807Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1805Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1784Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1771Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1754Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1734Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1721Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1716Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1698Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1677Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1669Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1659Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1653Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1647Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1647Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1634Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1628Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1619Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1613Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1611Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1604Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1602Epoch 2/15: [==============================] 63/63 batches, loss: 0.1590
[2025-04-29 21:28:20,445][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1590
[2025-04-29 21:28:20,639][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1168, Metrics: {'mse': 0.11806302517652512, 'rmse': 0.3436030051913474, 'r2': -0.8197438716888428}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1658Epoch 3/15: [                              ] 2/63 batches, loss: 0.1287Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1297Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1220Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1186Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1210Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1140Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1193Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1175Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1135Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1089Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1107Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1097Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1067Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1090Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1099Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1082Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1057Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1041Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1050Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1043Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1035Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1023Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1011Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1000Epoch 3/15: [============                  ] 26/63 batches, loss: 0.0987Epoch 3/15: [============                  ] 27/63 batches, loss: 0.0976Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.0967Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.0968Epoch 3/15: [==============                ] 30/63 batches, loss: 0.0956Epoch 3/15: [==============                ] 31/63 batches, loss: 0.0944Epoch 3/15: [===============               ] 32/63 batches, loss: 0.0932Epoch 3/15: [===============               ] 33/63 batches, loss: 0.0918Epoch 3/15: [================              ] 34/63 batches, loss: 0.0914Epoch 3/15: [================              ] 35/63 batches, loss: 0.0915Epoch 3/15: [=================             ] 36/63 batches, loss: 0.0903Epoch 3/15: [=================             ] 37/63 batches, loss: 0.0895Epoch 3/15: [==================            ] 38/63 batches, loss: 0.0892Epoch 3/15: [==================            ] 39/63 batches, loss: 0.0882Epoch 3/15: [===================           ] 40/63 batches, loss: 0.0891Epoch 3/15: [===================           ] 41/63 batches, loss: 0.0887Epoch 3/15: [====================          ] 42/63 batches, loss: 0.0885Epoch 3/15: [====================          ] 43/63 batches, loss: 0.0878Epoch 3/15: [====================          ] 44/63 batches, loss: 0.0872Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.0864Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.0867Epoch 3/15: [======================        ] 47/63 batches, loss: 0.0861Epoch 3/15: [======================        ] 48/63 batches, loss: 0.0861Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.0872Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.0870Epoch 3/15: [========================      ] 51/63 batches, loss: 0.0862Epoch 3/15: [========================      ] 52/63 batches, loss: 0.0863Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.0855Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.0846Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.0847Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.0842Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.0839Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.0831Epoch 3/15: [============================  ] 59/63 batches, loss: 0.0827Epoch 3/15: [============================  ] 60/63 batches, loss: 0.0819Epoch 3/15: [============================= ] 61/63 batches, loss: 0.0815Epoch 3/15: [============================= ] 62/63 batches, loss: 0.0816Epoch 3/15: [==============================] 63/63 batches, loss: 0.0815
[2025-04-29 21:28:22,988][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0815
[2025-04-29 21:28:23,188][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0762, Metrics: {'mse': 0.07663413882255554, 'rmse': 0.27682871748168675, 'r2': -0.18118703365325928}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1081Epoch 4/15: [                              ] 2/63 batches, loss: 0.0642Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0620Epoch 4/15: [=                             ] 4/63 batches, loss: 0.0612Epoch 4/15: [==                            ] 5/63 batches, loss: 0.0684Epoch 4/15: [==                            ] 6/63 batches, loss: 0.0672Epoch 4/15: [===                           ] 7/63 batches, loss: 0.0642Epoch 4/15: [===                           ] 8/63 batches, loss: 0.0660Epoch 4/15: [====                          ] 9/63 batches, loss: 0.0659Epoch 4/15: [====                          ] 10/63 batches, loss: 0.0671Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.0667Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.0680Epoch 4/15: [======                        ] 13/63 batches, loss: 0.0670Epoch 4/15: [======                        ] 14/63 batches, loss: 0.0668Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.0657Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.0630Epoch 4/15: [========                      ] 17/63 batches, loss: 0.0622Epoch 4/15: [========                      ] 18/63 batches, loss: 0.0616Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.0619Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.0613Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.0599Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.0591Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.0598Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.0594Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.0586Epoch 4/15: [============                  ] 26/63 batches, loss: 0.0584Epoch 4/15: [============                  ] 27/63 batches, loss: 0.0579Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.0571Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.0563Epoch 4/15: [==============                ] 30/63 batches, loss: 0.0570Epoch 4/15: [==============                ] 31/63 batches, loss: 0.0566Epoch 4/15: [===============               ] 32/63 batches, loss: 0.0564Epoch 4/15: [===============               ] 33/63 batches, loss: 0.0556Epoch 4/15: [================              ] 34/63 batches, loss: 0.0556Epoch 4/15: [================              ] 35/63 batches, loss: 0.0558Epoch 4/15: [=================             ] 36/63 batches, loss: 0.0557Epoch 4/15: [=================             ] 37/63 batches, loss: 0.0559Epoch 4/15: [==================            ] 38/63 batches, loss: 0.0557Epoch 4/15: [==================            ] 39/63 batches, loss: 0.0559Epoch 4/15: [===================           ] 40/63 batches, loss: 0.0555Epoch 4/15: [===================           ] 41/63 batches, loss: 0.0557Epoch 4/15: [====================          ] 42/63 batches, loss: 0.0560Epoch 4/15: [====================          ] 43/63 batches, loss: 0.0558Epoch 4/15: [====================          ] 44/63 batches, loss: 0.0562Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.0559Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.0553Epoch 4/15: [======================        ] 47/63 batches, loss: 0.0551Epoch 4/15: [======================        ] 48/63 batches, loss: 0.0551Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.0550Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.0552Epoch 4/15: [========================      ] 51/63 batches, loss: 0.0552Epoch 4/15: [========================      ] 52/63 batches, loss: 0.0560Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.0557Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.0554Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.0549Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.0549Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.0545Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.0546Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0546Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0544Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0549Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0544Epoch 4/15: [==============================] 63/63 batches, loss: 0.0538
[2025-04-29 21:28:25,454][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0538
[2025-04-29 21:28:25,660][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0661, Metrics: {'mse': 0.06620245426893234, 'rmse': 0.25729837595471206, 'r2': -0.020399928092956543}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.0499Epoch 5/15: [                              ] 2/63 batches, loss: 0.0584Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0492Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0590Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0520Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0492Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0461Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0454Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0441Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0427Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0420Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0433Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0451Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0448Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0448Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0446Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0434Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0435Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0428Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0438Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0445Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0467Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0465Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0458Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0457Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0464Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0461Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0455Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0467Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0463Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0455Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0448Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0451Epoch 5/15: [================              ] 34/63 batches, loss: 0.0452Epoch 5/15: [================              ] 35/63 batches, loss: 0.0453Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0449Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0459Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0457Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0453Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0455Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0450Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0455Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0455Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0457Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0457Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0456Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0458Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0454Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0454Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0458Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0455Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0459Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0456Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0451Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0448Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0444Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0446Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0450Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0452Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0448Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0449Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0446Epoch 5/15: [==============================] 63/63 batches, loss: 0.0446
[2025-04-29 21:28:28,004][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0446
[2025-04-29 21:28:28,209][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0650, Metrics: {'mse': 0.06477416306734085, 'rmse': 0.25450768763898046, 'r2': 0.001614689826965332}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0683Epoch 6/15: [                              ] 2/63 batches, loss: 0.0525Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0455Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0428Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0415Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0421Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0437Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0416Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0408Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0421Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0431Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0449Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0447Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0438Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0429Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0423Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0419Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0409Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0415Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0411Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0414Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0413Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0408Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0408Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0410Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0412Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0409Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0413Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0420Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0421Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0417Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0416Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0425Epoch 6/15: [================              ] 34/63 batches, loss: 0.0429Epoch 6/15: [================              ] 35/63 batches, loss: 0.0421Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0419Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0413Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0415Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0420Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0416Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0417Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0414Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0409Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0418Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0418Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0426Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0428Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0429Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0428Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0426Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0422Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0422Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0422Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0420Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0420Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0421Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0421Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0424Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0426Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0425Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0424Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0421Epoch 6/15: [==============================] 63/63 batches, loss: 0.0416
[2025-04-29 21:28:30,523][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0416
[2025-04-29 21:28:30,738][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0653, Metrics: {'mse': 0.06500847637653351, 'rmse': 0.2549675986797803, 'r2': -0.0019968748092651367}
[2025-04-29 21:28:30,739][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0668Epoch 7/15: [                              ] 2/63 batches, loss: 0.0613Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0477Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0427Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0444Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0435Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0439Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0441Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0423Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0407Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0455Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0451Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0448Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0450Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0428Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0426Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0427Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0431Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0422Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0421Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0414Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0411Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0408Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0395Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0401Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0408Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0415Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0422Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0421Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0433Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0431Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0427Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0425Epoch 7/15: [================              ] 34/63 batches, loss: 0.0414Epoch 7/15: [================              ] 35/63 batches, loss: 0.0414Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0412Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0410Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0414Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0409Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0415Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0410Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0409Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0408Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0413Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0410Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0409Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0405Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0406Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0409Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0410Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0413Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0411Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0416Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0414Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0415Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0417Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0414Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0411Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0409Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0411Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0407Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0405Epoch 7/15: [==============================] 63/63 batches, loss: 0.0402
[2025-04-29 21:28:32,659][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0402
[2025-04-29 21:28:32,851][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0655, Metrics: {'mse': 0.06520910561084747, 'rmse': 0.2553607362357171, 'r2': -0.005089163780212402}
[2025-04-29 21:28:32,852][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0567Epoch 8/15: [                              ] 2/63 batches, loss: 0.0524Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0552Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0687Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0605Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0576Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0558Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0528Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0510Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0502Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0487Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0469Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0469Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0454Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0448Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0438Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0429Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0418Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0433Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0425Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0420Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0420Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0411Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0402Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0405Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0411Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0412Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0407Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0405Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0403Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0407Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0408Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0406Epoch 8/15: [================              ] 34/63 batches, loss: 0.0407Epoch 8/15: [================              ] 35/63 batches, loss: 0.0413Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0413Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0408Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0409Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0406Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0413Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0410Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0416Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0411Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0410Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0408Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0404Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0401Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0408Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0408Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0405Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0407Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0406Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0405Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0404Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0403Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0403Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0406Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0406Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0404Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0405Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0405Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0409Epoch 8/15: [==============================] 63/63 batches, loss: 0.0411
[2025-04-29 21:28:34,758][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0411
[2025-04-29 21:28:34,961][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0657, Metrics: {'mse': 0.06535712629556656, 'rmse': 0.2556503985828431, 'r2': -0.007370710372924805}
[2025-04-29 21:28:34,962][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:28:34,962][src.training.lm_trainer][INFO] - Early stopping at epoch 8
[2025-04-29 21:28:34,962][src.training.lm_trainer][INFO] - Training completed in 19.66 seconds
[2025-04-29 21:28:34,962][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:28:37,381][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03189478814601898, 'rmse': 0.17859112000885985, 'r2': -0.03900730609893799}
[2025-04-29 21:28:37,381][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06477416306734085, 'rmse': 0.25450768763898046, 'r2': 0.001614689826965332}
[2025-04-29 21:28:37,381][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.058885641396045685, 'rmse': 0.2426636383887081, 'r2': -0.015162229537963867}
[2025-04-29 21:28:38,998][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁
wandb:     best_val_mse █▃▁▁▁
wandb:      best_val_r2 ▁▆███
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁
wandb:           val_r2 ▁▆██████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06496
wandb:     best_val_mse 0.06477
wandb:      best_val_r2 0.00161
wandb:    best_val_rmse 0.25451
wandb:            epoch 8
wandb:   final_test_mse 0.05889
wandb:    final_test_r2 -0.01516
wandb:  final_test_rmse 0.24266
wandb:  final_train_mse 0.03189
wandb:   final_train_r2 -0.03901
wandb: final_train_rmse 0.17859
wandb:    final_val_mse 0.06477
wandb:     final_val_r2 0.00161
wandb:   final_val_rmse 0.25451
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04111
wandb:       train_time 19.66318
wandb:         val_loss 0.06569
wandb:          val_mse 0.06536
wandb:           val_r2 -0.00737
wandb:         val_rmse 0.25565
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212803-ry3uyj5d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212803-ry3uyj5d/logs
Standard experiment completed successfully: layer_1_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity/results.json
Running question_type experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:28:52,225][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type
experiment_name: layer_2_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:28:52,225][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:28:52,225][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:28:52,225][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:28:52,270][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 21:28:52,270][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:28:54,054][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:28:56,297][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:28:56,298][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:56,390][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,435][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,533][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:28:56,540][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:56,540][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:28:56,542][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:56,572][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,633][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:28:56,634][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:56,634][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:28:56,635][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:28:56,665][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,712][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:28:56,726][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:28:56,727][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:28:56,727][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:28:56,728][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:28:56,729][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:28:56,729][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:28:56,729][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:28:56,729][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:28:56,729][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 21:28:56,729][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:28:56,730][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 21:28:56,730][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:28:56,730][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:28:56,731][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 21:28:56,731][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:28:56,731][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:28:56,732][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:28:56,732][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:29:00,889][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:29:00,890][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:29:00,891][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:29:00,891][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=2, freeze_model=True, finetune=False
[2025-04-29 21:29:00,892][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:29:00,892][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7326Epoch 1/15: [                              ] 2/63 batches, loss: 0.7367Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7108Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7149Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6972Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6999Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7020Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7002Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6974Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6977Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7008Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7054Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7030Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7019Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7038Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7050Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7050Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7040Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7048Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7047Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7037Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7016Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7031Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7043Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7037Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7037Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7032Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7029Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7030Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7006Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7014Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7015Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7021Epoch 1/15: [================              ] 34/63 batches, loss: 0.7034Epoch 1/15: [================              ] 35/63 batches, loss: 0.7033Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7019Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7022Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7023Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7032Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7032Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7042Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7051Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7048Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7048Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7048Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7044Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7050Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7050Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7038Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7041Epoch 1/15: [========================      ] 51/63 batches, loss: 0.7021Epoch 1/15: [========================      ] 52/63 batches, loss: 0.7013Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.7017Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.7022Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.7026Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.7017Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.7020Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.7010Epoch 1/15: [============================  ] 59/63 batches, loss: 0.7010Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7019Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7021Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7025Epoch 1/15: [==============================] 63/63 batches, loss: 0.7016
[2025-04-29 21:29:07,801][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7016
[2025-04-29 21:29:07,978][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6884, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6947Epoch 2/15: [                              ] 2/63 batches, loss: 0.7047Epoch 2/15: [=                             ] 3/63 batches, loss: 0.7070Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6986Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6988Epoch 2/15: [==                            ] 6/63 batches, loss: 0.7023Epoch 2/15: [===                           ] 7/63 batches, loss: 0.7124Epoch 2/15: [===                           ] 8/63 batches, loss: 0.7019Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6993Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6974Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6926Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6954Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6950Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6928Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6925Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6960Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6971Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6966Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6944Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6935Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6917Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6921Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6932Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6927Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6932Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6948Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6953Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6969Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6958Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6945Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6957Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6974Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6963Epoch 2/15: [================              ] 34/63 batches, loss: 0.6964Epoch 2/15: [================              ] 35/63 batches, loss: 0.6952Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6953Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6945Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6948Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6951Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6952Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6959Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6969Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6965Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6959Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6957Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6955Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6949Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6950Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6950Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6948Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6950Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6947Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6951Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6955Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6960Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6960Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6959Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6959Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6960Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6957Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6962Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6965Epoch 2/15: [==============================] 63/63 batches, loss: 0.6974
[2025-04-29 21:29:10,290][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6974
[2025-04-29 21:29:10,476][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6886, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:29:10,477][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6580Epoch 3/15: [                              ] 2/63 batches, loss: 0.6903Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6753Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6734Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6826Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6804Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6788Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6771Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6835Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6816Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6832Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6867Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6855Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6851Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6889Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6906Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6919Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6907Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6902Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6909Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6914Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6918Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6933Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6920Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6935Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6941Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6941Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6944Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6947Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6947Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6941Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6945Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6941Epoch 3/15: [================              ] 34/63 batches, loss: 0.6947Epoch 3/15: [================              ] 35/63 batches, loss: 0.6955Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6953Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6954Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6957Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6961Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6964Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6961Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6972Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6969Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6967Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6969Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6968Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6963Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6964Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6964Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6967Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6969Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6965Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6965Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6964Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6964Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6963Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6964Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6964Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6959Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6957Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6961Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6964Epoch 3/15: [==============================] 63/63 batches, loss: 0.6965
[2025-04-29 21:29:12,378][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6965
[2025-04-29 21:29:12,566][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6890, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:29:12,566][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6939Epoch 4/15: [                              ] 2/63 batches, loss: 0.6917Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6926Epoch 4/15: [=                             ] 4/63 batches, loss: 0.7024Epoch 4/15: [==                            ] 5/63 batches, loss: 0.7046Epoch 4/15: [==                            ] 6/63 batches, loss: 0.7072Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6984Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6974Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6956Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6928Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6934Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6945Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6979Epoch 4/15: [======                        ] 14/63 batches, loss: 0.7010Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.7003Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.7000Epoch 4/15: [========                      ] 17/63 batches, loss: 0.7002Epoch 4/15: [========                      ] 18/63 batches, loss: 0.7000Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6994Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6997Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6985Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6975Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6984Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6978Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6984Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6983Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6973Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6975Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6965Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6970Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6962Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6960Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6962Epoch 4/15: [================              ] 34/63 batches, loss: 0.6959Epoch 4/15: [================              ] 35/63 batches, loss: 0.6963Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6956Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6963Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6964Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6965Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6962Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6961Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6965Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6961Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6962Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6960Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6960Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6960Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6951Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6948Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6953Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6955Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6952Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6947Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6945Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6941Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6947Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6940Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6937Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6939Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6943Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6946Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6946Epoch 4/15: [==============================] 63/63 batches, loss: 0.6941
[2025-04-29 21:29:14,465][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6941
[2025-04-29 21:29:14,656][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6894, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:29:14,656][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:29:14,657][src.training.lm_trainer][INFO] - Early stopping at epoch 4
[2025-04-29 21:29:14,657][src.training.lm_trainer][INFO] - Training completed in 9.58 seconds
[2025-04-29 21:29:14,657][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:29:17,137][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0}
[2025-04-29 21:29:17,138][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:29:17,138][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0}
[2025-04-29 21:29:18,756][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▄▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.68838
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69413
wandb:           train_time 9.57517
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.68935
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212852-u8my6nbe
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212852-u8my6nbe/logs
Standard experiment completed successfully: layer_2_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type/results.json
Running complexity experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:29:38,492][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity
experiment_name: layer_2_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:29:38,492][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:29:38,492][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:29:38,492][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:29:38,496][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 21:29:38,496][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:29:40,090][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:29:42,343][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:29:42,344][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:42,410][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,445][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,549][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:29:42,557][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:42,558][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:29:42,559][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:42,583][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,621][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,636][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:29:42,637][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:42,638][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:29:42,640][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:29:42,685][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,737][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:29:42,756][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:29:42,757][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:29:42,757][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:29:42,760][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:42,761][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:42,761][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:29:42,761][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:42,762][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:42,762][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 21:29:42,762][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:29:42,763][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:29:42,763][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:29:42,763][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:29:42,764][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:29:42,764][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:29:47,226][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:29:47,228][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:29:47,229][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:29:47,229][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=2, freeze_model=True, finetune=False
[2025-04-29 21:29:47,230][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:29:47,230][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.5023Epoch 1/15: [                              ] 2/63 batches, loss: 0.4885Epoch 1/15: [=                             ] 3/63 batches, loss: 0.5221Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4958Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4776Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4790Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4746Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4595Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4582Epoch 1/15: [====                          ] 10/63 batches, loss: 0.4527Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.4581Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.4607Epoch 1/15: [======                        ] 13/63 batches, loss: 0.4570Epoch 1/15: [======                        ] 14/63 batches, loss: 0.4547Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.4468Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.4510Epoch 1/15: [========                      ] 17/63 batches, loss: 0.4560Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4538Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.4550Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.4581Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.4556Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.4500Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.4468Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.4462Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.4450Epoch 1/15: [============                  ] 26/63 batches, loss: 0.4392Epoch 1/15: [============                  ] 27/63 batches, loss: 0.4358Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.4300Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.4327Epoch 1/15: [==============                ] 30/63 batches, loss: 0.4316Epoch 1/15: [==============                ] 31/63 batches, loss: 0.4333Epoch 1/15: [===============               ] 32/63 batches, loss: 0.4319Epoch 1/15: [===============               ] 33/63 batches, loss: 0.4292Epoch 1/15: [================              ] 34/63 batches, loss: 0.4304Epoch 1/15: [================              ] 35/63 batches, loss: 0.4298Epoch 1/15: [=================             ] 36/63 batches, loss: 0.4264Epoch 1/15: [=================             ] 37/63 batches, loss: 0.4236Epoch 1/15: [==================            ] 38/63 batches, loss: 0.4234Epoch 1/15: [==================            ] 39/63 batches, loss: 0.4215Epoch 1/15: [===================           ] 40/63 batches, loss: 0.4211Epoch 1/15: [===================           ] 41/63 batches, loss: 0.4195Epoch 1/15: [====================          ] 42/63 batches, loss: 0.4183Epoch 1/15: [====================          ] 43/63 batches, loss: 0.4148Epoch 1/15: [====================          ] 44/63 batches, loss: 0.4143Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.4123Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.4105Epoch 1/15: [======================        ] 47/63 batches, loss: 0.4105Epoch 1/15: [======================        ] 48/63 batches, loss: 0.4076Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.4054Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.4050Epoch 1/15: [========================      ] 51/63 batches, loss: 0.4021Epoch 1/15: [========================      ] 52/63 batches, loss: 0.4009Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.4002Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3990Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3963Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.3939Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.3932Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.3921Epoch 1/15: [============================  ] 59/63 batches, loss: 0.3922Epoch 1/15: [============================  ] 60/63 batches, loss: 0.3921Epoch 1/15: [============================= ] 61/63 batches, loss: 0.3920Epoch 1/15: [============================= ] 62/63 batches, loss: 0.3919Epoch 1/15: [==============================] 63/63 batches, loss: 0.3907
[2025-04-29 21:29:52,059][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3907
[2025-04-29 21:29:52,229][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2986, Metrics: {'mse': 0.30136725306510925, 'rmse': 0.548969264226249, 'r2': -3.6450719833374023}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2965Epoch 2/15: [                              ] 2/63 batches, loss: 0.3367Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3488Epoch 2/15: [=                             ] 4/63 batches, loss: 0.3555Epoch 2/15: [==                            ] 5/63 batches, loss: 0.3457Epoch 2/15: [==                            ] 6/63 batches, loss: 0.3276Epoch 2/15: [===                           ] 7/63 batches, loss: 0.3225Epoch 2/15: [===                           ] 8/63 batches, loss: 0.3190Epoch 2/15: [====                          ] 9/63 batches, loss: 0.3163Epoch 2/15: [====                          ] 10/63 batches, loss: 0.3092Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.3077Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.3045Epoch 2/15: [======                        ] 13/63 batches, loss: 0.3069Epoch 2/15: [======                        ] 14/63 batches, loss: 0.3023Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.3008Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.3063Epoch 2/15: [========                      ] 17/63 batches, loss: 0.3041Epoch 2/15: [========                      ] 18/63 batches, loss: 0.3034Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.3022Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.3041Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.3010Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2977Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.2958Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.2938Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.2928Epoch 2/15: [============                  ] 26/63 batches, loss: 0.2937Epoch 2/15: [============                  ] 27/63 batches, loss: 0.2947Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.2927Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.2884Epoch 2/15: [==============                ] 30/63 batches, loss: 0.2875Epoch 2/15: [==============                ] 31/63 batches, loss: 0.2866Epoch 2/15: [===============               ] 32/63 batches, loss: 0.2855Epoch 2/15: [===============               ] 33/63 batches, loss: 0.2847Epoch 2/15: [================              ] 34/63 batches, loss: 0.2847Epoch 2/15: [================              ] 35/63 batches, loss: 0.2825Epoch 2/15: [=================             ] 36/63 batches, loss: 0.2823Epoch 2/15: [=================             ] 37/63 batches, loss: 0.2805Epoch 2/15: [==================            ] 38/63 batches, loss: 0.2793Epoch 2/15: [==================            ] 39/63 batches, loss: 0.2778Epoch 2/15: [===================           ] 40/63 batches, loss: 0.2768Epoch 2/15: [===================           ] 41/63 batches, loss: 0.2767Epoch 2/15: [====================          ] 42/63 batches, loss: 0.2771Epoch 2/15: [====================          ] 43/63 batches, loss: 0.2749Epoch 2/15: [====================          ] 44/63 batches, loss: 0.2735Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.2717Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.2698Epoch 2/15: [======================        ] 47/63 batches, loss: 0.2683Epoch 2/15: [======================        ] 48/63 batches, loss: 0.2686Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.2668Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.2648Epoch 2/15: [========================      ] 51/63 batches, loss: 0.2637Epoch 2/15: [========================      ] 52/63 batches, loss: 0.2627Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.2622Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.2617Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.2619Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.2604Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.2600Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.2592Epoch 2/15: [============================  ] 59/63 batches, loss: 0.2585Epoch 2/15: [============================  ] 60/63 batches, loss: 0.2586Epoch 2/15: [============================= ] 61/63 batches, loss: 0.2578Epoch 2/15: [============================= ] 62/63 batches, loss: 0.2582Epoch 2/15: [==============================] 63/63 batches, loss: 0.2562
[2025-04-29 21:29:54,547][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2562
[2025-04-29 21:29:54,737][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2009, Metrics: {'mse': 0.20305246114730835, 'rmse': 0.45061342761541, 'r2': -2.129713773727417}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.2859Epoch 3/15: [                              ] 2/63 batches, loss: 0.2456Epoch 3/15: [=                             ] 3/63 batches, loss: 0.2366Epoch 3/15: [=                             ] 4/63 batches, loss: 0.2246Epoch 3/15: [==                            ] 5/63 batches, loss: 0.2236Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2273Epoch 3/15: [===                           ] 7/63 batches, loss: 0.2166Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2196Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2206Epoch 3/15: [====                          ] 10/63 batches, loss: 0.2126Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.2066Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.2105Epoch 3/15: [======                        ] 13/63 batches, loss: 0.2108Epoch 3/15: [======                        ] 14/63 batches, loss: 0.2071Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.2110Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.2132Epoch 3/15: [========                      ] 17/63 batches, loss: 0.2114Epoch 3/15: [========                      ] 18/63 batches, loss: 0.2082Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.2070Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.2078Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.2064Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.2056Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.2038Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.2018Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1999Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1984Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1969Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1954Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1956Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1940Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1923Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1904Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1885Epoch 3/15: [================              ] 34/63 batches, loss: 0.1884Epoch 3/15: [================              ] 35/63 batches, loss: 0.1882Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1860Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1855Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1851Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1839Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1858Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1846Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1842Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1834Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1823Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1806Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1809Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1798Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1798Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1808Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1802Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1796Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1798Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1789Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1777Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1783Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1779Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1772Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1757Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1751Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1735Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1729Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1734Epoch 3/15: [==============================] 63/63 batches, loss: 0.1728
[2025-04-29 21:29:57,052][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1728
[2025-04-29 21:29:57,246][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1395, Metrics: {'mse': 0.14106836915016174, 'rmse': 0.37559069364157804, 'r2': -1.174332857131958}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.2011Epoch 4/15: [                              ] 2/63 batches, loss: 0.1340Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1413Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1355Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1496Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1504Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1402Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1432Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1407Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1435Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1432Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1445Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1445Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1442Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1410Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1373Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1359Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1346Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1345Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1351Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1335Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1333Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1339Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1329Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1318Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1313Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1310Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1297Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1284Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1292Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1288Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1282Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1259Epoch 4/15: [================              ] 34/63 batches, loss: 0.1256Epoch 4/15: [================              ] 35/63 batches, loss: 0.1259Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1252Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1252Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1245Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1248Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1244Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1247Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1254Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1250Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1257Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1253Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1241Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1236Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1231Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1226Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1229Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1225Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1233Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1224Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1212Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1204Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1202Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1195Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1188Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1193Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1195Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1202Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1193Epoch 4/15: [==============================] 63/63 batches, loss: 0.1185
[2025-04-29 21:29:59,528][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1185
[2025-04-29 21:29:59,739][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1041, Metrics: {'mse': 0.10517554730176926, 'rmse': 0.32430779716462144, 'r2': -0.6211049556732178}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1100Epoch 5/15: [                              ] 2/63 batches, loss: 0.1204Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1097Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1193Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1077Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1043Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1005Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0972Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0936Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0914Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0889Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0903Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0917Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0915Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0910Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0907Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0883Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0878Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0883Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0890Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0904Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0933Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0917Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0897Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0896Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0904Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0903Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0895Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0907Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0901Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0888Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0872Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0876Epoch 5/15: [================              ] 34/63 batches, loss: 0.0882Epoch 5/15: [================              ] 35/63 batches, loss: 0.0875Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0869Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0878Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0874Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0870Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0874Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0867Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0881Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0875Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0877Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0876Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0876Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0877Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0866Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0859Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0869Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0865Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0873Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0868Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0857Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0852Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0845Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0841Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0851Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0850Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0846Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0851Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0843Epoch 5/15: [==============================] 63/63 batches, loss: 0.0832
[2025-04-29 21:30:02,040][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0832
[2025-04-29 21:30:02,235][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0833, Metrics: {'mse': 0.0839855968952179, 'rmse': 0.28980268614217136, 'r2': -0.29449737071990967}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0954Epoch 6/15: [                              ] 2/63 batches, loss: 0.0807Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0701Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0670Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0635Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0681Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0699Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0669Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0673Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0699Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0728Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0740Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0717Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0711Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0709Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0699Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0693Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0673Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0669Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0676Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0671Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0675Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0668Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0659Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0661Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0661Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0657Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0660Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0670Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0667Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0663Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0665Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0680Epoch 6/15: [================              ] 34/63 batches, loss: 0.0681Epoch 6/15: [================              ] 35/63 batches, loss: 0.0674Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0663Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0651Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0655Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0663Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0653Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0647Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0643Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0637Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0648Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0647Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0657Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0652Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0657Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0654Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0652Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0648Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0642Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0638Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0632Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0633Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0633Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0633Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0635Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0637Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0633Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0629Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0625Epoch 6/15: [==============================] 63/63 batches, loss: 0.0619
[2025-04-29 21:30:04,541][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0619
[2025-04-29 21:30:04,759][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0726, Metrics: {'mse': 0.0729767307639122, 'rmse': 0.2701420566367114, 'r2': -0.12481415271759033}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0874Epoch 7/15: [                              ] 2/63 batches, loss: 0.0852Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0667Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0655Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0664Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0636Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0610Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0597Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0575Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0567Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0622Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0616Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0616Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0617Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0593Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0588Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0587Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0582Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0571Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0565Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0552Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0543Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0535Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0519Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0513Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0521Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0528Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0540Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0536Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0554Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0551Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0541Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0541Epoch 7/15: [================              ] 34/63 batches, loss: 0.0533Epoch 7/15: [================              ] 35/63 batches, loss: 0.0530Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0524Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0522Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0526Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0524Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0529Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0522Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0519Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0520Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0518Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0517Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0514Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0510Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0507Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0510Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0514Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0521Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0518Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0521Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0518Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0520Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0521Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0517Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0514Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0513Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0514Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0509Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0506Epoch 7/15: [==============================] 63/63 batches, loss: 0.0502
[2025-04-29 21:30:07,039][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0502
[2025-04-29 21:30:07,237][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0677, Metrics: {'mse': 0.06778743118047714, 'rmse': 0.2603601950768918, 'r2': -0.044829726219177246}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0728Epoch 8/15: [                              ] 2/63 batches, loss: 0.0602Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0621Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0757Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0670Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0653Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0647Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0616Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0586Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0572Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0554Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0541Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0531Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0511Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0497Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0485Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0483Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0475Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0490Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0486Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0483Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0478Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0469Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0463Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0463Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0469Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0474Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0470Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0462Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0459Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0460Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0461Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0463Epoch 8/15: [================              ] 34/63 batches, loss: 0.0462Epoch 8/15: [================              ] 35/63 batches, loss: 0.0473Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0472Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0465Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0467Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0464Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0469Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0465Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0471Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0464Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0464Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0459Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0454Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0452Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0463Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0464Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0460Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0462Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0460Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0460Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0459Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0458Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0457Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0459Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0460Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0457Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0457Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0458Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0461Epoch 8/15: [==============================] 63/63 batches, loss: 0.0463
[2025-04-29 21:30:09,593][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0463
[2025-04-29 21:30:09,810][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0658, Metrics: {'mse': 0.06577123701572418, 'rmse': 0.2564590357459144, 'r2': -0.013753533363342285}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0497Epoch 9/15: [                              ] 2/63 batches, loss: 0.0411Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0497Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0501Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0469Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0432Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0415Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0419Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0418Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0432Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0449Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0458Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0450Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0442Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0477Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0482Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0484Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0473Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0457Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0452Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0459Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0456Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0456Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0458Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0455Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0448Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0454Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0451Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0450Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0451Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0455Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0452Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0454Epoch 9/15: [================              ] 34/63 batches, loss: 0.0455Epoch 9/15: [================              ] 35/63 batches, loss: 0.0448Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0447Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0446Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0445Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0455Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0450Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0446Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0447Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0448Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0446Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0442Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0449Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0451Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0448Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0450Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0448Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0444Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0453Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0455Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0455Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0462Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0463Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0463Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0459Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0460Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0456Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0455Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0460Epoch 9/15: [==============================] 63/63 batches, loss: 0.0456
[2025-04-29 21:30:12,150][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0456
[2025-04-29 21:30:12,371][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0654, Metrics: {'mse': 0.06522632390260696, 'rmse': 0.25539444767380315, 'r2': -0.005354523658752441}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0400Epoch 10/15: [                              ] 2/63 batches, loss: 0.0397Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0445Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0446Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0431Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0463Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0453Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0438Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0471Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0446Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0439Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0427Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0413Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0416Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0426Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0418Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0414Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0433Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0434Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0444Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0439Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0460Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0467Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0459Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0472Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0471Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0464Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0454Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0463Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0474Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0468Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0468Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0467Epoch 10/15: [================              ] 34/63 batches, loss: 0.0469Epoch 10/15: [================              ] 35/63 batches, loss: 0.0463Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0465Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0458Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0453Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0454Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0461Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0462Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0458Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0455Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0453Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0449Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0454Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0455Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0451Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0451Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0449Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0446Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0448Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0443Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0442Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0448Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0448Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0450Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0446Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0444Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0440Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0442Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0441Epoch 10/15: [==============================] 63/63 batches, loss: 0.0434
[2025-04-29 21:30:14,683][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0434
[2025-04-29 21:30:14,906][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0655, Metrics: {'mse': 0.06526745110750198, 'rmse': 0.2554749520158522, 'r2': -0.005988597869873047}
[2025-04-29 21:30:14,907][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0273Epoch 11/15: [                              ] 2/63 batches, loss: 0.0399Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0436Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0467Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0400Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0395Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0387Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0390Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0393Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0425Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0432Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0415Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0428Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0451Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0442Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0446Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0454Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0462Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0466Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0474Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0466Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0455Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0450Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0450Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0460Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0458Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0450Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0443Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0444Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0452Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0460Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0460Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0464Epoch 11/15: [================              ] 34/63 batches, loss: 0.0470Epoch 11/15: [================              ] 35/63 batches, loss: 0.0468Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0461Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0467Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0478Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0477Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0482Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0485Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0483Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0479Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0483Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0484Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0484Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0484Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0481Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0477Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0478Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0477Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0477Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0476Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0476Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0475Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0470Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0467Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0467Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0464Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0463Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0459Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0457Epoch 11/15: [==============================] 63/63 batches, loss: 0.0463
[2025-04-29 21:30:16,848][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0463
[2025-04-29 21:30:17,075][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0657, Metrics: {'mse': 0.06544340401887894, 'rmse': 0.2558190845478088, 'r2': -0.00870060920715332}
[2025-04-29 21:30:17,075][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0270Epoch 12/15: [                              ] 2/63 batches, loss: 0.0445Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0445Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0496Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0427Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0434Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0413Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0412Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0417Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0426Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0424Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0398Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0411Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0404Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0391Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0389Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0394Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0389Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0398Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0407Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0402Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0396Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0401Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0395Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0422Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0419Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0413Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0410Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0409Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0412Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0425Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0444Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0443Epoch 12/15: [================              ] 34/63 batches, loss: 0.0445Epoch 12/15: [================              ] 35/63 batches, loss: 0.0447Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0444Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0449Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0447Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0456Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0456Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0452Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0457Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0463Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0461Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0459Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0457Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0458Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0458Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0456Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0457Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0457Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0453Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0449Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0462Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0468Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0466Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0467Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0470Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0474Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0475Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0474Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0475Epoch 12/15: [==============================] 63/63 batches, loss: 0.0476
[2025-04-29 21:30:19,004][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0476
[2025-04-29 21:30:19,211][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0661, Metrics: {'mse': 0.06573919206857681, 'rmse': 0.25639655237264175, 'r2': -0.013259649276733398}
[2025-04-29 21:30:19,211][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:30:19,211][src.training.lm_trainer][INFO] - Early stopping at epoch 12
[2025-04-29 21:30:19,212][src.training.lm_trainer][INFO] - Training completed in 29.84 seconds
[2025-04-29 21:30:19,212][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:30:21,654][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03258334845304489, 'rmse': 0.18050858276836837, 'r2': -0.06143808364868164}
[2025-04-29 21:30:21,655][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06522632390260696, 'rmse': 0.25539444767380315, 'r2': -0.005354523658752441}
[2025-04-29 21:30:21,655][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.059661537408828735, 'rmse': 0.2442571133228851, 'r2': -0.02853834629058838}
[2025-04-29 21:30:23,316][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▂▁▁▁▁
wandb:     best_val_mse █▅▃▂▂▁▁▁▁
wandb:      best_val_r2 ▁▄▆▇▇████
wandb:    best_val_rmse █▆▄▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▄▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▂▁▁▁▁▁▁▁
wandb:          val_mse █▅▃▂▂▁▁▁▁▁▁▁
wandb:           val_r2 ▁▄▆▇▇███████
wandb:         val_rmse █▆▄▃▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06538
wandb:     best_val_mse 0.06523
wandb:      best_val_r2 -0.00535
wandb:    best_val_rmse 0.25539
wandb:            epoch 12
wandb:   final_test_mse 0.05966
wandb:    final_test_r2 -0.02854
wandb:  final_test_rmse 0.24426
wandb:  final_train_mse 0.03258
wandb:   final_train_r2 -0.06144
wandb: final_train_rmse 0.18051
wandb:    final_val_mse 0.06523
wandb:     final_val_r2 -0.00535
wandb:   final_val_rmse 0.25539
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04759
wandb:       train_time 29.84087
wandb:         val_loss 0.06606
wandb:          val_mse 0.06574
wandb:           val_r2 -0.01326
wandb:         val_rmse 0.2564
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212938-k3ce2lhy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_212938-k3ce2lhy/logs
Standard experiment completed successfully: layer_2_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity/results.json
Running question_type experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:30:38,858][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type
experiment_name: layer_6_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:30:38,858][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:30:38,858][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:30:38,858][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:30:38,862][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 21:30:38,862][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:30:40,722][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:30:42,968][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:30:42,969][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:30:43,082][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,243][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:30:43,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:30:43,253][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:30:43,255][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:30:43,287][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,326][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,342][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:30:43,343][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:30:43,344][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:30:43,345][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:30:43,374][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,413][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:30:43,429][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:30:43,431][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:30:43,431][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:30:43,432][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:30:43,433][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:30:43,433][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:30:43,433][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:30:43,434][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 21:30:43,434][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:30:43,434][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:30:43,435][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 21:30:43,435][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:30:43,435][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:30:43,435][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 21:30:43,435][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 21:30:43,436][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:30:43,436][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:30:43,436][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:30:43,436][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:30:43,436][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:30:43,436][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:30:48,032][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:30:48,033][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:30:48,034][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:30:48,034][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 21:30:48,035][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:30:48,035][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7221Epoch 1/15: [                              ] 2/63 batches, loss: 0.7270Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7062Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7084Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6954Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6979Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7000Epoch 1/15: [===                           ] 8/63 batches, loss: 0.6985Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6960Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6963Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.6986Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7020Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7001Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6994Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7007Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7017Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7017Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7010Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7014Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7014Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7005Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6992Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7003Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7014Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7007Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7007Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7003Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6999Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7000Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6982Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6989Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6989Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6994Epoch 1/15: [================              ] 34/63 batches, loss: 0.7004Epoch 1/15: [================              ] 35/63 batches, loss: 0.7004Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6993Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6996Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6996Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7002Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7001Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7010Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7017Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7015Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7015Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7015Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7012Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7018Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7017Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7008Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7011Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6995Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6989Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6991Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6996Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6998Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6992Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6993Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6986Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6985Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6991Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6993Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6996Epoch 1/15: [==============================] 63/63 batches, loss: 0.6989
[2025-04-29 21:30:53,019][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6989
[2025-04-29 21:30:53,224][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6887, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6936Epoch 2/15: [                              ] 2/63 batches, loss: 0.7016Epoch 2/15: [=                             ] 3/63 batches, loss: 0.7034Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6953Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6956Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6985Epoch 2/15: [===                           ] 7/63 batches, loss: 0.7069Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6985Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6964Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6951Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6913Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6930Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6926Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6909Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6906Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6934Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6943Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6939Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6922Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6916Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6903Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6907Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6916Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6912Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6918Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6935Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6948Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6940Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6930Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6939Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6954Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6944Epoch 2/15: [================              ] 34/63 batches, loss: 0.6945Epoch 2/15: [================              ] 35/63 batches, loss: 0.6935Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6936Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6930Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6933Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6936Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6937Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6942Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6950Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6946Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6942Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6940Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6939Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6934Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6936Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6936Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6935Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6936Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6934Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6937Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6940Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6944Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6944Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6943Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6943Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6944Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6942Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6946Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6948Epoch 2/15: [==============================] 63/63 batches, loss: 0.6955
[2025-04-29 21:30:55,539][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6955
[2025-04-29 21:30:55,732][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6890, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:30:55,733][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6632Epoch 3/15: [                              ] 2/63 batches, loss: 0.6896Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6776Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6754Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6831Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6813Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6803Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6792Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6842Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6826Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6839Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6864Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6855Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6852Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6883Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6898Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6911Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6902Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6899Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6905Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6910Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6913Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6925Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6913Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6926Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6930Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6930Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6933Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6936Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6936Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6935Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 3/15: [================              ] 34/63 batches, loss: 0.6936Epoch 3/15: [================              ] 35/63 batches, loss: 0.6943Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6942Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6942Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6945Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6948Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6950Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6949Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6957Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6956Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6955Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6957Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6957Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6953Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6953Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6953Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6956Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6958Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6956Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6957Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6956Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6955Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6954Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6955Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6955Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6951Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6949Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6952Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6955Epoch 3/15: [==============================] 63/63 batches, loss: 0.6956
[2025-04-29 21:30:57,646][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6956
[2025-04-29 21:30:57,837][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6895, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:30:57,838][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6920Epoch 4/15: [                              ] 2/63 batches, loss: 0.6939Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6939Epoch 4/15: [=                             ] 4/63 batches, loss: 0.7013Epoch 4/15: [==                            ] 5/63 batches, loss: 0.7028Epoch 4/15: [==                            ] 6/63 batches, loss: 0.7055Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6979Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6967Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6950Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6925Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6929Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6937Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6966Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6989Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6984Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6980Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6981Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6980Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6975Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6977Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6967Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6959Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6968Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6962Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6968Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6966Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6957Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6959Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6951Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6956Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6949Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6947Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6948Epoch 4/15: [================              ] 34/63 batches, loss: 0.6946Epoch 4/15: [================              ] 35/63 batches, loss: 0.6951Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6944Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6951Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6952Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6952Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6949Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6948Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6953Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6948Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6950Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6948Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6948Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6949Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6941Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6938Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6942Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6944Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6942Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6938Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6937Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6934Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6939Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6933Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6930Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6932Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6936Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6937Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6938Epoch 4/15: [==============================] 63/63 batches, loss: 0.6934
[2025-04-29 21:30:59,736][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6934
[2025-04-29 21:30:59,927][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6898, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:30:59,928][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:30:59,929][src.training.lm_trainer][INFO] - Early stopping at epoch 4
[2025-04-29 21:30:59,929][src.training.lm_trainer][INFO] - Training completed in 9.62 seconds
[2025-04-29 21:30:59,929][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:31:02,623][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0}
[2025-04-29 21:31:02,624][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 21:31:02,624][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0}
[2025-04-29 21:31:04,264][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▄▄▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.68865
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69341
wandb:           train_time 9.61871
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.68981
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213038-vfxfnfpw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213038-vfxfnfpw/logs
Standard experiment completed successfully: layer_6_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type/results.json
Running complexity experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:31:15,674][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity
experiment_name: layer_6_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:31:15,674][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:31:15,674][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:31:15,674][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:31:15,679][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 21:31:15,679][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:31:17,312][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:31:19,536][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:31:19,537][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:31:19,620][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,663][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,761][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:31:19,768][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:31:19,769][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:31:19,770][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:31:19,800][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,845][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,864][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:31:19,865][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:31:19,865][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:31:19,867][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:31:19,906][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,953][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:31:19,968][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:31:19,969][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:31:19,970][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:31:19,971][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:31:19,972][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:31:19,972][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:31:19,973][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:31:19,973][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:31:19,973][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:31:19,974][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:31:19,974][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:31:19,974][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:31:19,975][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:31:19,975][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 21:31:19,975][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:31:19,975][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 21:31:19,975][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:31:19,975][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:31:19,975][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:31:19,976][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:31:24,150][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:31:24,151][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:31:24,151][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:31:24,152][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 21:31:24,153][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:31:24,153][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.4241Epoch 1/15: [                              ] 2/63 batches, loss: 0.4080Epoch 1/15: [=                             ] 3/63 batches, loss: 0.4369Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4142Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3988Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4007Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3967Epoch 1/15: [===                           ] 8/63 batches, loss: 0.3833Epoch 1/15: [====                          ] 9/63 batches, loss: 0.3823Epoch 1/15: [====                          ] 10/63 batches, loss: 0.3778Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.3819Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.3858Epoch 1/15: [======                        ] 13/63 batches, loss: 0.3829Epoch 1/15: [======                        ] 14/63 batches, loss: 0.3813Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.3747Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.3789Epoch 1/15: [========                      ] 17/63 batches, loss: 0.3837Epoch 1/15: [========                      ] 18/63 batches, loss: 0.3818Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.3827Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.3855Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.3834Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.3786Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.3760Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.3757Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.3745Epoch 1/15: [============                  ] 26/63 batches, loss: 0.3693Epoch 1/15: [============                  ] 27/63 batches, loss: 0.3664Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.3612Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.3639Epoch 1/15: [==============                ] 30/63 batches, loss: 0.3629Epoch 1/15: [==============                ] 31/63 batches, loss: 0.3649Epoch 1/15: [===============               ] 32/63 batches, loss: 0.3638Epoch 1/15: [===============               ] 33/63 batches, loss: 0.3616Epoch 1/15: [================              ] 34/63 batches, loss: 0.3628Epoch 1/15: [================              ] 35/63 batches, loss: 0.3628Epoch 1/15: [=================             ] 36/63 batches, loss: 0.3598Epoch 1/15: [=================             ] 37/63 batches, loss: 0.3575Epoch 1/15: [==================            ] 38/63 batches, loss: 0.3575Epoch 1/15: [==================            ] 39/63 batches, loss: 0.3560Epoch 1/15: [===================           ] 40/63 batches, loss: 0.3562Epoch 1/15: [===================           ] 41/63 batches, loss: 0.3549Epoch 1/15: [====================          ] 42/63 batches, loss: 0.3539Epoch 1/15: [====================          ] 43/63 batches, loss: 0.3510Epoch 1/15: [====================          ] 44/63 batches, loss: 0.3508Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.3491Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.3476Epoch 1/15: [======================        ] 47/63 batches, loss: 0.3477Epoch 1/15: [======================        ] 48/63 batches, loss: 0.3453Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.3435Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.3432Epoch 1/15: [========================      ] 51/63 batches, loss: 0.3407Epoch 1/15: [========================      ] 52/63 batches, loss: 0.3398Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.3394Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3384Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3362Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.3342Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.3338Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.3329Epoch 1/15: [============================  ] 59/63 batches, loss: 0.3333Epoch 1/15: [============================  ] 60/63 batches, loss: 0.3334Epoch 1/15: [============================= ] 61/63 batches, loss: 0.3335Epoch 1/15: [============================= ] 62/63 batches, loss: 0.3336Epoch 1/15: [==============================] 63/63 batches, loss: 0.3324
[2025-04-29 21:31:28,596][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3324
[2025-04-29 21:31:28,781][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2652, Metrics: {'mse': 0.2678278088569641, 'rmse': 0.5175208293943, 'r2': -3.128117561340332}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2607Epoch 2/15: [                              ] 2/63 batches, loss: 0.2971Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3066Epoch 2/15: [=                             ] 4/63 batches, loss: 0.3126Epoch 2/15: [==                            ] 5/63 batches, loss: 0.3036Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2874Epoch 2/15: [===                           ] 7/63 batches, loss: 0.2838Epoch 2/15: [===                           ] 8/63 batches, loss: 0.2806Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2778Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2720Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.2704Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.2666Epoch 2/15: [======                        ] 13/63 batches, loss: 0.2693Epoch 2/15: [======                        ] 14/63 batches, loss: 0.2656Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2641Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.2692Epoch 2/15: [========                      ] 17/63 batches, loss: 0.2673Epoch 2/15: [========                      ] 18/63 batches, loss: 0.2665Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.2654Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.2674Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.2647Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2619Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.2605Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.2588Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.2582Epoch 2/15: [============                  ] 26/63 batches, loss: 0.2594Epoch 2/15: [============                  ] 27/63 batches, loss: 0.2604Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.2587Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.2549Epoch 2/15: [==============                ] 30/63 batches, loss: 0.2546Epoch 2/15: [==============                ] 31/63 batches, loss: 0.2537Epoch 2/15: [===============               ] 32/63 batches, loss: 0.2529Epoch 2/15: [===============               ] 33/63 batches, loss: 0.2523Epoch 2/15: [================              ] 34/63 batches, loss: 0.2525Epoch 2/15: [================              ] 35/63 batches, loss: 0.2506Epoch 2/15: [=================             ] 36/63 batches, loss: 0.2505Epoch 2/15: [=================             ] 37/63 batches, loss: 0.2489Epoch 2/15: [==================            ] 38/63 batches, loss: 0.2479Epoch 2/15: [==================            ] 39/63 batches, loss: 0.2467Epoch 2/15: [===================           ] 40/63 batches, loss: 0.2460Epoch 2/15: [===================           ] 41/63 batches, loss: 0.2459Epoch 2/15: [====================          ] 42/63 batches, loss: 0.2464Epoch 2/15: [====================          ] 43/63 batches, loss: 0.2445Epoch 2/15: [====================          ] 44/63 batches, loss: 0.2432Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.2417Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.2399Epoch 2/15: [======================        ] 47/63 batches, loss: 0.2386Epoch 2/15: [======================        ] 48/63 batches, loss: 0.2391Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.2376Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.2358Epoch 2/15: [========================      ] 51/63 batches, loss: 0.2350Epoch 2/15: [========================      ] 52/63 batches, loss: 0.2341Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.2337Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.2334Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.2337Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.2325Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.2323Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.2317Epoch 2/15: [============================  ] 59/63 batches, loss: 0.2311Epoch 2/15: [============================  ] 60/63 batches, loss: 0.2314Epoch 2/15: [============================= ] 61/63 batches, loss: 0.2307Epoch 2/15: [============================= ] 62/63 batches, loss: 0.2311Epoch 2/15: [==============================] 63/63 batches, loss: 0.2293
[2025-04-29 21:31:31,090][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2293
[2025-04-29 21:31:31,282][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1897, Metrics: {'mse': 0.19170837104320526, 'rmse': 0.4378451450492574, 'r2': -1.9548635482788086}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.2637Epoch 3/15: [                              ] 2/63 batches, loss: 0.2253Epoch 3/15: [=                             ] 3/63 batches, loss: 0.2170Epoch 3/15: [=                             ] 4/63 batches, loss: 0.2054Epoch 3/15: [==                            ] 5/63 batches, loss: 0.2048Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2083Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1990Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2019Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2032Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1966Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1912Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1947Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1952Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1917Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1956Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1978Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1963Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1934Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1923Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1933Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1919Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1913Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1895Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1878Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1861Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1848Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1835Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1821Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1825Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1812Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1797Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1780Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1762Epoch 3/15: [================              ] 34/63 batches, loss: 0.1763Epoch 3/15: [================              ] 35/63 batches, loss: 0.1759Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1740Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1736Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1732Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1722Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1741Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1730Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1727Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1721Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1711Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1695Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1698Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1688Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1690Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1701Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1695Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1691Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1693Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1684Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1673Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1679Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1676Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1669Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1656Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1650Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1636Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1630Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1636Epoch 3/15: [==============================] 63/63 batches, loss: 0.1632
[2025-04-29 21:31:33,611][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1632
[2025-04-29 21:31:33,810][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1381, Metrics: {'mse': 0.13965144753456116, 'rmse': 0.37369967558797956, 'r2': -1.1524932384490967}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1937Epoch 4/15: [                              ] 2/63 batches, loss: 0.1283Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1354Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1303Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1440Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1440Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1340Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1369Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1351Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1375Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1380Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1390Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1390Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1383Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1350Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1315Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1302Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1293Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1292Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1301Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1288Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1287Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1295Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1288Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1276Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1273Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1270Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1258Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1245Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1253Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1250Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1244Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1223Epoch 4/15: [================              ] 34/63 batches, loss: 0.1220Epoch 4/15: [================              ] 35/63 batches, loss: 0.1223Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1216Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1217Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1211Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1216Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1212Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1216Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1221Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1218Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1226Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1222Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1211Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1207Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1202Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1197Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1201Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1197Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1204Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1196Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1185Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1177Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1175Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1168Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1161Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1166Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1168Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1176Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1167Epoch 4/15: [==============================] 63/63 batches, loss: 0.1159
[2025-04-29 21:31:36,084][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1159
[2025-04-29 21:31:36,283][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1061, Metrics: {'mse': 0.10721492767333984, 'rmse': 0.32743690640082074, 'r2': -0.6525386571884155}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1099Epoch 5/15: [                              ] 2/63 batches, loss: 0.1197Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1098Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1198Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1087Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1048Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1010Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0974Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0939Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0919Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0895Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0904Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0916Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0915Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0910Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0908Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0885Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0881Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0887Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0895Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0910Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0939Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0924Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0903Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0903Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0912Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0912Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0904Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0916Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0910Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0899Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0883Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0887Epoch 5/15: [================              ] 34/63 batches, loss: 0.0895Epoch 5/15: [================              ] 35/63 batches, loss: 0.0888Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0883Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0892Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0887Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0882Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0887Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0881Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0894Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0888Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0890Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0889Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0889Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0889Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0879Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0873Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0883Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0880Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0888Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0883Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0872Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0869Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0861Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0857Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0868Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0866Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0863Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0867Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0861Epoch 5/15: [==============================] 63/63 batches, loss: 0.0849
[2025-04-29 21:31:38,555][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0849
[2025-04-29 21:31:38,759][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0865, Metrics: {'mse': 0.08719846606254578, 'rmse': 0.29529386390940426, 'r2': -0.3440183401107788}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0962Epoch 6/15: [                              ] 2/63 batches, loss: 0.0841Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0731Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0694Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0651Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0695Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0714Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0685Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0691Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0721Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0752Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0762Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0737Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0733Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0731Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0719Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0714Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0693Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0687Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0692Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0687Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0692Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0684Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0675Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0676Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0676Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0672Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0677Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0686Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0683Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0679Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0683Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0697Epoch 6/15: [================              ] 34/63 batches, loss: 0.0699Epoch 6/15: [================              ] 35/63 batches, loss: 0.0691Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0680Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0667Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0673Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0680Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0671Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0664Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0660Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0654Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0665Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0664Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0674Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0667Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0672Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0669Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0667Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0662Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0656Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0652Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0646Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0646Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0647Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0647Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0649Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0651Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0647Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0643Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0640Epoch 6/15: [==============================] 63/63 batches, loss: 0.0634
[2025-04-29 21:31:41,028][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0634
[2025-04-29 21:31:41,239][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0752, Metrics: {'mse': 0.0756973996758461, 'rmse': 0.27513160428392464, 'r2': -0.16674864292144775}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0879Epoch 7/15: [                              ] 2/63 batches, loss: 0.0836Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0651Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0655Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0665Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0650Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0623Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0605Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0586Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0583Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0639Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0635Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0635Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0637Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0612Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0607Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0607Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0600Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0590Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0582Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0570Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0560Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0551Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0535Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0527Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0537Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0541Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0554Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0551Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0569Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0565Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0555Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0554Epoch 7/15: [================              ] 34/63 batches, loss: 0.0546Epoch 7/15: [================              ] 35/63 batches, loss: 0.0544Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0537Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0536Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0539Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0537Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0541Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0533Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0530Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0532Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0529Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0528Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0525Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0521Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0518Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0521Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0525Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0531Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0528Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0530Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0527Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0529Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0531Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0527Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0524Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0523Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0524Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0519Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0517Epoch 7/15: [==============================] 63/63 batches, loss: 0.0514
[2025-04-29 21:31:43,559][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0514
[2025-04-29 21:31:43,765][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0693, Metrics: {'mse': 0.06950197368860245, 'rmse': 0.2636322698165049, 'r2': -0.07125651836395264}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0724Epoch 8/15: [                              ] 2/63 batches, loss: 0.0599Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0613Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0753Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0660Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0649Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0646Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0620Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0592Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0573Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0556Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0545Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0533Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0511Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0497Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0486Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0484Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0479Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0493Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0490Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0488Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0483Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0474Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0470Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0468Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0475Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0481Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0476Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0467Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0464Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0464Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0465Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0467Epoch 8/15: [================              ] 34/63 batches, loss: 0.0466Epoch 8/15: [================              ] 35/63 batches, loss: 0.0478Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0477Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0469Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0472Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0468Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0471Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0468Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0475Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0467Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0467Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0462Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0456Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0455Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0466Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0468Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0462Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0466Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0464Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0462Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0462Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0462Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0460Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0461Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0463Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0459Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0460Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0460Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0464Epoch 8/15: [==============================] 63/63 batches, loss: 0.0465
[2025-04-29 21:31:46,133][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0465
[2025-04-29 21:31:46,360][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0665, Metrics: {'mse': 0.06652151048183441, 'rmse': 0.25791764282777246, 'r2': -0.025317668914794922}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0495Epoch 9/15: [                              ] 2/63 batches, loss: 0.0400Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0507Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0494Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0471Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0432Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0411Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0418Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0421Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0434Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0451Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0458Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0448Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0443Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0476Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0479Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0483Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0472Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0456Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0451Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0457Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0453Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0454Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0455Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0450Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0444Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0450Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0446Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0445Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0447Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0451Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0448Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0448Epoch 9/15: [================              ] 34/63 batches, loss: 0.0450Epoch 9/15: [================              ] 35/63 batches, loss: 0.0443Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0441Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0440Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0440Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0450Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0445Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0441Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0442Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0443Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0441Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0437Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0444Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0446Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0443Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0445Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0443Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0440Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0447Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0450Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0449Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0456Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0456Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0454Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0451Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0450Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0447Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0446Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0450Epoch 9/15: [==============================] 63/63 batches, loss: 0.0446
[2025-04-29 21:31:48,710][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0446
[2025-04-29 21:31:48,926][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0654, Metrics: {'mse': 0.06532575190067291, 'rmse': 0.255589029304219, 'r2': -0.006887078285217285}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0376Epoch 10/15: [                              ] 2/63 batches, loss: 0.0371Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0425Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0426Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0417Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0459Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0450Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0433Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0464Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0441Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0435Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0423Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0407Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0410Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0419Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0410Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0404Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0422Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0423Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0433Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0430Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0449Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0453Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0446Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0458Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0458Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0451Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0441Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0452Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0463Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0458Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0458Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0456Epoch 10/15: [================              ] 34/63 batches, loss: 0.0457Epoch 10/15: [================              ] 35/63 batches, loss: 0.0453Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0454Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0447Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0444Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0446Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0452Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0453Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0449Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0446Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0444Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0440Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0445Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0446Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0443Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0443Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0441Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0437Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0440Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0436Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0433Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0438Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0439Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0441Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0437Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0435Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0432Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0432Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0432Epoch 10/15: [==============================] 63/63 batches, loss: 0.0425
[2025-04-29 21:31:51,259][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0425
[2025-04-29 21:31:51,475][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0652, Metrics: {'mse': 0.06507474929094315, 'rmse': 0.25509752897851273, 'r2': -0.0030182600021362305}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0240Epoch 11/15: [                              ] 2/63 batches, loss: 0.0373Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0403Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0417Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0361Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0361Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0354Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0357Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0364Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0403Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0413Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0399Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0413Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0434Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0425Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0426Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0433Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0441Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0446Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0454Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0446Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0436Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0431Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0433Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0443Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0441Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0433Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0427Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0427Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0434Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0441Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0442Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0445Epoch 11/15: [================              ] 34/63 batches, loss: 0.0450Epoch 11/15: [================              ] 35/63 batches, loss: 0.0448Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0442Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0448Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0458Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0457Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0461Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0464Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0463Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0458Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0463Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0464Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0464Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0463Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0461Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0457Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0457Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0456Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0456Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0456Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0455Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0454Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0449Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0446Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0445Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0443Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0441Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0437Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0436Epoch 11/15: [==============================] 63/63 batches, loss: 0.0442
[2025-04-29 21:31:53,837][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0442
[2025-04-29 21:31:54,059][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0654, Metrics: {'mse': 0.0651485025882721, 'rmse': 0.255242047061749, 'r2': -0.0041550397872924805}
[2025-04-29 21:31:54,059][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0268Epoch 12/15: [                              ] 2/63 batches, loss: 0.0429Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0430Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0474Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0407Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0415Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0397Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0395Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0402Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0412Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0409Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0383Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0395Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0387Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0376Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0375Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0379Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0372Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0381Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0390Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0385Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0379Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0382Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0376Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0396Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0395Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0390Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0388Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0388Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0389Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0402Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0421Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0420Epoch 12/15: [================              ] 34/63 batches, loss: 0.0422Epoch 12/15: [================              ] 35/63 batches, loss: 0.0424Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0421Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0426Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0424Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0432Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0432Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0428Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0433Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0441Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0438Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0435Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0434Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0434Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0435Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0434Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0435Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0435Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0431Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0427Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0439Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0444Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0442Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0444Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0446Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0449Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0451Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0450Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0451Epoch 12/15: [==============================] 63/63 batches, loss: 0.0452
[2025-04-29 21:31:56,024][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0452
[2025-04-29 21:31:56,229][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0657, Metrics: {'mse': 0.06538800895214081, 'rmse': 0.25571079162237326, 'r2': -0.007846713066101074}
[2025-04-29 21:31:56,229][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0536Epoch 13/15: [                              ] 2/63 batches, loss: 0.0496Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0495Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0465Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0496Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0472Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0483Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0500Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0492Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0490Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0465Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0468Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0453Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0464Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0467Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0483Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0476Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0470Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0483Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0473Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0484Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0485Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0481Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0479Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0476Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0476Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0493Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0500Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0492Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0489Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0487Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0481Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0480Epoch 13/15: [================              ] 34/63 batches, loss: 0.0482Epoch 13/15: [================              ] 35/63 batches, loss: 0.0484Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0479Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0473Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0471Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0466Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0461Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0460Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0459Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0457Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0453Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0454Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0458Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0464Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0462Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0455Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0454Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0448Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0449Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0450Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0450Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0446Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0443Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0438Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0438Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0440Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0435Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0435Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0442Epoch 13/15: [==============================] 63/63 batches, loss: 0.0437
[2025-04-29 21:31:58,152][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0437
[2025-04-29 21:31:58,367][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0659, Metrics: {'mse': 0.0655578151345253, 'rmse': 0.2560426041394777, 'r2': -0.010463953018188477}
[2025-04-29 21:31:58,368][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 21:31:58,368][src.training.lm_trainer][INFO] - Early stopping at epoch 13
[2025-04-29 21:31:58,368][src.training.lm_trainer][INFO] - Training completed in 32.51 seconds
[2025-04-29 21:31:58,368][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:32:00,862][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.032394543290138245, 'rmse': 0.17998484183435629, 'r2': -0.05528748035430908}
[2025-04-29 21:32:00,862][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06507474929094315, 'rmse': 0.25509752897851273, 'r2': -0.0030182600021362305}
[2025-04-29 21:32:00,862][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05936342850327492, 'rmse': 0.24364611325296145, 'r2': -0.02339911460876465}
[2025-04-29 21:32:02,487][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▂▂▁▁▁▁▁
wandb:     best_val_mse █▅▄▂▂▁▁▁▁▁
wandb:      best_val_r2 ▁▄▅▇▇█████
wandb:    best_val_rmse █▆▄▃▂▂▁▁▁▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▄▃▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▄▂▂▁▁▁▁▁▁▁▁
wandb:          val_mse █▅▄▂▂▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▄▅▇▇████████
wandb:         val_rmse █▆▄▃▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06523
wandb:     best_val_mse 0.06507
wandb:      best_val_r2 -0.00302
wandb:    best_val_rmse 0.2551
wandb:            epoch 13
wandb:   final_test_mse 0.05936
wandb:    final_test_r2 -0.0234
wandb:  final_test_rmse 0.24365
wandb:  final_train_mse 0.03239
wandb:   final_train_r2 -0.05529
wandb: final_train_rmse 0.17998
wandb:    final_val_mse 0.06507
wandb:     final_val_r2 -0.00302
wandb:   final_val_rmse 0.2551
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04366
wandb:       train_time 32.51054
wandb:         val_loss 0.06586
wandb:          val_mse 0.06556
wandb:           val_r2 -0.01046
wandb:         val_rmse 0.25604
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213115-ncgvjjnq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213115-ncgvjjnq/logs
Standard experiment completed successfully: layer_6_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity/results.json
Running question_type experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:32:16,567][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type
experiment_name: layer_11_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 21:32:16,568][__main__][INFO] - Normalized task: question_type
[2025-04-29 21:32:16,568][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 21:32:16,568][__main__][INFO] - Determined Task Type: classification
[2025-04-29 21:32:16,572][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 21:32:16,572][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:32:18,737][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:32:20,966][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:32:20,967][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:21,040][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,079][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,216][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:32:21,223][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:21,224][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:32:21,225][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:21,257][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,300][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,316][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:32:21,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:21,317][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:32:21,319][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:32:21,356][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,409][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:32:21,428][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:32:21,429][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:32:21,429][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:32:21,431][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:32:21,431][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:21,431][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:21,431][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:21,431][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:21,432][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 21:32:21,432][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:21,432][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:21,432][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 21:32:21,432][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 21:32:21,433][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 21:32:21,433][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 21:32:21,433][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:32:21,434][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:32:21,434][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:32:21,434][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:32:26,344][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:32:26,345][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:32:26,346][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 21:32:26,346][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=11, freeze_model=True, finetune=False
[2025-04-29 21:32:26,347][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:32:26,347][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6864Epoch 1/15: [                              ] 2/63 batches, loss: 0.6846Epoch 1/15: [=                             ] 3/63 batches, loss: 0.6896Epoch 1/15: [=                             ] 4/63 batches, loss: 0.6896Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6953Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6937Epoch 1/15: [===                           ] 7/63 batches, loss: 0.6970Epoch 1/15: [===                           ] 8/63 batches, loss: 0.6949Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6973Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6969Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.6972Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.6949Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6973Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6969Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6959Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6965Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6959Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6958Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6958Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6956Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6958Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6964Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6959Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6963Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6972Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6975Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6970Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6965Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6964Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6966Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6965Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6962Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6960Epoch 1/15: [================              ] 34/63 batches, loss: 0.6962Epoch 1/15: [================              ] 35/63 batches, loss: 0.6961Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6963Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6963Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6964Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6960Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6957Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6955Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6947Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6948Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6948Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6946Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6945Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6943Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6941Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6943Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6935Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6945Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6946Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6941Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6938Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6934Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6935Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6927Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6925Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6921Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6919Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6913Epoch 1/15: [==============================] 63/63 batches, loss: 0.6918
[2025-04-29 21:32:31,041][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6918
[2025-04-29 21:32:31,215][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6639, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8333333333333334}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6460Epoch 2/15: [                              ] 2/63 batches, loss: 0.6545Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6559Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6617Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6612Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6611Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6635Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6668Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6683Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6690Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6710Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6713Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6711Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6716Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6723Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6707Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6703Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6706Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6724Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6737Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6750Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6756Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6757Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6758Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6759Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6755Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6747Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6740Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6739Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6738Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6730Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6722Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6727Epoch 2/15: [================              ] 34/63 batches, loss: 0.6722Epoch 2/15: [================              ] 35/63 batches, loss: 0.6730Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6730Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6734Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6731Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6730Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6718Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6717Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6716Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6715Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6715Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6714Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6714Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6716Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6717Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6718Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6721Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6716Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6713Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6708Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6700Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6696Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6693Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6692Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6688Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6686Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6684Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6682Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6677Epoch 2/15: [==============================] 63/63 batches, loss: 0.6679
[2025-04-29 21:32:33,513][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6679
[2025-04-29 21:32:33,708][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6367, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8333333333333334}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6893Epoch 3/15: [                              ] 2/63 batches, loss: 0.6733Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6838Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6729Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6689Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6673Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6672Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6688Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6624Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6620Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6599Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6568Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6602Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6609Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6585Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6565Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6560Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6572Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6586Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6584Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6591Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6578Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6565Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6570Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6562Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6560Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6550Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6548Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6543Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6552Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6543Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6552Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6546Epoch 3/15: [================              ] 34/63 batches, loss: 0.6530Epoch 3/15: [================              ] 35/63 batches, loss: 0.6530Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6526Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6520Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6519Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6513Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6514Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6519Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6514Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6517Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6516Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6517Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6518Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6513Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6510Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6516Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6520Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6513Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6508Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6509Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6505Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6499Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6495Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6495Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6497Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6502Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6500Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6499Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6496Epoch 3/15: [==============================] 63/63 batches, loss: 0.6493
[2025-04-29 21:32:36,039][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6493
[2025-04-29 21:32:36,238][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6050, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8333333333333334}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6153Epoch 4/15: [                              ] 2/63 batches, loss: 0.6345Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6328Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6316Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6338Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6301Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6325Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6342Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6376Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6394Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6405Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6403Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6401Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6387Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6375Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6371Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6365Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6355Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6356Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6357Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6350Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6343Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6334Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6332Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6330Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6322Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6318Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6315Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6314Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6305Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6314Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6319Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6323Epoch 4/15: [================              ] 34/63 batches, loss: 0.6324Epoch 4/15: [================              ] 35/63 batches, loss: 0.6319Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6324Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6324Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6328Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6326Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6319Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6308Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6304Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6308Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6300Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6301Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6307Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6305Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6309Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6304Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6299Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6294Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6294Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6296Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6298Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6301Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6301Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6311Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6315Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6305Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6301Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6294Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6293Epoch 4/15: [==============================] 63/63 batches, loss: 0.6293
[2025-04-29 21:32:38,484][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6293
[2025-04-29 21:32:38,691][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5727, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5800Epoch 5/15: [                              ] 2/63 batches, loss: 0.5845Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5928Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5904Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5927Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5964Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5935Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5973Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6008Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6040Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6028Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6044Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6062Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6086Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6073Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6068Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6092Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6077Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6046Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6037Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6020Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6007Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6024Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6037Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6027Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6034Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6031Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6020Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6035Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6029Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6018Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6023Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6017Epoch 5/15: [================              ] 34/63 batches, loss: 0.6001Epoch 5/15: [================              ] 35/63 batches, loss: 0.6001Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6007Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5999Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5991Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5995Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6004Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6006Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6001Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5999Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5996Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5996Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6003Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5995Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5991Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5984Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5984Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5986Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5978Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5974Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5973Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5978Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5972Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5967Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5953Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5953Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5958Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5951Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5954Epoch 5/15: [==============================] 63/63 batches, loss: 0.5955
[2025-04-29 21:32:40,966][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5955
[2025-04-29 21:32:41,172][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5368, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5725Epoch 6/15: [                              ] 2/63 batches, loss: 0.5397Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5577Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5626Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5725Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5790Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5812Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5795Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5775Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5762Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5758Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5790Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5824Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5803Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5820Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5831Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5806Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5808Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5804Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5794Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5795Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5788Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5824Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5807Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5805Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5802Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5786Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5770Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5771Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5773Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5772Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5751Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5739Epoch 6/15: [================              ] 34/63 batches, loss: 0.5730Epoch 6/15: [================              ] 35/63 batches, loss: 0.5723Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5723Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5737Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5723Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5717Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5723Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5742Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5745Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5757Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5747Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5731Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5730Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5735Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5747Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5757Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5750Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5753Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5749Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5750Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5765Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5770Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5759Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5750Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5755Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5758Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5749Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5753Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5752Epoch 6/15: [==============================] 63/63 batches, loss: 0.5749
[2025-04-29 21:32:43,469][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5749
[2025-04-29 21:32:43,674][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5061, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5486Epoch 7/15: [                              ] 2/63 batches, loss: 0.5808Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5782Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5822Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5887Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5850Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5818Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5859Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5839Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5799Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5707Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5674Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5662Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5658Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5646Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5633Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5600Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5608Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5602Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5596Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5594Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5631Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5651Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5650Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5668Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5647Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5669Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5652Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5633Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5620Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5611Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5616Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5606Epoch 7/15: [================              ] 34/63 batches, loss: 0.5584Epoch 7/15: [================              ] 35/63 batches, loss: 0.5592Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5605Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5608Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5607Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5600Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5604Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5586Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5564Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5551Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5546Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5545Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5538Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5528Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5527Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5528Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5535Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5529Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5530Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5537Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5536Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5524Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5510Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5507Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5505Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5512Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5513Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5516Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5511Epoch 7/15: [==============================] 63/63 batches, loss: 0.5501
[2025-04-29 21:32:45,968][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5501
[2025-04-29 21:32:46,166][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.4822, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.4899Epoch 8/15: [                              ] 2/63 batches, loss: 0.5026Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5193Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5318Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5222Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5195Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5232Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5228Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5298Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5328Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5320Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5316Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5315Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5322Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5335Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5360Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5333Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5322Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5330Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5311Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5343Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5375Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5350Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5324Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5333Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5319Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5313Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5298Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5308Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5303Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5333Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5338Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5325Epoch 8/15: [================              ] 34/63 batches, loss: 0.5329Epoch 8/15: [================              ] 35/63 batches, loss: 0.5334Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5333Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5325Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5313Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5310Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5309Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5302Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5305Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5309Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5313Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5305Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5322Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5307Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5308Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5308Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5304Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5289Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5283Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5286Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5268Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5266Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5261Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5261Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5252Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5252Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5256Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5252Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5262Epoch 8/15: [==============================] 63/63 batches, loss: 0.5252
[2025-04-29 21:32:48,520][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5252
[2025-04-29 21:32:48,730][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.4479, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.5593Epoch 9/15: [                              ] 2/63 batches, loss: 0.5696Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5430Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5396Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5396Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5485Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5364Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5322Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5275Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5188Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5198Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5170Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5170Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5166Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5200Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5194Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5199Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5165Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5189Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5205Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5169Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5183Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5191Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5204Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5215Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5165Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5152Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5145Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5132Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5125Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5126Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5101Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5108Epoch 9/15: [================              ] 34/63 batches, loss: 0.5090Epoch 9/15: [================              ] 35/63 batches, loss: 0.5093Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5079Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5089Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5069Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5064Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5057Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5073Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5072Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5088Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5089Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5090Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5099Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5092Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5089Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5085Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5083Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5086Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5092Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5085Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5069Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5083Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5088Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5091Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5093Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5104Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5107Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5118Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5119Epoch 9/15: [==============================] 63/63 batches, loss: 0.5128
[2025-04-29 21:32:51,065][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5128
[2025-04-29 21:32:51,273][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.4247, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.4851Epoch 10/15: [                              ] 2/63 batches, loss: 0.4563Epoch 10/15: [=                             ] 3/63 batches, loss: 0.4742Epoch 10/15: [=                             ] 4/63 batches, loss: 0.4831Epoch 10/15: [==                            ] 5/63 batches, loss: 0.4765Epoch 10/15: [==                            ] 6/63 batches, loss: 0.4770Epoch 10/15: [===                           ] 7/63 batches, loss: 0.4974Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5035Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5081Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5107Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5023Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.4935Epoch 10/15: [======                        ] 13/63 batches, loss: 0.4995Epoch 10/15: [======                        ] 14/63 batches, loss: 0.4994Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.4967Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5046Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5087Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5079Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5072Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5065Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5061Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5047Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5052Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5044Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5014Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5008Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5010Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.4979Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.4954Epoch 10/15: [==============                ] 30/63 batches, loss: 0.4913Epoch 10/15: [==============                ] 31/63 batches, loss: 0.4939Epoch 10/15: [===============               ] 32/63 batches, loss: 0.4953Epoch 10/15: [===============               ] 33/63 batches, loss: 0.4927Epoch 10/15: [================              ] 34/63 batches, loss: 0.4946Epoch 10/15: [================              ] 35/63 batches, loss: 0.4937Epoch 10/15: [=================             ] 36/63 batches, loss: 0.4946Epoch 10/15: [=================             ] 37/63 batches, loss: 0.4941Epoch 10/15: [==================            ] 38/63 batches, loss: 0.4939Epoch 10/15: [==================            ] 39/63 batches, loss: 0.4920Epoch 10/15: [===================           ] 40/63 batches, loss: 0.4914Epoch 10/15: [===================           ] 41/63 batches, loss: 0.4905Epoch 10/15: [====================          ] 42/63 batches, loss: 0.4903Epoch 10/15: [====================          ] 43/63 batches, loss: 0.4892Epoch 10/15: [====================          ] 44/63 batches, loss: 0.4899Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.4888Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.4893Epoch 10/15: [======================        ] 47/63 batches, loss: 0.4898Epoch 10/15: [======================        ] 48/63 batches, loss: 0.4915Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.4900Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.4904Epoch 10/15: [========================      ] 51/63 batches, loss: 0.4896Epoch 10/15: [========================      ] 52/63 batches, loss: 0.4905Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.4899Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.4901Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.4904Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.4897Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.4902Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.4907Epoch 10/15: [============================  ] 59/63 batches, loss: 0.4906Epoch 10/15: [============================  ] 60/63 batches, loss: 0.4899Epoch 10/15: [============================= ] 61/63 batches, loss: 0.4897Epoch 10/15: [============================= ] 62/63 batches, loss: 0.4902Epoch 10/15: [==============================] 63/63 batches, loss: 0.4870
[2025-04-29 21:32:53,575][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.4870
[2025-04-29 21:32:53,785][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.4061, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5074Epoch 11/15: [                              ] 2/63 batches, loss: 0.5049Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5118Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5120Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5080Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5191Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5054Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5039Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5021Epoch 11/15: [====                          ] 10/63 batches, loss: 0.4896Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.4873Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.4839Epoch 11/15: [======                        ] 13/63 batches, loss: 0.4822Epoch 11/15: [======                        ] 14/63 batches, loss: 0.4873Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.4882Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.4894Epoch 11/15: [========                      ] 17/63 batches, loss: 0.4931Epoch 11/15: [========                      ] 18/63 batches, loss: 0.4911Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.4888Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.4879Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.4848Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.4858Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.4839Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.4818Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.4822Epoch 11/15: [============                  ] 26/63 batches, loss: 0.4815Epoch 11/15: [============                  ] 27/63 batches, loss: 0.4872Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.4874Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.4898Epoch 11/15: [==============                ] 30/63 batches, loss: 0.4891Epoch 11/15: [==============                ] 31/63 batches, loss: 0.4903Epoch 11/15: [===============               ] 32/63 batches, loss: 0.4923Epoch 11/15: [===============               ] 33/63 batches, loss: 0.4911Epoch 11/15: [================              ] 34/63 batches, loss: 0.4876Epoch 11/15: [================              ] 35/63 batches, loss: 0.4859Epoch 11/15: [=================             ] 36/63 batches, loss: 0.4852Epoch 11/15: [=================             ] 37/63 batches, loss: 0.4849Epoch 11/15: [==================            ] 38/63 batches, loss: 0.4834Epoch 11/15: [==================            ] 39/63 batches, loss: 0.4832Epoch 11/15: [===================           ] 40/63 batches, loss: 0.4815Epoch 11/15: [===================           ] 41/63 batches, loss: 0.4822Epoch 11/15: [====================          ] 42/63 batches, loss: 0.4809Epoch 11/15: [====================          ] 43/63 batches, loss: 0.4807Epoch 11/15: [====================          ] 44/63 batches, loss: 0.4802Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.4803Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.4801Epoch 11/15: [======================        ] 47/63 batches, loss: 0.4802Epoch 11/15: [======================        ] 48/63 batches, loss: 0.4827Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.4822Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.4825Epoch 11/15: [========================      ] 51/63 batches, loss: 0.4821Epoch 11/15: [========================      ] 52/63 batches, loss: 0.4792Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.4781Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.4775Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.4765Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.4760Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.4777Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.4786Epoch 11/15: [============================  ] 59/63 batches, loss: 0.4778Epoch 11/15: [============================  ] 60/63 batches, loss: 0.4773Epoch 11/15: [============================= ] 61/63 batches, loss: 0.4770Epoch 11/15: [============================= ] 62/63 batches, loss: 0.4771Epoch 11/15: [==============================] 63/63 batches, loss: 0.4773
[2025-04-29 21:32:56,153][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.4773
[2025-04-29 21:32:56,388][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.3907, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.4998Epoch 12/15: [                              ] 2/63 batches, loss: 0.4985Epoch 12/15: [=                             ] 3/63 batches, loss: 0.4621Epoch 12/15: [=                             ] 4/63 batches, loss: 0.4660Epoch 12/15: [==                            ] 5/63 batches, loss: 0.4754Epoch 12/15: [==                            ] 6/63 batches, loss: 0.4551Epoch 12/15: [===                           ] 7/63 batches, loss: 0.4492Epoch 12/15: [===                           ] 8/63 batches, loss: 0.4557Epoch 12/15: [====                          ] 9/63 batches, loss: 0.4621Epoch 12/15: [====                          ] 10/63 batches, loss: 0.4683Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.4610Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.4647Epoch 12/15: [======                        ] 13/63 batches, loss: 0.4631Epoch 12/15: [======                        ] 14/63 batches, loss: 0.4610Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.4569Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.4554Epoch 12/15: [========                      ] 17/63 batches, loss: 0.4601Epoch 12/15: [========                      ] 18/63 batches, loss: 0.4560Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.4600Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.4633Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.4638Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.4648Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.4663Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.4680Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.4711Epoch 12/15: [============                  ] 26/63 batches, loss: 0.4672Epoch 12/15: [============                  ] 27/63 batches, loss: 0.4690Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.4641Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.4625Epoch 12/15: [==============                ] 30/63 batches, loss: 0.4649Epoch 12/15: [==============                ] 31/63 batches, loss: 0.4602Epoch 12/15: [===============               ] 32/63 batches, loss: 0.4606Epoch 12/15: [===============               ] 33/63 batches, loss: 0.4628Epoch 12/15: [================              ] 34/63 batches, loss: 0.4635Epoch 12/15: [================              ] 35/63 batches, loss: 0.4647Epoch 12/15: [=================             ] 36/63 batches, loss: 0.4672Epoch 12/15: [=================             ] 37/63 batches, loss: 0.4669Epoch 12/15: [==================            ] 38/63 batches, loss: 0.4652Epoch 12/15: [==================            ] 39/63 batches, loss: 0.4659Epoch 12/15: [===================           ] 40/63 batches, loss: 0.4656Epoch 12/15: [===================           ] 41/63 batches, loss: 0.4644Epoch 12/15: [====================          ] 42/63 batches, loss: 0.4648Epoch 12/15: [====================          ] 43/63 batches, loss: 0.4635Epoch 12/15: [====================          ] 44/63 batches, loss: 0.4635Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.4638Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.4632Epoch 12/15: [======================        ] 47/63 batches, loss: 0.4640Epoch 12/15: [======================        ] 48/63 batches, loss: 0.4624Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.4628Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.4610Epoch 12/15: [========================      ] 51/63 batches, loss: 0.4623Epoch 12/15: [========================      ] 52/63 batches, loss: 0.4608Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.4609Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.4603Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.4602Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.4605Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.4601Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.4606Epoch 12/15: [============================  ] 59/63 batches, loss: 0.4609Epoch 12/15: [============================  ] 60/63 batches, loss: 0.4597Epoch 12/15: [============================= ] 61/63 batches, loss: 0.4584Epoch 12/15: [============================= ] 62/63 batches, loss: 0.4584Epoch 12/15: [==============================] 63/63 batches, loss: 0.4611
[2025-04-29 21:32:58,912][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.4611
[2025-04-29 21:32:59,141][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.3755, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.4139Epoch 13/15: [                              ] 2/63 batches, loss: 0.4329Epoch 13/15: [=                             ] 3/63 batches, loss: 0.4340Epoch 13/15: [=                             ] 4/63 batches, loss: 0.4431Epoch 13/15: [==                            ] 5/63 batches, loss: 0.4355Epoch 13/15: [==                            ] 6/63 batches, loss: 0.4430Epoch 13/15: [===                           ] 7/63 batches, loss: 0.4510Epoch 13/15: [===                           ] 8/63 batches, loss: 0.4506Epoch 13/15: [====                          ] 9/63 batches, loss: 0.4510Epoch 13/15: [====                          ] 10/63 batches, loss: 0.4396Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.4363Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.4365Epoch 13/15: [======                        ] 13/63 batches, loss: 0.4411Epoch 13/15: [======                        ] 14/63 batches, loss: 0.4365Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.4378Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.4381Epoch 13/15: [========                      ] 17/63 batches, loss: 0.4343Epoch 13/15: [========                      ] 18/63 batches, loss: 0.4392Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.4448Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.4428Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.4376Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.4400Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.4375Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.4361Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.4358Epoch 13/15: [============                  ] 26/63 batches, loss: 0.4344Epoch 13/15: [============                  ] 27/63 batches, loss: 0.4325Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.4318Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.4327Epoch 13/15: [==============                ] 30/63 batches, loss: 0.4341Epoch 13/15: [==============                ] 31/63 batches, loss: 0.4355Epoch 13/15: [===============               ] 32/63 batches, loss: 0.4353Epoch 13/15: [===============               ] 33/63 batches, loss: 0.4376Epoch 13/15: [================              ] 34/63 batches, loss: 0.4385Epoch 13/15: [================              ] 35/63 batches, loss: 0.4366Epoch 13/15: [=================             ] 36/63 batches, loss: 0.4347Epoch 13/15: [=================             ] 37/63 batches, loss: 0.4352Epoch 13/15: [==================            ] 38/63 batches, loss: 0.4359Epoch 13/15: [==================            ] 39/63 batches, loss: 0.4349Epoch 13/15: [===================           ] 40/63 batches, loss: 0.4327Epoch 13/15: [===================           ] 41/63 batches, loss: 0.4339Epoch 13/15: [====================          ] 42/63 batches, loss: 0.4315Epoch 13/15: [====================          ] 43/63 batches, loss: 0.4332Epoch 13/15: [====================          ] 44/63 batches, loss: 0.4347Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.4339Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.4354Epoch 13/15: [======================        ] 47/63 batches, loss: 0.4348Epoch 13/15: [======================        ] 48/63 batches, loss: 0.4355Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.4359Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.4362Epoch 13/15: [========================      ] 51/63 batches, loss: 0.4359Epoch 13/15: [========================      ] 52/63 batches, loss: 0.4367Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.4377Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.4382Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.4388Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.4397Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.4417Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.4428Epoch 13/15: [============================  ] 59/63 batches, loss: 0.4431Epoch 13/15: [============================  ] 60/63 batches, loss: 0.4426Epoch 13/15: [============================= ] 61/63 batches, loss: 0.4443Epoch 13/15: [============================= ] 62/63 batches, loss: 0.4458Epoch 13/15: [==============================] 63/63 batches, loss: 0.4454
[2025-04-29 21:33:01,603][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.4454
[2025-04-29 21:33:01,819][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.3627, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.4857Epoch 14/15: [                              ] 2/63 batches, loss: 0.4316Epoch 14/15: [=                             ] 3/63 batches, loss: 0.4241Epoch 14/15: [=                             ] 4/63 batches, loss: 0.4300Epoch 14/15: [==                            ] 5/63 batches, loss: 0.4456Epoch 14/15: [==                            ] 6/63 batches, loss: 0.4430Epoch 14/15: [===                           ] 7/63 batches, loss: 0.4382Epoch 14/15: [===                           ] 8/63 batches, loss: 0.4248Epoch 14/15: [====                          ] 9/63 batches, loss: 0.4248Epoch 14/15: [====                          ] 10/63 batches, loss: 0.4188Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.4278Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.4266Epoch 14/15: [======                        ] 13/63 batches, loss: 0.4306Epoch 14/15: [======                        ] 14/63 batches, loss: 0.4337Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.4293Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.4354Epoch 14/15: [========                      ] 17/63 batches, loss: 0.4336Epoch 14/15: [========                      ] 18/63 batches, loss: 0.4273Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.4248Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.4248Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.4219Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.4212Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.4177Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.4146Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.4113Epoch 14/15: [============                  ] 26/63 batches, loss: 0.4131Epoch 14/15: [============                  ] 27/63 batches, loss: 0.4141Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.4172Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.4199Epoch 14/15: [==============                ] 30/63 batches, loss: 0.4213Epoch 14/15: [==============                ] 31/63 batches, loss: 0.4205Epoch 14/15: [===============               ] 32/63 batches, loss: 0.4200Epoch 14/15: [===============               ] 33/63 batches, loss: 0.4200Epoch 14/15: [================              ] 34/63 batches, loss: 0.4195Epoch 14/15: [================              ] 35/63 batches, loss: 0.4186Epoch 14/15: [=================             ] 36/63 batches, loss: 0.4170Epoch 14/15: [=================             ] 37/63 batches, loss: 0.4200Epoch 14/15: [==================            ] 38/63 batches, loss: 0.4197Epoch 14/15: [==================            ] 39/63 batches, loss: 0.4180Epoch 14/15: [===================           ] 40/63 batches, loss: 0.4179Epoch 14/15: [===================           ] 41/63 batches, loss: 0.4184Epoch 14/15: [====================          ] 42/63 batches, loss: 0.4192Epoch 14/15: [====================          ] 43/63 batches, loss: 0.4170Epoch 14/15: [====================          ] 44/63 batches, loss: 0.4170Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.4167Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.4179Epoch 14/15: [======================        ] 47/63 batches, loss: 0.4184Epoch 14/15: [======================        ] 48/63 batches, loss: 0.4157Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.4153Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.4160Epoch 14/15: [========================      ] 51/63 batches, loss: 0.4166Epoch 14/15: [========================      ] 52/63 batches, loss: 0.4186Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.4194Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.4187Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.4175Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.4200Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.4194Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.4215Epoch 14/15: [============================  ] 59/63 batches, loss: 0.4226Epoch 14/15: [============================  ] 60/63 batches, loss: 0.4233Epoch 14/15: [============================= ] 61/63 batches, loss: 0.4225Epoch 14/15: [============================= ] 62/63 batches, loss: 0.4228Epoch 14/15: [==============================] 63/63 batches, loss: 0.4277
[2025-04-29 21:33:04,216][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.4277
[2025-04-29 21:33:04,424][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.3555, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.4275Epoch 15/15: [                              ] 2/63 batches, loss: 0.4265Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4185Epoch 15/15: [=                             ] 4/63 batches, loss: 0.3886Epoch 15/15: [==                            ] 5/63 batches, loss: 0.4133Epoch 15/15: [==                            ] 6/63 batches, loss: 0.4101Epoch 15/15: [===                           ] 7/63 batches, loss: 0.4087Epoch 15/15: [===                           ] 8/63 batches, loss: 0.4106Epoch 15/15: [====                          ] 9/63 batches, loss: 0.4245Epoch 15/15: [====                          ] 10/63 batches, loss: 0.4235Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.4193Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.4228Epoch 15/15: [======                        ] 13/63 batches, loss: 0.4180Epoch 15/15: [======                        ] 14/63 batches, loss: 0.4193Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.4191Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.4182Epoch 15/15: [========                      ] 17/63 batches, loss: 0.4156Epoch 15/15: [========                      ] 18/63 batches, loss: 0.4172Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.4228Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.4176Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.4185Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.4172Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.4219Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.4263Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.4216Epoch 15/15: [============                  ] 26/63 batches, loss: 0.4207Epoch 15/15: [============                  ] 27/63 batches, loss: 0.4242Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.4227Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.4197Epoch 15/15: [==============                ] 30/63 batches, loss: 0.4143Epoch 15/15: [==============                ] 31/63 batches, loss: 0.4149Epoch 15/15: [===============               ] 32/63 batches, loss: 0.4165Epoch 15/15: [===============               ] 33/63 batches, loss: 0.4157Epoch 15/15: [================              ] 34/63 batches, loss: 0.4135Epoch 15/15: [================              ] 35/63 batches, loss: 0.4138Epoch 15/15: [=================             ] 36/63 batches, loss: 0.4130Epoch 15/15: [=================             ] 37/63 batches, loss: 0.4143Epoch 15/15: [==================            ] 38/63 batches, loss: 0.4116Epoch 15/15: [==================            ] 39/63 batches, loss: 0.4120Epoch 15/15: [===================           ] 40/63 batches, loss: 0.4113Epoch 15/15: [===================           ] 41/63 batches, loss: 0.4106Epoch 15/15: [====================          ] 42/63 batches, loss: 0.4115Epoch 15/15: [====================          ] 43/63 batches, loss: 0.4099Epoch 15/15: [====================          ] 44/63 batches, loss: 0.4117Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.4136Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.4115Epoch 15/15: [======================        ] 47/63 batches, loss: 0.4105Epoch 15/15: [======================        ] 48/63 batches, loss: 0.4119Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.4106Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.4116Epoch 15/15: [========================      ] 51/63 batches, loss: 0.4122Epoch 15/15: [========================      ] 52/63 batches, loss: 0.4120Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.4106Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.4114Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.4113Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.4116Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.4108Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.4106Epoch 15/15: [============================  ] 59/63 batches, loss: 0.4097Epoch 15/15: [============================  ] 60/63 batches, loss: 0.4108Epoch 15/15: [============================= ] 61/63 batches, loss: 0.4099Epoch 15/15: [============================= ] 62/63 batches, loss: 0.4091Epoch 15/15: [==============================] 63/63 batches, loss: 0.4109
[2025-04-29 21:33:06,826][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.4109
[2025-04-29 21:33:07,053][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.3478, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
[2025-04-29 21:33:07,434][src.training.lm_trainer][INFO] - Training completed in 39.07 seconds
[2025-04-29 21:33:07,434][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 21:33:09,954][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9829145728643216, 'f1': 0.9831181727904668}
[2025-04-29 21:33:09,955][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
[2025-04-29 21:33:09,955][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5064935064935064, 'f1': 0.5365853658536586}
[2025-04-29 21:33:11,602][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▃▃▃▃▅▅▆▆▆█▆▆
wandb:          best_val_f1 ▁▁▁▃▃▃▃▄▄▆▆▆█▆▆
wandb:        best_val_loss █▇▇▆▅▅▄▃▃▂▂▂▁▁▁
wandb:                epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▇▆▆▅▄▄▄▃▃▂▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▃▃▃▃▅▅▆▆▆█▆▆
wandb:               val_f1 ▁▁▁▃▃▃▃▄▄▆▆▆█▆▆
wandb:             val_loss █▇▇▆▅▅▄▃▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88636
wandb:          best_val_f1 0.88889
wandb:        best_val_loss 0.34779
wandb:                epoch 15
wandb:  final_test_accuracy 0.50649
wandb:        final_test_f1 0.53659
wandb: final_train_accuracy 0.98291
wandb:       final_train_f1 0.98312
wandb:   final_val_accuracy 0.88636
wandb:         final_val_f1 0.88889
wandb:        learning_rate 2e-05
wandb:           train_loss 0.41093
wandb:           train_time 39.06919
wandb:         val_accuracy 0.88636
wandb:               val_f1 0.88889
wandb:             val_loss 0.34779
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213216-bv3smh8u
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_213216-bv3smh8u/logs
Standard experiment completed successfully: layer_11_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type/results.json
Running complexity experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 21:33:28,874][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/complexity
experiment_name: layer_11_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 21:33:28,875][__main__][INFO] - Normalized task: complexity
[2025-04-29 21:33:28,875][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 21:33:28,875][__main__][INFO] - Determined Task Type: regression
[2025-04-29 21:33:28,879][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 21:33:28,879][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 21:33:30,491][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 21:33:32,691][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 21:33:32,692][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:33:32,760][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:32,794][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:32,898][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 21:33:32,905][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:33:32,906][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 21:33:32,907][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:33:32,937][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:32,974][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:32,989][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 21:33:32,990][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:33:32,990][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 21:33:32,991][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 21:33:33,016][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:33,056][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 21:33:33,070][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 21:33:33,071][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 21:33:33,071][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 21:33:33,072][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 21:33:33,072][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:33:33,072][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:33:33,073][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:33:33,073][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:33:33,073][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:33:33,074][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:33:33,074][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 21:33:33,074][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 21:33:33,075][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 21:33:33,075][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 21:33:33,075][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 21:33:33,075][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 21:33:33,075][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 21:33:33,075][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 21:33:33,075][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 21:33:33,075][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 21:33:37,275][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 21:33:37,276][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 21:33:37,277][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 21:33:37,277][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=11, freeze_model=True, finetune=False
[2025-04-29 21:33:37,278][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 21:33:37,278][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.2449Epoch 1/15: [                              ] 2/63 batches, loss: 0.2009Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2003Epoch 1/15: [=                             ] 4/63 batches, loss: 0.1834Epoch 1/15: [==                            ] 5/63 batches, loss: 0.1744Epoch 1/15: [==                            ] 6/63 batches, loss: 0.1753Epoch 1/15: [===                           ] 7/63 batches, loss: 0.1742Epoch 1/15: [===                           ] 8/63 batches, loss: 0.1672Epoch 1/15: [====                          ] 9/63 batches, loss: 0.1648Epoch 1/15: [====                          ] 10/63 batches, loss: 0.1598Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.1579Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.1610Epoch 1/15: [======                        ] 13/63 batches, loss: 0.1568Epoch 1/15: [======                        ] 14/63 batches, loss: 0.1533Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.1494Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.1530Epoch 1/15: [========                      ] 17/63 batches, loss: 0.1534Epoch 1/15: [========                      ] 18/63 batches, loss: 0.1499Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.1484Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.1473Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.1450Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.1412Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.1385Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.1380Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.1344Epoch 1/15: [============                  ] 26/63 batches, loss: 0.1314Epoch 1/15: [============                  ] 27/63 batches, loss: 0.1295Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.1264Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.1266Epoch 1/15: [==============                ] 30/63 batches, loss: 0.1255Epoch 1/15: [==============                ] 31/63 batches, loss: 0.1265Epoch 1/15: [===============               ] 32/63 batches, loss: 0.1255Epoch 1/15: [===============               ] 33/63 batches, loss: 0.1234Epoch 1/15: [================              ] 34/63 batches, loss: 0.1244Epoch 1/15: [================              ] 35/63 batches, loss: 0.1243Epoch 1/15: [=================             ] 36/63 batches, loss: 0.1226Epoch 1/15: [=================             ] 37/63 batches, loss: 0.1206Epoch 1/15: [==================            ] 38/63 batches, loss: 0.1200Epoch 1/15: [==================            ] 39/63 batches, loss: 0.1187Epoch 1/15: [===================           ] 40/63 batches, loss: 0.1184Epoch 1/15: [===================           ] 41/63 batches, loss: 0.1167Epoch 1/15: [====================          ] 42/63 batches, loss: 0.1157Epoch 1/15: [====================          ] 43/63 batches, loss: 0.1142Epoch 1/15: [====================          ] 44/63 batches, loss: 0.1133Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.1119Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.1106Epoch 1/15: [======================        ] 47/63 batches, loss: 0.1099Epoch 1/15: [======================        ] 48/63 batches, loss: 0.1082Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.1069Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.1057Epoch 1/15: [========================      ] 51/63 batches, loss: 0.1040Epoch 1/15: [========================      ] 52/63 batches, loss: 0.1031Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.1024Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.1013Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.1002Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.0990Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.0983Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.0972Epoch 1/15: [============================  ] 59/63 batches, loss: 0.0968Epoch 1/15: [============================  ] 60/63 batches, loss: 0.0959Epoch 1/15: [============================= ] 61/63 batches, loss: 0.0960Epoch 1/15: [============================= ] 62/63 batches, loss: 0.0952Epoch 1/15: [==============================] 63/63 batches, loss: 0.0939
[2025-04-29 21:33:41,921][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.0939
[2025-04-29 21:33:42,098][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0705, Metrics: {'mse': 0.07014802098274231, 'rmse': 0.2648547167462613, 'r2': -0.0812143087387085}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.0301Epoch 2/15: [                              ] 2/63 batches, loss: 0.0434Epoch 2/15: [=                             ] 3/63 batches, loss: 0.0577Epoch 2/15: [=                             ] 4/63 batches, loss: 0.0530Epoch 2/15: [==                            ] 5/63 batches, loss: 0.0484Epoch 2/15: [==                            ] 6/63 batches, loss: 0.0471Epoch 2/15: [===                           ] 7/63 batches, loss: 0.0458Epoch 2/15: [===                           ] 8/63 batches, loss: 0.0457Epoch 2/15: [====                          ] 9/63 batches, loss: 0.0449Epoch 2/15: [====                          ] 10/63 batches, loss: 0.0435Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.0427Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.0418Epoch 2/15: [======                        ] 13/63 batches, loss: 0.0423Epoch 2/15: [======                        ] 14/63 batches, loss: 0.0440Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.0432Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.0439Epoch 2/15: [========                      ] 17/63 batches, loss: 0.0440Epoch 2/15: [========                      ] 18/63 batches, loss: 0.0435Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.0425Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.0447Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.0450Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.0447Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.0442Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.0434Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.0433Epoch 2/15: [============                  ] 26/63 batches, loss: 0.0429Epoch 2/15: [============                  ] 27/63 batches, loss: 0.0431Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.0432Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.0429Epoch 2/15: [==============                ] 30/63 batches, loss: 0.0439Epoch 2/15: [==============                ] 31/63 batches, loss: 0.0433Epoch 2/15: [===============               ] 32/63 batches, loss: 0.0432Epoch 2/15: [===============               ] 33/63 batches, loss: 0.0427Epoch 2/15: [================              ] 34/63 batches, loss: 0.0429Epoch 2/15: [================              ] 35/63 batches, loss: 0.0425Epoch 2/15: [=================             ] 36/63 batches, loss: 0.0429Epoch 2/15: [=================             ] 37/63 batches, loss: 0.0426Epoch 2/15: [==================            ] 38/63 batches, loss: 0.0425Epoch 2/15: [==================            ] 39/63 batches, loss: 0.0428Epoch 2/15: [===================           ] 40/63 batches, loss: 0.0426Epoch 2/15: [===================           ] 41/63 batches, loss: 0.0427Epoch 2/15: [====================          ] 42/63 batches, loss: 0.0428Epoch 2/15: [====================          ] 43/63 batches, loss: 0.0429Epoch 2/15: [====================          ] 44/63 batches, loss: 0.0425Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.0421Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.0417Epoch 2/15: [======================        ] 47/63 batches, loss: 0.0413Epoch 2/15: [======================        ] 48/63 batches, loss: 0.0412Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.0418Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.0414Epoch 2/15: [========================      ] 51/63 batches, loss: 0.0415Epoch 2/15: [========================      ] 52/63 batches, loss: 0.0413Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.0410Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.0415Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.0414Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.0411Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.0408Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.0409Epoch 2/15: [============================  ] 59/63 batches, loss: 0.0407Epoch 2/15: [============================  ] 60/63 batches, loss: 0.0406Epoch 2/15: [============================= ] 61/63 batches, loss: 0.0407Epoch 2/15: [============================= ] 62/63 batches, loss: 0.0408Epoch 2/15: [==============================] 63/63 batches, loss: 0.0411
[2025-04-29 21:33:44,401][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.0411
[2025-04-29 21:33:44,581][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0748, Metrics: {'mse': 0.07432592660188675, 'rmse': 0.2726278169994521, 'r2': -0.14560973644256592}
[2025-04-29 21:33:44,582][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0278Epoch 3/15: [                              ] 2/63 batches, loss: 0.0367Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0381Epoch 3/15: [=                             ] 4/63 batches, loss: 0.0382Epoch 3/15: [==                            ] 5/63 batches, loss: 0.0346Epoch 3/15: [==                            ] 6/63 batches, loss: 0.0342Epoch 3/15: [===                           ] 7/63 batches, loss: 0.0359Epoch 3/15: [===                           ] 8/63 batches, loss: 0.0379Epoch 3/15: [====                          ] 9/63 batches, loss: 0.0376Epoch 3/15: [====                          ] 10/63 batches, loss: 0.0392Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.0386Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.0406Epoch 3/15: [======                        ] 13/63 batches, loss: 0.0405Epoch 3/15: [======                        ] 14/63 batches, loss: 0.0390Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.0395Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.0393Epoch 3/15: [========                      ] 17/63 batches, loss: 0.0383Epoch 3/15: [========                      ] 18/63 batches, loss: 0.0377Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.0378Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.0383Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.0379Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.0374Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.0370Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.0367Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.0380Epoch 3/15: [============                  ] 26/63 batches, loss: 0.0377Epoch 3/15: [============                  ] 27/63 batches, loss: 0.0380Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.0373Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.0382Epoch 3/15: [==============                ] 30/63 batches, loss: 0.0385Epoch 3/15: [==============                ] 31/63 batches, loss: 0.0385Epoch 3/15: [===============               ] 32/63 batches, loss: 0.0382Epoch 3/15: [===============               ] 33/63 batches, loss: 0.0379Epoch 3/15: [================              ] 34/63 batches, loss: 0.0377Epoch 3/15: [================              ] 35/63 batches, loss: 0.0375Epoch 3/15: [=================             ] 36/63 batches, loss: 0.0379Epoch 3/15: [=================             ] 37/63 batches, loss: 0.0376Epoch 3/15: [==================            ] 38/63 batches, loss: 0.0375Epoch 3/15: [==================            ] 39/63 batches, loss: 0.0371Epoch 3/15: [===================           ] 40/63 batches, loss: 0.0379Epoch 3/15: [===================           ] 41/63 batches, loss: 0.0384Epoch 3/15: [====================          ] 42/63 batches, loss: 0.0389Epoch 3/15: [====================          ] 43/63 batches, loss: 0.0396Epoch 3/15: [====================          ] 44/63 batches, loss: 0.0394Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.0390Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.0394Epoch 3/15: [======================        ] 47/63 batches, loss: 0.0389Epoch 3/15: [======================        ] 48/63 batches, loss: 0.0389Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.0390Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.0387Epoch 3/15: [========================      ] 51/63 batches, loss: 0.0383Epoch 3/15: [========================      ] 52/63 batches, loss: 0.0382Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.0384Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.0382Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.0382Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.0381Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.0377Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.0378Epoch 3/15: [============================  ] 59/63 batches, loss: 0.0379Epoch 3/15: [============================  ] 60/63 batches, loss: 0.0376Epoch 3/15: [============================= ] 61/63 batches, loss: 0.0374Epoch 3/15: [============================= ] 62/63 batches, loss: 0.0377Epoch 3/15: [==============================] 63/63 batches, loss: 0.0376
[2025-04-29 21:33:46,518][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0376
[2025-04-29 21:33:46,706][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0667, Metrics: {'mse': 0.06633675843477249, 'rmse': 0.25755923286648547, 'r2': -0.02246999740600586}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0447Epoch 4/15: [                              ] 2/63 batches, loss: 0.0355Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0380Epoch 4/15: [=                             ] 4/63 batches, loss: 0.0335Epoch 4/15: [==                            ] 5/63 batches, loss: 0.0376Epoch 4/15: [==                            ] 6/63 batches, loss: 0.0377Epoch 4/15: [===                           ] 7/63 batches, loss: 0.0362Epoch 4/15: [===                           ] 8/63 batches, loss: 0.0368Epoch 4/15: [====                          ] 9/63 batches, loss: 0.0377Epoch 4/15: [====                          ] 10/63 batches, loss: 0.0394Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.0386Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.0417Epoch 4/15: [======                        ] 13/63 batches, loss: 0.0425Epoch 4/15: [======                        ] 14/63 batches, loss: 0.0418Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.0414Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.0399Epoch 4/15: [========                      ] 17/63 batches, loss: 0.0394Epoch 4/15: [========                      ] 18/63 batches, loss: 0.0387Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.0384Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.0384Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.0375Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.0371Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.0368Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.0366Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.0365Epoch 4/15: [============                  ] 26/63 batches, loss: 0.0359Epoch 4/15: [============                  ] 27/63 batches, loss: 0.0359Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.0353Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.0352Epoch 4/15: [==============                ] 30/63 batches, loss: 0.0357Epoch 4/15: [==============                ] 31/63 batches, loss: 0.0351Epoch 4/15: [===============               ] 32/63 batches, loss: 0.0348Epoch 4/15: [===============               ] 33/63 batches, loss: 0.0342Epoch 4/15: [================              ] 34/63 batches, loss: 0.0337Epoch 4/15: [================              ] 35/63 batches, loss: 0.0345Epoch 4/15: [=================             ] 36/63 batches, loss: 0.0341Epoch 4/15: [=================             ] 37/63 batches, loss: 0.0337Epoch 4/15: [==================            ] 38/63 batches, loss: 0.0332Epoch 4/15: [==================            ] 39/63 batches, loss: 0.0332Epoch 4/15: [===================           ] 40/63 batches, loss: 0.0331Epoch 4/15: [===================           ] 41/63 batches, loss: 0.0331Epoch 4/15: [====================          ] 42/63 batches, loss: 0.0337Epoch 4/15: [====================          ] 43/63 batches, loss: 0.0335Epoch 4/15: [====================          ] 44/63 batches, loss: 0.0336Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.0334Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.0335Epoch 4/15: [======================        ] 47/63 batches, loss: 0.0335Epoch 4/15: [======================        ] 48/63 batches, loss: 0.0334Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.0334Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.0334Epoch 4/15: [========================      ] 51/63 batches, loss: 0.0336Epoch 4/15: [========================      ] 52/63 batches, loss: 0.0338Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.0337Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.0336Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.0334Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.0333Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.0330Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.0333Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0331Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0330Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0331Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0329Epoch 4/15: [==============================] 63/63 batches, loss: 0.0324
[2025-04-29 21:33:49,038][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0324
[2025-04-29 21:33:49,232][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0635, Metrics: {'mse': 0.06319864094257355, 'rmse': 0.25139339876491096, 'r2': 0.025898754596710205}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.0325Epoch 5/15: [                              ] 2/63 batches, loss: 0.0317Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0313Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0353Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0342Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0319Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0319Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0306Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0301Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0296Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0313Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0306Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0307Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0310Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0310Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0303Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0303Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0313Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0311Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0316Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0320Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0332Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0329Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0329Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0325Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0327Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0329Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0322Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0325Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0319Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0315Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0314Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0314Epoch 5/15: [================              ] 34/63 batches, loss: 0.0316Epoch 5/15: [================              ] 35/63 batches, loss: 0.0314Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0311Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0318Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0319Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0313Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0318Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0319Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0325Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0322Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0325Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0327Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0329Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0329Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0328Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0328Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0343Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0343Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0344Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0343Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0338Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0339Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0338Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0336Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0338Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0340Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0338Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0336Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0333Epoch 5/15: [==============================] 63/63 batches, loss: 0.0339
[2025-04-29 21:33:51,521][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0339
[2025-04-29 21:33:51,723][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0588, Metrics: {'mse': 0.058499518781900406, 'rmse': 0.24186673765092298, 'r2': 0.09832781553268433}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0364Epoch 6/15: [                              ] 2/63 batches, loss: 0.0279Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0273Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0244Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0221Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0254Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0247Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0250Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0260Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0271Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0281Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0280Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0279Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0283Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0279Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0268Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0262Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0265Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0262Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0260Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0255Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0264Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0269Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0268Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0265Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0268Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0267Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0269Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0272Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0271Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0268Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0267Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0274Epoch 6/15: [================              ] 34/63 batches, loss: 0.0274Epoch 6/15: [================              ] 35/63 batches, loss: 0.0271Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0269Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0268Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0268Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0270Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0266Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0269Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0267Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0265Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0267Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0266Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0271Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0270Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0273Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0274Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0274Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0273Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0273Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0272Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0273Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0276Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0274Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0273Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0273Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0275Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0275Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0277Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0278Epoch 6/15: [==============================] 63/63 batches, loss: 0.0276
[2025-04-29 21:33:54,037][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0276
[2025-04-29 21:33:54,254][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0580, Metrics: {'mse': 0.057645704597234726, 'rmse': 0.24009519902995713, 'r2': 0.1114879846572876}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0395Epoch 7/15: [                              ] 2/63 batches, loss: 0.0440Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0343Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0342Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0332Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0305Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0311Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0305Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0283Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0280Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0299Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0292Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0292Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0294Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0283Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0275Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0279Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0292Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0289Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0289Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0286Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0281Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0282Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0278Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0272Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0274Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0277Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0286Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0281Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0291Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0292Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0289Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0293Epoch 7/15: [================              ] 34/63 batches, loss: 0.0290Epoch 7/15: [================              ] 35/63 batches, loss: 0.0289Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0289Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0286Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0289Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0294Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0293Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0293Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0288Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0287Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0288Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0284Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0285Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0281Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0282Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0285Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0282Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0284Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0281Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0283Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0282Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0280Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0281Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0278Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0277Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0277Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0277Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0275Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0272Epoch 7/15: [==============================] 63/63 batches, loss: 0.0276
[2025-04-29 21:33:56,564][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0276
[2025-04-29 21:33:56,769][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0526, Metrics: {'mse': 0.05229305848479271, 'rmse': 0.2286767554536156, 'r2': 0.1939900517463684}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0462Epoch 8/15: [                              ] 2/63 batches, loss: 0.0431Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0347Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0412Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0360Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0379Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0387Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0366Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0355Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0340Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0326Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0328Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0320Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0311Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0305Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0297Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0299Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0290Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0292Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0288Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0283Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0287Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0283Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0275Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0275Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0273Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0284Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0283Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0277Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0275Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0273Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0274Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0274Epoch 8/15: [================              ] 34/63 batches, loss: 0.0271Epoch 8/15: [================              ] 35/63 batches, loss: 0.0270Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0268Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0266Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0264Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0267Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0266Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0266Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0268Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0266Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0267Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0265Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0264Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0264Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0268Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0269Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0267Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0266Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0267Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0266Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0269Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0268Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0265Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0267Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0266Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0265Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0264Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0264Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0264Epoch 8/15: [==============================] 63/63 batches, loss: 0.0262
[2025-04-29 21:33:59,139][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0262
[2025-04-29 21:33:59,362][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0517, Metrics: {'mse': 0.05130082741379738, 'rmse': 0.22649685961133628, 'r2': 0.20928359031677246}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0210Epoch 9/15: [                              ] 2/63 batches, loss: 0.0151Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0223Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0251Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0237Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0236Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0241Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0237Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0246Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0248Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0269Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0274Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0267Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0269Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0281Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0281Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0277Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0274Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0271Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0267Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0270Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0273Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0274Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0275Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0272Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0270Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0271Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0267Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0264Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0269Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0270Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0267Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0268Epoch 9/15: [================              ] 34/63 batches, loss: 0.0271Epoch 9/15: [================              ] 35/63 batches, loss: 0.0267Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0265Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0270Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0269Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0269Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0266Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0264Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0265Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0264Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0265Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0265Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0266Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0265Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0262Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0265Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0262Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0260Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0264Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0262Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0261Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0263Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0261Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0260Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0260Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0260Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0261Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0264Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0264Epoch 9/15: [==============================] 63/63 batches, loss: 0.0262
[2025-04-29 21:34:01,703][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0262
[2025-04-29 21:34:01,916][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0552, Metrics: {'mse': 0.05478736013174057, 'rmse': 0.23406699923684365, 'r2': 0.15554457902908325}
[2025-04-29 21:34:01,917][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0166Epoch 10/15: [                              ] 2/63 batches, loss: 0.0182Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0242Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0268Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0246Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0266Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0257Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0267Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0282Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0270Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0261Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0262Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0255Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0250Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0248Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0249Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0245Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0245Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0245Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0247Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0247Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0266Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0268Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0265Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0268Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0264Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0261Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0259Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0263Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0265Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0263Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0265Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0264Epoch 10/15: [================              ] 34/63 batches, loss: 0.0265Epoch 10/15: [================              ] 35/63 batches, loss: 0.0262Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0263Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0262Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0264Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0264Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0271Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0272Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0279Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0278Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0278Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0275Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0275Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0271Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0272Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0269Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0269Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0268Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0269Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0268Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0268Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0269Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0270Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0270Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0270Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0269Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0268Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0270Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0268Epoch 10/15: [==============================] 63/63 batches, loss: 0.0265
[2025-04-29 21:34:03,851][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0265
[2025-04-29 21:34:04,071][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0492, Metrics: {'mse': 0.04883687198162079, 'rmse': 0.2209906603945533, 'r2': 0.2472614049911499}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0080Epoch 11/15: [                              ] 2/63 batches, loss: 0.0150Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0172Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0215Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0201Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0196Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0192Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0184Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0179Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0205Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0209Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0199Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0204Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0221Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0218Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0217Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0221Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0228Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0230Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0231Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0231Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0236Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0240Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0245Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0248Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0247Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0252Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0250Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0251Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0249Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0251Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0252Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0249Epoch 11/15: [================              ] 34/63 batches, loss: 0.0246Epoch 11/15: [================              ] 35/63 batches, loss: 0.0242Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0244Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0245Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0248Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0248Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0249Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0254Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0254Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0255Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0258Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0255Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0255Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0254Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0253Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0252Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0253Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0251Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0252Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0253Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0254Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0258Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0256Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0255Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0257Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0255Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0253Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0251Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0252Epoch 11/15: [==============================] 63/63 batches, loss: 0.0254
[2025-04-29 21:34:06,382][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0254
[2025-04-29 21:34:06,593][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0477, Metrics: {'mse': 0.0473659485578537, 'rmse': 0.21763719479411991, 'r2': 0.26993322372436523}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0395Epoch 12/15: [                              ] 2/63 batches, loss: 0.0375Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0330Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0295Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0282Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0275Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0281Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0280Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0285Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0297Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0293Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0281Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0278Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0272Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0267Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0267Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0265Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0261Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0267Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0272Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0269Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0269Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0265Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0259Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0261Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0256Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0254Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0254Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0255Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0253Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0259Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0264Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0264Epoch 12/15: [================              ] 34/63 batches, loss: 0.0265Epoch 12/15: [================              ] 35/63 batches, loss: 0.0265Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0262Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0258Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0256Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0261Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0260Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0257Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0256Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0259Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0258Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0260Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0256Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0260Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0259Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0255Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0256Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0256Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0253Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0252Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0254Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0257Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0257Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0258Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0258Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0260Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0261Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0262Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0262Epoch 12/15: [==============================] 63/63 batches, loss: 0.0268
[2025-04-29 21:34:08,898][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0268
[2025-04-29 21:34:09,120][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0493, Metrics: {'mse': 0.04893531650304794, 'rmse': 0.22121328283592726, 'r2': 0.24574404954910278}
[2025-04-29 21:34:09,120][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0261Epoch 13/15: [                              ] 2/63 batches, loss: 0.0329Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0277Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0278Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0305Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0273Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0299Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0297Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0270Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0281Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0281Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0273Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0272Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0273Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0279Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0283Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0282Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0276Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0278Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0273Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0270Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0269Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0269Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0264Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0259Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0261Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0265Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0269Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0265Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0264Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0264Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0264Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0266Epoch 13/15: [================              ] 34/63 batches, loss: 0.0263Epoch 13/15: [================              ] 35/63 batches, loss: 0.0262Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0261Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0260Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0263Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0261Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0260Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0260Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0257Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0257Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0255Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0254Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0255Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0258Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0255Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0253Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0251Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0249Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0249Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0248Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0248Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0246Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0243Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0241Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0243Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0242Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0241Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0242Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0245Epoch 13/15: [==============================] 63/63 batches, loss: 0.0242
[2025-04-29 21:34:11,074][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0242
[2025-04-29 21:34:11,297][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0488, Metrics: {'mse': 0.048382677137851715, 'rmse': 0.21996062633537783, 'r2': 0.2542620301246643}
[2025-04-29 21:34:11,298][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0395Epoch 14/15: [                              ] 2/63 batches, loss: 0.0304Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0238Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0260Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0245Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0237Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0238Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0221Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0241Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0238Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0246Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0241Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0234Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0236Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0253Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0256Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0250Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0249Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0245Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0238Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0243Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0249Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0242Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0238Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0241Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0239Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0235Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0233Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0230Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0234Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0232Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0230Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0229Epoch 14/15: [================              ] 34/63 batches, loss: 0.0227Epoch 14/15: [================              ] 35/63 batches, loss: 0.0230Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0232Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0234Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0233Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0230Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0228Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0227Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0231Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0230Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0229Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0230Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0233Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0233Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0239Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0240Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0238Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0237Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0235Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0235Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0237Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0237Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0237Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0235Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0238Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0237Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0235Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0238Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0238Epoch 14/15: [==============================] 63/63 batches, loss: 0.0239
[2025-04-29 21:34:13,249][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0239
[2025-04-29 21:34:13,457][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0460, Metrics: {'mse': 0.04560254514217377, 'rmse': 0.21354752431759486, 'r2': 0.2971131205558777}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0397Epoch 15/15: [                              ] 2/63 batches, loss: 0.0295Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0255Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0266Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0250Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0254Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0243Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0233Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0229Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0222Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0226Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0228Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0240Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0257Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0250Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0245Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0248Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0249Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0251Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0261Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0271Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0272Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0267Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0273Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0266Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0266Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0268Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0263Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0267Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0263Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0266Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0263Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0260Epoch 15/15: [================              ] 34/63 batches, loss: 0.0259Epoch 15/15: [================              ] 35/63 batches, loss: 0.0257Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0257Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0253Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0250Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0246Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0245Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0246Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0247Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0246Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0247Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0247Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0248Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0247Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0246Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0246Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0246Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0244Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0242Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0243Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0243Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0240Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0239Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0237Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0238Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0238Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0238Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0237Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0235Epoch 15/15: [==============================] 63/63 batches, loss: 0.0243
[2025-04-29 21:34:15,847][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0243
[2025-04-29 21:34:16,074][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0479, Metrics: {'mse': 0.04749322310090065, 'rmse': 0.21792939934965325, 'r2': 0.2679714560508728}
[2025-04-29 21:34:16,075][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-04-29 21:34:16,075][src.training.lm_trainer][INFO] - Training completed in 36.76 seconds
[2025-04-29 21:34:16,075][src.training.lm_trainer][INFO] - Loading best model for final evaluation
slurmstepd: error: *** JOB 64423787 ON k28i22 CANCELLED AT 2025-04-29T21:34:16 ***
