SLURM_JOB_ID: 64436568
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_h100
SLURM_NNODES: 1
SLURM_NODELIST: s16g09
SLURM_JOB_CPUS_PER_NODE: 16
SLURM_JOB_GPUS: 2
Date: Thu May  1 22:31:41 CEST 2025
Walltime: 00-00:10:00
========================================================================
Running main finetuning experiments (non-control)...
Experiment finetune_question_type_ar has failed 2 times already. Skipping.
Experiment finetune_complexity_ar has failed 2 times already. Skipping.
Running experiment: finetune_question_type_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=false"         "model.finetune=true"                     "model.head_hidden_size=768"             "model.head_layers=2"             "model.dropout=0.2"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"                     "training.lr=2e-5"             "training.patience=3"             "training.scheduler_factor=0.5"             "training.scheduler_patience=2"             "+training.gradient_accumulation_steps=4"         +training.debug_mode=true         "experiment_name=finetune_question_type_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 22:31:57,895][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type
experiment_name: finetune_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 22:31:57,895][__main__][INFO] - Normalized task: question_type
[2025-05-01 22:31:57,895][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 22:31:57,895][__main__][INFO] - Determined Task Type: classification
[2025-05-01 22:31:57,898][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-05-01 22:31:57,898][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 22:31:59,771][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 22:32:01,736][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 22:32:01,737][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:01,814][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:01,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:01,917][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 22:32:01,924][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:01,924][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 22:32:01,925][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:01,941][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:01,964][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:01,974][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 22:32:01,975][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:01,975][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 22:32:01,976][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:01,992][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:02,022][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:02,032][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 22:32:02,033][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:02,033][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 22:32:02,034][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 22:32:02,034][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 22:32:02,034][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 22:32:02,034][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 22:32:02,034][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 22:32:02,035][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-05-01 22:32:02,035][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 22:32:02,035][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-01 22:32:02,035][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 22:32:02,035][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 22:32:02,036][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-01 22:32:02,036][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-01 22:32:02,036][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 22:32:02,036][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 22:32:02,036][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 22:32:02,036][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 22:32:02,036][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 22:32:02,036][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 22:32:02,036][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 22:32:05,838][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 22:32:05,838][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 22:32:05,839][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 22:32:05,839][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-01 22:32:05,840][src.models.model_factory][INFO] - Model has 394,196,929 trainable parameters out of 394,196,929 total parameters
[2025-05-01 22:32:05,841][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 75,457 trainable parameters
[2025-05-01 22:32:05,841][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=1, activation=gelu, normalization=layer
[2025-05-01 22:32:05,841][src.models.model_factory][INFO] - Created specialized classification probe with 1 layers, 96 hidden size
[2025-05-01 22:32:05,841][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 22:32:05,841][__main__][INFO] - Total parameters: 394,196,929
[2025-05-01 22:32:05,842][__main__][INFO] - Trainable parameters: 394,196,929 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.4528Epoch 1/10: [                              ] 2/75 batches, loss: 0.6293Epoch 1/10: [=                             ] 3/75 batches, loss: 0.7090Epoch 1/10: [=                             ] 4/75 batches, loss: 0.6905Epoch 1/10: [==                            ] 5/75 batches, loss: 0.6995Epoch 1/10: [==                            ] 6/75 batches, loss: 0.7049Epoch 1/10: [==                            ] 7/75 batches, loss: 0.7238Epoch 1/10: [===                           ] 8/75 batches, loss: 0.7308Epoch 1/10: [===                           ] 9/75 batches, loss: 0.7214Epoch 1/10: [====                          ] 10/75 batches, loss: 0.7184Epoch 1/10: [====                          ] 11/75 batches, loss: 0.7231Epoch 1/10: [====                          ] 12/75 batches, loss: 0.7168Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.7131Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.7098Epoch 1/10: [======                        ] 15/75 batches, loss: 0.7034Epoch 1/10: [======                        ] 16/75 batches, loss: 0.6992Epoch 1/10: [======                        ] 17/75 batches, loss: 0.6961Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.6913Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.6910Epoch 1/10: [========                      ] 20/75 batches, loss: 0.6881Epoch 1/10: [========                      ] 21/75 batches, loss: 0.6850Epoch 1/10: [========                      ] 22/75 batches, loss: 0.6835Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.6800Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.6750Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.6716Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.6682Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.6624Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.6562Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.6526Epoch 1/10: [============                  ] 30/75 batches, loss: 0.6495Epoch 1/10: [============                  ] 31/75 batches, loss: 0.6449Epoch 1/10: [============                  ] 32/75 batches, loss: 0.6406Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.6364Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.6342Epoch 1/10: [==============                ] 35/75 batches, loss: 0.6311Epoch 1/10: [==============                ] 36/75 batches, loss: 0.6283Epoch 1/10: [==============                ] 37/75 batches, loss: 0.6253Epoch 1/10: [===============               ] 38/75 batches, loss: 0.6216Epoch 1/10: [===============               ] 39/75 batches, loss: 0.6208Epoch 1/10: [================              ] 40/75 batches, loss: 0.6162Epoch 1/10: [================              ] 41/75 batches, loss: 0.6143Epoch 1/10: [================              ] 42/75 batches, loss: 0.6123Epoch 1/10: [=================             ] 43/75 batches, loss: 0.6120Epoch 1/10: [=================             ] 44/75 batches, loss: 0.6092Epoch 1/10: [==================            ] 45/75 batches, loss: 0.6064Epoch 1/10: [==================            ] 46/75 batches, loss: 0.6047Epoch 1/10: [==================            ] 47/75 batches, loss: 0.6022Epoch 1/10: [===================           ] 48/75 batches, loss: 0.6011Epoch 1/10: [===================           ] 49/75 batches, loss: 0.5984Epoch 1/10: [====================          ] 50/75 batches, loss: 0.5975Epoch 1/10: [====================          ] 51/75 batches, loss: 0.5958Epoch 1/10: [====================          ] 52/75 batches, loss: 0.5935Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.5923Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.5925Epoch 1/10: [======================        ] 55/75 batches, loss: 0.5914Epoch 1/10: [======================        ] 56/75 batches, loss: 0.5886Epoch 1/10: [======================        ] 57/75 batches, loss: 0.5875Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.5881Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.5859Epoch 1/10: [========================      ] 60/75 batches, loss: 0.5841Epoch 1/10: [========================      ] 61/75 batches, loss: 0.5820Epoch 1/10: [========================      ] 62/75 batches, loss: 0.5806Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.5786Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.5782Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.5780Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.5783Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.5765Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.5754Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.5747Epoch 1/10: [============================  ] 70/75 batches, loss: 0.5754Epoch 1/10: [============================  ] 71/75 batches, loss: 0.5747Epoch 1/10: [============================  ] 72/75 batches, loss: 0.5734Epoch 1/10: [============================= ] 73/75 batches, loss: 0.5728Epoch 1/10: [============================= ] 74/75 batches, loss: 0.5715Epoch 1/10: [==============================] 75/75 batches, loss: 0.5706
[2025-05-01 22:32:13,156][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.5706
[2025-05-01 22:32:13,557][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.5355, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.5508Epoch 2/10: [                              ] 2/75 batches, loss: 0.5277Epoch 2/10: [=                             ] 3/75 batches, loss: 0.5116Epoch 2/10: [=                             ] 4/75 batches, loss: 0.4858Epoch 2/10: [==                            ] 5/75 batches, loss: 0.4988Epoch 2/10: [==                            ] 6/75 batches, loss: 0.4956Epoch 2/10: [==                            ] 7/75 batches, loss: 0.4967Epoch 2/10: [===                           ] 8/75 batches, loss: 0.4946Epoch 2/10: [===                           ] 9/75 batches, loss: 0.4982Epoch 2/10: [====                          ] 10/75 batches, loss: 0.5058Epoch 2/10: [====                          ] 11/75 batches, loss: 0.5099Epoch 2/10: [====                          ] 12/75 batches, loss: 0.5035Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.5053Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.5018Epoch 2/10: [======                        ] 15/75 batches, loss: 0.5003Epoch 2/10: [======                        ] 16/75 batches, loss: 0.5079Epoch 2/10: [======                        ] 17/75 batches, loss: 0.5049Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.5048Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.5067Epoch 2/10: [========                      ] 20/75 batches, loss: 0.5078Epoch 2/10: [========                      ] 21/75 batches, loss: 0.5049Epoch 2/10: [========                      ] 22/75 batches, loss: 0.4994Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.5017Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.5014Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.5015Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.5015Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.5007Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.5008Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.4976Epoch 2/10: [============                  ] 30/75 batches, loss: 0.4971Epoch 2/10: [============                  ] 31/75 batches, loss: 0.4981Epoch 2/10: [============                  ] 32/75 batches, loss: 0.4968Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.4999Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.5007Epoch 2/10: [==============                ] 35/75 batches, loss: 0.4994Epoch 2/10: [==============                ] 36/75 batches, loss: 0.4995Epoch 2/10: [==============                ] 37/75 batches, loss: 0.4996Epoch 2/10: [===============               ] 38/75 batches, loss: 0.4991Epoch 2/10: [===============               ] 39/75 batches, loss: 0.4992Epoch 2/10: [================              ] 40/75 batches, loss: 0.4987Epoch 2/10: [================              ] 41/75 batches, loss: 0.4994Epoch 2/10: [================              ] 42/75 batches, loss: 0.5006Epoch 2/10: [=================             ] 43/75 batches, loss: 0.5013Epoch 2/10: [=================             ] 44/75 batches, loss: 0.5008Epoch 2/10: [==================            ] 45/75 batches, loss: 0.5008Epoch 2/10: [==================            ] 46/75 batches, loss: 0.5014Epoch 2/10: [==================            ] 47/75 batches, loss: 0.5025Epoch 2/10: [===================           ] 48/75 batches, loss: 0.5020Epoch 2/10: [===================           ] 49/75 batches, loss: 0.5025Epoch 2/10: [====================          ] 50/75 batches, loss: 0.5025Epoch 2/10: [====================          ] 51/75 batches, loss: 0.5040Epoch 2/10: [====================          ] 52/75 batches, loss: 0.5030Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.5031Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.5029Epoch 2/10: [======================        ] 55/75 batches, loss: 0.5016Epoch 2/10: [======================        ] 56/75 batches, loss: 0.5004Epoch 2/10: [======================        ] 57/75 batches, loss: 0.5008Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.4996Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.4997Epoch 2/10: [========================      ] 60/75 batches, loss: 0.5006Epoch 2/10: [========================      ] 61/75 batches, loss: 0.5007Epoch 2/10: [========================      ] 62/75 batches, loss: 0.5027Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.5038Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.5038Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.5041Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.5045Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.5038Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.5048Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.5051Epoch 2/10: [============================  ] 70/75 batches, loss: 0.5051Epoch 2/10: [============================  ] 71/75 batches, loss: 0.5054Epoch 2/10: [============================  ] 72/75 batches, loss: 0.5054Epoch 2/10: [============================= ] 73/75 batches, loss: 0.5050Epoch 2/10: [============================= ] 74/75 batches, loss: 0.5050Epoch 2/10: [==============================] 75/75 batches, loss: 0.5062
[2025-05-01 22:32:17,601][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5062
[2025-05-01 22:32:18,012][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5481, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.8985507246376812, 'precision': 0.9393939393939394, 'recall': 0.8611111111111112}
[2025-05-01 22:32:18,013][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.4801Epoch 3/10: [                              ] 2/75 batches, loss: 0.4941Epoch 3/10: [=                             ] 3/75 batches, loss: 0.5130Epoch 3/10: [=                             ] 4/75 batches, loss: 0.5165Epoch 3/10: [==                            ] 5/75 batches, loss: 0.5233Epoch 3/10: [==                            ] 6/75 batches, loss: 0.5243Epoch 3/10: [==                            ] 7/75 batches, loss: 0.5336Epoch 3/10: [===                           ] 8/75 batches, loss: 0.5387Epoch 3/10: [===                           ] 9/75 batches, loss: 0.5427Epoch 3/10: [====                          ] 10/75 batches, loss: 0.5316Epoch 3/10: [====                          ] 11/75 batches, loss: 0.5312Epoch 3/10: [====                          ] 12/75 batches, loss: 0.5361Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.5336Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.5348Epoch 3/10: [======                        ] 15/75 batches, loss: 0.5311Epoch 3/10: [======                        ] 16/75 batches, loss: 0.5333Epoch 3/10: [======                        ] 17/75 batches, loss: 0.5301Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.5286Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.5223Epoch 3/10: [========                      ] 20/75 batches, loss: 0.5237Epoch 3/10: [========                      ] 21/75 batches, loss: 0.5193Epoch 3/10: [========                      ] 22/75 batches, loss: 0.5182Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.5155Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.5170Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.5136Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.5132Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.5128Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.5125Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.5138Epoch 3/10: [============                  ] 30/75 batches, loss: 0.5119Epoch 3/10: [============                  ] 31/75 batches, loss: 0.5139Epoch 3/10: [============                  ] 32/75 batches, loss: 0.5136Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.5118Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.5123Epoch 3/10: [==============                ] 35/75 batches, loss: 0.5140Epoch 3/10: [==============                ] 36/75 batches, loss: 0.5138Epoch 3/10: [==============                ] 37/75 batches, loss: 0.5154Epoch 3/10: [===============               ] 38/75 batches, loss: 0.5145Epoch 3/10: [===============               ] 39/75 batches, loss: 0.5130Epoch 3/10: [================              ] 40/75 batches, loss: 0.5134Epoch 3/10: [================              ] 41/75 batches, loss: 0.5119Epoch 3/10: [================              ] 42/75 batches, loss: 0.5106Epoch 3/10: [=================             ] 43/75 batches, loss: 0.5093Epoch 3/10: [=================             ] 44/75 batches, loss: 0.5108Epoch 3/10: [==================            ] 45/75 batches, loss: 0.5112Epoch 3/10: [==================            ] 46/75 batches, loss: 0.5115Epoch 3/10: [==================            ] 47/75 batches, loss: 0.5098Epoch 3/10: [===================           ] 48/75 batches, loss: 0.5122Epoch 3/10: [===================           ] 49/75 batches, loss: 0.5105Epoch 3/10: [====================          ] 50/75 batches, loss: 0.5118Epoch 3/10: [====================          ] 51/75 batches, loss: 0.5121Epoch 3/10: [====================          ] 52/75 batches, loss: 0.5115Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.5118Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.5094Epoch 3/10: [======================        ] 55/75 batches, loss: 0.5093Epoch 3/10: [======================        ] 56/75 batches, loss: 0.5101Epoch 3/10: [======================        ] 57/75 batches, loss: 0.5108Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.5094Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.5101Epoch 3/10: [========================      ] 60/75 batches, loss: 0.5104Epoch 3/10: [========================      ] 61/75 batches, loss: 0.5115Epoch 3/10: [========================      ] 62/75 batches, loss: 0.5109Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.5104Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.5089Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.5073Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.5083Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.5072Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.5071Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.5054Epoch 3/10: [============================  ] 70/75 batches, loss: 0.5054Epoch 3/10: [============================  ] 71/75 batches, loss: 0.5057Epoch 3/10: [============================  ] 72/75 batches, loss: 0.5060Epoch 3/10: [============================= ] 73/75 batches, loss: 0.5053Epoch 3/10: [============================= ] 74/75 batches, loss: 0.5049Epoch 3/10: [==============================] 75/75 batches, loss: 0.5062
[2025-05-01 22:32:21,834][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5062
[2025-05-01 22:32:22,246][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5409, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.935064935064935, 'precision': 0.8780487804878049, 'recall': 1.0}
[2025-05-01 22:32:22,246][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.4320Epoch 4/10: [                              ] 2/75 batches, loss: 0.4913Epoch 4/10: [=                             ] 3/75 batches, loss: 0.4874Epoch 4/10: [=                             ] 4/75 batches, loss: 0.4973Epoch 4/10: [==                            ] 5/75 batches, loss: 0.4890Epoch 4/10: [==                            ] 6/75 batches, loss: 0.5032Epoch 4/10: [==                            ] 7/75 batches, loss: 0.5066Epoch 4/10: [===                           ] 8/75 batches, loss: 0.5003Epoch 4/10: [===                           ] 9/75 batches, loss: 0.4927Epoch 4/10: [====                          ] 10/75 batches, loss: 0.4866Epoch 4/10: [====                          ] 11/75 batches, loss: 0.4903Epoch 4/10: [====                          ] 12/75 batches, loss: 0.4894Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.4843Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.4907Epoch 4/10: [======                        ] 15/75 batches, loss: 0.4900Epoch 4/10: [======                        ] 16/75 batches, loss: 0.4893Epoch 4/10: [======                        ] 17/75 batches, loss: 0.4845Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.4882Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.4853Epoch 4/10: [========                      ] 20/75 batches, loss: 0.4850Epoch 4/10: [========                      ] 21/75 batches, loss: 0.4858Epoch 4/10: [========                      ] 22/75 batches, loss: 0.4895Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.4911Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.4926Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.4949Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.4953Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.4938Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.4933Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.4944Epoch 4/10: [============                  ] 30/75 batches, loss: 0.4971Epoch 4/10: [============                  ] 31/75 batches, loss: 0.4958Epoch 4/10: [============                  ] 32/75 batches, loss: 0.4953Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.4941Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.4957Epoch 4/10: [==============                ] 35/75 batches, loss: 0.4973Epoch 4/10: [==============                ] 36/75 batches, loss: 0.4955Epoch 4/10: [==============                ] 37/75 batches, loss: 0.4970Epoch 4/10: [===============               ] 38/75 batches, loss: 0.4971Epoch 4/10: [===============               ] 39/75 batches, loss: 0.4979Epoch 4/10: [================              ] 40/75 batches, loss: 0.5010Epoch 4/10: [================              ] 41/75 batches, loss: 0.4982Epoch 4/10: [================              ] 42/75 batches, loss: 0.4983Epoch 4/10: [=================             ] 43/75 batches, loss: 0.4999Epoch 4/10: [=================             ] 44/75 batches, loss: 0.5010Epoch 4/10: [==================            ] 45/75 batches, loss: 0.5011Epoch 4/10: [==================            ] 46/75 batches, loss: 0.5021Epoch 4/10: [==================            ] 47/75 batches, loss: 0.5042Epoch 4/10: [===================           ] 48/75 batches, loss: 0.5057Epoch 4/10: [===================           ] 49/75 batches, loss: 0.5056Epoch 4/10: [====================          ] 50/75 batches, loss: 0.5056Epoch 4/10: [====================          ] 51/75 batches, loss: 0.5046Epoch 4/10: [====================          ] 52/75 batches, loss: 0.5046Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.5036Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.5045Epoch 4/10: [======================        ] 55/75 batches, loss: 0.5041Epoch 4/10: [======================        ] 56/75 batches, loss: 0.5036Epoch 4/10: [======================        ] 57/75 batches, loss: 0.5036Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.5040Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.5028Epoch 4/10: [========================      ] 60/75 batches, loss: 0.5024Epoch 4/10: [========================      ] 61/75 batches, loss: 0.5024Epoch 4/10: [========================      ] 62/75 batches, loss: 0.5020Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.5013Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.5006Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.5010Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.5014Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.5025Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.5043Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.5039Epoch 4/10: [============================  ] 70/75 batches, loss: 0.5048Epoch 4/10: [============================  ] 71/75 batches, loss: 0.5051Epoch 4/10: [============================  ] 72/75 batches, loss: 0.5051Epoch 4/10: [============================= ] 73/75 batches, loss: 0.5057Epoch 4/10: [============================= ] 74/75 batches, loss: 0.5053Epoch 4/10: [==============================] 75/75 batches, loss: 0.5053
[2025-05-01 22:32:26,077][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5053
[2025-05-01 22:32:26,456][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5370, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
[2025-05-01 22:32:26,457][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 22:32:26,457][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-01 22:32:26,457][src.training.lm_trainer][INFO] - Training completed in 18.27 seconds
[2025-05-01 22:32:26,457][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 22:32:28,474][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9966442953020134, 'f1': 0.9966555183946488, 'precision': 0.9933333333333333, 'recall': 1.0}
[2025-05-01 22:32:28,474][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315, 'precision': 0.9, 'recall': 1.0}
[2025-05-01 22:32:28,475][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.9272727272727272, 'f1': 0.9285714285714286, 'precision': 0.9122807017543859, 'recall': 0.9454545454545454}
[2025-05-01 22:32:29,707][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/en/model.pt
[2025-05-01 22:32:29,714][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy █▁▆█
wandb:                val_f1 █▁▆█
wandb:              val_loss ▁█▄▂
wandb:         val_precision ▄█▁▄
wandb:            val_recall █▁██
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.94444
wandb:           best_val_f1 0.94737
wandb:         best_val_loss 0.53546
wandb:    best_val_precision 0.9
wandb:       best_val_recall 1
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.92727
wandb:         final_test_f1 0.92857
wandb:  final_test_precision 0.91228
wandb:     final_test_recall 0.94545
wandb:  final_train_accuracy 0.99664
wandb:        final_train_f1 0.99666
wandb: final_train_precision 0.99333
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.94444
wandb:          final_val_f1 0.94737
wandb:   final_val_precision 0.9
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50531
wandb:            train_time 18.27119
wandb:          val_accuracy 0.94444
wandb:                val_f1 0.94737
wandb:              val_loss 0.53697
wandb:         val_precision 0.9
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_223157-d2st0nsy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_223157-d2st0nsy/logs
Experiment finetune_question_type_en completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/finetune_output/question_type/en/results.json
Running experiment: finetune_complexity_en
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=false"         "model.finetune=true"                     "model.head_hidden_size=512"             "model.head_layers=3"             "model.dropout=0.1"         "data.languages=[en]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"                     "training.lr=1e-5"             "training.patience=4"             "training.scheduler_factor=0.5"             "training.scheduler_patience=3"             "+training.gradient_accumulation_steps=4"         +training.debug_mode=true         "experiment_name=finetune_complexity_en"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 22:32:39,486][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity
experiment_name: finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 512
  head_layers: 3
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 3
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 22:32:39,486][__main__][INFO] - Normalized task: complexity
[2025-05-01 22:32:39,486][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 22:32:39,486][__main__][INFO] - Determined Task Type: regression
[2025-05-01 22:32:39,490][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-05-01 22:32:39,490][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 22:32:40,580][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 22:32:42,557][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 22:32:42,557][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:42,593][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,675][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-05-01 22:32:42,685][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:42,685][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-05-01 22:32:42,686][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:42,705][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,730][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,740][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-05-01 22:32:42,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:42,741][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-05-01 22:32:42,742][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 22:32:42,760][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,790][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 22:32:42,800][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-05-01 22:32:42,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 22:32:42,801][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-05-01 22:32:42,802][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-05-01 22:32:42,802][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 22:32:42,802][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 22:32:42,802][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 22:32:42,803][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 22:32:42,803][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 22:32:42,803][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 22:32:42,803][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-05-01 22:32:42,803][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 22:32:42,804][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 22:32:42,804][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 22:32:42,804][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 22:32:42,805][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 22:32:42,805][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 22:32:45,569][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 22:32:45,570][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 22:32:45,570][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 22:32:45,570][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-01 22:32:45,572][src.models.model_factory][INFO] - Model has 394,196,929 trainable parameters out of 394,196,929 total parameters
[2025-05-01 22:32:45,572][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 75,457 trainable parameters
[2025-05-01 22:32:45,572][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=1, activation=silu, normalization=layer
[2025-05-01 22:32:45,572][src.models.model_factory][INFO] - Created specialized regression probe with 1 layers, 96 hidden size
[2025-05-01 22:32:45,572][__main__][INFO] - Successfully created lm_probe model for en
[2025-05-01 22:32:45,573][__main__][INFO] - Total parameters: 394,196,929
[2025-05-01 22:32:45,573][__main__][INFO] - Trainable parameters: 394,196,929 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/75 batches, loss: 0.2158Epoch 1/10: [                              ] 2/75 batches, loss: 0.1729Epoch 1/10: [=                             ] 3/75 batches, loss: 0.1752Epoch 1/10: [=                             ] 4/75 batches, loss: 0.1582Epoch 1/10: [==                            ] 5/75 batches, loss: 0.1522Epoch 1/10: [==                            ] 6/75 batches, loss: 0.1494Epoch 1/10: [==                            ] 7/75 batches, loss: 0.1569Epoch 1/10: [===                           ] 8/75 batches, loss: 0.1491Epoch 1/10: [===                           ] 9/75 batches, loss: 0.1555Epoch 1/10: [====                          ] 10/75 batches, loss: 0.1533Epoch 1/10: [====                          ] 11/75 batches, loss: 0.1553Epoch 1/10: [====                          ] 12/75 batches, loss: 0.1573Epoch 1/10: [=====                         ] 13/75 batches, loss: 0.1572Epoch 1/10: [=====                         ] 14/75 batches, loss: 0.1561Epoch 1/10: [======                        ] 15/75 batches, loss: 0.1542Epoch 1/10: [======                        ] 16/75 batches, loss: 0.1497Epoch 1/10: [======                        ] 17/75 batches, loss: 0.1488Epoch 1/10: [=======                       ] 18/75 batches, loss: 0.1477Epoch 1/10: [=======                       ] 19/75 batches, loss: 0.1457Epoch 1/10: [========                      ] 20/75 batches, loss: 0.1419Epoch 1/10: [========                      ] 21/75 batches, loss: 0.1400Epoch 1/10: [========                      ] 22/75 batches, loss: 0.1384Epoch 1/10: [=========                     ] 23/75 batches, loss: 0.1379Epoch 1/10: [=========                     ] 24/75 batches, loss: 0.1387Epoch 1/10: [==========                    ] 25/75 batches, loss: 0.1350Epoch 1/10: [==========                    ] 26/75 batches, loss: 0.1324Epoch 1/10: [==========                    ] 27/75 batches, loss: 0.1309Epoch 1/10: [===========                   ] 28/75 batches, loss: 0.1305Epoch 1/10: [===========                   ] 29/75 batches, loss: 0.1285Epoch 1/10: [============                  ] 30/75 batches, loss: 0.1262Epoch 1/10: [============                  ] 31/75 batches, loss: 0.1253Epoch 1/10: [============                  ] 32/75 batches, loss: 0.1243Epoch 1/10: [=============                 ] 33/75 batches, loss: 0.1223Epoch 1/10: [=============                 ] 34/75 batches, loss: 0.1206Epoch 1/10: [==============                ] 35/75 batches, loss: 0.1181Epoch 1/10: [==============                ] 36/75 batches, loss: 0.1185Epoch 1/10: [==============                ] 37/75 batches, loss: 0.1162Epoch 1/10: [===============               ] 38/75 batches, loss: 0.1143Epoch 1/10: [===============               ] 39/75 batches, loss: 0.1124Epoch 1/10: [================              ] 40/75 batches, loss: 0.1114Epoch 1/10: [================              ] 41/75 batches, loss: 0.1101Epoch 1/10: [================              ] 42/75 batches, loss: 0.1094Epoch 1/10: [=================             ] 43/75 batches, loss: 0.1076Epoch 1/10: [=================             ] 44/75 batches, loss: 0.1058Epoch 1/10: [==================            ] 45/75 batches, loss: 0.1041Epoch 1/10: [==================            ] 46/75 batches, loss: 0.1028Epoch 1/10: [==================            ] 47/75 batches, loss: 0.1018Epoch 1/10: [===================           ] 48/75 batches, loss: 0.1006Epoch 1/10: [===================           ] 49/75 batches, loss: 0.1005Epoch 1/10: [====================          ] 50/75 batches, loss: 0.0989Epoch 1/10: [====================          ] 51/75 batches, loss: 0.0982Epoch 1/10: [====================          ] 52/75 batches, loss: 0.0969Epoch 1/10: [=====================         ] 53/75 batches, loss: 0.0962Epoch 1/10: [=====================         ] 54/75 batches, loss: 0.0954Epoch 1/10: [======================        ] 55/75 batches, loss: 0.0941Epoch 1/10: [======================        ] 56/75 batches, loss: 0.0930Epoch 1/10: [======================        ] 57/75 batches, loss: 0.0916Epoch 1/10: [=======================       ] 58/75 batches, loss: 0.0903Epoch 1/10: [=======================       ] 59/75 batches, loss: 0.0892Epoch 1/10: [========================      ] 60/75 batches, loss: 0.0886Epoch 1/10: [========================      ] 61/75 batches, loss: 0.0875Epoch 1/10: [========================      ] 62/75 batches, loss: 0.0869Epoch 1/10: [=========================     ] 63/75 batches, loss: 0.0859Epoch 1/10: [=========================     ] 64/75 batches, loss: 0.0850Epoch 1/10: [==========================    ] 65/75 batches, loss: 0.0841Epoch 1/10: [==========================    ] 66/75 batches, loss: 0.0835Epoch 1/10: [==========================    ] 67/75 batches, loss: 0.0827Epoch 1/10: [===========================   ] 68/75 batches, loss: 0.0820Epoch 1/10: [===========================   ] 69/75 batches, loss: 0.0811Epoch 1/10: [============================  ] 70/75 batches, loss: 0.0805Epoch 1/10: [============================  ] 71/75 batches, loss: 0.0800Epoch 1/10: [============================  ] 72/75 batches, loss: 0.0792Epoch 1/10: [============================= ] 73/75 batches, loss: 0.0784Epoch 1/10: [============================= ] 74/75 batches, loss: 0.0778Epoch 1/10: [==============================] 75/75 batches, loss: 0.0770
[2025-05-01 22:32:52,671][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0770
[2025-05-01 22:32:53,021][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0537, Metrics: {'mse': 0.05267707258462906, 'rmse': 0.22951486353748216, 'r2': -0.2586749792098999}
Epoch 2/10: [Epoch 2/10: [                              ] 1/75 batches, loss: 0.0297Epoch 2/10: [                              ] 2/75 batches, loss: 0.0316Epoch 2/10: [=                             ] 3/75 batches, loss: 0.0278Epoch 2/10: [=                             ] 4/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 5/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 6/75 batches, loss: 0.0283Epoch 2/10: [==                            ] 7/75 batches, loss: 0.0280Epoch 2/10: [===                           ] 8/75 batches, loss: 0.0271Epoch 2/10: [===                           ] 9/75 batches, loss: 0.0292Epoch 2/10: [====                          ] 10/75 batches, loss: 0.0286Epoch 2/10: [====                          ] 11/75 batches, loss: 0.0284Epoch 2/10: [====                          ] 12/75 batches, loss: 0.0272Epoch 2/10: [=====                         ] 13/75 batches, loss: 0.0270Epoch 2/10: [=====                         ] 14/75 batches, loss: 0.0270Epoch 2/10: [======                        ] 15/75 batches, loss: 0.0282Epoch 2/10: [======                        ] 16/75 batches, loss: 0.0278Epoch 2/10: [======                        ] 17/75 batches, loss: 0.0274Epoch 2/10: [=======                       ] 18/75 batches, loss: 0.0264Epoch 2/10: [=======                       ] 19/75 batches, loss: 0.0263Epoch 2/10: [========                      ] 20/75 batches, loss: 0.0271Epoch 2/10: [========                      ] 21/75 batches, loss: 0.0277Epoch 2/10: [========                      ] 22/75 batches, loss: 0.0276Epoch 2/10: [=========                     ] 23/75 batches, loss: 0.0272Epoch 2/10: [=========                     ] 24/75 batches, loss: 0.0277Epoch 2/10: [==========                    ] 25/75 batches, loss: 0.0283Epoch 2/10: [==========                    ] 26/75 batches, loss: 0.0280Epoch 2/10: [==========                    ] 27/75 batches, loss: 0.0279Epoch 2/10: [===========                   ] 28/75 batches, loss: 0.0279Epoch 2/10: [===========                   ] 29/75 batches, loss: 0.0274Epoch 2/10: [============                  ] 30/75 batches, loss: 0.0274Epoch 2/10: [============                  ] 31/75 batches, loss: 0.0271Epoch 2/10: [============                  ] 32/75 batches, loss: 0.0271Epoch 2/10: [=============                 ] 33/75 batches, loss: 0.0271Epoch 2/10: [=============                 ] 34/75 batches, loss: 0.0272Epoch 2/10: [==============                ] 35/75 batches, loss: 0.0277Epoch 2/10: [==============                ] 36/75 batches, loss: 0.0281Epoch 2/10: [==============                ] 37/75 batches, loss: 0.0278Epoch 2/10: [===============               ] 38/75 batches, loss: 0.0278Epoch 2/10: [===============               ] 39/75 batches, loss: 0.0274Epoch 2/10: [================              ] 40/75 batches, loss: 0.0272Epoch 2/10: [================              ] 41/75 batches, loss: 0.0273Epoch 2/10: [================              ] 42/75 batches, loss: 0.0270Epoch 2/10: [=================             ] 43/75 batches, loss: 0.0268Epoch 2/10: [=================             ] 44/75 batches, loss: 0.0267Epoch 2/10: [==================            ] 45/75 batches, loss: 0.0266Epoch 2/10: [==================            ] 46/75 batches, loss: 0.0269Epoch 2/10: [==================            ] 47/75 batches, loss: 0.0265Epoch 2/10: [===================           ] 48/75 batches, loss: 0.0266Epoch 2/10: [===================           ] 49/75 batches, loss: 0.0269Epoch 2/10: [====================          ] 50/75 batches, loss: 0.0272Epoch 2/10: [====================          ] 51/75 batches, loss: 0.0270Epoch 2/10: [====================          ] 52/75 batches, loss: 0.0269Epoch 2/10: [=====================         ] 53/75 batches, loss: 0.0267Epoch 2/10: [=====================         ] 54/75 batches, loss: 0.0266Epoch 2/10: [======================        ] 55/75 batches, loss: 0.0267Epoch 2/10: [======================        ] 56/75 batches, loss: 0.0266Epoch 2/10: [======================        ] 57/75 batches, loss: 0.0269Epoch 2/10: [=======================       ] 58/75 batches, loss: 0.0270Epoch 2/10: [=======================       ] 59/75 batches, loss: 0.0270Epoch 2/10: [========================      ] 60/75 batches, loss: 0.0268Epoch 2/10: [========================      ] 61/75 batches, loss: 0.0268Epoch 2/10: [========================      ] 62/75 batches, loss: 0.0267Epoch 2/10: [=========================     ] 63/75 batches, loss: 0.0266Epoch 2/10: [=========================     ] 64/75 batches, loss: 0.0265Epoch 2/10: [==========================    ] 65/75 batches, loss: 0.0264Epoch 2/10: [==========================    ] 66/75 batches, loss: 0.0264Epoch 2/10: [==========================    ] 67/75 batches, loss: 0.0263Epoch 2/10: [===========================   ] 68/75 batches, loss: 0.0261Epoch 2/10: [===========================   ] 69/75 batches, loss: 0.0259Epoch 2/10: [============================  ] 70/75 batches, loss: 0.0260Epoch 2/10: [============================  ] 71/75 batches, loss: 0.0261Epoch 2/10: [============================  ] 72/75 batches, loss: 0.0260Epoch 2/10: [============================= ] 73/75 batches, loss: 0.0257Epoch 2/10: [============================= ] 74/75 batches, loss: 0.0258Epoch 2/10: [==============================] 75/75 batches, loss: 0.0258
[2025-05-01 22:32:57,067][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0258
[2025-05-01 22:32:57,436][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0569, Metrics: {'mse': 0.05557546392083168, 'rmse': 0.23574448863299366, 'r2': -0.3279297351837158}
[2025-05-01 22:32:57,437][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/10: [Epoch 3/10: [                              ] 1/75 batches, loss: 0.0175Epoch 3/10: [                              ] 2/75 batches, loss: 0.0162Epoch 3/10: [=                             ] 3/75 batches, loss: 0.0156Epoch 3/10: [=                             ] 4/75 batches, loss: 0.0196Epoch 3/10: [==                            ] 5/75 batches, loss: 0.0188Epoch 3/10: [==                            ] 6/75 batches, loss: 0.0209Epoch 3/10: [==                            ] 7/75 batches, loss: 0.0198Epoch 3/10: [===                           ] 8/75 batches, loss: 0.0190Epoch 3/10: [===                           ] 9/75 batches, loss: 0.0181Epoch 3/10: [====                          ] 10/75 batches, loss: 0.0194Epoch 3/10: [====                          ] 11/75 batches, loss: 0.0201Epoch 3/10: [====                          ] 12/75 batches, loss: 0.0202Epoch 3/10: [=====                         ] 13/75 batches, loss: 0.0202Epoch 3/10: [=====                         ] 14/75 batches, loss: 0.0195Epoch 3/10: [======                        ] 15/75 batches, loss: 0.0197Epoch 3/10: [======                        ] 16/75 batches, loss: 0.0199Epoch 3/10: [======                        ] 17/75 batches, loss: 0.0200Epoch 3/10: [=======                       ] 18/75 batches, loss: 0.0192Epoch 3/10: [=======                       ] 19/75 batches, loss: 0.0190Epoch 3/10: [========                      ] 20/75 batches, loss: 0.0190Epoch 3/10: [========                      ] 21/75 batches, loss: 0.0191Epoch 3/10: [========                      ] 22/75 batches, loss: 0.0188Epoch 3/10: [=========                     ] 23/75 batches, loss: 0.0189Epoch 3/10: [=========                     ] 24/75 batches, loss: 0.0183Epoch 3/10: [==========                    ] 25/75 batches, loss: 0.0187Epoch 3/10: [==========                    ] 26/75 batches, loss: 0.0187Epoch 3/10: [==========                    ] 27/75 batches, loss: 0.0185Epoch 3/10: [===========                   ] 28/75 batches, loss: 0.0185Epoch 3/10: [===========                   ] 29/75 batches, loss: 0.0187Epoch 3/10: [============                  ] 30/75 batches, loss: 0.0186Epoch 3/10: [============                  ] 31/75 batches, loss: 0.0186Epoch 3/10: [============                  ] 32/75 batches, loss: 0.0189Epoch 3/10: [=============                 ] 33/75 batches, loss: 0.0188Epoch 3/10: [=============                 ] 34/75 batches, loss: 0.0189Epoch 3/10: [==============                ] 35/75 batches, loss: 0.0187Epoch 3/10: [==============                ] 36/75 batches, loss: 0.0186Epoch 3/10: [==============                ] 37/75 batches, loss: 0.0185Epoch 3/10: [===============               ] 38/75 batches, loss: 0.0185Epoch 3/10: [===============               ] 39/75 batches, loss: 0.0186Epoch 3/10: [================              ] 40/75 batches, loss: 0.0186Epoch 3/10: [================              ] 41/75 batches, loss: 0.0189Epoch 3/10: [================              ] 42/75 batches, loss: 0.0189Epoch 3/10: [=================             ] 43/75 batches, loss: 0.0188Epoch 3/10: [=================             ] 44/75 batches, loss: 0.0186Epoch 3/10: [==================            ] 45/75 batches, loss: 0.0185Epoch 3/10: [==================            ] 46/75 batches, loss: 0.0186Epoch 3/10: [==================            ] 47/75 batches, loss: 0.0189Epoch 3/10: [===================           ] 48/75 batches, loss: 0.0191Epoch 3/10: [===================           ] 49/75 batches, loss: 0.0191Epoch 3/10: [====================          ] 50/75 batches, loss: 0.0191Epoch 3/10: [====================          ] 51/75 batches, loss: 0.0190Epoch 3/10: [====================          ] 52/75 batches, loss: 0.0189Epoch 3/10: [=====================         ] 53/75 batches, loss: 0.0188Epoch 3/10: [=====================         ] 54/75 batches, loss: 0.0190Epoch 3/10: [======================        ] 55/75 batches, loss: 0.0188Epoch 3/10: [======================        ] 56/75 batches, loss: 0.0189Epoch 3/10: [======================        ] 57/75 batches, loss: 0.0188Epoch 3/10: [=======================       ] 58/75 batches, loss: 0.0188Epoch 3/10: [=======================       ] 59/75 batches, loss: 0.0186Epoch 3/10: [========================      ] 60/75 batches, loss: 0.0185Epoch 3/10: [========================      ] 61/75 batches, loss: 0.0183Epoch 3/10: [========================      ] 62/75 batches, loss: 0.0184Epoch 3/10: [=========================     ] 63/75 batches, loss: 0.0186Epoch 3/10: [=========================     ] 64/75 batches, loss: 0.0187Epoch 3/10: [==========================    ] 65/75 batches, loss: 0.0186Epoch 3/10: [==========================    ] 66/75 batches, loss: 0.0186Epoch 3/10: [==========================    ] 67/75 batches, loss: 0.0188Epoch 3/10: [===========================   ] 68/75 batches, loss: 0.0190Epoch 3/10: [===========================   ] 69/75 batches, loss: 0.0191Epoch 3/10: [============================  ] 70/75 batches, loss: 0.0191Epoch 3/10: [============================  ] 71/75 batches, loss: 0.0190Epoch 3/10: [============================  ] 72/75 batches, loss: 0.0192Epoch 3/10: [============================= ] 73/75 batches, loss: 0.0192Epoch 3/10: [============================= ] 74/75 batches, loss: 0.0195Epoch 3/10: [==============================] 75/75 batches, loss: 0.0193
[2025-05-01 22:33:01,274][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0193
[2025-05-01 22:33:01,631][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0431, Metrics: {'mse': 0.04222438111901283, 'rmse': 0.20548571998806348, 'r2': -0.008916616439819336}
Epoch 4/10: [Epoch 4/10: [                              ] 1/75 batches, loss: 0.0235Epoch 4/10: [                              ] 2/75 batches, loss: 0.0177Epoch 4/10: [=                             ] 3/75 batches, loss: 0.0156Epoch 4/10: [=                             ] 4/75 batches, loss: 0.0144Epoch 4/10: [==                            ] 5/75 batches, loss: 0.0139Epoch 4/10: [==                            ] 6/75 batches, loss: 0.0152Epoch 4/10: [==                            ] 7/75 batches, loss: 0.0183Epoch 4/10: [===                           ] 8/75 batches, loss: 0.0176Epoch 4/10: [===                           ] 9/75 batches, loss: 0.0173Epoch 4/10: [====                          ] 10/75 batches, loss: 0.0176Epoch 4/10: [====                          ] 11/75 batches, loss: 0.0185Epoch 4/10: [====                          ] 12/75 batches, loss: 0.0190Epoch 4/10: [=====                         ] 13/75 batches, loss: 0.0200Epoch 4/10: [=====                         ] 14/75 batches, loss: 0.0195Epoch 4/10: [======                        ] 15/75 batches, loss: 0.0185Epoch 4/10: [======                        ] 16/75 batches, loss: 0.0185Epoch 4/10: [======                        ] 17/75 batches, loss: 0.0190Epoch 4/10: [=======                       ] 18/75 batches, loss: 0.0189Epoch 4/10: [=======                       ] 19/75 batches, loss: 0.0184Epoch 4/10: [========                      ] 20/75 batches, loss: 0.0188Epoch 4/10: [========                      ] 21/75 batches, loss: 0.0188Epoch 4/10: [========                      ] 22/75 batches, loss: 0.0185Epoch 4/10: [=========                     ] 23/75 batches, loss: 0.0183Epoch 4/10: [=========                     ] 24/75 batches, loss: 0.0187Epoch 4/10: [==========                    ] 25/75 batches, loss: 0.0192Epoch 4/10: [==========                    ] 26/75 batches, loss: 0.0191Epoch 4/10: [==========                    ] 27/75 batches, loss: 0.0194Epoch 4/10: [===========                   ] 28/75 batches, loss: 0.0191Epoch 4/10: [===========                   ] 29/75 batches, loss: 0.0191Epoch 4/10: [============                  ] 30/75 batches, loss: 0.0194Epoch 4/10: [============                  ] 31/75 batches, loss: 0.0189Epoch 4/10: [============                  ] 32/75 batches, loss: 0.0187Epoch 4/10: [=============                 ] 33/75 batches, loss: 0.0187Epoch 4/10: [=============                 ] 34/75 batches, loss: 0.0184Epoch 4/10: [==============                ] 35/75 batches, loss: 0.0182Epoch 4/10: [==============                ] 36/75 batches, loss: 0.0179Epoch 4/10: [==============                ] 37/75 batches, loss: 0.0179Epoch 4/10: [===============               ] 38/75 batches, loss: 0.0177Epoch 4/10: [===============               ] 39/75 batches, loss: 0.0179Epoch 4/10: [================              ] 40/75 batches, loss: 0.0178Epoch 4/10: [================              ] 41/75 batches, loss: 0.0179Epoch 4/10: [================              ] 42/75 batches, loss: 0.0179Epoch 4/10: [=================             ] 43/75 batches, loss: 0.0177Epoch 4/10: [=================             ] 44/75 batches, loss: 0.0176Epoch 4/10: [==================            ] 45/75 batches, loss: 0.0175Epoch 4/10: [==================            ] 46/75 batches, loss: 0.0173Epoch 4/10: [==================            ] 47/75 batches, loss: 0.0172Epoch 4/10: [===================           ] 48/75 batches, loss: 0.0171Epoch 4/10: [===================           ] 49/75 batches, loss: 0.0170Epoch 4/10: [====================          ] 50/75 batches, loss: 0.0169Epoch 4/10: [====================          ] 51/75 batches, loss: 0.0167Epoch 4/10: [====================          ] 52/75 batches, loss: 0.0166Epoch 4/10: [=====================         ] 53/75 batches, loss: 0.0165Epoch 4/10: [=====================         ] 54/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 55/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 56/75 batches, loss: 0.0164Epoch 4/10: [======================        ] 57/75 batches, loss: 0.0163Epoch 4/10: [=======================       ] 58/75 batches, loss: 0.0162Epoch 4/10: [=======================       ] 59/75 batches, loss: 0.0161Epoch 4/10: [========================      ] 60/75 batches, loss: 0.0162Epoch 4/10: [========================      ] 61/75 batches, loss: 0.0161Epoch 4/10: [========================      ] 62/75 batches, loss: 0.0161Epoch 4/10: [=========================     ] 63/75 batches, loss: 0.0160Epoch 4/10: [=========================     ] 64/75 batches, loss: 0.0162Epoch 4/10: [==========================    ] 65/75 batches, loss: 0.0161Epoch 4/10: [==========================    ] 66/75 batches, loss: 0.0162Epoch 4/10: [==========================    ] 67/75 batches, loss: 0.0162Epoch 4/10: [===========================   ] 68/75 batches, loss: 0.0162Epoch 4/10: [===========================   ] 69/75 batches, loss: 0.0163Epoch 4/10: [============================  ] 70/75 batches, loss: 0.0164Epoch 4/10: [============================  ] 71/75 batches, loss: 0.0162Epoch 4/10: [============================  ] 72/75 batches, loss: 0.0161Epoch 4/10: [============================= ] 73/75 batches, loss: 0.0160Epoch 4/10: [============================= ] 74/75 batches, loss: 0.0160Epoch 4/10: [==============================] 75/75 batches, loss: 0.0158
[2025-05-01 22:33:05,692][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0158
[2025-05-01 22:33:06,058][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0471, Metrics: {'mse': 0.04527391865849495, 'rmse': 0.21277668730031246, 'r2': -0.08178281784057617}
[2025-05-01 22:33:06,058][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/10: [Epoch 5/10: [                              ] 1/75 batches, loss: 0.0040Epoch 5/10: [                              ] 2/75 batches, loss: 0.0085Epoch 5/10: [=                             ] 3/75 batches, loss: 0.0098Epoch 5/10: [=                             ] 4/75 batches, loss: 0.0111Epoch 5/10: [==                            ] 5/75 batches, loss: 0.0113Epoch 5/10: [==                            ] 6/75 batches, loss: 0.0130Epoch 5/10: [==                            ] 7/75 batches, loss: 0.0125Epoch 5/10: [===                           ] 8/75 batches, loss: 0.0124Epoch 5/10: [===                           ] 9/75 batches, loss: 0.0116Epoch 5/10: [====                          ] 10/75 batches, loss: 0.0111Epoch 5/10: [====                          ] 11/75 batches, loss: 0.0112Epoch 5/10: [====                          ] 12/75 batches, loss: 0.0109Epoch 5/10: [=====                         ] 13/75 batches, loss: 0.0106Epoch 5/10: [=====                         ] 14/75 batches, loss: 0.0104Epoch 5/10: [======                        ] 15/75 batches, loss: 0.0116Epoch 5/10: [======                        ] 16/75 batches, loss: 0.0121Epoch 5/10: [======                        ] 17/75 batches, loss: 0.0119Epoch 5/10: [=======                       ] 18/75 batches, loss: 0.0118Epoch 5/10: [=======                       ] 19/75 batches, loss: 0.0116Epoch 5/10: [========                      ] 20/75 batches, loss: 0.0118Epoch 5/10: [========                      ] 21/75 batches, loss: 0.0117Epoch 5/10: [========                      ] 22/75 batches, loss: 0.0119Epoch 5/10: [=========                     ] 23/75 batches, loss: 0.0116Epoch 5/10: [=========                     ] 24/75 batches, loss: 0.0115Epoch 5/10: [==========                    ] 25/75 batches, loss: 0.0115Epoch 5/10: [==========                    ] 26/75 batches, loss: 0.0114Epoch 5/10: [==========                    ] 27/75 batches, loss: 0.0113Epoch 5/10: [===========                   ] 28/75 batches, loss: 0.0113Epoch 5/10: [===========                   ] 29/75 batches, loss: 0.0112Epoch 5/10: [============                  ] 30/75 batches, loss: 0.0115Epoch 5/10: [============                  ] 31/75 batches, loss: 0.0115Epoch 5/10: [============                  ] 32/75 batches, loss: 0.0115Epoch 5/10: [=============                 ] 33/75 batches, loss: 0.0114Epoch 5/10: [=============                 ] 34/75 batches, loss: 0.0112Epoch 5/10: [==============                ] 35/75 batches, loss: 0.0111Epoch 5/10: [==============                ] 36/75 batches, loss: 0.0115Epoch 5/10: [==============                ] 37/75 batches, loss: 0.0115Epoch 5/10: [===============               ] 38/75 batches, loss: 0.0115Epoch 5/10: [===============               ] 39/75 batches, loss: 0.0116Epoch 5/10: [================              ] 40/75 batches, loss: 0.0114Epoch 5/10: [================              ] 41/75 batches, loss: 0.0114Epoch 5/10: [================              ] 42/75 batches, loss: 0.0114Epoch 5/10: [=================             ] 43/75 batches, loss: 0.0112Epoch 5/10: [=================             ] 44/75 batches, loss: 0.0113Epoch 5/10: [==================            ] 45/75 batches, loss: 0.0113Epoch 5/10: [==================            ] 46/75 batches, loss: 0.0113Epoch 5/10: [==================            ] 47/75 batches, loss: 0.0112Epoch 5/10: [===================           ] 48/75 batches, loss: 0.0111Epoch 5/10: [===================           ] 49/75 batches, loss: 0.0115Epoch 5/10: [====================          ] 50/75 batches, loss: 0.0115Epoch 5/10: [====================          ] 51/75 batches, loss: 0.0114Epoch 5/10: [====================          ] 52/75 batches, loss: 0.0113Epoch 5/10: [=====================         ] 53/75 batches, loss: 0.0112Epoch 5/10: [=====================         ] 54/75 batches, loss: 0.0111Epoch 5/10: [======================        ] 55/75 batches, loss: 0.0112Epoch 5/10: [======================        ] 56/75 batches, loss: 0.0111Epoch 5/10: [======================        ] 57/75 batches, loss: 0.0113Epoch 5/10: [=======================       ] 58/75 batches, loss: 0.0112Epoch 5/10: [=======================       ] 59/75 batches, loss: 0.0112Epoch 5/10: [========================      ] 60/75 batches, loss: 0.0112Epoch 5/10: [========================      ] 61/75 batches, loss: 0.0112Epoch 5/10: [========================      ] 62/75 batches, loss: 0.0112Epoch 5/10: [=========================     ] 63/75 batches, loss: 0.0111Epoch 5/10: [=========================     ] 64/75 batches, loss: 0.0113Epoch 5/10: [==========================    ] 65/75 batches, loss: 0.0111Epoch 5/10: [==========================    ] 66/75 batches, loss: 0.0112Epoch 5/10: [==========================    ] 67/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 68/75 batches, loss: 0.0113Epoch 5/10: [===========================   ] 69/75 batches, loss: 0.0113Epoch 5/10: [============================  ] 70/75 batches, loss: 0.0113Epoch 5/10: [============================  ] 71/75 batches, loss: 0.0112Epoch 5/10: [============================  ] 72/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 73/75 batches, loss: 0.0112Epoch 5/10: [============================= ] 74/75 batches, loss: 0.0111Epoch 5/10: [==============================] 75/75 batches, loss: 0.0111
[2025-05-01 22:33:09,905][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0111
[2025-05-01 22:33:10,340][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0535, Metrics: {'mse': 0.05215701833367348, 'rmse': 0.22837911098363062, 'r2': -0.24624872207641602}
[2025-05-01 22:33:10,341][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/10: [Epoch 6/10: [                              ] 1/75 batches, loss: 0.0131Epoch 6/10: [                              ] 2/75 batches, loss: 0.0097Epoch 6/10: [=                             ] 3/75 batches, loss: 0.0107Epoch 6/10: [=                             ] 4/75 batches, loss: 0.0093Epoch 6/10: [==                            ] 5/75 batches, loss: 0.0086Epoch 6/10: [==                            ] 6/75 batches, loss: 0.0087Epoch 6/10: [==                            ] 7/75 batches, loss: 0.0095Epoch 6/10: [===                           ] 8/75 batches, loss: 0.0098Epoch 6/10: [===                           ] 9/75 batches, loss: 0.0092Epoch 6/10: [====                          ] 10/75 batches, loss: 0.0089Epoch 6/10: [====                          ] 11/75 batches, loss: 0.0090Epoch 6/10: [====                          ] 12/75 batches, loss: 0.0092Epoch 6/10: [=====                         ] 13/75 batches, loss: 0.0092Epoch 6/10: [=====                         ] 14/75 batches, loss: 0.0095Epoch 6/10: [======                        ] 15/75 batches, loss: 0.0098Epoch 6/10: [======                        ] 16/75 batches, loss: 0.0097Epoch 6/10: [======                        ] 17/75 batches, loss: 0.0098Epoch 6/10: [=======                       ] 18/75 batches, loss: 0.0103Epoch 6/10: [=======                       ] 19/75 batches, loss: 0.0100Epoch 6/10: [========                      ] 20/75 batches, loss: 0.0099Epoch 6/10: [========                      ] 21/75 batches, loss: 0.0098Epoch 6/10: [========                      ] 22/75 batches, loss: 0.0099Epoch 6/10: [=========                     ] 23/75 batches, loss: 0.0098Epoch 6/10: [=========                     ] 24/75 batches, loss: 0.0100Epoch 6/10: [==========                    ] 25/75 batches, loss: 0.0100Epoch 6/10: [==========                    ] 26/75 batches, loss: 0.0098Epoch 6/10: [==========                    ] 27/75 batches, loss: 0.0096Epoch 6/10: [===========                   ] 28/75 batches, loss: 0.0099Epoch 6/10: [===========                   ] 29/75 batches, loss: 0.0104Epoch 6/10: [============                  ] 30/75 batches, loss: 0.0103Epoch 6/10: [============                  ] 31/75 batches, loss: 0.0102Epoch 6/10: [============                  ] 32/75 batches, loss: 0.0100Epoch 6/10: [=============                 ] 33/75 batches, loss: 0.0100Epoch 6/10: [=============                 ] 34/75 batches, loss: 0.0099Epoch 6/10: [==============                ] 35/75 batches, loss: 0.0098Epoch 6/10: [==============                ] 36/75 batches, loss: 0.0098Epoch 6/10: [==============                ] 37/75 batches, loss: 0.0097Epoch 6/10: [===============               ] 38/75 batches, loss: 0.0097Epoch 6/10: [===============               ] 39/75 batches, loss: 0.0096Epoch 6/10: [================              ] 40/75 batches, loss: 0.0096Epoch 6/10: [================              ] 41/75 batches, loss: 0.0096Epoch 6/10: [================              ] 42/75 batches, loss: 0.0095Epoch 6/10: [=================             ] 43/75 batches, loss: 0.0094Epoch 6/10: [=================             ] 44/75 batches, loss: 0.0093Epoch 6/10: [==================            ] 45/75 batches, loss: 0.0094Epoch 6/10: [==================            ] 46/75 batches, loss: 0.0094Epoch 6/10: [==================            ] 47/75 batches, loss: 0.0093Epoch 6/10: [===================           ] 48/75 batches, loss: 0.0091Epoch 6/10: [===================           ] 49/75 batches, loss: 0.0091Epoch 6/10: [====================          ] 50/75 batches, loss: 0.0093Epoch 6/10: [====================          ] 51/75 batches, loss: 0.0092Epoch 6/10: [====================          ] 52/75 batches, loss: 0.0092Epoch 6/10: [=====================         ] 53/75 batches, loss: 0.0092Epoch 6/10: [=====================         ] 54/75 batches, loss: 0.0091Epoch 6/10: [======================        ] 55/75 batches, loss: 0.0090Epoch 6/10: [======================        ] 56/75 batches, loss: 0.0090Epoch 6/10: [======================        ] 57/75 batches, loss: 0.0091Epoch 6/10: [=======================       ] 58/75 batches, loss: 0.0091Epoch 6/10: [=======================       ] 59/75 batches, loss: 0.0090Epoch 6/10: [========================      ] 60/75 batches, loss: 0.0089Epoch 6/10: [========================      ] 61/75 batches, loss: 0.0090Epoch 6/10: [========================      ] 62/75 batches, loss: 0.0090Epoch 6/10: [=========================     ] 63/75 batches, loss: 0.0090Epoch 6/10: [=========================     ] 64/75 batches, loss: 0.0089Epoch 6/10: [==========================    ] 65/75 batches, loss: 0.0088Epoch 6/10: [==========================    ] 66/75 batches, loss: 0.0088Epoch 6/10: [==========================    ] 67/75 batches, loss: 0.0088Epoch 6/10: [===========================   ] 68/75 batches, loss: 0.0088Epoch 6/10: [===========================   ] 69/75 batches, loss: 0.0087Epoch 6/10: [============================  ] 70/75 batches, loss: 0.0087Epoch 6/10: [============================  ] 71/75 batches, loss: 0.0087Epoch 6/10: [============================  ] 72/75 batches, loss: 0.0086Epoch 6/10: [============================= ] 73/75 batches, loss: 0.0086Epoch 6/10: [============================= ] 74/75 batches, loss: 0.0087Epoch 6/10: [==============================] 75/75 batches, loss: 0.0087
[2025-05-01 22:33:14,210][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0087
[2025-05-01 22:33:14,618][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0407, Metrics: {'mse': 0.04052799195051193, 'rmse': 0.2013156525223807, 'r2': 0.031617164611816406}
Epoch 7/10: [Epoch 7/10: [                              ] 1/75 batches, loss: 0.0121Epoch 7/10: [                              ] 2/75 batches, loss: 0.0091Epoch 7/10: [=                             ] 3/75 batches, loss: 0.0120Epoch 7/10: [=                             ] 4/75 batches, loss: 0.0104Epoch 7/10: [==                            ] 5/75 batches, loss: 0.0099Epoch 7/10: [==                            ] 6/75 batches, loss: 0.0111Epoch 7/10: [==                            ] 7/75 batches, loss: 0.0106Epoch 7/10: [===                           ] 8/75 batches, loss: 0.0101Epoch 7/10: [===                           ] 9/75 batches, loss: 0.0099Epoch 7/10: [====                          ] 10/75 batches, loss: 0.0098Epoch 7/10: [====                          ] 11/75 batches, loss: 0.0093Epoch 7/10: [====                          ] 12/75 batches, loss: 0.0093Epoch 7/10: [=====                         ] 13/75 batches, loss: 0.0092Epoch 7/10: [=====                         ] 14/75 batches, loss: 0.0091Epoch 7/10: [======                        ] 15/75 batches, loss: 0.0088Epoch 7/10: [======                        ] 16/75 batches, loss: 0.0088Epoch 7/10: [======                        ] 17/75 batches, loss: 0.0085Epoch 7/10: [=======                       ] 18/75 batches, loss: 0.0084Epoch 7/10: [=======                       ] 19/75 batches, loss: 0.0082Epoch 7/10: [========                      ] 20/75 batches, loss: 0.0082Epoch 7/10: [========                      ] 21/75 batches, loss: 0.0083Epoch 7/10: [========                      ] 22/75 batches, loss: 0.0084Epoch 7/10: [=========                     ] 23/75 batches, loss: 0.0082Epoch 7/10: [=========                     ] 24/75 batches, loss: 0.0081Epoch 7/10: [==========                    ] 25/75 batches, loss: 0.0081Epoch 7/10: [==========                    ] 26/75 batches, loss: 0.0080Epoch 7/10: [==========                    ] 27/75 batches, loss: 0.0079Epoch 7/10: [===========                   ] 28/75 batches, loss: 0.0077Epoch 7/10: [===========                   ] 29/75 batches, loss: 0.0076Epoch 7/10: [============                  ] 30/75 batches, loss: 0.0076Epoch 7/10: [============                  ] 31/75 batches, loss: 0.0076Epoch 7/10: [============                  ] 32/75 batches, loss: 0.0076Epoch 7/10: [=============                 ] 33/75 batches, loss: 0.0076Epoch 7/10: [=============                 ] 34/75 batches, loss: 0.0075Epoch 7/10: [==============                ] 35/75 batches, loss: 0.0075Epoch 7/10: [==============                ] 36/75 batches, loss: 0.0075Epoch 7/10: [==============                ] 37/75 batches, loss: 0.0074Epoch 7/10: [===============               ] 38/75 batches, loss: 0.0074Epoch 7/10: [===============               ] 39/75 batches, loss: 0.0076Epoch 7/10: [================              ] 40/75 batches, loss: 0.0076Epoch 7/10: [================              ] 41/75 batches, loss: 0.0075Epoch 7/10: [================              ] 42/75 batches, loss: 0.0075Epoch 7/10: [=================             ] 43/75 batches, loss: 0.0075Epoch 7/10: [=================             ] 44/75 batches, loss: 0.0075Epoch 7/10: [==================            ] 45/75 batches, loss: 0.0075Epoch 7/10: [==================            ] 46/75 batches, loss: 0.0075Epoch 7/10: [==================            ] 47/75 batches, loss: 0.0074Epoch 7/10: [===================           ] 48/75 batches, loss: 0.0073Epoch 7/10: [===================           ] 49/75 batches, loss: 0.0073Epoch 7/10: [====================          ] 50/75 batches, loss: 0.0072Epoch 7/10: [====================          ] 51/75 batches, loss: 0.0072Epoch 7/10: [====================          ] 52/75 batches, loss: 0.0072Epoch 7/10: [=====================         ] 53/75 batches, loss: 0.0071Epoch 7/10: [=====================         ] 54/75 batches, loss: 0.0071Epoch 7/10: [======================        ] 55/75 batches, loss: 0.0071Epoch 7/10: [======================        ] 56/75 batches, loss: 0.0071Epoch 7/10: [======================        ] 57/75 batches, loss: 0.0071Epoch 7/10: [=======================       ] 58/75 batches, loss: 0.0072Epoch 7/10: [=======================       ] 59/75 batches, loss: 0.0072Epoch 7/10: [========================      ] 60/75 batches, loss: 0.0072Epoch 7/10: [========================      ] 61/75 batches, loss: 0.0071Epoch 7/10: [========================      ] 62/75 batches, loss: 0.0071Epoch 7/10: [=========================     ] 63/75 batches, loss: 0.0071Epoch 7/10: [=========================     ] 64/75 batches, loss: 0.0072Epoch 7/10: [==========================    ] 65/75 batches, loss: 0.0072Epoch 7/10: [==========================    ] 66/75 batches, loss: 0.0072Epoch 7/10: [==========================    ] 67/75 batches, loss: 0.0072Epoch 7/10: [===========================   ] 68/75 batches, loss: 0.0072Epoch 7/10: [===========================   ] 69/75 batches, loss: 0.0072Epoch 7/10: [============================  ] 70/75 batches, loss: 0.0072Epoch 7/10: [============================  ] 71/75 batches, loss: 0.0072Epoch 7/10: [============================  ] 72/75 batches, loss: 0.0073Epoch 7/10: [============================= ] 73/75 batches, loss: 0.0074Epoch 7/10: [============================= ] 74/75 batches, loss: 0.0074Epoch 7/10: [==============================] 75/75 batches, loss: 0.0074
[2025-05-01 22:33:18,725][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0074
[2025-05-01 22:33:19,090][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0418, Metrics: {'mse': 0.042119987308979034, 'rmse': 0.20523154559905998, 'r2': -0.006422162055969238}
[2025-05-01 22:33:19,090][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/10: [Epoch 8/10: [                              ] 1/75 batches, loss: 0.0054Epoch 8/10: [                              ] 2/75 batches, loss: 0.0079Epoch 8/10: [=                             ] 3/75 batches, loss: 0.0065Epoch 8/10: [=                             ] 4/75 batches, loss: 0.0059Epoch 8/10: [==                            ] 5/75 batches, loss: 0.0056Epoch 8/10: [==                            ] 6/75 batches, loss: 0.0056Epoch 8/10: [==                            ] 7/75 batches, loss: 0.0058Epoch 8/10: [===                           ] 8/75 batches, loss: 0.0065Epoch 8/10: [===                           ] 9/75 batches, loss: 0.0065Epoch 8/10: [====                          ] 10/75 batches, loss: 0.0065Epoch 8/10: [====                          ] 11/75 batches, loss: 0.0062Epoch 8/10: [====                          ] 12/75 batches, loss: 0.0065Epoch 8/10: [=====                         ] 13/75 batches, loss: 0.0062Epoch 8/10: [=====                         ] 14/75 batches, loss: 0.0062Epoch 8/10: [======                        ] 15/75 batches, loss: 0.0064Epoch 8/10: [======                        ] 16/75 batches, loss: 0.0062Epoch 8/10: [======                        ] 17/75 batches, loss: 0.0063Epoch 8/10: [=======                       ] 18/75 batches, loss: 0.0063Epoch 8/10: [=======                       ] 19/75 batches, loss: 0.0063Epoch 8/10: [========                      ] 20/75 batches, loss: 0.0063Epoch 8/10: [========                      ] 21/75 batches, loss: 0.0063Epoch 8/10: [========                      ] 22/75 batches, loss: 0.0065Epoch 8/10: [=========                     ] 23/75 batches, loss: 0.0066Epoch 8/10: [=========                     ] 24/75 batches, loss: 0.0067Epoch 8/10: [==========                    ] 25/75 batches, loss: 0.0068Epoch 8/10: [==========                    ] 26/75 batches, loss: 0.0067Epoch 8/10: [==========                    ] 27/75 batches, loss: 0.0066Epoch 8/10: [===========                   ] 28/75 batches, loss: 0.0065Epoch 8/10: [===========                   ] 29/75 batches, loss: 0.0064Epoch 8/10: [============                  ] 30/75 batches, loss: 0.0065Epoch 8/10: [============                  ] 31/75 batches, loss: 0.0065Epoch 8/10: [============                  ] 32/75 batches, loss: 0.0064Epoch 8/10: [=============                 ] 33/75 batches, loss: 0.0065Epoch 8/10: [=============                 ] 34/75 batches, loss: 0.0066Epoch 8/10: [==============                ] 35/75 batches, loss: 0.0068Epoch 8/10: [==============                ] 36/75 batches, loss: 0.0068Epoch 8/10: [==============                ] 37/75 batches, loss: 0.0067Epoch 8/10: [===============               ] 38/75 batches, loss: 0.0067Epoch 8/10: [===============               ] 39/75 batches, loss: 0.0067Epoch 8/10: [================              ] 40/75 batches, loss: 0.0066Epoch 8/10: [================              ] 41/75 batches, loss: 0.0066Epoch 8/10: [================              ] 42/75 batches, loss: 0.0067Epoch 8/10: [=================             ] 43/75 batches, loss: 0.0067Epoch 8/10: [=================             ] 44/75 batches, loss: 0.0068Epoch 8/10: [==================            ] 45/75 batches, loss: 0.0069Epoch 8/10: [==================            ] 46/75 batches, loss: 0.0068Epoch 8/10: [==================            ] 47/75 batches, loss: 0.0069Epoch 8/10: [===================           ] 48/75 batches, loss: 0.0068Epoch 8/10: [===================           ] 49/75 batches, loss: 0.0068Epoch 8/10: [====================          ] 50/75 batches, loss: 0.0067Epoch 8/10: [====================          ] 51/75 batches, loss: 0.0067Epoch 8/10: [====================          ] 52/75 batches, loss: 0.0068Epoch 8/10: [=====================         ] 53/75 batches, loss: 0.0067Epoch 8/10: [=====================         ] 54/75 batches, loss: 0.0066Epoch 8/10: [======================        ] 55/75 batches, loss: 0.0066Epoch 8/10: [======================        ] 56/75 batches, loss: 0.0066Epoch 8/10: [======================        ] 57/75 batches, loss: 0.0065Epoch 8/10: [=======================       ] 58/75 batches, loss: 0.0064Epoch 8/10: [=======================       ] 59/75 batches, loss: 0.0064Epoch 8/10: [========================      ] 60/75 batches, loss: 0.0064Epoch 8/10: [========================      ] 61/75 batches, loss: 0.0064Epoch 8/10: [========================      ] 62/75 batches, loss: 0.0065Epoch 8/10: [=========================     ] 63/75 batches, loss: 0.0064Epoch 8/10: [=========================     ] 64/75 batches, loss: 0.0065Epoch 8/10: [==========================    ] 65/75 batches, loss: 0.0064Epoch 8/10: [==========================    ] 66/75 batches, loss: 0.0064Epoch 8/10: [==========================    ] 67/75 batches, loss: 0.0064Epoch 8/10: [===========================   ] 68/75 batches, loss: 0.0064Epoch 8/10: [===========================   ] 69/75 batches, loss: 0.0064Epoch 8/10: [============================  ] 70/75 batches, loss: 0.0064Epoch 8/10: [============================  ] 71/75 batches, loss: 0.0063Epoch 8/10: [============================  ] 72/75 batches, loss: 0.0063Epoch 8/10: [============================= ] 73/75 batches, loss: 0.0064Epoch 8/10: [============================= ] 74/75 batches, loss: 0.0065Epoch 8/10: [==============================] 75/75 batches, loss: 0.0065
[2025-05-01 22:33:22,965][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0065
[2025-05-01 22:33:23,384][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0377, Metrics: {'mse': 0.03820870816707611, 'rmse': 0.19547047901684825, 'r2': 0.08703458309173584}
Epoch 9/10: [Epoch 9/10: [                              ] 1/75 batches, loss: 0.0064Epoch 9/10: [                              ] 2/75 batches, loss: 0.0064Epoch 9/10: [=                             ] 3/75 batches, loss: 0.0079Epoch 9/10: [=                             ] 4/75 batches, loss: 0.0081Epoch 9/10: [==                            ] 5/75 batches, loss: 0.0074Epoch 9/10: [==                            ] 6/75 batches, loss: 0.0070Epoch 9/10: [==                            ] 7/75 batches, loss: 0.0069Epoch 9/10: [===                           ] 8/75 batches, loss: 0.0068Epoch 9/10: [===                           ] 9/75 batches, loss: 0.0067Epoch 9/10: [====                          ] 10/75 batches, loss: 0.0066Epoch 9/10: [====                          ] 11/75 batches, loss: 0.0067Epoch 9/10: [====                          ] 12/75 batches, loss: 0.0064Epoch 9/10: [=====                         ] 13/75 batches, loss: 0.0062Epoch 9/10: [=====                         ] 14/75 batches, loss: 0.0065Epoch 9/10: [======                        ] 15/75 batches, loss: 0.0063Epoch 9/10: [======                        ] 16/75 batches, loss: 0.0062Epoch 9/10: [======                        ] 17/75 batches, loss: 0.0060Epoch 9/10: [=======                       ] 18/75 batches, loss: 0.0059Epoch 9/10: [=======                       ] 19/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 20/75 batches, loss: 0.0058Epoch 9/10: [========                      ] 21/75 batches, loss: 0.0057Epoch 9/10: [========                      ] 22/75 batches, loss: 0.0057Epoch 9/10: [=========                     ] 23/75 batches, loss: 0.0058Epoch 9/10: [=========                     ] 24/75 batches, loss: 0.0057Epoch 9/10: [==========                    ] 25/75 batches, loss: 0.0056Epoch 9/10: [==========                    ] 26/75 batches, loss: 0.0056Epoch 9/10: [==========                    ] 27/75 batches, loss: 0.0056Epoch 9/10: [===========                   ] 28/75 batches, loss: 0.0056Epoch 9/10: [===========                   ] 29/75 batches, loss: 0.0055Epoch 9/10: [============                  ] 30/75 batches, loss: 0.0054Epoch 9/10: [============                  ] 31/75 batches, loss: 0.0055Epoch 9/10: [============                  ] 32/75 batches, loss: 0.0054Epoch 9/10: [=============                 ] 33/75 batches, loss: 0.0054Epoch 9/10: [=============                 ] 34/75 batches, loss: 0.0054Epoch 9/10: [==============                ] 35/75 batches, loss: 0.0054Epoch 9/10: [==============                ] 36/75 batches, loss: 0.0054Epoch 9/10: [==============                ] 37/75 batches, loss: 0.0053Epoch 9/10: [===============               ] 38/75 batches, loss: 0.0052Epoch 9/10: [===============               ] 39/75 batches, loss: 0.0052Epoch 9/10: [================              ] 40/75 batches, loss: 0.0052Epoch 9/10: [================              ] 41/75 batches, loss: 0.0053Epoch 9/10: [================              ] 42/75 batches, loss: 0.0052Epoch 9/10: [=================             ] 43/75 batches, loss: 0.0052Epoch 9/10: [=================             ] 44/75 batches, loss: 0.0053Epoch 9/10: [==================            ] 45/75 batches, loss: 0.0052Epoch 9/10: [==================            ] 46/75 batches, loss: 0.0053Epoch 9/10: [==================            ] 47/75 batches, loss: 0.0053Epoch 9/10: [===================           ] 48/75 batches, loss: 0.0053Epoch 9/10: [===================           ] 49/75 batches, loss: 0.0052Epoch 9/10: [====================          ] 50/75 batches, loss: 0.0052Epoch 9/10: [====================          ] 51/75 batches, loss: 0.0053Epoch 9/10: [====================          ] 52/75 batches, loss: 0.0053Epoch 9/10: [=====================         ] 53/75 batches, loss: 0.0052Epoch 9/10: [=====================         ] 54/75 batches, loss: 0.0052Epoch 9/10: [======================        ] 55/75 batches, loss: 0.0053Epoch 9/10: [======================        ] 56/75 batches, loss: 0.0053Epoch 9/10: [======================        ] 57/75 batches, loss: 0.0052Epoch 9/10: [=======================       ] 58/75 batches, loss: 0.0052Epoch 9/10: [=======================       ] 59/75 batches, loss: 0.0051Epoch 9/10: [========================      ] 60/75 batches, loss: 0.0051Epoch 9/10: [========================      ] 61/75 batches, loss: 0.0051Epoch 9/10: [========================      ] 62/75 batches, loss: 0.0050Epoch 9/10: [=========================     ] 63/75 batches, loss: 0.0050Epoch 9/10: [=========================     ] 64/75 batches, loss: 0.0050Epoch 9/10: [==========================    ] 65/75 batches, loss: 0.0050Epoch 9/10: [==========================    ] 66/75 batches, loss: 0.0049Epoch 9/10: [==========================    ] 67/75 batches, loss: 0.0049Epoch 9/10: [===========================   ] 68/75 batches, loss: 0.0050Epoch 9/10: [===========================   ] 69/75 batches, loss: 0.0050Epoch 9/10: [============================  ] 70/75 batches, loss: 0.0049Epoch 9/10: [============================  ] 71/75 batches, loss: 0.0049Epoch 9/10: [============================  ] 72/75 batches, loss: 0.0049Epoch 9/10: [============================= ] 73/75 batches, loss: 0.0050Epoch 9/10: [============================= ] 74/75 batches, loss: 0.0050Epoch 9/10: [==============================] 75/75 batches, loss: 0.0051
[2025-05-01 22:33:27,421][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0051
[2025-05-01 22:33:27,844][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0255, Metrics: {'mse': 0.026619765907526016, 'rmse': 0.16315564932764667, 'r2': 0.3639427423477173}
Epoch 10/10: [Epoch 10/10: [                              ] 1/75 batches, loss: 0.0038Epoch 10/10: [                              ] 2/75 batches, loss: 0.0032Epoch 10/10: [=                             ] 3/75 batches, loss: 0.0043Epoch 10/10: [=                             ] 4/75 batches, loss: 0.0040Epoch 10/10: [==                            ] 5/75 batches, loss: 0.0037Epoch 10/10: [==                            ] 6/75 batches, loss: 0.0038Epoch 10/10: [==                            ] 7/75 batches, loss: 0.0052Epoch 10/10: [===                           ] 8/75 batches, loss: 0.0049Epoch 10/10: [===                           ] 9/75 batches, loss: 0.0045Epoch 10/10: [====                          ] 10/75 batches, loss: 0.0048Epoch 10/10: [====                          ] 11/75 batches, loss: 0.0047Epoch 10/10: [====                          ] 12/75 batches, loss: 0.0049Epoch 10/10: [=====                         ] 13/75 batches, loss: 0.0049Epoch 10/10: [=====                         ] 14/75 batches, loss: 0.0049Epoch 10/10: [======                        ] 15/75 batches, loss: 0.0048Epoch 10/10: [======                        ] 16/75 batches, loss: 0.0050Epoch 10/10: [======                        ] 17/75 batches, loss: 0.0049Epoch 10/10: [=======                       ] 18/75 batches, loss: 0.0048Epoch 10/10: [=======                       ] 19/75 batches, loss: 0.0047Epoch 10/10: [========                      ] 20/75 batches, loss: 0.0046Epoch 10/10: [========                      ] 21/75 batches, loss: 0.0046Epoch 10/10: [========                      ] 22/75 batches, loss: 0.0047Epoch 10/10: [=========                     ] 23/75 batches, loss: 0.0050Epoch 10/10: [=========                     ] 24/75 batches, loss: 0.0049Epoch 10/10: [==========                    ] 25/75 batches, loss: 0.0049Epoch 10/10: [==========                    ] 26/75 batches, loss: 0.0050Epoch 10/10: [==========                    ] 27/75 batches, loss: 0.0050Epoch 10/10: [===========                   ] 28/75 batches, loss: 0.0049Epoch 10/10: [===========                   ] 29/75 batches, loss: 0.0049Epoch 10/10: [============                  ] 30/75 batches, loss: 0.0049Epoch 10/10: [============                  ] 31/75 batches, loss: 0.0050Epoch 10/10: [============                  ] 32/75 batches, loss: 0.0049Epoch 10/10: [=============                 ] 33/75 batches, loss: 0.0049Epoch 10/10: [=============                 ] 34/75 batches, loss: 0.0049Epoch 10/10: [==============                ] 35/75 batches, loss: 0.0051Epoch 10/10: [==============                ] 36/75 batches, loss: 0.0051Epoch 10/10: [==============                ] 37/75 batches, loss: 0.0051Epoch 10/10: [===============               ] 38/75 batches, loss: 0.0051Epoch 10/10: [===============               ] 39/75 batches, loss: 0.0051Epoch 10/10: [================              ] 40/75 batches, loss: 0.0052Epoch 10/10: [================              ] 41/75 batches, loss: 0.0052Epoch 10/10: [================              ] 42/75 batches, loss: 0.0053Epoch 10/10: [=================             ] 43/75 batches, loss: 0.0052Epoch 10/10: [=================             ] 44/75 batches, loss: 0.0052Epoch 10/10: [==================            ] 45/75 batches, loss: 0.0052Epoch 10/10: [==================            ] 46/75 batches, loss: 0.0051Epoch 10/10: [==================            ] 47/75 batches, loss: 0.0051Epoch 10/10: [===================           ] 48/75 batches, loss: 0.0052Epoch 10/10: [===================           ] 49/75 batches, loss: 0.0052Epoch 10/10: [====================          ] 50/75 batches, loss: 0.0052Epoch 10/10: [====================          ] 51/75 batches, loss: 0.0051Epoch 10/10: [====================          ] 52/75 batches, loss: 0.0052Epoch 10/10: [=====================         ] 53/75 batches, loss: 0.0051Epoch 10/10: [=====================         ] 54/75 batches, loss: 0.0051Epoch 10/10: [======================        ] 55/75 batches, loss: 0.0051Epoch 10/10: [======================        ] 56/75 batches, loss: 0.0051Epoch 10/10: [======================        ] 57/75 batches, loss: 0.0051Epoch 10/10: [=======================       ] 58/75 batches, loss: 0.0050Epoch 10/10: [=======================       ] 59/75 batches, loss: 0.0050Epoch 10/10: [========================      ] 60/75 batches, loss: 0.0050Epoch 10/10: [========================      ] 61/75 batches, loss: 0.0050Epoch 10/10: [========================      ] 62/75 batches, loss: 0.0050Epoch 10/10: [=========================     ] 63/75 batches, loss: 0.0050Epoch 10/10: [=========================     ] 64/75 batches, loss: 0.0050Epoch 10/10: [==========================    ] 65/75 batches, loss: 0.0050Epoch 10/10: [==========================    ] 66/75 batches, loss: 0.0049Epoch 10/10: [==========================    ] 67/75 batches, loss: 0.0049Epoch 10/10: [===========================   ] 68/75 batches, loss: 0.0049Epoch 10/10: [===========================   ] 69/75 batches, loss: 0.0048Epoch 10/10: [============================  ] 70/75 batches, loss: 0.0048Epoch 10/10: [============================  ] 71/75 batches, loss: 0.0049Epoch 10/10: [============================  ] 72/75 batches, loss: 0.0049Epoch 10/10: [============================= ] 73/75 batches, loss: 0.0048Epoch 10/10: [============================= ] 74/75 batches, loss: 0.0049Epoch 10/10: [==============================] 75/75 batches, loss: 0.0049
[2025-05-01 22:33:31,905][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0049
[2025-05-01 22:33:32,275][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0284, Metrics: {'mse': 0.029286589473485947, 'rmse': 0.17113325063670692, 'r2': 0.30022120475769043}
[2025-05-01 22:33:32,276][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-01 22:33:32,276][src.training.lm_trainer][INFO] - Training completed in 44.68 seconds
[2025-05-01 22:33:32,276][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 22:33:34,341][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0029778608586639166, 'rmse': 0.05456977971976721, 'r2': 0.8890045285224915}
[2025-05-01 22:33:34,341][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.026619765907526016, 'rmse': 0.16315564932764667, 'r2': 0.3639427423477173}
[2025-05-01 22:33:34,341][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.013519830070436, 'rmse': 0.11627480410835359, 'r2': 0.649187445640564}
[2025-05-01 22:33:35,559][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/complexity/en/model.pt
[2025-05-01 22:33:35,567][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▄▁
wandb:     best_val_mse █▅▅▄▁
wandb:      best_val_r2 ▁▄▄▅█
wandb:    best_val_rmse █▅▅▄▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▃▂▄▄▅▇
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▅▆▇▄▅▄▁▂
wandb:          val_mse ▇█▅▆▇▄▅▄▁▂
wandb:           val_r2 ▂▁▄▃▂▅▄▅█▇
wandb:         val_rmse ▇█▅▆▇▅▅▄▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02552
wandb:     best_val_mse 0.02662
wandb:      best_val_r2 0.36394
wandb:    best_val_rmse 0.16316
wandb:            epoch 10
wandb:   final_test_mse 0.01352
wandb:    final_test_r2 0.64919
wandb:  final_test_rmse 0.11627
wandb:  final_train_mse 0.00298
wandb:   final_train_r2 0.889
wandb: final_train_rmse 0.05457
wandb:    final_val_mse 0.02662
wandb:     final_val_r2 0.36394
wandb:   final_val_rmse 0.16316
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00487
wandb:       train_time 44.68401
wandb:         val_loss 0.02836
wandb:          val_mse 0.02929
wandb:           val_r2 0.30022
wandb:         val_rmse 0.17113
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_223239-875qosql
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_223239-875qosql/logs
Experiment finetune_complexity_en completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/finetune_output/complexity/en/results.json
Running experiment: finetune_question_type_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=false"         "model.finetune=true"                     "model.head_hidden_size=768"             "model.head_layers=2"             "model.dropout=0.2"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"                     "training.lr=2e-5"             "training.patience=3"             "training.scheduler_factor=0.5"             "training.scheduler_patience=2"             "+training.gradient_accumulation_steps=4"                  "experiment_name=finetune_question_type_fi"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type"         "wandb.mode=offline"
slurmstepd: error: *** JOB 64436568 ON s16g09 CANCELLED AT 2025-05-01T22:33:40 ***
