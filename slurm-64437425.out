SLURM_JOB_ID: 64437425
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Fri May  2 11:30:16 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 1
=======================
Running experiment: probe_layer1_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:30:30,039][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar
experiment_name: probe_layer1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:30:30,039][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:30:30,039][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:30:30,039][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:30:30,043][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 11:30:30,043][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:30:31,697][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:30:33,996][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:30:33,997][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:30:34,096][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,136][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,371][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:30:34,380][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:30:34,381][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:30:34,382][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:30:34,402][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,443][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,467][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:30:34,468][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:30:34,468][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:30:34,469][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:30:34,489][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,521][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:30:34,535][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:30:34,537][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:30:34,538][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:30:34,539][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:30:34,540][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 11:30:34,540][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:30:34,540][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:30:34,541][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 11:30:34,541][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:30:34,541][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:30:34,542][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 11:30:34,542][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:30:34,542][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:30:34,543][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:30:34,543][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:30:34,543][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:30:39,089][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:30:39,090][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:30:39,090][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 11:30:39,090][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:30:39,096][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:30:39,097][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:30:39,097][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:30:39,097][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:30:39,097][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:30:39,098][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:30:39,098][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7303Epoch 1/15: [                              ] 2/63 batches, loss: 0.7000Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7045Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7060Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7067Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7078Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7074Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7037Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7015Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6992Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.6991Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.6987Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6982Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6969Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6967Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6960Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6955Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6946Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6944Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6946Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6944Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6943Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6943Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6934Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6924Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6918Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6915Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6904Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6898Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6892Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6900Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6876Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6878Epoch 1/15: [================              ] 34/63 batches, loss: 0.6868Epoch 1/15: [================              ] 35/63 batches, loss: 0.6856Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6836Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6825Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6817Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6783Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6749Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6775Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6785Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6782Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6741Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6746Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6748Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6734Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6724Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6708Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6697Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6696Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6676Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6672Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6660Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6642Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6626Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6620Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6612Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6612Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6590Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6578Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6574Epoch 1/15: [==============================] 63/63 batches, loss: 0.6579
[2025-05-02 11:30:43,759][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6579
[2025-05-02 11:30:43,965][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6387, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8636363636363636, 'precision': 0.7916666666666666, 'recall': 0.95}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.5882Epoch 2/15: [                              ] 2/63 batches, loss: 0.6116Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6305Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6264Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6165Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6102Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6158Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6112Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6170Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6131Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6186Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6181Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6132Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6102Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6080Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6124Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6119Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6119Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6114Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6100Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6103Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6083Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6040Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6043Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6064Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6041Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6030Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6011Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.5997Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6004Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6036Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6032Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6028Epoch 2/15: [================              ] 34/63 batches, loss: 0.6017Epoch 2/15: [================              ] 35/63 batches, loss: 0.5995Epoch 2/15: [=================             ] 36/63 batches, loss: 0.5998Epoch 2/15: [=================             ] 37/63 batches, loss: 0.5979Epoch 2/15: [==================            ] 38/63 batches, loss: 0.5956Epoch 2/15: [==================            ] 39/63 batches, loss: 0.5945Epoch 2/15: [===================           ] 40/63 batches, loss: 0.5950Epoch 2/15: [===================           ] 41/63 batches, loss: 0.5942Epoch 2/15: [====================          ] 42/63 batches, loss: 0.5927Epoch 2/15: [====================          ] 43/63 batches, loss: 0.5927Epoch 2/15: [====================          ] 44/63 batches, loss: 0.5924Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.5908Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.5895Epoch 2/15: [======================        ] 47/63 batches, loss: 0.5874Epoch 2/15: [======================        ] 48/63 batches, loss: 0.5874Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.5877Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.5858Epoch 2/15: [========================      ] 51/63 batches, loss: 0.5857Epoch 2/15: [========================      ] 52/63 batches, loss: 0.5834Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.5827Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.5825Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.5817Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.5804Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.5803Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.5783Epoch 2/15: [============================  ] 59/63 batches, loss: 0.5777Epoch 2/15: [============================  ] 60/63 batches, loss: 0.5778Epoch 2/15: [============================= ] 61/63 batches, loss: 0.5778Epoch 2/15: [============================= ] 62/63 batches, loss: 0.5763Epoch 2/15: [==============================] 63/63 batches, loss: 0.5765
[2025-05-02 11:30:46,304][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.5765
[2025-05-02 11:30:46,517][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.5882, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5166Epoch 3/15: [                              ] 2/63 batches, loss: 0.5433Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5277Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5316Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5411Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5462Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5414Epoch 3/15: [===                           ] 8/63 batches, loss: 0.5578Epoch 3/15: [====                          ] 9/63 batches, loss: 0.5486Epoch 3/15: [====                          ] 10/63 batches, loss: 0.5498Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5470Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.5520Epoch 3/15: [======                        ] 13/63 batches, loss: 0.5546Epoch 3/15: [======                        ] 14/63 batches, loss: 0.5522Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.5497Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.5495Epoch 3/15: [========                      ] 17/63 batches, loss: 0.5490Epoch 3/15: [========                      ] 18/63 batches, loss: 0.5466Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.5485Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.5471Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5470Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.5485Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5478Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5445Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5402Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5429Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5454Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5447Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5472Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5485Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5484Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5489Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5507Epoch 3/15: [================              ] 34/63 batches, loss: 0.5497Epoch 3/15: [================              ] 35/63 batches, loss: 0.5472Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5490Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5491Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5482Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5480Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5460Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5479Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5470Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5469Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5468Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5451Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5456Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5447Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5446Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5436Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5437Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5446Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5432Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5426Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5412Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5407Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5402Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5395Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5400Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5399Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5409Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5405Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5404Epoch 3/15: [==============================] 63/63 batches, loss: 0.5409
[2025-05-02 11:30:48,877][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5409
[2025-05-02 11:30:49,106][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5704, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5556Epoch 4/15: [                              ] 2/63 batches, loss: 0.5405Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5184Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5004Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5144Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5126Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5157Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5185Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5024Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5095Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5179Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5196Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5213Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5214Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5292Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5307Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5337Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5323Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5299Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5289Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5258Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5266Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5272Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5275Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5283Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5317Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5294Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5319Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5336Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5323Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5307Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5314Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5287Epoch 4/15: [================              ] 34/63 batches, loss: 0.5289Epoch 4/15: [================              ] 35/63 batches, loss: 0.5292Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5281Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5287Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5280Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5283Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5273Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5286Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5285Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5269Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5271Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5267Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5247Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5239Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5241Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5239Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5238Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5246Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5257Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5265Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5265Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5263Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5259Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5281Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5269Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5277Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5274Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5272Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5276Epoch 4/15: [==============================] 63/63 batches, loss: 0.5247
[2025-05-02 11:30:51,387][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5247
[2025-05-02 11:30:51,610][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5678, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5857Epoch 5/15: [                              ] 2/63 batches, loss: 0.5363Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5296Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5368Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5655Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5574Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5551Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5404Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5374Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5305Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5242Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5237Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5223Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5184Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5132Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5159Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5173Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5154Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5195Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5239Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5214Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5198Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5177Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5197Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5180Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5200Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5186Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5158Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5181Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5191Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5170Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5156Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5163Epoch 5/15: [================              ] 34/63 batches, loss: 0.5181Epoch 5/15: [================              ] 35/63 batches, loss: 0.5198Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5194Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5194Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5173Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5166Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5181Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5178Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5193Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5186Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5184Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5173Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5174Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5176Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5185Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5182Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5186Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5184Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5187Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5197Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5199Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5201Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5191Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5198Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5208Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5210Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5207Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5205Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5207Epoch 5/15: [==============================] 63/63 batches, loss: 0.5197
[2025-05-02 11:30:53,909][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5197
[2025-05-02 11:30:54,125][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5658, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5012Epoch 6/15: [                              ] 2/63 batches, loss: 0.5176Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5072Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5008Epoch 6/15: [==                            ] 5/63 batches, loss: 0.4941Epoch 6/15: [==                            ] 6/63 batches, loss: 0.4928Epoch 6/15: [===                           ] 7/63 batches, loss: 0.4783Epoch 6/15: [===                           ] 8/63 batches, loss: 0.4885Epoch 6/15: [====                          ] 9/63 batches, loss: 0.4941Epoch 6/15: [====                          ] 10/63 batches, loss: 0.4986Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5037Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5083Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5099Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5102Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5126Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5190Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5202Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5208Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5200Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5210Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5193Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5221Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5204Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5217Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5265Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5262Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5255Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5254Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5247Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5220Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5249Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5239Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5226Epoch 6/15: [================              ] 34/63 batches, loss: 0.5186Epoch 6/15: [================              ] 35/63 batches, loss: 0.5160Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5165Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5176Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5148Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5166Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5180Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5177Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5169Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5183Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5166Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5171Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5169Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5161Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5156Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5171Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5184Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5172Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5155Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5148Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5159Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5166Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5176Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5171Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5183Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5186Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5174Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5176Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5186Epoch 6/15: [==============================] 63/63 batches, loss: 0.5176
[2025-05-02 11:30:56,419][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5176
[2025-05-02 11:30:56,645][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5736, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 11:30:56,645][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.4880Epoch 7/15: [                              ] 2/63 batches, loss: 0.4749Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5386Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5262Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5268Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5192Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5212Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5164Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5169Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5086Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.4980Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5005Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5028Epoch 7/15: [======                        ] 14/63 batches, loss: 0.4962Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5043Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5067Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5080Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5068Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5075Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5062Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5062Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5090Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5078Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5070Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5069Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5075Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5084Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5108Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5145Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5185Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5173Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5154Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5138Epoch 7/15: [================              ] 34/63 batches, loss: 0.5152Epoch 7/15: [================              ] 35/63 batches, loss: 0.5171Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5169Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5148Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5167Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5164Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5187Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5192Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5177Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5185Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5173Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5161Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5154Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5167Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5150Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5153Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5139Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5128Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5155Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5158Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5161Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5163Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5157Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5147Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5145Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5127Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5125Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5138Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5145Epoch 7/15: [==============================] 63/63 batches, loss: 0.5144
[2025-05-02 11:30:58,569][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5144
[2025-05-02 11:30:58,778][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5880, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:30:58,778][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5743Epoch 8/15: [                              ] 2/63 batches, loss: 0.5521Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5189Epoch 8/15: [=                             ] 4/63 batches, loss: 0.4922Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5136Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5383Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5338Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5347Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5359Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5357Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5330Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5341Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5323Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5308Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5325Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5339Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5365Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5401Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5419Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5358Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5367Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5331Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5329Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5338Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5330Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5310Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5291Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5275Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5273Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5241Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5242Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5243Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5216Epoch 8/15: [================              ] 34/63 batches, loss: 0.5211Epoch 8/15: [================              ] 35/63 batches, loss: 0.5200Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5183Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5189Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5175Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5185Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5160Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5181Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5184Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5181Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5189Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5201Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5203Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5205Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5192Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5184Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5186Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5191Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5199Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5196Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5194Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5174Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5168Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5158Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5164Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5156Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5153Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5156Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5147Epoch 8/15: [==============================] 63/63 batches, loss: 0.5155
[2025-05-02 11:31:00,694][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5155
[2025-05-02 11:31:00,909][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5687, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 11:31:00,910][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:31:00,910][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 11:31:00,910][src.training.lm_trainer][INFO] - Training completed in 19.88 seconds
[2025-05-02 11:31:00,910][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:31:03,382][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9979899497487437, 'f1': 0.9979838709677419, 'precision': 1.0, 'recall': 0.9959758551307847}
[2025-05-02 11:31:03,383][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 11:31:03,383][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.6666666666666666, 'precision': 0.5, 'recall': 1.0}
[2025-05-02 11:31:05,026][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/ar/model.pt
[2025-05-02 11:31:05,027][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▁▅███
wandb:       best_val_recall ▁████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▂▃▂▂
wandb:            train_loss █▄▂▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆████▆█
wandb:                val_f1 ▁▆████▆█
wandb:              val_loss █▃▁▁▁▂▃▁
wandb:         val_precision ▁▅████▅█
wandb:            val_recall ▁███████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.5658
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0.66667
wandb:  final_test_precision 0.5
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99799
wandb:        final_train_f1 0.99798
wandb: final_train_precision 1
wandb:    final_train_recall 0.99598
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.5155
wandb:            train_time 19.88462
wandb:          val_accuracy 0.93182
wandb:                val_f1 0.93023
wandb:              val_loss 0.56874
wandb:         val_precision 0.86957
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113030-cgc1cjtl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113030-cgc1cjtl/logs
Experiment probe_layer1_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/results.json
Running experiment: probe_layer1_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:31:16,259][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar
experiment_name: probe_layer1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:31:16,259][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:31:16,259][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:31:16,260][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:31:16,264][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 11:31:16,264][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:31:17,749][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:31:20,016][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:31:20,017][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:31:20,085][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,113][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,206][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:31:20,214][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:31:20,214][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:31:20,215][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:31:20,237][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,268][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,280][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:31:20,281][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:31:20,281][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:31:20,282][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:31:20,297][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,321][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:31:20,332][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:31:20,334][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:31:20,334][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:31:20,335][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:31:20,335][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:31:20,335][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:31:20,336][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:31:20,336][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:31:20,336][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:31:20,336][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:31:20,336][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:31:20,337][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:31:20,337][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:31:20,337][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:31:20,338][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:31:20,338][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:31:20,338][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:31:20,339][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:31:20,339][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:31:20,339][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:31:24,193][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:31:24,193][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:31:24,194][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 11:31:24,194][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:31:24,198][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:31:24,198][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:31:24,198][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:31:24,199][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:31:24,199][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:31:24,200][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:31:24,200][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.3937Epoch 1/15: [                              ] 2/63 batches, loss: 0.3394Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2897Epoch 1/15: [=                             ] 4/63 batches, loss: 0.3218Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3499Epoch 1/15: [==                            ] 6/63 batches, loss: 0.3244Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3247Epoch 1/15: [===                           ] 8/63 batches, loss: 0.3055Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2949Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2886Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2927Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2903Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2891Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2856Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2856Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2839Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2802Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2795Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2770Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2727Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2704Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2680Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2727Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2707Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2659Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2634Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2589Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2585Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2593Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2672Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2660Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2670Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2688Epoch 1/15: [================              ] 34/63 batches, loss: 0.2729Epoch 1/15: [================              ] 35/63 batches, loss: 0.2709Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2741Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2752Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2769Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2775Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2746Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2723Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2711Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2678Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2647Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2628Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2655Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2645Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2670Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2659Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2677Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2655Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2653Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2648Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2620Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2617Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2645Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2628Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2615Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2620Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2610Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2592Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2575Epoch 1/15: [==============================] 63/63 batches, loss: 0.2572
[2025-05-02 11:31:28,486][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2572
[2025-05-02 11:31:28,682][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1720, Metrics: {'mse': 0.1731313169002533, 'rmse': 0.41609051527312335, 'r2': -1.6685292720794678}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2201Epoch 2/15: [                              ] 2/63 batches, loss: 0.2542Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2481Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2320Epoch 2/15: [==                            ] 5/63 batches, loss: 0.2165Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2055Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1999Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1922Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2026Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2049Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1999Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1963Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1981Epoch 2/15: [======                        ] 14/63 batches, loss: 0.2022Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2083Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.2078Epoch 2/15: [========                      ] 17/63 batches, loss: 0.2017Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1990Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.2027Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.2090Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.2059Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2089Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.2041Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.2000Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1942Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1916Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1895Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1885Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1863Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1854Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1856Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1839Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1848Epoch 2/15: [================              ] 34/63 batches, loss: 0.1824Epoch 2/15: [================              ] 35/63 batches, loss: 0.1835Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1808Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1806Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1797Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1776Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1769Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1762Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1768Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1763Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1771Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1764Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1776Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1761Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1757Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1744Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1733Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1718Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1732Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1727Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1722Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1715Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1715Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1711Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1701Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1681Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1686Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1679Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1670Epoch 2/15: [==============================] 63/63 batches, loss: 0.1719
[2025-05-02 11:31:31,026][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1719
[2025-05-02 11:31:31,238][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1135, Metrics: {'mse': 0.11422435939311981, 'rmse': 0.33797094459896965, 'r2': -0.7605772018432617}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1501Epoch 3/15: [                              ] 2/63 batches, loss: 0.1221Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1043Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1195Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1296Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1228Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1187Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1223Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1353Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1356Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1372Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1337Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1389Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1408Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1420Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1407Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1416Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1428Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1435Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1413Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1419Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1420Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1451Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1424Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1450Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1444Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1446Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1413Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1398Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1390Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1427Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1403Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1404Epoch 3/15: [================              ] 34/63 batches, loss: 0.1392Epoch 3/15: [================              ] 35/63 batches, loss: 0.1433Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1411Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1390Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1396Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1406Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1400Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1395Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1393Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1399Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1392Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1387Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1379Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1373Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1370Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1365Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1371Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1379Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1410Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1423Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1419Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1410Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1412Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1403Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1423Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1409Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1421Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1414Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1403Epoch 3/15: [==============================] 63/63 batches, loss: 0.1382
[2025-05-02 11:31:33,569][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1382
[2025-05-02 11:31:33,779][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0664, Metrics: {'mse': 0.06732013076543808, 'rmse': 0.25946123171957325, 'r2': -0.03762710094451904}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1038Epoch 4/15: [                              ] 2/63 batches, loss: 0.1254Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1404Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1238Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1287Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1276Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1251Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1257Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1331Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1356Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1348Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1314Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1316Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1251Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1225Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1189Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1167Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1208Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1208Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1201Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1199Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1226Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1234Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1253Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1230Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1225Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1212Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1184Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1172Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1164Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1162Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1179Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1183Epoch 4/15: [================              ] 34/63 batches, loss: 0.1183Epoch 4/15: [================              ] 35/63 batches, loss: 0.1172Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1152Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1155Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1173Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1172Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1181Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1185Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1201Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1194Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1182Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1174Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1162Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1165Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1174Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1177Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1167Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1161Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1145Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1144Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1149Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1144Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1136Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1127Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1133Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1135Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1130Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1130Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1139Epoch 4/15: [==============================] 63/63 batches, loss: 0.1142
[2025-05-02 11:31:36,062][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1142
[2025-05-02 11:31:36,269][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0660, Metrics: {'mse': 0.06668505072593689, 'rmse': 0.2582344878708824, 'r2': -0.027838468551635742}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1239Epoch 5/15: [                              ] 2/63 batches, loss: 0.1118Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1099Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1097Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1025Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0989Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0960Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0983Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0992Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0974Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0992Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0934Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0946Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0913Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0936Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0946Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0950Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0931Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0919Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0905Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0961Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0969Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0965Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0971Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0986Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0985Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0991Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0997Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1002Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0998Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0984Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0978Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0980Epoch 5/15: [================              ] 34/63 batches, loss: 0.0986Epoch 5/15: [================              ] 35/63 batches, loss: 0.0991Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0988Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0977Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0966Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0958Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0962Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0982Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0978Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0980Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0975Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0967Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0967Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0965Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0963Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0966Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0972Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0970Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0973Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0972Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0969Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0960Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0962Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0964Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0969Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0964Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0962Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0957Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0954Epoch 5/15: [==============================] 63/63 batches, loss: 0.0966
[2025-05-02 11:31:38,562][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0966
[2025-05-02 11:31:38,783][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0458, Metrics: {'mse': 0.046286631375551224, 'rmse': 0.21514328103743147, 'r2': 0.2865690588951111}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1252Epoch 6/15: [                              ] 2/63 batches, loss: 0.0903Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0825Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0799Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0841Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0898Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0881Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0830Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0928Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0913Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0880Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0859Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0884Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0890Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0890Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0874Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0888Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0869Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0870Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0862Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0864Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0887Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0882Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0890Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0902Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0884Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0888Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0899Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0886Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0880Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0892Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0882Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0880Epoch 6/15: [================              ] 34/63 batches, loss: 0.0874Epoch 6/15: [================              ] 35/63 batches, loss: 0.0875Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0865Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0874Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0876Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0875Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0887Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0886Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0889Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0900Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0895Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0928Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0919Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0917Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0915Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0911Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0904Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0898Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0899Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0892Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0890Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0891Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0887Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0899Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0895Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0889Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0886Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0879Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0870Epoch 6/15: [==============================] 63/63 batches, loss: 0.0859
[2025-05-02 11:31:41,068][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0859
[2025-05-02 11:31:41,292][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0493, Metrics: {'mse': 0.04952831566333771, 'rmse': 0.2225495802362649, 'r2': 0.23660391569137573}
[2025-05-02 11:31:41,293][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0866Epoch 7/15: [                              ] 2/63 batches, loss: 0.0563Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0649Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0726Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0804Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0877Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0831Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0833Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0832Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0841Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0872Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0852Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0856Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0885Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0858Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0860Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0849Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0825Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0836Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0834Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0841Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0838Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0833Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0831Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0848Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0843Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0832Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0836Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0836Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0850Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0845Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0834Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0835Epoch 7/15: [================              ] 34/63 batches, loss: 0.0829Epoch 7/15: [================              ] 35/63 batches, loss: 0.0826Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0827Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0822Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0824Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0828Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0829Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0830Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0823Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0820Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0807Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0808Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0816Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0813Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0814Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0816Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0815Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0808Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0823Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0824Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0821Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0816Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0816Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0807Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0807Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0810Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0810Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0804Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0804Epoch 7/15: [==============================] 63/63 batches, loss: 0.0798
[2025-05-02 11:31:43,214][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0798
[2025-05-02 11:31:43,429][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0313, Metrics: {'mse': 0.03169049322605133, 'rmse': 0.1780182384646341, 'r2': 0.5115441083908081}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0405Epoch 8/15: [                              ] 2/63 batches, loss: 0.0516Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0523Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0508Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0519Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0524Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0589Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0638Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0650Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0674Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0650Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0672Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0696Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0694Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0717Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0705Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0702Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0685Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0689Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0692Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0702Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0709Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0718Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0721Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0720Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0710Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0715Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0711Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0714Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0725Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0724Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0740Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0742Epoch 8/15: [================              ] 34/63 batches, loss: 0.0752Epoch 8/15: [================              ] 35/63 batches, loss: 0.0753Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0751Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0766Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0757Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0759Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0761Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0758Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0752Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0758Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0757Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0756Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0755Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0766Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0766Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0766Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0753Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0751Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0753Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0750Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0746Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0744Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0749Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0747Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0755Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0745Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0740Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0741Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0735Epoch 8/15: [==============================] 63/63 batches, loss: 0.0724
[2025-05-02 11:31:45,799][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0724
[2025-05-02 11:31:46,026][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0351, Metrics: {'mse': 0.03519883379340172, 'rmse': 0.18761352241616733, 'r2': 0.4574688673019409}
[2025-05-02 11:31:46,026][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0550Epoch 9/15: [                              ] 2/63 batches, loss: 0.0561Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0696Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0704Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0672Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0607Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0665Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0618Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0661Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0657Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0678Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0673Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0666Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0685Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0670Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0683Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0683Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0680Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0674Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0678Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0662Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0677Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0676Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0682Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0692Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0708Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0723Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0714Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0711Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0700Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0698Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0698Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0695Epoch 9/15: [================              ] 34/63 batches, loss: 0.0688Epoch 9/15: [================              ] 35/63 batches, loss: 0.0683Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0685Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0682Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0680Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0680Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0679Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0673Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0667Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0668Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0670Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0667Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0679Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0675Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0680Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0691Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0690Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0687Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0690Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0687Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0681Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0675Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0675Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0673Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0670Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0673Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0668Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0665Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0665Epoch 9/15: [==============================] 63/63 batches, loss: 0.0659
[2025-05-02 11:31:47,997][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0659
[2025-05-02 11:31:48,234][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0354, Metrics: {'mse': 0.0353972464799881, 'rmse': 0.18814155968309632, 'r2': 0.45441073179244995}
[2025-05-02 11:31:48,234][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0661Epoch 10/15: [                              ] 2/63 batches, loss: 0.0665Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0536Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0456Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0418Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0446Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0456Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0468Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0482Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0511Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0518Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0602Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0607Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0603Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0590Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0580Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0625Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0616Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0612Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0620Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0629Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0616Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0616Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0613Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0615Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0606Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0605Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0593Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0611Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0604Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0595Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0596Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0594Epoch 10/15: [================              ] 34/63 batches, loss: 0.0592Epoch 10/15: [================              ] 35/63 batches, loss: 0.0610Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0601Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0596Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0601Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0596Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0587Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0589Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0587Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0587Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0588Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0586Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0584Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0586Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0590Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0582Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0587Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0583Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0581Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0583Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0584Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0586Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0590Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0591Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0590Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0586Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0587Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0585Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0581Epoch 10/15: [==============================] 63/63 batches, loss: 0.0586
[2025-05-02 11:31:50,170][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0586
[2025-05-02 11:31:50,393][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0263, Metrics: {'mse': 0.026337474584579468, 'rmse': 0.16228824536786227, 'r2': 0.5940519571304321}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0615Epoch 11/15: [                              ] 2/63 batches, loss: 0.0611Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0654Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0607Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0594Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0615Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0592Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0610Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0613Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0593Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0560Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0536Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0516Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0516Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0519Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0523Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0519Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0504Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0502Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0504Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0498Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0492Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0488Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0492Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0485Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0485Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0480Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0487Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0481Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0482Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0484Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0484Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0483Epoch 11/15: [================              ] 34/63 batches, loss: 0.0486Epoch 11/15: [================              ] 35/63 batches, loss: 0.0493Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0494Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0497Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0493Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0492Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0490Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0495Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0494Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0489Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0492Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0496Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0503Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0500Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0503Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0499Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0504Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0505Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0508Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0506Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0506Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0505Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0508Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0514Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0511Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0507Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0505Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0507Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0506Epoch 11/15: [==============================] 63/63 batches, loss: 0.0501
[2025-05-02 11:31:52,714][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0501
[2025-05-02 11:31:52,932][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0414, Metrics: {'mse': 0.041288282722234726, 'rmse': 0.20319518380669047, 'r2': 0.3636102080345154}
[2025-05-02 11:31:52,932][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0747Epoch 12/15: [                              ] 2/63 batches, loss: 0.0622Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0620Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0632Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0645Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0668Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0617Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0614Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0608Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0571Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0578Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0582Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0566Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0562Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0559Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0544Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0540Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0558Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0550Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0544Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0544Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0538Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0541Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0530Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0531Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0527Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0528Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0524Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0521Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0512Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0505Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0498Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0497Epoch 12/15: [================              ] 34/63 batches, loss: 0.0496Epoch 12/15: [================              ] 35/63 batches, loss: 0.0495Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0493Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0486Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0482Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0479Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0478Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0482Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0484Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0492Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0488Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0484Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0483Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0478Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0472Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0481Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0484Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0479Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0484Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0484Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0486Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0487Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0489Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0491Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0494Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0495Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0491Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0490Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0490Epoch 12/15: [==============================] 63/63 batches, loss: 0.0488
[2025-05-02 11:31:54,884][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0488
[2025-05-02 11:31:55,102][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0180, Metrics: {'mse': 0.018039880320429802, 'rmse': 0.13431262159763616, 'r2': 0.7219454646110535}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0578Epoch 13/15: [                              ] 2/63 batches, loss: 0.0518Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0498Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0479Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0493Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0493Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0471Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0504Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0485Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0498Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0486Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0489Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0513Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0521Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0536Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0542Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0535Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0527Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0526Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0520Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0534Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0534Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0528Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0523Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0523Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0520Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0517Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0515Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0513Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0511Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0512Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0507Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0499Epoch 13/15: [================              ] 34/63 batches, loss: 0.0495Epoch 13/15: [================              ] 35/63 batches, loss: 0.0493Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0489Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0488Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0487Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0488Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0488Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0483Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0487Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0486Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0487Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0492Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0488Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0490Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0492Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0488Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0484Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0482Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0482Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0483Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0482Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0485Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0483Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0484Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0486Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0484Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0480Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0480Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0489Epoch 13/15: [==============================] 63/63 batches, loss: 0.0490
[2025-05-02 11:31:57,537][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0490
[2025-05-02 11:31:57,792][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0260, Metrics: {'mse': 0.025860970839858055, 'rmse': 0.16081346597800214, 'r2': 0.6013964414596558}
[2025-05-02 11:31:57,793][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0550Epoch 14/15: [                              ] 2/63 batches, loss: 0.0597Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0528Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0496Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0481Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0503Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0498Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0533Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0525Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0530Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0517Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0521Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0522Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0499Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0489Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0481Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0486Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0501Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0505Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0492Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0483Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0474Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0471Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0464Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0454Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0451Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0443Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0438Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0433Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0439Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0439Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0441Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0439Epoch 14/15: [================              ] 34/63 batches, loss: 0.0446Epoch 14/15: [================              ] 35/63 batches, loss: 0.0439Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0441Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0441Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0445Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0442Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0444Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0436Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0430Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0433Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0431Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0427Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0427Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0426Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0426Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0427Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0425Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0429Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0427Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0426Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0422Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0419Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0418Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0416Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0416Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0418Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0416Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0420Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0419Epoch 14/15: [==============================] 63/63 batches, loss: 0.0417
[2025-05-02 11:31:59,776][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0417
[2025-05-02 11:32:00,002][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0285, Metrics: {'mse': 0.02830641344189644, 'rmse': 0.16824509931019221, 'r2': 0.5637040138244629}
[2025-05-02 11:32:00,003][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0229Epoch 15/15: [                              ] 2/63 batches, loss: 0.0364Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0293Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0340Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0415Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0420Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0417Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0413Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0408Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0436Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0471Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0460Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0442Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0443Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0444Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0441Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0432Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0421Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0418Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0409Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0408Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0396Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0396Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0398Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0404Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0404Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0403Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0397Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0413Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0410Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0410Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0408Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0410Epoch 15/15: [================              ] 34/63 batches, loss: 0.0407Epoch 15/15: [================              ] 35/63 batches, loss: 0.0407Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0412Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0411Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0412Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0408Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0405Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0404Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0408Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0417Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0416Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0419Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0427Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0426Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0421Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0420Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0418Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0413Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0412Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0408Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0405Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0410Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0413Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0419Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0424Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0421Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0423Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0428Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0428Epoch 15/15: [==============================] 63/63 batches, loss: 0.0423
[2025-05-02 11:32:01,958][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0423
[2025-05-02 11:32:02,183][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0292, Metrics: {'mse': 0.02904665470123291, 'rmse': 0.17043079152909227, 'r2': 0.5522944331169128}
[2025-05-02 11:32:02,184][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
[2025-05-02 11:32:02,184][src.training.lm_trainer][INFO] - Training completed in 36.35 seconds
[2025-05-02 11:32:02,184][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:32:04,729][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014927377924323082, 'rmse': 0.12217764903746954, 'r2': 0.5137244462966919}
[2025-05-02 11:32:04,730][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.018039880320429802, 'rmse': 0.13431262159763616, 'r2': 0.7219454646110535}
[2025-05-02 11:32:04,730][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.057846855372190475, 'rmse': 0.2405137321904728, 'r2': 0.0027459263801574707}
[2025-05-02 11:32:06,404][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/ar/model.pt
[2025-05-02 11:32:06,406][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▂▂▁▁
wandb:     best_val_mse █▅▃▃▂▂▁▁
wandb:      best_val_r2 ▁▄▆▆▇▇██
wandb:    best_val_rmse █▆▄▄▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▇▇█▇████████
wandb:       train_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▃▂▂▂▂▂▁▂▁▁▁▂
wandb:          val_mse █▅▃▃▂▂▂▂▂▁▂▁▁▁▁
wandb:           val_r2 ▁▄▆▆▇▇▇▇▇█▇████
wandb:         val_rmse █▆▄▄▃▃▂▂▂▂▃▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01804
wandb:     best_val_mse 0.01804
wandb:      best_val_r2 0.72195
wandb:    best_val_rmse 0.13431
wandb:            epoch 15
wandb:   final_test_mse 0.05785
wandb:    final_test_r2 0.00275
wandb:  final_test_rmse 0.24051
wandb:  final_train_mse 0.01493
wandb:   final_train_r2 0.51372
wandb: final_train_rmse 0.12218
wandb:    final_val_mse 0.01804
wandb:     final_val_r2 0.72195
wandb:   final_val_rmse 0.13431
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04235
wandb:       train_time 36.3459
wandb:         val_loss 0.02917
wandb:          val_mse 0.02905
wandb:           val_r2 0.55229
wandb:         val_rmse 0.17043
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113116-z9r2uwim
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113116-z9r2uwim/logs
Experiment probe_layer1_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/results.json
=======================
PROBING LAYER 4
=======================
Running experiment: probe_layer4_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:32:20,521][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar
experiment_name: probe_layer4_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:32:20,521][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:32:20,521][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:32:20,521][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:32:20,526][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 11:32:20,526][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:32:22,239][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:32:24,605][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:32:24,606][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:32:24,671][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,702][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,802][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:32:24,810][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:32:24,810][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:32:24,812][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:32:24,835][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,868][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,881][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:32:24,882][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:32:24,882][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:32:24,883][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:32:24,905][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,946][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:32:24,960][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:32:24,961][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:32:24,961][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:32:24,962][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:32:24,963][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:32:24,963][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:32:24,963][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:32:24,963][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:32:24,963][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 11:32:24,964][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:32:24,964][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 11:32:24,964][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:32:24,964][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:32:24,965][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 11:32:24,965][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:32:24,965][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:32:24,966][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:32:24,966][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:32:24,966][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:32:29,235][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:32:29,237][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:32:29,237][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 11:32:29,237][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:32:29,243][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:32:29,243][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:32:29,243][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:32:29,243][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:32:29,243][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:32:29,244][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:32:29,244][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7334Epoch 1/15: [                              ] 2/63 batches, loss: 0.6922Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7079Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7141Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7144Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7142Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7132Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7080Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7049Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7025Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7024Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7015Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7002Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6988Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6985Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6984Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6987Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6977Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6973Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6969Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6964Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6965Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6964Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6959Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6952Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6948Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6946Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6941Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6935Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6928Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6923Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6930Epoch 1/15: [================              ] 34/63 batches, loss: 0.6928Epoch 1/15: [================              ] 35/63 batches, loss: 0.6922Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6916Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6911Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6908Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6894Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6874Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6887Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6899Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6901Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6872Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6876Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6878Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6872Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6865Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6856Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6845Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6849Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6834Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6837Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6828Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6811Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6803Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6804Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6800Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6803Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6787Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6780Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6773Epoch 1/15: [==============================] 63/63 batches, loss: 0.6784
[2025-05-02 11:32:34,071][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6784
[2025-05-02 11:32:34,272][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6762, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.6285714285714286, 'precision': 0.7333333333333333, 'recall': 0.55}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6437Epoch 2/15: [                              ] 2/63 batches, loss: 0.6571Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6703Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6754Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6621Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6554Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6580Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6563Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6601Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6576Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6606Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6621Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6555Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6539Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6519Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6547Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6548Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6540Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6538Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6545Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6557Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6536Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6502Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6506Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6541Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6526Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6508Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6490Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6477Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6481Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6519Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6528Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6518Epoch 2/15: [================              ] 34/63 batches, loss: 0.6505Epoch 2/15: [================              ] 35/63 batches, loss: 0.6481Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6484Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6469Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6461Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6457Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6446Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6442Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6428Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6432Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6438Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6425Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6415Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6397Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6390Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6391Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6367Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6370Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6349Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6338Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6335Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6335Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6324Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6322Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6303Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6299Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6298Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6295Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6279Epoch 2/15: [==============================] 63/63 batches, loss: 0.6275
[2025-05-02 11:32:36,598][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6275
[2025-05-02 11:32:36,807][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6341, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888, 'precision': 0.8, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5791Epoch 3/15: [                              ] 2/63 batches, loss: 0.5957Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5803Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5910Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5989Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5977Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5939Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6106Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6011Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6052Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6013Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6068Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6147Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6131Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6080Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6060Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6051Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6034Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6018Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6000Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5997Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6015Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5998Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5985Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5929Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5942Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5963Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5961Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5988Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5978Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5982Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5984Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6013Epoch 3/15: [================              ] 34/63 batches, loss: 0.6002Epoch 3/15: [================              ] 35/63 batches, loss: 0.5976Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5994Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5995Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5987Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5981Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5965Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5979Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5977Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5968Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5964Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5948Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5944Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5927Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5918Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5904Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5899Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5898Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5890Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5885Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5875Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5874Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5865Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5857Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5861Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5858Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5868Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5858Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5852Epoch 3/15: [==============================] 63/63 batches, loss: 0.5850
[2025-05-02 11:32:39,169][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5850
[2025-05-02 11:32:39,391][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5993, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5533Epoch 4/15: [                              ] 2/63 batches, loss: 0.5653Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5421Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5302Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5521Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5493Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5519Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5546Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5398Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5472Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5561Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5580Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5592Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5595Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5693Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5701Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5744Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5743Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5726Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5703Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5655Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5652Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5658Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5671Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5676Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5698Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5685Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5699Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5710Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5700Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5684Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5684Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5659Epoch 4/15: [================              ] 34/63 batches, loss: 0.5661Epoch 4/15: [================              ] 35/63 batches, loss: 0.5676Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5657Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5669Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5664Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5663Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5645Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5656Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5652Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5633Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5633Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5628Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5598Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5590Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5591Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5588Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5582Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5585Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5591Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5596Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5600Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5591Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5587Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5610Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5599Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5603Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5597Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5595Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5602Epoch 4/15: [==============================] 63/63 batches, loss: 0.5585
[2025-05-02 11:32:41,671][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5585
[2025-05-02 11:32:41,880][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5852, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6010Epoch 5/15: [                              ] 2/63 batches, loss: 0.5594Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5549Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5629Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5900Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5805Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5740Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5596Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5555Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5489Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5417Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5428Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5432Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5401Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5362Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5416Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5427Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5396Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5427Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5457Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5434Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5406Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5382Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5406Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5384Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5404Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5394Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5375Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5409Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5422Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5402Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5385Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5402Epoch 5/15: [================              ] 34/63 batches, loss: 0.5418Epoch 5/15: [================              ] 35/63 batches, loss: 0.5437Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5431Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5430Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5412Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5401Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5410Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5412Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5429Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5422Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5422Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5422Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5416Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5415Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5418Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5413Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5428Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5429Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5432Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5440Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5438Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5440Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5428Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5431Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5439Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5442Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5437Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5431Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5431Epoch 5/15: [==============================] 63/63 batches, loss: 0.5427
[2025-05-02 11:32:44,173][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5427
[2025-05-02 11:32:44,400][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5744, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5001Epoch 6/15: [                              ] 2/63 batches, loss: 0.5291Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5284Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5213Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5165Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5140Epoch 6/15: [===                           ] 7/63 batches, loss: 0.4985Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5102Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5132Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5205Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5229Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5278Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5286Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5316Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5329Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5392Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5394Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5428Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5449Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5453Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5427Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5449Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5424Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5449Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5485Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5476Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5469Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5460Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5458Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5434Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5477Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5458Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5443Epoch 6/15: [================              ] 34/63 batches, loss: 0.5400Epoch 6/15: [================              ] 35/63 batches, loss: 0.5377Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5383Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5382Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5355Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5371Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5385Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5381Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5371Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5383Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5363Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5369Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5377Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5367Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5363Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5379Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5394Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5380Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5365Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5359Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5369Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5376Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5389Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5383Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5391Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5395Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5383Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5391Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5398Epoch 6/15: [==============================] 63/63 batches, loss: 0.5413
[2025-05-02 11:32:46,721][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5413
[2025-05-02 11:32:46,942][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5769, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:32:46,942][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5232Epoch 7/15: [                              ] 2/63 batches, loss: 0.5008Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5580Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5460Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5454Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5355Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5406Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5354Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5360Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5297Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5235Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5250Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5274Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5191Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5250Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5283Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5300Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5286Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5301Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5279Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5299Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5324Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5315Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5303Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5296Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5306Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5319Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5340Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5370Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5411Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5400Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5381Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5362Epoch 7/15: [================              ] 34/63 batches, loss: 0.5372Epoch 7/15: [================              ] 35/63 batches, loss: 0.5398Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5386Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5367Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5388Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5389Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5418Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5419Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5399Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5403Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5387Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5373Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5365Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5383Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5368Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5366Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5348Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5336Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5364Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5367Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5376Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5379Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5371Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5358Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5355Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5336Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5330Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5341Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5348Epoch 7/15: [==============================] 63/63 batches, loss: 0.5350
[2025-05-02 11:32:48,875][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5350
[2025-05-02 11:32:49,085][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5757, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:32:49,086][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6117Epoch 8/15: [                              ] 2/63 batches, loss: 0.5769Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5518Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5195Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5375Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5557Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5532Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5494Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5507Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5483Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5474Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5498Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5469Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5442Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5459Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5483Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5525Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5551Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5552Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5521Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5534Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5495Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5485Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5497Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5488Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5471Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5447Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5427Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5411Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5385Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5384Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5382Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5353Epoch 8/15: [================              ] 34/63 batches, loss: 0.5354Epoch 8/15: [================              ] 35/63 batches, loss: 0.5340Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5336Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5342Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5332Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5346Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5321Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5341Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5343Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5339Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5348Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5358Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5357Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5361Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5353Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5349Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5350Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5355Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5364Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5358Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5358Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5336Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5327Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5316Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5322Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5312Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5305Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5309Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5298Epoch 8/15: [==============================] 63/63 batches, loss: 0.5304
[2025-05-02 11:32:51,017][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5304
[2025-05-02 11:32:51,228][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5750, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:32:51,229][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:32:51,229][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 11:32:51,230][src.training.lm_trainer][INFO] - Training completed in 20.08 seconds
[2025-05-02 11:32:51,230][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:32:53,839][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.978894472361809, 'f1': 0.9789368104312939, 'precision': 0.976, 'recall': 0.9818913480885312}
[2025-05-02 11:32:53,840][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:32:53,840][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6363636363636364, 'f1': 0.6, 'precision': 0.4375, 'recall': 0.9545454545454546}
[2025-05-02 11:32:55,467][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar/ar/model.pt
[2025-05-02 11:32:55,468][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▇█▇█
wandb:           best_val_f1 ▁▇█▇█
wandb:         best_val_loss █▅▃▂▁
wandb:    best_val_precision ▁▆█▇█
wandb:       best_val_recall ▁██▇█
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃
wandb:            train_loss █▆▄▂▂▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▇█▇████
wandb:                val_f1 ▁▇█▇████
wandb:              val_loss █▅▃▂▁▁▁▁
wandb:         val_precision ▁▆█▇████
wandb:            val_recall ▁██▇████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.57444
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.63636
wandb:         final_test_f1 0.6
wandb:  final_test_precision 0.4375
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 0.97889
wandb:        final_train_f1 0.97894
wandb: final_train_precision 0.976
wandb:    final_train_recall 0.98189
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.5304
wandb:            train_time 20.0762
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57501
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113220-m9c2d1yd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113220-m9c2d1yd/logs
Experiment probe_layer4_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar/results.json
Running experiment: probe_layer4_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:33:06,198][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar
experiment_name: probe_layer4_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:33:06,198][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:33:06,198][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:33:06,198][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:33:06,202][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 11:33:06,203][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:33:07,575][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:33:10,003][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:33:10,003][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:33:10,035][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,065][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,167][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:33:10,174][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:33:10,174][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:33:10,176][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:33:10,196][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,224][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,236][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:33:10,237][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:33:10,237][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:33:10,238][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:33:10,257][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,287][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:33:10,299][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:33:10,301][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:33:10,301][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:33:10,302][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:33:10,303][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:33:10,303][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:33:10,303][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:33:10,303][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:33:10,303][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:33:10,303][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:33:10,304][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:33:10,304][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:33:10,304][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:33:10,305][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:33:10,305][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:33:10,305][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:33:10,306][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:33:10,306][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:33:10,306][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:33:10,306][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:33:14,140][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:33:14,141][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:33:14,141][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 11:33:14,141][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:33:14,145][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:33:14,146][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:33:14,146][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:33:14,146][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:33:14,146][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:33:14,147][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:33:14,147][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7622Epoch 1/15: [                              ] 2/63 batches, loss: 0.5325Epoch 1/15: [=                             ] 3/63 batches, loss: 0.4102Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4713Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4446Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4093Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3953Epoch 1/15: [===                           ] 8/63 batches, loss: 0.3917Epoch 1/15: [====                          ] 9/63 batches, loss: 0.3791Epoch 1/15: [====                          ] 10/63 batches, loss: 0.3581Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.3444Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.3302Epoch 1/15: [======                        ] 13/63 batches, loss: 0.3210Epoch 1/15: [======                        ] 14/63 batches, loss: 0.3094Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2996Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2954Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2914Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2806Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2848Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2784Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2791Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2798Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2768Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2739Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2779Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2747Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2753Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2748Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2719Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2693Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2631Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2619Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2564Epoch 1/15: [================              ] 34/63 batches, loss: 0.2539Epoch 1/15: [================              ] 35/63 batches, loss: 0.2518Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2495Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2487Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2486Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2497Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2453Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2430Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2413Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2404Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2370Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2363Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2365Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2379Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2391Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2385Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2395Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2376Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2381Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2365Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2350Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2345Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2353Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2330Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2321Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2322Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2309Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2285Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2269Epoch 1/15: [==============================] 63/63 batches, loss: 0.2236
[2025-05-02 11:33:18,657][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2236
[2025-05-02 11:33:18,836][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1108, Metrics: {'mse': 0.1094176322221756, 'rmse': 0.33078336146513715, 'r2': -0.6864895820617676}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2393Epoch 2/15: [                              ] 2/63 batches, loss: 0.2035Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2127Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2031Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1884Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1693Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1617Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1579Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1612Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1657Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1569Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1534Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1574Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1572Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1561Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1521Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1494Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1466Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1484Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1545Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1517Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1521Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1513Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1492Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1478Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1445Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1417Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1403Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1380Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1403Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1395Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1401Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1389Epoch 2/15: [================              ] 34/63 batches, loss: 0.1378Epoch 2/15: [================              ] 35/63 batches, loss: 0.1363Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1352Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1344Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1331Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1344Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1353Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1351Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1352Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1356Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1367Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1361Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1365Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1361Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1346Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1338Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1353Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1358Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1357Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1361Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1356Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1356Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1349Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1336Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1340Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1328Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1319Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1318Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1311Epoch 2/15: [==============================] 63/63 batches, loss: 0.1298
[2025-05-02 11:33:21,167][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1298
[2025-05-02 11:33:21,379][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0688, Metrics: {'mse': 0.06813698261976242, 'rmse': 0.2610306162498231, 'r2': -0.050217509269714355}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0782Epoch 3/15: [                              ] 2/63 batches, loss: 0.0846Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0948Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1011Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1041Epoch 3/15: [==                            ] 6/63 batches, loss: 0.0977Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1112Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1098Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1119Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1156Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1138Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1143Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1160Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1207Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1217Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1227Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1179Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1167Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1187Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1160Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1172Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1165Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1166Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1155Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1160Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1156Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1145Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1137Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1133Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1117Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1148Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1151Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1143Epoch 3/15: [================              ] 34/63 batches, loss: 0.1152Epoch 3/15: [================              ] 35/63 batches, loss: 0.1147Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1152Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1144Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1150Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1159Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1152Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1171Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1160Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1170Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1164Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1149Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1140Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1150Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1159Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1163Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1185Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1201Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1231Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1246Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1245Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1249Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1257Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1251Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1247Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1239Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1245Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1242Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1234Epoch 3/15: [==============================] 63/63 batches, loss: 0.1217
[2025-05-02 11:33:23,707][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1217
[2025-05-02 11:33:23,918][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0519, Metrics: {'mse': 0.051565561443567276, 'rmse': 0.2270805175341277, 'r2': 0.20520323514938354}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1341Epoch 4/15: [                              ] 2/63 batches, loss: 0.1079Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1132Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1221Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1236Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1259Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1217Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1176Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1194Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1180Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1164Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1142Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1190Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1201Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1166Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1131Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1112Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1113Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1112Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1094Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1091Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1111Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1142Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1119Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1117Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1102Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1103Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1088Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1100Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1084Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1097Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1083Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1075Epoch 4/15: [================              ] 34/63 batches, loss: 0.1076Epoch 4/15: [================              ] 35/63 batches, loss: 0.1069Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1058Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1061Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1060Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1064Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1069Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1064Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1066Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1055Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1049Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1057Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1057Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1047Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1049Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1065Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1061Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1047Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1040Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1036Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1034Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1029Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1026Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1024Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1024Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1016Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1015Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1018Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1010Epoch 4/15: [==============================] 63/63 batches, loss: 0.1013
[2025-05-02 11:33:26,206][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1013
[2025-05-02 11:33:26,408][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0574, Metrics: {'mse': 0.05730462446808815, 'rmse': 0.23938384337312354, 'r2': 0.11674511432647705}
[2025-05-02 11:33:26,409][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.2293Epoch 5/15: [                              ] 2/63 batches, loss: 0.1789Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1349Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1263Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1126Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1027Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0975Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1013Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1053Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1011Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0966Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0970Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1021Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1021Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1040Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1048Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1062Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1027Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1014Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1020Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1008Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0993Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0975Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0996Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0993Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0996Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0995Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1007Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0999Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1034Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1022Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1022Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1020Epoch 5/15: [================              ] 34/63 batches, loss: 0.1026Epoch 5/15: [================              ] 35/63 batches, loss: 0.1019Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1019Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1028Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1017Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1009Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0997Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1011Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1009Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1019Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1014Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1002Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1007Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1000Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0989Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0985Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0982Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0982Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0974Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0976Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0973Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0965Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0958Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0953Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0948Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0954Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0952Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0946Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0938Epoch 5/15: [==============================] 63/63 batches, loss: 0.0946
[2025-05-02 11:33:28,349][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0946
[2025-05-02 11:33:28,576][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0621, Metrics: {'mse': 0.06217733398079872, 'rmse': 0.24935383289774937, 'r2': 0.041640520095825195}
[2025-05-02 11:33:28,576][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1357Epoch 6/15: [                              ] 2/63 batches, loss: 0.1403Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1327Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1160Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1083Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1000Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1031Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0924Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0971Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0950Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0883Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0916Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0887Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0897Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0889Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0870Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0863Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0870Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0903Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0886Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0873Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0873Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0872Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0871Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0863Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0873Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0867Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0861Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0856Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0850Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0842Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0836Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0828Epoch 6/15: [================              ] 34/63 batches, loss: 0.0817Epoch 6/15: [================              ] 35/63 batches, loss: 0.0812Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0798Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0796Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0798Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0795Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0798Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0802Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0796Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0790Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0788Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0791Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0791Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0792Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0788Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0787Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0780Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0780Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0777Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0778Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0779Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0772Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0769Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0773Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0771Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0779Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0784Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0779Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0775Epoch 6/15: [==============================] 63/63 batches, loss: 0.0770
[2025-05-02 11:33:30,513][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0770
[2025-05-02 11:33:30,734][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0469, Metrics: {'mse': 0.04705316945910454, 'rmse': 0.21691742543904705, 'r2': 0.2747541666030884}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0715Epoch 7/15: [                              ] 2/63 batches, loss: 0.0760Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0744Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0741Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0717Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0765Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0859Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0845Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0822Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0781Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0786Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0774Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0821Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0839Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0801Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0800Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0785Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0769Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0761Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0744Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0724Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0719Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0716Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0716Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0702Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0692Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0704Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0717Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0721Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0717Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0731Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0730Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0727Epoch 7/15: [================              ] 34/63 batches, loss: 0.0720Epoch 7/15: [================              ] 35/63 batches, loss: 0.0715Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0714Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0719Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0732Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0726Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0730Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0729Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0734Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0725Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0725Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0731Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0730Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0735Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0737Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0759Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0753Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0748Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0748Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0757Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0760Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0752Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0747Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0748Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0743Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0743Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0739Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0733Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0729Epoch 7/15: [==============================] 63/63 batches, loss: 0.0719
[2025-05-02 11:33:33,013][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0719
[2025-05-02 11:33:33,213][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0459, Metrics: {'mse': 0.04617976024746895, 'rmse': 0.21489476551900688, 'r2': 0.2882162928581238}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0576Epoch 8/15: [                              ] 2/63 batches, loss: 0.0649Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0577Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0573Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0560Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0523Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0532Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0563Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0549Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0549Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0530Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0550Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0555Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0575Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0581Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0579Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0580Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0600Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0609Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0612Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0632Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0633Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0619Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0618Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0619Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0608Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0595Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0612Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0621Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0634Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0645Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0652Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0654Epoch 8/15: [================              ] 34/63 batches, loss: 0.0670Epoch 8/15: [================              ] 35/63 batches, loss: 0.0676Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0677Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0681Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0673Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0667Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0669Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0669Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0663Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0676Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0700Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0694Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0687Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0687Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0679Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0674Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0667Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0663Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0667Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0663Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0663Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0657Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0654Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0658Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0655Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0648Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0642Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0642Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0644Epoch 8/15: [==============================] 63/63 batches, loss: 0.0641
[2025-05-02 11:33:35,561][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0641
[2025-05-02 11:33:35,770][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0441, Metrics: {'mse': 0.044389739632606506, 'rmse': 0.2106887268759449, 'r2': 0.31580644845962524}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0896Epoch 9/15: [                              ] 2/63 batches, loss: 0.0660Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0645Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0624Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0586Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0548Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0529Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0500Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0493Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0508Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0511Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0511Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0501Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0530Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0524Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0525Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0534Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0554Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0554Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0549Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0542Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0552Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0561Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0560Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0552Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0561Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0563Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0558Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0557Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0554Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0565Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0583Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0585Epoch 9/15: [================              ] 34/63 batches, loss: 0.0582Epoch 9/15: [================              ] 35/63 batches, loss: 0.0575Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0572Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0565Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0562Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0564Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0566Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0565Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0576Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0585Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0584Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0585Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0586Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0589Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0592Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0602Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0601Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0600Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0602Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0601Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0601Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0606Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0609Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0603Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0600Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0596Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0596Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0597Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0596Epoch 9/15: [==============================] 63/63 batches, loss: 0.0594
[2025-05-02 11:33:38,066][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0594
[2025-05-02 11:33:38,280][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0475, Metrics: {'mse': 0.04767557606101036, 'rmse': 0.21834737475181687, 'r2': 0.26516079902648926}
[2025-05-02 11:33:38,281][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1466Epoch 10/15: [                              ] 2/63 batches, loss: 0.1016Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0836Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0710Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0691Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0665Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0635Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0579Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0626Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0656Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0628Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0645Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0650Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0647Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0662Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0642Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0646Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0644Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0635Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0640Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0633Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0617Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0603Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0597Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0596Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0591Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0592Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0582Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0580Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0574Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0563Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0557Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0554Epoch 10/15: [================              ] 34/63 batches, loss: 0.0552Epoch 10/15: [================              ] 35/63 batches, loss: 0.0556Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0550Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0544Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0546Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0545Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0537Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0534Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0531Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0534Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0540Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0537Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0533Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0538Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0548Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0544Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0541Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0538Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0538Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0553Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0553Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0548Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0544Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0543Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0541Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0535Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0537Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0544Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0544Epoch 10/15: [==============================] 63/63 batches, loss: 0.0542
[2025-05-02 11:33:40,220][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0542
[2025-05-02 11:33:40,445][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0516, Metrics: {'mse': 0.051657769829034805, 'rmse': 0.22728345700696037, 'r2': 0.20378196239471436}
[2025-05-02 11:33:40,446][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0615Epoch 11/15: [                              ] 2/63 batches, loss: 0.0601Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0566Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0542Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0530Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0535Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0502Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0532Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0542Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0564Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0529Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0551Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0550Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0541Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0538Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0528Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0524Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0527Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0539Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0540Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0547Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0534Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0532Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0521Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0517Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0510Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0516Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0512Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0504Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0494Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0490Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0492Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0492Epoch 11/15: [================              ] 34/63 batches, loss: 0.0504Epoch 11/15: [================              ] 35/63 batches, loss: 0.0511Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0513Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0520Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0519Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0519Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0512Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0515Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0512Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0510Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0510Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0511Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0512Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0512Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0519Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0514Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0520Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0520Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0516Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0514Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0511Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0508Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0514Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0521Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0517Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0517Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0514Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0512Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0508Epoch 11/15: [==============================] 63/63 batches, loss: 0.0503
[2025-05-02 11:33:42,389][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0503
[2025-05-02 11:33:42,611][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0629, Metrics: {'mse': 0.0629342794418335, 'rmse': 0.2508670553138325, 'r2': 0.02997344732284546}
[2025-05-02 11:33:42,611][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0515Epoch 12/15: [                              ] 2/63 batches, loss: 0.0620Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0593Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0648Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0672Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0736Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0700Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0692Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0707Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0665Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0671Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0646Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0645Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0638Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0616Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0593Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0611Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0603Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0599Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0583Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0579Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0574Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0579Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0571Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0571Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0556Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0560Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0553Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0548Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0548Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0547Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0553Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0548Epoch 12/15: [================              ] 34/63 batches, loss: 0.0553Epoch 12/15: [================              ] 35/63 batches, loss: 0.0551Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0545Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0543Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0542Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0538Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0536Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0541Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0534Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0531Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0528Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0527Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0525Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0529Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0521Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0522Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0515Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0515Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0518Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0518Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0521Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0523Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0522Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0519Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0522Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0520Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0516Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0512Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0513Epoch 12/15: [==============================] 63/63 batches, loss: 0.0512
[2025-05-02 11:33:44,560][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0512
[2025-05-02 11:33:44,790][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0408, Metrics: {'mse': 0.04072019085288048, 'rmse': 0.20179244498464377, 'r2': 0.37236642837524414}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0792Epoch 13/15: [                              ] 2/63 batches, loss: 0.0651Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0596Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0550Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0501Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0480Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0441Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0432Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0440Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0425Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0438Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0436Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0435Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0438Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0449Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0463Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0455Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0447Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0444Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0451Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0457Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0458Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0450Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0454Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0459Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0454Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0454Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0453Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0458Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0463Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0470Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0468Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0467Epoch 13/15: [================              ] 34/63 batches, loss: 0.0471Epoch 13/15: [================              ] 35/63 batches, loss: 0.0472Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0467Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0467Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0468Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0472Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0475Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0468Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0476Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0473Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0473Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0472Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0467Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0475Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0481Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0477Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0472Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0469Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0467Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0473Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0471Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0472Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0469Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0469Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0467Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0465Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0464Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0468Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0472Epoch 13/15: [==============================] 63/63 batches, loss: 0.0468
[2025-05-02 11:33:47,170][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0468
[2025-05-02 11:33:47,394][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0544, Metrics: {'mse': 0.054376911371946335, 'rmse': 0.23318857470284932, 'r2': 0.16187095642089844}
[2025-05-02 11:33:47,395][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0668Epoch 14/15: [                              ] 2/63 batches, loss: 0.0606Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0517Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0451Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0464Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0454Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0448Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0451Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0453Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0459Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0457Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0458Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0439Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0431Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0426Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0434Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0427Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0440Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0437Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0438Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0430Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0422Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0423Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0418Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0418Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0415Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0409Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0406Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0401Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0399Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0403Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0407Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0404Epoch 14/15: [================              ] 34/63 batches, loss: 0.0409Epoch 14/15: [================              ] 35/63 batches, loss: 0.0407Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0408Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0410Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0422Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0430Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0431Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0431Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0426Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0433Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0435Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0430Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0427Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0430Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0433Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0432Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0433Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0435Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0438Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0436Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0435Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0431Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0428Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0427Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0427Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0433Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0431Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0430Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0428Epoch 14/15: [==============================] 63/63 batches, loss: 0.0427
[2025-05-02 11:33:49,352][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0427
[2025-05-02 11:33:49,561][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0363, Metrics: {'mse': 0.03627631068229675, 'rmse': 0.19046341035037873, 'r2': 0.44086140394210815}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0274Epoch 15/15: [                              ] 2/63 batches, loss: 0.0271Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0237Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0334Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0339Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0325Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0352Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0337Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0330Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0318Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0348Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0359Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0361Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0361Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0359Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0367Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0379Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0373Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0384Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0380Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0378Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0368Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0366Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0369Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0376Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0369Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0369Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0378Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0374Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0370Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0368Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0364Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0362Epoch 15/15: [================              ] 34/63 batches, loss: 0.0358Epoch 15/15: [================              ] 35/63 batches, loss: 0.0363Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0369Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0365Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0369Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0362Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0359Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0359Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0356Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0358Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0357Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0366Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0370Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0376Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0372Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0369Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0367Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0365Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0361Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0367Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0366Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0369Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0371Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0373Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0374Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0374Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0374Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0373Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0374Epoch 15/15: [==============================] 63/63 batches, loss: 0.0369
[2025-05-02 11:33:51,960][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0369
[2025-05-02 11:33:52,194][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0413, Metrics: {'mse': 0.04132948815822601, 'rmse': 0.20329655225366222, 'r2': 0.36297518014907837}
[2025-05-02 11:33:52,195][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 11:33:52,195][src.training.lm_trainer][INFO] - Training completed in 36.23 seconds
[2025-05-02 11:33:52,195][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:33:54,771][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.012962846085429192, 'rmse': 0.11385449523593344, 'r2': 0.5777211785316467}
[2025-05-02 11:33:54,772][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03627631068229675, 'rmse': 0.19046341035037873, 'r2': 0.44086140394210815}
[2025-05-02 11:33:54,772][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05878124013543129, 'rmse': 0.24244842778502668, 'r2': -0.013362407684326172}
[2025-05-02 11:33:56,484][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar/ar/model.pt
[2025-05-02 11:33:56,486][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▂▂▁▁
wandb:     best_val_mse █▄▂▂▂▂▁▁
wandb:      best_val_r2 ▁▅▇▇▇▇██
wandb:    best_val_rmse █▅▃▂▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▆▆▆▇▇▇▇▆▆▇▆▇
wandb:       train_loss █▄▄▃▃▃▂▂▂▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▃▃▂▂▂▂▂▄▁▃▁▁
wandb:          val_mse █▄▂▃▃▂▂▂▂▂▄▁▃▁▁
wandb:           val_r2 ▁▅▇▆▆▇▇▇▇▇▅█▆██
wandb:         val_rmse █▅▃▃▄▂▂▂▂▃▄▂▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03627
wandb:     best_val_mse 0.03628
wandb:      best_val_r2 0.44086
wandb:    best_val_rmse 0.19046
wandb:            epoch 15
wandb:   final_test_mse 0.05878
wandb:    final_test_r2 -0.01336
wandb:  final_test_rmse 0.24245
wandb:  final_train_mse 0.01296
wandb:   final_train_r2 0.57772
wandb: final_train_rmse 0.11385
wandb:    final_val_mse 0.03628
wandb:     final_val_r2 0.44086
wandb:   final_val_rmse 0.19046
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0369
wandb:       train_time 36.22888
wandb:         val_loss 0.04129
wandb:          val_mse 0.04133
wandb:           val_r2 0.36298
wandb:         val_rmse 0.2033
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113306-e3j4l3xr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113306-e3j4l3xr/logs
Experiment probe_layer4_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar/results.json
=======================
PROBING LAYER 6
=======================
Running experiment: probe_layer6_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:34:09,107][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar
experiment_name: probe_layer6_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:34:09,108][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:34:09,108][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:34:09,108][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:34:09,112][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 11:34:09,112][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:34:10,634][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:34:12,901][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:34:12,901][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:12,960][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:12,989][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:13,105][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:34:13,112][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:13,113][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:34:13,114][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:13,135][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:13,165][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:13,179][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:34:13,180][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:13,180][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:34:13,181][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:13,199][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:13,228][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:13,247][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:34:13,248][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:13,248][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:34:13,250][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:34:13,250][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:34:13,251][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 11:34:13,251][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:34:13,251][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:34:13,252][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 11:34:13,252][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:34:13,252][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:34:13,253][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 11:34:13,253][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 11:34:13,253][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:34:13,253][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:34:13,253][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:34:13,253][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:34:13,253][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:34:13,254][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:34:13,254][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:34:17,441][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:34:17,443][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:34:17,443][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 11:34:17,443][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:34:17,448][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:34:17,449][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:34:17,449][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:34:17,449][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:34:17,449][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:34:17,450][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:34:17,450][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7331Epoch 1/15: [                              ] 2/63 batches, loss: 0.6932Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7102Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7167Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7171Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7161Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7148Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7094Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7065Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7038Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7040Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7025Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7006Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6994Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6988Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6985Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6990Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6977Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6976Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6969Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6963Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6961Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6959Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6953Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6944Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6939Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6936Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6932Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6923Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6910Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6913Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6904Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6914Epoch 1/15: [================              ] 34/63 batches, loss: 0.6907Epoch 1/15: [================              ] 35/63 batches, loss: 0.6903Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6894Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6893Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6893Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6878Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6858Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6874Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6884Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6885Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6855Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6861Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6862Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6853Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6843Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6833Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6823Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6825Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6805Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6810Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6802Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6786Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6775Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6781Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6778Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6775Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6762Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6755Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6748Epoch 1/15: [==============================] 63/63 batches, loss: 0.6757
[2025-05-02 11:34:22,287][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6757
[2025-05-02 11:34:22,484][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6696, Metrics: {'accuracy': 0.7954545454545454, 'f1': 0.7906976744186046, 'precision': 0.7391304347826086, 'recall': 0.85}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6427Epoch 2/15: [                              ] 2/63 batches, loss: 0.6558Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6694Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6705Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6561Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6535Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6553Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6522Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6553Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6535Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6576Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6583Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6511Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6482Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6468Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6493Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6494Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6480Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6480Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6489Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6497Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6470Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6435Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6435Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6472Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6456Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6445Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6427Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6412Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6423Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6456Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6461Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6452Epoch 2/15: [================              ] 34/63 batches, loss: 0.6442Epoch 2/15: [================              ] 35/63 batches, loss: 0.6411Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6414Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6398Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6391Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6389Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6379Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6371Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6359Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6362Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6371Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6363Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6353Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6333Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6328Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6330Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6308Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6312Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6293Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6282Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6281Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6280Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6271Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6268Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6248Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6247Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6247Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6244Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6228Epoch 2/15: [==============================] 63/63 batches, loss: 0.6225
[2025-05-02 11:34:24,820][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6225
[2025-05-02 11:34:25,037][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6290, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043, 'precision': 0.7692307692307693, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5622Epoch 3/15: [                              ] 2/63 batches, loss: 0.5730Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5614Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5754Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5870Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5851Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5841Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6038Epoch 3/15: [====                          ] 9/63 batches, loss: 0.5945Epoch 3/15: [====                          ] 10/63 batches, loss: 0.5980Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5930Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.5978Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6048Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6024Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.5986Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.5982Epoch 3/15: [========                      ] 17/63 batches, loss: 0.5979Epoch 3/15: [========                      ] 18/63 batches, loss: 0.5966Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.5956Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.5939Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5935Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.5950Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5934Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5928Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5877Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5895Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5932Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5930Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5966Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5957Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5953Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5945Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5975Epoch 3/15: [================              ] 34/63 batches, loss: 0.5964Epoch 3/15: [================              ] 35/63 batches, loss: 0.5933Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5952Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5954Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5947Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5945Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5927Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5938Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5934Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5928Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5923Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5909Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5901Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5885Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5878Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5862Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5858Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5860Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5846Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5841Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5828Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5823Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5813Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5806Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5810Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5807Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5818Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5810Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5805Epoch 3/15: [==============================] 63/63 batches, loss: 0.5807
[2025-05-02 11:34:27,390][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5807
[2025-05-02 11:34:27,610][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5881, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5643Epoch 4/15: [                              ] 2/63 batches, loss: 0.5590Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5403Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5272Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5441Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5442Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5486Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5506Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5360Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5416Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5496Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5512Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5524Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5524Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5600Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5611Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5658Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5639Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5650Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5631Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5598Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5608Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5617Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5627Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5633Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5651Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5648Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5662Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5677Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5670Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5653Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5652Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5623Epoch 4/15: [================              ] 34/63 batches, loss: 0.5625Epoch 4/15: [================              ] 35/63 batches, loss: 0.5636Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5619Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5624Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5626Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5628Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5610Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5619Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5617Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5607Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5607Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5601Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5583Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5574Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5570Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5573Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5566Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5571Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5575Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5583Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5585Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5584Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5582Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5605Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5589Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5594Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5587Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5585Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5595Epoch 4/15: [==============================] 63/63 batches, loss: 0.5572
[2025-05-02 11:34:29,905][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5572
[2025-05-02 11:34:30,120][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5776, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6005Epoch 5/15: [                              ] 2/63 batches, loss: 0.5591Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5477Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5492Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5828Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5734Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5709Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5551Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5511Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5446Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5388Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5392Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5408Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5373Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5333Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5369Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5392Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5371Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5407Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5439Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5413Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5381Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5359Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5379Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5363Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5383Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5395Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5375Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5402Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5422Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5404Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5387Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5403Epoch 5/15: [================              ] 34/63 batches, loss: 0.5415Epoch 5/15: [================              ] 35/63 batches, loss: 0.5435Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5427Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5422Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5399Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5391Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5398Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5406Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5422Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5426Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5429Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5419Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5415Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5417Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5426Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5422Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5441Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5449Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5455Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5463Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5459Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5457Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5452Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5453Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5459Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5461Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5457Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5455Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5454Epoch 5/15: [==============================] 63/63 batches, loss: 0.5451
[2025-05-02 11:34:32,437][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5451
[2025-05-02 11:34:32,664][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5716, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5094Epoch 6/15: [                              ] 2/63 batches, loss: 0.5258Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5211Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5174Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5098Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5092Epoch 6/15: [===                           ] 7/63 batches, loss: 0.4928Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5053Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5087Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5134Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5174Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5232Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5242Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5269Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5295Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5351Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5346Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5364Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5374Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5382Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5362Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5384Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5366Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5395Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5438Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5443Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5445Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5430Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5426Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5402Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5440Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5436Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5417Epoch 6/15: [================              ] 34/63 batches, loss: 0.5373Epoch 6/15: [================              ] 35/63 batches, loss: 0.5353Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5361Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5362Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5334Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5355Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5370Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5370Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5366Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5380Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5362Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5370Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5372Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5361Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5358Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5372Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5387Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5374Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5367Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5362Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5370Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5381Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5390Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5383Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5391Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5393Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5383Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5387Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5394Epoch 6/15: [==============================] 63/63 batches, loss: 0.5390
[2025-05-02 11:34:34,964][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5390
[2025-05-02 11:34:35,199][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5739, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:34:35,200][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5351Epoch 7/15: [                              ] 2/63 batches, loss: 0.5014Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5537Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5356Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5403Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5334Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5352Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5295Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5284Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5222Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5145Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5165Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5198Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5127Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5186Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5212Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5225Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5217Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5223Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5203Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5230Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5259Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5246Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5232Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5226Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5231Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5245Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5264Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5302Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5345Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5337Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5320Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5302Epoch 7/15: [================              ] 34/63 batches, loss: 0.5313Epoch 7/15: [================              ] 35/63 batches, loss: 0.5330Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5327Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5307Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5320Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5325Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5360Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5364Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5345Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5352Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5338Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5328Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5320Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5331Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5315Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5315Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5296Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5284Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5316Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5317Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5320Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5324Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5314Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5304Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5302Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5283Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5276Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5290Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5299Epoch 7/15: [==============================] 63/63 batches, loss: 0.5320
[2025-05-02 11:34:37,166][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5320
[2025-05-02 11:34:37,390][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5761, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:34:37,390][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6273Epoch 8/15: [                              ] 2/63 batches, loss: 0.5651Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5319Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5022Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5226Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5434Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5389Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5350Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5384Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5347Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5335Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5346Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5337Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5321Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5342Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5376Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5417Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5443Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5449Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5392Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5417Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5381Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5374Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5383Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5377Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5366Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5346Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5330Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5321Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5293Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5293Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5305Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5276Epoch 8/15: [================              ] 34/63 batches, loss: 0.5271Epoch 8/15: [================              ] 35/63 batches, loss: 0.5260Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5255Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5259Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5253Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5267Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5244Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5269Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5280Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5279Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5290Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5302Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5304Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5307Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5296Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5290Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5294Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5295Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5305Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5304Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5304Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5282Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5275Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5267Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5272Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5263Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5259Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5274Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5263Epoch 8/15: [==============================] 63/63 batches, loss: 0.5269
[2025-05-02 11:34:39,317][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5269
[2025-05-02 11:34:39,529][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5742, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:34:39,529][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:34:39,529][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 11:34:39,529][src.training.lm_trainer][INFO] - Training completed in 20.26 seconds
[2025-05-02 11:34:39,530][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:34:41,992][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9869346733668342, 'f1': 0.9868819374369324, 'precision': 0.9898785425101214, 'recall': 0.9839034205231388}
[2025-05-02 11:34:41,992][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:34:41,992][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6103896103896104, 'f1': 0.5714285714285714, 'precision': 0.4166666666666667, 'recall': 0.9090909090909091}
[2025-05-02 11:34:43,622][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar/ar/model.pt
[2025-05-02 11:34:43,623][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▅███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▅▂▁▁
wandb:    best_val_precision ▁▃███
wandb:       best_val_recall ▁████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃
wandb:            train_loss █▅▄▂▂▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▅██████
wandb:                val_f1 ▁▆██████
wandb:              val_loss █▅▂▁▁▁▁▁
wandb:         val_precision ▁▃██████
wandb:            val_recall ▁███████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.57157
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.61039
wandb:         final_test_f1 0.57143
wandb:  final_test_precision 0.41667
wandb:     final_test_recall 0.90909
wandb:  final_train_accuracy 0.98693
wandb:        final_train_f1 0.98688
wandb: final_train_precision 0.98988
wandb:    final_train_recall 0.9839
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52692
wandb:            train_time 20.26453
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57422
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113409-gk015o3j
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113409-gk015o3j/logs
Experiment probe_layer6_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar/results.json
Running experiment: probe_layer6_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:34:54,814][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar
experiment_name: probe_layer6_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:34:54,814][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:34:54,814][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:34:54,814][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:34:54,818][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 11:34:54,819][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:34:56,247][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:34:58,527][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:34:58,527][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:58,568][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,598][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,685][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:34:58,693][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:58,694][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:34:58,695][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:58,716][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,747][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,761][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:34:58,762][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:58,762][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:34:58,763][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:34:58,783][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,815][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:34:58,828][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:34:58,829][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:34:58,830][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:34:58,830][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:34:58,831][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:34:58,831][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:34:58,831][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:34:58,831][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:34:58,832][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:34:58,832][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:34:58,832][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:34:58,833][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:34:58,833][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:34:58,833][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:34:58,833][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:34:58,834][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:34:58,834][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:34:58,834][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:34:58,834][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:34:58,834][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:34:58,834][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:34:58,834][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:34:58,835][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:35:02,949][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:35:02,950][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:35:02,950][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 11:35:02,950][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:35:02,954][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:35:02,955][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:35:02,955][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:35:02,955][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:35:02,955][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:35:02,956][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:35:02,956][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 1.2258Epoch 1/15: [                              ] 2/63 batches, loss: 0.9837Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7821Epoch 1/15: [=                             ] 4/63 batches, loss: 0.8226Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7633Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6891Epoch 1/15: [===                           ] 7/63 batches, loss: 0.6665Epoch 1/15: [===                           ] 8/63 batches, loss: 0.6340Epoch 1/15: [====                          ] 9/63 batches, loss: 0.5875Epoch 1/15: [====                          ] 10/63 batches, loss: 0.5509Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.5259Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.5028Epoch 1/15: [======                        ] 13/63 batches, loss: 0.4776Epoch 1/15: [======                        ] 14/63 batches, loss: 0.4578Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.4407Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.4279Epoch 1/15: [========                      ] 17/63 batches, loss: 0.4166Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4043Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.3982Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.3870Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.3820Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.3772Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.3707Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.3642Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.3601Epoch 1/15: [============                  ] 26/63 batches, loss: 0.3600Epoch 1/15: [============                  ] 27/63 batches, loss: 0.3594Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.3570Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.3512Epoch 1/15: [==============                ] 30/63 batches, loss: 0.3483Epoch 1/15: [==============                ] 31/63 batches, loss: 0.3424Epoch 1/15: [===============               ] 32/63 batches, loss: 0.3392Epoch 1/15: [===============               ] 33/63 batches, loss: 0.3338Epoch 1/15: [================              ] 34/63 batches, loss: 0.3302Epoch 1/15: [================              ] 35/63 batches, loss: 0.3279Epoch 1/15: [=================             ] 36/63 batches, loss: 0.3267Epoch 1/15: [=================             ] 37/63 batches, loss: 0.3230Epoch 1/15: [==================            ] 38/63 batches, loss: 0.3205Epoch 1/15: [==================            ] 39/63 batches, loss: 0.3180Epoch 1/15: [===================           ] 40/63 batches, loss: 0.3118Epoch 1/15: [===================           ] 41/63 batches, loss: 0.3063Epoch 1/15: [====================          ] 42/63 batches, loss: 0.3026Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2995Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2948Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2925Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2915Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2927Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2939Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2910Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2912Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2903Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2910Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2886Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2874Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2845Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2830Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2809Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2805Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2813Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2806Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2778Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2754Epoch 1/15: [==============================] 63/63 batches, loss: 0.2730
[2025-05-02 11:35:07,449][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2730
[2025-05-02 11:35:07,639][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0854, Metrics: {'mse': 0.08553603291511536, 'rmse': 0.2924654388387034, 'r2': -0.3183947801589966}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2156Epoch 2/15: [                              ] 2/63 batches, loss: 0.2181Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2068Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2017Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1836Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1720Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1646Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1612Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1595Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1659Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1723Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1659Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1712Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1701Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1691Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1655Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1612Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1574Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1591Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1617Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1584Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1588Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1574Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1604Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1595Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1562Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1528Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1537Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1545Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1544Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1531Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1523Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1517Epoch 2/15: [================              ] 34/63 batches, loss: 0.1498Epoch 2/15: [================              ] 35/63 batches, loss: 0.1491Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1466Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1457Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1461Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1466Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1472Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1460Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1454Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1476Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1487Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1477Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1494Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1483Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1472Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1456Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1460Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1468Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1472Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1483Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1481Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1487Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1488Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1478Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1487Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1477Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1471Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1475Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1464Epoch 2/15: [==============================] 63/63 batches, loss: 0.1464
[2025-05-02 11:35:09,959][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1464
[2025-05-02 11:35:10,166][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0583, Metrics: {'mse': 0.05829407274723053, 'rmse': 0.24144165495462983, 'r2': 0.1014944314956665}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1080Epoch 3/15: [                              ] 2/63 batches, loss: 0.1378Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1316Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1329Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1292Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1245Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1291Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1405Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1396Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1450Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1430Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1462Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1457Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1525Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1506Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1490Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1478Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1472Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1461Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1461Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1479Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1464Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1458Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1437Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1452Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1437Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1406Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1438Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1431Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1428Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1483Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1492Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1487Epoch 3/15: [================              ] 34/63 batches, loss: 0.1481Epoch 3/15: [================              ] 35/63 batches, loss: 0.1459Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1443Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1440Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1443Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1440Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1436Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1443Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1423Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1425Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1421Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1402Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1406Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1406Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1400Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1397Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1413Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1426Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1442Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1444Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1435Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1427Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1428Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1415Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1417Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1412Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1411Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1399Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1398Epoch 3/15: [==============================] 63/63 batches, loss: 0.1392
[2025-05-02 11:35:12,518][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1392
[2025-05-02 11:35:12,728][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0490, Metrics: {'mse': 0.04927320405840874, 'rmse': 0.22197568348449506, 'r2': 0.24053603410720825}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1078Epoch 4/15: [                              ] 2/63 batches, loss: 0.1052Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1046Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1158Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1485Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1455Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1449Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1335Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1309Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1271Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1298Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1259Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1323Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1305Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1288Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1277Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1244Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1259Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1253Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1238Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1259Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1282Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1309Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1277Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1285Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1273Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1271Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1266Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1301Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1276Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1277Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1267Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1273Epoch 4/15: [================              ] 34/63 batches, loss: 0.1294Epoch 4/15: [================              ] 35/63 batches, loss: 0.1296Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1280Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1280Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1259Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1261Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1256Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1241Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1243Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1228Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1216Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1219Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1211Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1201Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1193Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1219Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1204Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1195Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1181Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1181Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1175Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1171Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1170Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1164Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1165Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1157Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1157Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1161Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1157Epoch 4/15: [==============================] 63/63 batches, loss: 0.1158
[2025-05-02 11:35:15,022][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1158
[2025-05-02 11:35:15,236][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0499, Metrics: {'mse': 0.05031634867191315, 'rmse': 0.2243130595215382, 'r2': 0.22445768117904663}
[2025-05-02 11:35:15,237][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.2357Epoch 5/15: [                              ] 2/63 batches, loss: 0.1580Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1339Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1221Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1245Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1143Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1092Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1146Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1152Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1102Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1080Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1087Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1057Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1042Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1044Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1062Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1069Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1043Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1035Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1033Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1019Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0999Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0993Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1013Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0997Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0996Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0988Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1004Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1002Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1039Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1021Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1028Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1020Epoch 5/15: [================              ] 34/63 batches, loss: 0.1047Epoch 5/15: [================              ] 35/63 batches, loss: 0.1036Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1035Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1032Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1028Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1020Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1013Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1019Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1017Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1032Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1024Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1027Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1043Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1040Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1035Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1034Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1029Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1016Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1007Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.1006Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.1007Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.1002Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0994Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0999Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0992Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0998Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1000Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0993Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0985Epoch 5/15: [==============================] 63/63 batches, loss: 0.0981
[2025-05-02 11:35:17,164][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0981
[2025-05-02 11:35:17,381][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0632, Metrics: {'mse': 0.06362875550985336, 'rmse': 0.25224740932238204, 'r2': 0.019269287586212158}
[2025-05-02 11:35:17,382][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1493Epoch 6/15: [                              ] 2/63 batches, loss: 0.1800Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1438Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1339Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1286Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1244Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1270Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1149Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1156Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1126Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1069Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1100Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1047Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1023Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1015Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0982Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0971Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0972Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1037Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1026Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1013Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0996Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1006Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1010Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1013Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1014Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1011Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1001Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0986Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0986Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0981Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0971Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0957Epoch 6/15: [================              ] 34/63 batches, loss: 0.0948Epoch 6/15: [================              ] 35/63 batches, loss: 0.0948Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0930Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0924Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0931Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0934Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0942Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0940Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0941Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0934Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0935Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0938Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0941Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0939Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0934Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0930Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0922Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0925Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0918Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0919Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0915Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0912Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0903Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0916Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0912Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0918Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0917Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0913Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0912Epoch 6/15: [==============================] 63/63 batches, loss: 0.0927
[2025-05-02 11:35:19,307][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0927
[2025-05-02 11:35:19,504][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0472, Metrics: {'mse': 0.047734931111335754, 'rmse': 0.21848325132910246, 'r2': 0.2642459273338318}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0733Epoch 7/15: [                              ] 2/63 batches, loss: 0.1016Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0877Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0927Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0856Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0871Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0948Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0893Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0902Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0860Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0867Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0824Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0847Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0872Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0829Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0832Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0826Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0820Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0824Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0813Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0788Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0789Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0790Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0779Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0763Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0756Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0756Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0769Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0770Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0768Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0790Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0799Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0816Epoch 7/15: [================              ] 34/63 batches, loss: 0.0806Epoch 7/15: [================              ] 35/63 batches, loss: 0.0802Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0802Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0802Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0808Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0804Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0806Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0797Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0799Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0793Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0795Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0803Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0794Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0794Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0789Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0806Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0801Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0800Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0797Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0802Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0799Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0793Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0797Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0794Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0797Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0796Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0791Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0783Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0782Epoch 7/15: [==============================] 63/63 batches, loss: 0.0779
[2025-05-02 11:35:21,845][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0779
[2025-05-02 11:35:22,036][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0395, Metrics: {'mse': 0.03998734429478645, 'rmse': 0.19996835823396275, 'r2': 0.38366204500198364}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0606Epoch 8/15: [                              ] 2/63 batches, loss: 0.0597Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0658Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0676Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0686Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0628Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0698Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0682Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0683Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0676Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0660Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0694Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0674Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0698Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0705Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0704Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0697Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0725Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0720Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0722Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0742Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0750Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0742Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0727Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0721Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0700Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0698Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0703Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0712Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0723Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0738Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0737Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0755Epoch 8/15: [================              ] 34/63 batches, loss: 0.0768Epoch 8/15: [================              ] 35/63 batches, loss: 0.0778Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0779Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0785Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0785Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0783Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0786Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0780Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0777Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0781Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0794Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0788Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0784Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0784Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0778Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0771Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0764Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0761Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0772Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0766Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0770Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0761Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0756Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0766Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0766Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0761Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0752Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0748Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0750Epoch 8/15: [==============================] 63/63 batches, loss: 0.0739
[2025-05-02 11:35:24,406][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0739
[2025-05-02 11:35:24,609][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0478, Metrics: {'mse': 0.048358798027038574, 'rmse': 0.2199063392152181, 'r2': 0.25463008880615234}
[2025-05-02 11:35:24,610][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1328Epoch 9/15: [                              ] 2/63 batches, loss: 0.0833Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0717Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0675Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0621Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0588Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0627Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0604Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0608Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0609Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0631Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0629Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0614Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0648Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0628Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0636Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0629Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0638Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0627Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0621Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0606Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0624Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0623Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0628Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0615Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0608Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0632Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0625Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0614Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0620Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0637Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0678Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0681Epoch 9/15: [================              ] 34/63 batches, loss: 0.0677Epoch 9/15: [================              ] 35/63 batches, loss: 0.0667Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0665Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0659Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0658Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0658Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0651Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0647Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0652Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0655Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0658Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0658Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0664Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0669Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0670Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0678Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0676Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0677Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0675Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0673Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0669Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0674Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0676Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0669Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0668Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0665Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0664Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0659Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0658Epoch 9/15: [==============================] 63/63 batches, loss: 0.0651
[2025-05-02 11:35:26,538][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0651
[2025-05-02 11:35:26,754][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0514, Metrics: {'mse': 0.05176516994833946, 'rmse': 0.2275196034374609, 'r2': 0.20212656259536743}
[2025-05-02 11:35:26,755][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1316Epoch 10/15: [                              ] 2/63 batches, loss: 0.1048Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0895Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0837Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0815Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0780Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0748Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0693Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0749Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0753Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0720Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0765Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0745Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0725Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0738Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0720Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0714Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0709Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0701Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0734Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0726Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0712Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0708Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0694Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0690Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0690Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0697Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0678Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0674Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0672Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0665Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0660Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0653Epoch 10/15: [================              ] 34/63 batches, loss: 0.0653Epoch 10/15: [================              ] 35/63 batches, loss: 0.0655Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0650Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0646Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0641Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0635Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0630Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0626Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0625Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0624Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0633Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0626Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0626Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0635Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0640Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0633Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0626Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0619Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0619Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0631Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0635Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0630Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0628Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0628Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0628Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0622Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0621Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0626Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0630Epoch 10/15: [==============================] 63/63 batches, loss: 0.0633
[2025-05-02 11:35:28,681][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0633
[2025-05-02 11:35:28,900][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0520, Metrics: {'mse': 0.052332840859889984, 'rmse': 0.22876372277939958, 'r2': 0.19337689876556396}
[2025-05-02 11:35:28,901][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0750Epoch 11/15: [                              ] 2/63 batches, loss: 0.0668Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0649Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0565Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0525Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0579Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0573Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0585Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0579Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0599Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0568Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0588Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0595Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0593Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0575Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0570Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0575Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0567Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0575Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0584Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0591Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0574Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0571Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0574Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0567Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0555Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0562Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0558Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0547Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0550Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0542Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0560Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0563Epoch 11/15: [================              ] 34/63 batches, loss: 0.0570Epoch 11/15: [================              ] 35/63 batches, loss: 0.0573Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0573Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0574Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0572Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0569Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0561Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0567Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0567Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0564Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0560Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0565Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0561Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0567Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0570Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0563Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0570Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0574Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0571Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0567Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0562Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0562Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0568Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0570Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0566Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0567Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0562Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0562Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0558Epoch 11/15: [==============================] 63/63 batches, loss: 0.0555
[2025-05-02 11:35:30,858][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0555
[2025-05-02 11:35:31,081][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0585, Metrics: {'mse': 0.05890987813472748, 'rmse': 0.2427135722095645, 'r2': 0.09200286865234375}
[2025-05-02 11:35:31,082][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:35:31,082][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-02 11:35:31,082][src.training.lm_trainer][INFO] - Training completed in 26.45 seconds
[2025-05-02 11:35:31,082][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:35:33,575][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.025891272351145744, 'rmse': 0.16090765162398507, 'r2': 0.1565636396408081}
[2025-05-02 11:35:33,576][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03998734429478645, 'rmse': 0.19996835823396275, 'r2': 0.38366204500198364}
[2025-05-02 11:35:33,576][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06517498195171356, 'rmse': 0.2552939128763425, 'r2': -0.1235877275466919}
[2025-05-02 11:35:35,226][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar/ar/model.pt
[2025-05-02 11:35:35,227][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▁
wandb:     best_val_mse █▄▂▂▁
wandb:      best_val_r2 ▁▅▇▇█
wandb:    best_val_rmse █▄▃▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▆▆▄▆▇▆▅▅
wandb:       train_loss █▄▄▃▂▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▃▅▂▁▂▃▃▄
wandb:          val_mse █▄▂▃▅▂▁▂▃▃▄
wandb:           val_r2 ▁▅▇▆▄▇█▇▆▆▅
wandb:         val_rmse █▄▃▃▅▂▁▃▃▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03949
wandb:     best_val_mse 0.03999
wandb:      best_val_r2 0.38366
wandb:    best_val_rmse 0.19997
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.06517
wandb:    final_test_r2 -0.12359
wandb:  final_test_rmse 0.25529
wandb:  final_train_mse 0.02589
wandb:   final_train_r2 0.15656
wandb: final_train_rmse 0.16091
wandb:    final_val_mse 0.03999
wandb:     final_val_r2 0.38366
wandb:   final_val_rmse 0.19997
wandb:    learning_rate 2e-05
wandb:       train_loss 0.05551
wandb:       train_time 26.45175
wandb:         val_loss 0.05853
wandb:          val_mse 0.05891
wandb:           val_r2 0.092
wandb:         val_rmse 0.24271
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113454-5ky3jyso
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113454-5ky3jyso/logs
Experiment probe_layer6_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar/results.json
=======================
PROBING LAYER 9
=======================
Running experiment: probe_layer9_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:35:48,479][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar
experiment_name: probe_layer9_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:35:48,479][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:35:48,479][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:35:48,479][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:35:48,483][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 11:35:48,483][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:35:50,082][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:35:52,347][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:35:52,347][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:35:52,400][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,428][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,517][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:35:52,524][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:35:52,524][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:35:52,525][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:35:52,545][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,586][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:35:52,587][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:35:52,587][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:35:52,588][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:35:52,608][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,639][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:35:52,651][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:35:52,653][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:35:52,653][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:35:52,654][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:35:52,654][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:35:52,654][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:35:52,654][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:35:52,655][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 11:35:52,655][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:35:52,655][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:35:52,656][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 11:35:52,656][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:35:52,656][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:35:52,656][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 11:35:52,656][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 11:35:52,657][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:35:52,657][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:35:52,657][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:35:52,657][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:35:52,657][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:35:52,657][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:35:52,657][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:35:56,729][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:35:56,730][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:35:56,730][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 11:35:56,730][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:35:56,736][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:35:56,737][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:35:56,737][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:35:56,737][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:35:56,737][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:35:56,738][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:35:56,738][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7300Epoch 1/15: [                              ] 2/63 batches, loss: 0.6967Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7150Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7223Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7242Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7215Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7182Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7128Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7086Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7066Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7064Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7045Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7026Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7013Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7007Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7007Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7007Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6993Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6992Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6985Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6981Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6980Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6980Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6973Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6964Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6959Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6956Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6952Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6942Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6930Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6923Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 1/15: [================              ] 34/63 batches, loss: 0.6926Epoch 1/15: [================              ] 35/63 batches, loss: 0.6925Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6918Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6916Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6915Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6902Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6883Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6895Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6902Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6898Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6872Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6876Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6874Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6865Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6855Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6844Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6831Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6832Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6810Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6812Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6806Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6790Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6782Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6785Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6781Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6778Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6764Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6758Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6747Epoch 1/15: [==============================] 63/63 batches, loss: 0.6759
[2025-05-02 11:36:01,376][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6759
[2025-05-02 11:36:01,574][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6573, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8636363636363636, 'precision': 0.7916666666666666, 'recall': 0.95}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6242Epoch 2/15: [                              ] 2/63 batches, loss: 0.6423Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6526Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6632Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6518Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6498Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6502Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6447Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6490Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6472Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6487Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6491Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6425Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6410Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6408Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6435Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6435Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6423Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6419Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6426Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6432Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6403Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6371Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6369Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6403Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6381Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6370Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6351Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6332Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6339Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6371Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6372Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6363Epoch 2/15: [================              ] 34/63 batches, loss: 0.6356Epoch 2/15: [================              ] 35/63 batches, loss: 0.6326Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6322Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6306Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6296Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6292Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6284Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6278Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6269Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6269Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6277Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6264Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6252Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6229Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6224Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6229Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6204Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6208Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6188Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6179Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6176Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6175Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6170Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6166Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6142Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6138Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6137Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6131Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6117Epoch 2/15: [==============================] 63/63 batches, loss: 0.6114
[2025-05-02 11:36:03,921][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6114
[2025-05-02 11:36:04,127][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6078, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888, 'precision': 0.8, 'recall': 1.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5497Epoch 3/15: [                              ] 2/63 batches, loss: 0.5640Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5467Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5594Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5686Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5702Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5686Epoch 3/15: [===                           ] 8/63 batches, loss: 0.5858Epoch 3/15: [====                          ] 9/63 batches, loss: 0.5784Epoch 3/15: [====                          ] 10/63 batches, loss: 0.5823Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5803Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.5855Epoch 3/15: [======                        ] 13/63 batches, loss: 0.5915Epoch 3/15: [======                        ] 14/63 batches, loss: 0.5883Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.5857Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.5841Epoch 3/15: [========                      ] 17/63 batches, loss: 0.5829Epoch 3/15: [========                      ] 18/63 batches, loss: 0.5811Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.5809Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.5797Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5790Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.5811Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5798Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5799Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5748Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5764Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5796Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5795Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5829Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5819Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5827Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5821Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5851Epoch 3/15: [================              ] 34/63 batches, loss: 0.5843Epoch 3/15: [================              ] 35/63 batches, loss: 0.5803Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5825Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5820Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5814Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5810Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5795Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5807Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5804Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5799Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5795Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5779Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5771Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5754Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5745Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5732Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5726Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5727Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5711Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5712Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5691Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5684Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5679Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5672Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5675Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5674Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5688Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5679Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5680Epoch 3/15: [==============================] 63/63 batches, loss: 0.5683
[2025-05-02 11:36:06,492][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5683
[2025-05-02 11:36:06,723][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5822, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5351Epoch 4/15: [                              ] 2/63 batches, loss: 0.5313Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5180Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5055Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5225Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5242Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5298Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5323Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5181Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5264Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5332Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5347Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5363Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5367Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5444Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5454Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5496Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5478Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5500Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5481Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5443Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5449Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5466Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5487Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5492Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5509Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5490Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5501Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5516Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5505Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5489Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5492Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5469Epoch 4/15: [================              ] 34/63 batches, loss: 0.5472Epoch 4/15: [================              ] 35/63 batches, loss: 0.5477Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5462Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5472Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5475Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5473Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5458Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5466Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5464Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5459Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5463Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5458Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5445Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5433Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5430Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5432Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5424Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5430Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5436Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5446Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5447Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5448Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5454Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5477Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5461Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5467Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5463Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5462Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5469Epoch 4/15: [==============================] 63/63 batches, loss: 0.5447
[2025-05-02 11:36:09,033][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5447
[2025-05-02 11:36:09,254][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5748, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5944Epoch 5/15: [                              ] 2/63 batches, loss: 0.5437Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5401Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5436Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5696Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5635Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5631Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5503Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5479Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5399Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5332Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5318Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5349Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5312Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5259Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5294Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5297Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5280Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5316Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5351Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5332Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5310Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5282Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5324Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5296Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5317Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5328Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5302Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5320Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5325Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5304Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5286Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5299Epoch 5/15: [================              ] 34/63 batches, loss: 0.5315Epoch 5/15: [================              ] 35/63 batches, loss: 0.5329Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5326Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5322Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5302Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5297Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5302Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5303Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5319Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5319Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5320Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5319Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5316Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5317Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5328Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5323Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5343Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5346Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5351Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5357Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5358Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5359Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5349Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5351Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5355Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5356Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5355Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5353Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5353Epoch 5/15: [==============================] 63/63 batches, loss: 0.5343
[2025-05-02 11:36:11,687][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5343
[2025-05-02 11:36:11,901][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5721, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5096Epoch 6/15: [                              ] 2/63 batches, loss: 0.5197Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5144Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5110Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5053Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5070Epoch 6/15: [===                           ] 7/63 batches, loss: 0.4927Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5028Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5071Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5134Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5132Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5172Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5184Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5191Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5206Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5259Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5258Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5280Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5297Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5309Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5285Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5310Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5289Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5319Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5373Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5358Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5359Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5346Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5341Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5318Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5348Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5339Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5324Epoch 6/15: [================              ] 34/63 batches, loss: 0.5281Epoch 6/15: [================              ] 35/63 batches, loss: 0.5268Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5272Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5274Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5248Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5265Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5282Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5281Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5275Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5289Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5272Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5279Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5281Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5273Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5272Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5298Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5314Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5305Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5297Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5293Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5307Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5315Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5320Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5314Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5326Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5331Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5320Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5324Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5333Epoch 6/15: [==============================] 63/63 batches, loss: 0.5319
[2025-05-02 11:36:14,217][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5319
[2025-05-02 11:36:14,431][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5750, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:36:14,431][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.4891Epoch 7/15: [                              ] 2/63 batches, loss: 0.4827Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5385Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5214Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5241Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5223Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5247Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5195Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5199Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5119Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5026Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5057Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5082Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5023Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5080Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5119Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5136Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5130Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5140Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5123Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5135Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5164Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5154Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5142Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5141Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5146Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5161Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5182Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5227Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5266Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5257Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5243Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5225Epoch 7/15: [================              ] 34/63 batches, loss: 0.5241Epoch 7/15: [================              ] 35/63 batches, loss: 0.5265Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5260Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5237Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5254Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5261Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5283Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5293Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5276Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5284Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5280Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5268Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5263Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5276Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5261Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5262Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5243Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5234Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5268Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5277Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5280Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5281Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5275Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5265Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5263Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5245Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5238Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5249Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5255Epoch 7/15: [==============================] 63/63 batches, loss: 0.5260
[2025-05-02 11:36:16,386][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5260
[2025-05-02 11:36:16,595][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5778, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:36:16,595][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5774Epoch 8/15: [                              ] 2/63 batches, loss: 0.5382Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5138Epoch 8/15: [=                             ] 4/63 batches, loss: 0.4901Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5138Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5367Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5295Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5282Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5314Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5271Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5271Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5275Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5261Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5249Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5282Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5325Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5364Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5392Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5412Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5347Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5370Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5334Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5331Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5341Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5331Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5314Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5296Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5289Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5282Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5267Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5268Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5277Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5252Epoch 8/15: [================              ] 34/63 batches, loss: 0.5249Epoch 8/15: [================              ] 35/63 batches, loss: 0.5237Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5223Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5234Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5232Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5249Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5222Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5242Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5249Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5246Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5255Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5262Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5263Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5266Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5254Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5249Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5251Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5253Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5266Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5267Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5273Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5251Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5252Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5246Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5252Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5246Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5242Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5249Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5238Epoch 8/15: [==============================] 63/63 batches, loss: 0.5245
[2025-05-02 11:36:18,530][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5245
[2025-05-02 11:36:18,733][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5755, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:36:18,733][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:36:18,733][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 11:36:18,734][src.training.lm_trainer][INFO] - Training completed in 20.15 seconds
[2025-05-02 11:36:18,734][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:36:21,183][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.992964824120603, 'f1': 0.992964824120603, 'precision': 0.9919678714859438, 'recall': 0.993963782696177}
[2025-05-02 11:36:21,183][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 11:36:21,183][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6623376623376623, 'f1': 0.6285714285714286, 'precision': 0.4583333333333333, 'recall': 1.0}
[2025-05-02 11:36:22,840][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar/ar/model.pt
[2025-05-02 11:36:22,841][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▃▆▆█
wandb:           best_val_f1 ▁▄▆▆█
wandb:         best_val_loss █▄▂▁▁
wandb:    best_val_precision ▁▂▅▅█
wandb:       best_val_recall ▁████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▃▃▃▃▃
wandb:            train_loss █▅▃▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▃▆▆█▆▆▆
wandb:                val_f1 ▁▄▆▆█▆▆▆
wandb:              val_loss █▄▂▁▁▁▁▁
wandb:         val_precision ▁▂▅▅█▅▅▅
wandb:            val_recall ▁███████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.57207
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.66234
wandb:         final_test_f1 0.62857
wandb:  final_test_precision 0.45833
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99296
wandb:        final_train_f1 0.99296
wandb: final_train_precision 0.99197
wandb:    final_train_recall 0.99396
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52452
wandb:            train_time 20.14758
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57547
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113548-wr4775vk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113548-wr4775vk/logs
Experiment probe_layer9_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar/results.json
Running experiment: probe_layer9_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:36:33,779][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar
experiment_name: probe_layer9_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:36:33,779][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:36:33,779][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:36:33,779][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:36:33,784][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 11:36:33,784][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:36:35,348][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:36:37,592][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:36:37,593][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:36:37,640][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,689][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,762][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:36:37,770][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:36:37,770][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:36:37,771][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:36:37,791][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,821][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,932][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:36:37,934][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:36:37,934][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:36:37,935][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:36:37,955][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,984][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:36:37,996][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:36:37,998][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:36:37,998][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:36:37,999][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:36:37,999][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:36:37,999][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:36:38,000][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:36:38,000][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:36:38,000][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:36:38,000][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:36:38,000][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:36:38,000][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:36:38,000][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:36:38,001][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:36:38,001][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:36:38,001][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:36:38,002][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:36:38,002][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:36:38,002][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:36:38,003][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:36:38,003][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:36:41,977][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:36:41,978][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:36:41,978][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 11:36:41,978][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:36:41,982][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:36:41,982][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:36:41,982][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:36:41,982][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:36:41,983][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:36:41,983][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:36:41,983][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7822Epoch 1/15: [                              ] 2/63 batches, loss: 0.5895Epoch 1/15: [=                             ] 3/63 batches, loss: 0.4799Epoch 1/15: [=                             ] 4/63 batches, loss: 0.5117Epoch 1/15: [==                            ] 5/63 batches, loss: 0.5189Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4628Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4506Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4250Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4055Epoch 1/15: [====                          ] 10/63 batches, loss: 0.3822Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.3652Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.3493Epoch 1/15: [======                        ] 13/63 batches, loss: 0.3361Epoch 1/15: [======                        ] 14/63 batches, loss: 0.3231Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.3136Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.3047Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2990Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2892Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2864Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2785Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2792Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2762Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2720Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2669Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2688Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2646Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2684Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2696Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2683Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2668Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2611Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2582Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2538Epoch 1/15: [================              ] 34/63 batches, loss: 0.2508Epoch 1/15: [================              ] 35/63 batches, loss: 0.2494Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2492Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2492Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2495Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2499Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2465Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2429Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2401Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2384Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2350Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2330Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2335Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2344Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2348Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2337Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2352Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2338Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2362Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2364Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2363Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2333Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2329Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2319Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2319Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2341Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2332Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2314Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2300Epoch 1/15: [==============================] 63/63 batches, loss: 0.2333
[2025-05-02 11:36:46,707][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2333
[2025-05-02 11:36:46,885][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0975, Metrics: {'mse': 0.09753081947565079, 'rmse': 0.3122992466780072, 'r2': -0.5032743215560913}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.3302Epoch 2/15: [                              ] 2/63 batches, loss: 0.2775Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2591Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2161Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1968Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1867Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1867Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1772Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1727Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1642Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1673Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1636Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1686Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1647Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1636Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1615Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1584Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1576Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1591Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1622Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1584Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1587Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1593Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1609Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1584Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1566Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1534Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1543Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1572Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1561Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1540Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1539Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1536Epoch 2/15: [================              ] 34/63 batches, loss: 0.1522Epoch 2/15: [================              ] 35/63 batches, loss: 0.1525Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1535Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1517Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1497Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1515Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1525Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1515Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1507Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1501Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1516Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1507Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1528Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1515Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1504Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1491Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1505Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1521Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1525Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1548Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1549Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1546Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1539Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1539Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1542Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1533Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1522Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1537Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1528Epoch 2/15: [==============================] 63/63 batches, loss: 0.1534
[2025-05-02 11:36:49,227][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1534
[2025-05-02 11:36:49,424][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0630, Metrics: {'mse': 0.06309663504362106, 'rmse': 0.25119043581239525, 'r2': 0.027471065521240234}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0618Epoch 3/15: [                              ] 2/63 batches, loss: 0.0633Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0829Epoch 3/15: [=                             ] 4/63 batches, loss: 0.0974Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1042Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1050Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1176Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1268Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1360Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1338Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1315Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1313Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1340Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1378Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1342Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1367Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1336Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1325Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1329Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1344Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1347Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1338Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1341Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1323Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1327Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1320Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1307Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1302Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1277Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1267Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1329Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1340Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1331Epoch 3/15: [================              ] 34/63 batches, loss: 0.1323Epoch 3/15: [================              ] 35/63 batches, loss: 0.1314Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1299Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1312Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1309Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1305Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1320Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1320Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1303Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1302Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1300Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1286Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1281Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1278Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1281Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1285Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1297Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1306Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1328Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1320Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1318Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1315Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1322Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1312Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1334Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1342Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1343Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1338Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1331Epoch 3/15: [==============================] 63/63 batches, loss: 0.1318
[2025-05-02 11:36:51,756][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1318
[2025-05-02 11:36:51,967][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0499, Metrics: {'mse': 0.050014857202768326, 'rmse': 0.22364001699778224, 'r2': 0.22910469770431519}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0771Epoch 4/15: [                              ] 2/63 batches, loss: 0.0754Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0832Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1146Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1494Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1438Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1385Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1277Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1296Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1223Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1225Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1204Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1233Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1238Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1178Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1157Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1133Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1142Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1140Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1115Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1126Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1140Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1168Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1135Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1141Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1154Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1167Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1156Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1166Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1141Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1134Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1119Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1126Epoch 4/15: [================              ] 34/63 batches, loss: 0.1138Epoch 4/15: [================              ] 35/63 batches, loss: 0.1142Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1135Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1124Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1129Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1119Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1113Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1101Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1112Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1100Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1098Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1091Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1095Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1081Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1083Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1105Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1118Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1114Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1099Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1099Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1094Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1091Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1098Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1097Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1094Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1094Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1093Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1087Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1076Epoch 4/15: [==============================] 63/63 batches, loss: 0.1083
[2025-05-02 11:36:54,241][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1083
[2025-05-02 11:36:54,456][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0553, Metrics: {'mse': 0.05483943596482277, 'rmse': 0.23417821411229262, 'r2': 0.15474188327789307}
[2025-05-02 11:36:54,457][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1739Epoch 5/15: [                              ] 2/63 batches, loss: 0.1250Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1148Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1115Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1047Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0971Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0957Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0960Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0941Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0931Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0916Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0970Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0949Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0947Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0936Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0944Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0948Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0945Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0950Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0953Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0955Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0945Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0929Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0941Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0949Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0953Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0946Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0955Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0966Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0983Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0966Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0971Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0961Epoch 5/15: [================              ] 34/63 batches, loss: 0.0980Epoch 5/15: [================              ] 35/63 batches, loss: 0.0976Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0979Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0986Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0991Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0991Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0988Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0991Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0984Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1008Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1001Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1005Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1020Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1017Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1009Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1000Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0988Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0985Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0978Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0984Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0974Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0967Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0964Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0972Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0967Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0969Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0965Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0962Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0956Epoch 5/15: [==============================] 63/63 batches, loss: 0.0956
[2025-05-02 11:36:56,390][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0956
[2025-05-02 11:36:56,604][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0667, Metrics: {'mse': 0.06589438766241074, 'rmse': 0.2566990215454877, 'r2': -0.015651702880859375}
[2025-05-02 11:36:56,605][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0913Epoch 6/15: [                              ] 2/63 batches, loss: 0.1505Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1460Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1245Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1260Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1170Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1155Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1045Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1065Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1005Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0971Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0985Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0934Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0920Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0914Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0919Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0913Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0933Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0999Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0981Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1007Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0985Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0974Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0967Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0964Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0965Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0960Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0960Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0948Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0959Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0955Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0941Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0931Epoch 6/15: [================              ] 34/63 batches, loss: 0.0926Epoch 6/15: [================              ] 35/63 batches, loss: 0.0927Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0914Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0913Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0934Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0933Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0946Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0948Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0950Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0946Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0948Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0953Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0946Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0940Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0930Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0929Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0922Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0921Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0919Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0921Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0917Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0909Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0901Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0904Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0905Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0911Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0921Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0914Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0914Epoch 6/15: [==============================] 63/63 batches, loss: 0.0919
[2025-05-02 11:36:58,529][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0919
[2025-05-02 11:36:58,736][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0518, Metrics: {'mse': 0.05105791240930557, 'rmse': 0.22595997966300488, 'r2': 0.21302777528762817}
[2025-05-02 11:36:58,736][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0904Epoch 7/15: [                              ] 2/63 batches, loss: 0.0796Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0679Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0676Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0635Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0626Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0708Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0697Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0719Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0684Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0686Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0694Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0711Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0734Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0707Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0691Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0678Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0662Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0652Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0649Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0648Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0637Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0642Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0639Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0629Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0619Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0627Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0638Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0642Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0636Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0641Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0646Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0655Epoch 7/15: [================              ] 34/63 batches, loss: 0.0648Epoch 7/15: [================              ] 35/63 batches, loss: 0.0649Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0645Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0646Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0656Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0653Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0654Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0652Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0655Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0649Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0646Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0648Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0657Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0653Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0654Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0672Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0666Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0673Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0673Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0690Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0689Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0683Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0691Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0688Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0690Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0689Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0685Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0683Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0682Epoch 7/15: [==============================] 63/63 batches, loss: 0.0678
[2025-05-02 11:37:00,674][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0678
[2025-05-02 11:37:00,887][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0430, Metrics: {'mse': 0.04231855273246765, 'rmse': 0.20571473630361936, 'r2': 0.3477303385734558}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0395Epoch 8/15: [                              ] 2/63 batches, loss: 0.0602Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0625Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0672Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0686Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0626Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0626Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0642Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0623Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0608Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0607Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0602Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0587Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0596Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0622Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0632Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0630Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0638Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0636Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0651Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0690Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0686Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0673Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0668Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0666Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0654Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0664Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0661Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0669Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0688Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0708Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0709Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0735Epoch 8/15: [================              ] 34/63 batches, loss: 0.0744Epoch 8/15: [================              ] 35/63 batches, loss: 0.0749Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0747Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0747Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0751Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0742Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0748Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0745Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0754Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0757Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0769Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0759Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0756Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0757Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0749Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0739Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0731Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0730Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0733Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0725Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0726Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0720Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0719Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0721Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0730Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0728Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0724Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0722Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0731Epoch 8/15: [==============================] 63/63 batches, loss: 0.0726
[2025-05-02 11:37:03,252][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0726
[2025-05-02 11:37:03,464][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0459, Metrics: {'mse': 0.0450984388589859, 'rmse': 0.21236393022117928, 'r2': 0.30488306283950806}
[2025-05-02 11:37:03,465][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1339Epoch 9/15: [                              ] 2/63 batches, loss: 0.0852Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0731Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0821Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0751Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0698Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0700Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0694Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0647Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0656Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0647Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0633Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0608Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0633Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0613Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0617Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0614Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0653Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0649Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0638Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0622Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0650Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0650Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0654Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0636Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0631Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0636Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0628Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0626Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0633Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0644Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0673Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0674Epoch 9/15: [================              ] 34/63 batches, loss: 0.0670Epoch 9/15: [================              ] 35/63 batches, loss: 0.0660Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0654Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0644Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0646Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0648Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0649Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0645Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0658Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0657Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0653Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0651Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0645Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0653Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0657Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0661Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0658Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0657Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0657Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0658Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0653Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0661Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0660Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0651Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0651Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0644Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0652Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0648Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0645Epoch 9/15: [==============================] 63/63 batches, loss: 0.0640
[2025-05-02 11:37:05,385][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0640
[2025-05-02 11:37:05,609][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0493, Metrics: {'mse': 0.04847172275185585, 'rmse': 0.22016294591019592, 'r2': 0.25288957357406616}
[2025-05-02 11:37:05,610][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0823Epoch 10/15: [                              ] 2/63 batches, loss: 0.0857Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0807Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0847Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0828Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0763Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0751Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0735Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0769Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0797Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0759Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0767Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0760Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0753Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0773Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0741Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0733Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0742Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0724Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0766Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0747Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0741Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0734Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0723Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0712Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0709Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0711Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0693Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0700Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0696Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0686Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0682Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0671Epoch 10/15: [================              ] 34/63 batches, loss: 0.0670Epoch 10/15: [================              ] 35/63 batches, loss: 0.0672Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0662Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0661Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0657Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0655Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0645Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0641Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0642Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0641Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0643Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0636Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0636Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0645Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0650Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0649Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0651Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0646Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0655Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0659Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0664Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0658Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0658Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0653Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0653Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0647Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0649Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0648Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0655Epoch 10/15: [==============================] 63/63 batches, loss: 0.0647
[2025-05-02 11:37:07,540][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0647
[2025-05-02 11:37:07,756][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0471, Metrics: {'mse': 0.04620619863271713, 'rmse': 0.21495627144309407, 'r2': 0.2878088355064392}
[2025-05-02 11:37:07,757][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0756Epoch 11/15: [                              ] 2/63 batches, loss: 0.0698Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0652Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0566Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0544Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0560Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0563Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0552Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0529Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0553Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0537Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0558Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0541Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0571Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0552Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0536Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0527Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0523Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0521Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0543Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0552Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0540Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0558Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0564Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0557Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0566Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0566Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0561Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0546Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0548Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0543Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0551Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0553Epoch 11/15: [================              ] 34/63 batches, loss: 0.0550Epoch 11/15: [================              ] 35/63 batches, loss: 0.0552Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0551Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0549Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0544Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0540Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0534Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0542Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0548Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0542Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0538Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0540Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0542Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0540Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0533Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0532Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0540Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0549Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0544Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0541Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0536Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0536Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0542Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0542Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0536Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0540Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0535Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0531Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0531Epoch 11/15: [==============================] 63/63 batches, loss: 0.0527
[2025-05-02 11:37:09,714][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0527
[2025-05-02 11:37:09,930][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0580, Metrics: {'mse': 0.05697409808635712, 'rmse': 0.23869247597349422, 'r2': 0.12183970212936401}
[2025-05-02 11:37:09,930][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:37:09,930][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-02 11:37:09,931][src.training.lm_trainer][INFO] - Training completed in 26.11 seconds
[2025-05-02 11:37:09,931][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:37:12,393][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021382903680205345, 'rmse': 0.14622894268989756, 'r2': 0.30342864990234375}
[2025-05-02 11:37:12,393][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04231855273246765, 'rmse': 0.20571473630361936, 'r2': 0.3477303385734558}
[2025-05-02 11:37:12,393][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06317862868309021, 'rmse': 0.25135359293849413, 'r2': -0.08917152881622314}
[2025-05-02 11:37:14,046][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar/ar/model.pt
[2025-05-02 11:37:14,048][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁
wandb:     best_val_mse █▄▂▁
wandb:      best_val_r2 ▁▅▇█
wandb:    best_val_rmse █▄▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▆▆▅▆▇▆▆▆
wandb:       train_loss █▅▄▃▃▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▃▄▂▁▁▂▂▃
wandb:          val_mse █▄▂▃▄▂▁▁▂▁▃
wandb:           val_r2 ▁▅▇▆▅▇██▇█▆
wandb:         val_rmse █▄▂▃▄▂▁▁▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04301
wandb:     best_val_mse 0.04232
wandb:      best_val_r2 0.34773
wandb:    best_val_rmse 0.20571
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.06318
wandb:    final_test_r2 -0.08917
wandb:  final_test_rmse 0.25135
wandb:  final_train_mse 0.02138
wandb:   final_train_r2 0.30343
wandb: final_train_rmse 0.14623
wandb:    final_val_mse 0.04232
wandb:     final_val_r2 0.34773
wandb:   final_val_rmse 0.20571
wandb:    learning_rate 2e-05
wandb:       train_loss 0.05274
wandb:       train_time 26.1059
wandb:         val_loss 0.05797
wandb:          val_mse 0.05697
wandb:           val_r2 0.12184
wandb:         val_rmse 0.23869
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113633-imv4mgnm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113633-imv4mgnm/logs
Experiment probe_layer9_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar/results.json
=======================
PROBING LAYER 11
=======================
Running experiment: probe_layer11_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=384" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:37:26,723][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar
experiment_name: probe_layer11_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 384
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 11:37:26,723][__main__][INFO] - Normalized task: question_type
[2025-05-02 11:37:26,723][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 11:37:26,723][__main__][INFO] - Determined Task Type: classification
[2025-05-02 11:37:26,727][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 11:37:26,727][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:37:28,598][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:37:30,845][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:37:30,845][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:37:30,918][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:30,963][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:31,074][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:37:31,082][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:37:31,082][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:37:31,084][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:37:31,113][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:31,146][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:31,158][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:37:31,159][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:37:31,159][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:37:31,160][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:37:31,183][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:31,215][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:37:31,227][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:37:31,228][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:37:31,228][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:37:31,230][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:37:31,230][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:37:31,230][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:37:31,230][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:37:31,230][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:37:31,230][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 11:37:31,230][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:37:31,231][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 11:37:31,231][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:37:31,231][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 11:37:31,232][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 11:37:31,232][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:37:31,232][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:37:31,233][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:37:31,233][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 11:37:31,233][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:37:35,463][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:37:35,464][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:37:35,464][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 11:37:35,464][src.models.model_factory][INFO] - Using provided probe_hidden_size: 384
[2025-05-02 11:37:35,470][src.models.model_factory][INFO] - Model has 445,825 trainable parameters out of 394,567,297 total parameters
[2025-05-02 11:37:35,471][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 445,825 trainable parameters
[2025-05-02 11:37:35,471][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=384, depth=2, activation=gelu, normalization=layer
[2025-05-02 11:37:35,471][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 384 hidden size
[2025-05-02 11:37:35,471][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:37:35,472][__main__][INFO] - Total parameters: 394,567,297
[2025-05-02 11:37:35,472][__main__][INFO] - Trainable parameters: 445,825 (0.11%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7383Epoch 1/15: [                              ] 2/63 batches, loss: 0.6988Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7096Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7204Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7227Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7209Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7183Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7137Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7093Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7073Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7077Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7054Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7035Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7024Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7017Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7025Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7025Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7011Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7007Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6999Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6994Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6990Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6986Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6980Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6970Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6966Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6959Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6953Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6944Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6932Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6932Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6922Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6930Epoch 1/15: [================              ] 34/63 batches, loss: 0.6925Epoch 1/15: [================              ] 35/63 batches, loss: 0.6922Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6914Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6909Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6908Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6888Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6864Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6879Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6887Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6885Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6852Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6857Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6857Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6851Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6838Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6824Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6810Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6810Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6789Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6789Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6782Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6767Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6759Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6761Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6759Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6759Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6744Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6738Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6729Epoch 1/15: [==============================] 63/63 batches, loss: 0.6742
[2025-05-02 11:37:40,296][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6742
[2025-05-02 11:37:40,492][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6577, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8292682926829268, 'precision': 0.8095238095238095, 'recall': 0.85}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6144Epoch 2/15: [                              ] 2/63 batches, loss: 0.6365Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6537Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6547Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6460Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6455Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6468Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6413Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6476Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6464Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6474Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6479Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6414Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6398Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6407Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6433Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6428Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6419Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6425Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6434Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6440Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6414Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6394Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6394Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6417Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6396Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6384Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6367Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6346Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6350Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6379Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6383Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6377Epoch 2/15: [================              ] 34/63 batches, loss: 0.6378Epoch 2/15: [================              ] 35/63 batches, loss: 0.6345Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6344Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6323Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6312Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6309Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6304Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6303Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6296Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6303Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6314Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6305Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6299Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6280Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6274Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6282Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6255Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6258Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6238Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6229Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6229Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6228Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6224Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6220Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6197Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6196Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6198Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6191Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6178Epoch 2/15: [==============================] 63/63 batches, loss: 0.6174
[2025-05-02 11:37:42,846][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6174
[2025-05-02 11:37:43,054][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6101, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.5458Epoch 3/15: [                              ] 2/63 batches, loss: 0.5607Epoch 3/15: [=                             ] 3/63 batches, loss: 0.5460Epoch 3/15: [=                             ] 4/63 batches, loss: 0.5649Epoch 3/15: [==                            ] 5/63 batches, loss: 0.5745Epoch 3/15: [==                            ] 6/63 batches, loss: 0.5770Epoch 3/15: [===                           ] 7/63 batches, loss: 0.5754Epoch 3/15: [===                           ] 8/63 batches, loss: 0.5922Epoch 3/15: [====                          ] 9/63 batches, loss: 0.5851Epoch 3/15: [====                          ] 10/63 batches, loss: 0.5888Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.5873Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.5908Epoch 3/15: [======                        ] 13/63 batches, loss: 0.5991Epoch 3/15: [======                        ] 14/63 batches, loss: 0.5979Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.5959Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.5934Epoch 3/15: [========                      ] 17/63 batches, loss: 0.5925Epoch 3/15: [========                      ] 18/63 batches, loss: 0.5905Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.5895Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.5889Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.5890Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.5903Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.5888Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.5891Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.5836Epoch 3/15: [============                  ] 26/63 batches, loss: 0.5855Epoch 3/15: [============                  ] 27/63 batches, loss: 0.5889Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.5890Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.5921Epoch 3/15: [==============                ] 30/63 batches, loss: 0.5908Epoch 3/15: [==============                ] 31/63 batches, loss: 0.5916Epoch 3/15: [===============               ] 32/63 batches, loss: 0.5910Epoch 3/15: [===============               ] 33/63 batches, loss: 0.5949Epoch 3/15: [================              ] 34/63 batches, loss: 0.5942Epoch 3/15: [================              ] 35/63 batches, loss: 0.5904Epoch 3/15: [=================             ] 36/63 batches, loss: 0.5928Epoch 3/15: [=================             ] 37/63 batches, loss: 0.5928Epoch 3/15: [==================            ] 38/63 batches, loss: 0.5915Epoch 3/15: [==================            ] 39/63 batches, loss: 0.5911Epoch 3/15: [===================           ] 40/63 batches, loss: 0.5894Epoch 3/15: [===================           ] 41/63 batches, loss: 0.5905Epoch 3/15: [====================          ] 42/63 batches, loss: 0.5907Epoch 3/15: [====================          ] 43/63 batches, loss: 0.5902Epoch 3/15: [====================          ] 44/63 batches, loss: 0.5894Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.5882Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.5875Epoch 3/15: [======================        ] 47/63 batches, loss: 0.5857Epoch 3/15: [======================        ] 48/63 batches, loss: 0.5850Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.5836Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.5832Epoch 3/15: [========================      ] 51/63 batches, loss: 0.5832Epoch 3/15: [========================      ] 52/63 batches, loss: 0.5816Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.5819Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.5794Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.5789Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.5784Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.5777Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.5779Epoch 3/15: [============================  ] 59/63 batches, loss: 0.5780Epoch 3/15: [============================  ] 60/63 batches, loss: 0.5796Epoch 3/15: [============================= ] 61/63 batches, loss: 0.5785Epoch 3/15: [============================= ] 62/63 batches, loss: 0.5785Epoch 3/15: [==============================] 63/63 batches, loss: 0.5791
[2025-05-02 11:37:45,416][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.5791
[2025-05-02 11:37:45,624][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.5908, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5493Epoch 4/15: [                              ] 2/63 batches, loss: 0.5464Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5359Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5232Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5422Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5413Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5468Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5505Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5362Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5442Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5521Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5513Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5533Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5532Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5608Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5610Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5642Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5613Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5654Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5627Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5597Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5598Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5604Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5638Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5651Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5670Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5645Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5660Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5678Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5670Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5655Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5655Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5637Epoch 4/15: [================              ] 34/63 batches, loss: 0.5644Epoch 4/15: [================              ] 35/63 batches, loss: 0.5641Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5625Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5640Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5637Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5640Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5622Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5627Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5632Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5627Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5628Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5618Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5607Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5595Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5596Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5597Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5591Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5599Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5603Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5612Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5611Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5610Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5612Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5633Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5616Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5621Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5616Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5611Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5620Epoch 4/15: [==============================] 63/63 batches, loss: 0.5597
[2025-05-02 11:37:47,917][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5597
[2025-05-02 11:37:48,137][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5868, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5952Epoch 5/15: [                              ] 2/63 batches, loss: 0.5363Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5386Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5474Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5719Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5636Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5647Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5521Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5505Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5434Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5371Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5358Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5376Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5342Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5292Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5321Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5324Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5311Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5350Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5389Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5373Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5347Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5316Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5369Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5345Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5359Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5360Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5344Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5365Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5378Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5356Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5338Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5355Epoch 5/15: [================              ] 34/63 batches, loss: 0.5370Epoch 5/15: [================              ] 35/63 batches, loss: 0.5382Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5377Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5372Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5354Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5348Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5352Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5356Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5373Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5373Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5376Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5371Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5371Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5370Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5378Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5371Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5398Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5401Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5404Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5413Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5412Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5415Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5406Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5406Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5413Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5415Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5415Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5413Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5411Epoch 5/15: [==============================] 63/63 batches, loss: 0.5395
[2025-05-02 11:37:50,425][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5395
[2025-05-02 11:37:50,644][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5816, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5133Epoch 6/15: [                              ] 2/63 batches, loss: 0.5254Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5221Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5202Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5148Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5149Epoch 6/15: [===                           ] 7/63 batches, loss: 0.4986Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5094Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5118Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5180Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5201Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5246Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5264Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5264Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5280Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5333Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5321Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5343Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5357Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5370Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5352Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5374Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5352Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5379Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5431Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5419Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5422Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5417Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5411Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5388Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5418Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5406Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5389Epoch 6/15: [================              ] 34/63 batches, loss: 0.5345Epoch 6/15: [================              ] 35/63 batches, loss: 0.5338Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5339Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5342Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5314Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5327Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5343Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5340Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5332Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5348Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5333Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5343Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5345Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5335Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5339Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5360Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5374Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5367Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5352Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5347Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5364Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5371Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5378Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5372Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5386Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5389Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5376Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5378Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5385Epoch 6/15: [==============================] 63/63 batches, loss: 0.5375
[2025-05-02 11:37:52,958][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5375
[2025-05-02 11:37:53,194][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5818, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888, 'precision': 0.8, 'recall': 1.0}
[2025-05-02 11:37:53,195][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5218Epoch 7/15: [                              ] 2/63 batches, loss: 0.4902Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5431Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5292Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5319Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5299Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5335Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5280Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5281Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5191Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5113Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5147Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5160Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5086Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5148Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5187Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5203Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5188Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5199Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5181Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5188Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5215Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5201Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5188Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5184Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5202Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5221Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5241Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5279Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5331Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5319Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5306Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5287Epoch 7/15: [================              ] 34/63 batches, loss: 0.5302Epoch 7/15: [================              ] 35/63 batches, loss: 0.5326Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5326Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5303Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5316Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5327Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5350Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5360Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5345Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5354Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5353Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5339Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5331Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5347Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5330Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5333Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5313Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5302Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5328Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5338Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5344Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5349Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5342Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5330Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5331Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5312Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5304Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5315Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5324Epoch 7/15: [==============================] 63/63 batches, loss: 0.5328
[2025-05-02 11:37:55,141][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5328
[2025-05-02 11:37:55,350][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5866, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888, 'precision': 0.8, 'recall': 1.0}
[2025-05-02 11:37:55,351][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5860Epoch 8/15: [                              ] 2/63 batches, loss: 0.5422Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5247Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5011Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5216Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5463Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5383Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5363Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5370Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5349Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5330Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5331Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5310Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5307Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5333Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5386Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5421Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5441Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5450Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5393Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5422Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5388Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5379Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5388Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5376Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5366Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5346Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5333Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5336Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5318Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5319Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5323Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5302Epoch 8/15: [================              ] 34/63 batches, loss: 0.5300Epoch 8/15: [================              ] 35/63 batches, loss: 0.5287Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5275Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5284Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5281Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5292Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5267Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5287Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5290Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5288Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5302Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5307Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5308Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5314Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5298Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5290Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5292Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5294Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5315Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5314Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5325Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5304Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5304Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5304Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5310Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5304Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5297Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5305Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5294Epoch 8/15: [==============================] 63/63 batches, loss: 0.5299
[2025-05-02 11:37:57,291][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5299
[2025-05-02 11:37:57,502][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5876, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888, 'precision': 0.8, 'recall': 1.0}
[2025-05-02 11:37:57,503][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 11:37:57,503][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-02 11:37:57,503][src.training.lm_trainer][INFO] - Training completed in 20.00 seconds
[2025-05-02 11:37:57,503][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:37:59,980][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.985929648241206, 'f1': 0.9858870967741935, 'precision': 0.9878787878787879, 'recall': 0.9839034205231388}
[2025-05-02 11:37:59,980][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 11:37:59,980][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6233766233766234, 'f1': 0.6027397260273972, 'precision': 0.43137254901960786, 'recall': 1.0}
[2025-05-02 11:38:01,645][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar/ar/model.pt
[2025-05-02 11:38:01,646][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆▆██
wandb:           best_val_f1 ▁▆▆██
wandb:         best_val_loss █▄▂▁▁
wandb:    best_val_precision ▁▆▆██
wandb:       best_val_recall ▁▆▆██
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▂▃▃▂
wandb:            train_loss █▅▃▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆▆██▆▆▆
wandb:                val_f1 ▁▆▆██▆▆▆
wandb:              val_loss █▄▂▁▁▁▁▂
wandb:         val_precision ▃▆▆██▁▁▁
wandb:            val_recall ▁▆▆█████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.58163
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.62338
wandb:         final_test_f1 0.60274
wandb:  final_test_precision 0.43137
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.98593
wandb:        final_train_f1 0.98589
wandb: final_train_precision 0.98788
wandb:    final_train_recall 0.9839
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52994
wandb:            train_time 20.00175
wandb:          val_accuracy 0.88636
wandb:                val_f1 0.88889
wandb:              val_loss 0.58763
wandb:         val_precision 0.8
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113726-aosfk18t
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113726-aosfk18t/logs
Experiment probe_layer11_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar/results.json
Running experiment: probe_layer11_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:38:12,169][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar
experiment_name: probe_layer11_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 11:38:12,170][__main__][INFO] - Normalized task: complexity
[2025-05-02 11:38:12,170][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:38:12,170][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:38:12,174][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 11:38:12,174][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:38:13,591][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:38:15,880][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:38:15,881][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:38:15,920][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:15,964][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:16,042][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:38:16,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:38:16,050][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:38:16,051][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:38:16,072][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:16,105][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:16,116][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:38:16,117][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:38:16,118][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:38:16,119][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:38:16,139][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:16,171][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:38:16,182][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:38:16,183][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:38:16,183][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:38:16,184][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:38:16,185][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:38:16,185][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:38:16,185][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:38:16,185][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:38:16,185][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:38:16,186][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:38:16,186][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:38:16,186][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:38:16,187][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 11:38:16,187][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 11:38:16,187][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:38:16,187][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 11:38:16,188][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:38:16,188][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 11:38:16,188][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:38:16,188][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:38:16,188][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:38:16,188][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 11:38:16,188][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:38:19,930][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:38:19,931][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:38:19,931][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 11:38:19,931][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:38:19,935][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:38:19,936][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:38:19,936][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:38:19,936][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:38:19,936][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:38:19,937][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:38:19,937][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.4990Epoch 1/15: [                              ] 2/63 batches, loss: 0.3729Epoch 1/15: [=                             ] 3/63 batches, loss: 0.3084Epoch 1/15: [=                             ] 4/63 batches, loss: 0.3307Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3368Epoch 1/15: [==                            ] 6/63 batches, loss: 0.3134Epoch 1/15: [===                           ] 7/63 batches, loss: 0.3015Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2771Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2838Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2717Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2618Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2477Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2454Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2364Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2298Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2285Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2273Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2225Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2241Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2183Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2179Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2135Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2139Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2097Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2094Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2066Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2066Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2086Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2090Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2072Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2025Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2020Epoch 1/15: [===============               ] 33/63 batches, loss: 0.1984Epoch 1/15: [================              ] 34/63 batches, loss: 0.1972Epoch 1/15: [================              ] 35/63 batches, loss: 0.1962Epoch 1/15: [=================             ] 36/63 batches, loss: 0.1967Epoch 1/15: [=================             ] 37/63 batches, loss: 0.1980Epoch 1/15: [==================            ] 38/63 batches, loss: 0.1990Epoch 1/15: [==================            ] 39/63 batches, loss: 0.1996Epoch 1/15: [===================           ] 40/63 batches, loss: 0.1982Epoch 1/15: [===================           ] 41/63 batches, loss: 0.1967Epoch 1/15: [====================          ] 42/63 batches, loss: 0.1947Epoch 1/15: [====================          ] 43/63 batches, loss: 0.1937Epoch 1/15: [====================          ] 44/63 batches, loss: 0.1917Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.1905Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.1942Epoch 1/15: [======================        ] 47/63 batches, loss: 0.1944Epoch 1/15: [======================        ] 48/63 batches, loss: 0.1949Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.1939Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.1952Epoch 1/15: [========================      ] 51/63 batches, loss: 0.1935Epoch 1/15: [========================      ] 52/63 batches, loss: 0.1943Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.1935Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.1942Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.1925Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.1925Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.1911Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.1912Epoch 1/15: [============================  ] 59/63 batches, loss: 0.1927Epoch 1/15: [============================  ] 60/63 batches, loss: 0.1921Epoch 1/15: [============================= ] 61/63 batches, loss: 0.1918Epoch 1/15: [============================= ] 62/63 batches, loss: 0.1917Epoch 1/15: [==============================] 63/63 batches, loss: 0.1915
[2025-05-02 11:38:24,069][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1915
[2025-05-02 11:38:24,355][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0846, Metrics: {'mse': 0.0852268636226654, 'rmse': 0.2919364033872196, 'r2': -0.31362950801849365}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1859Epoch 2/15: [                              ] 2/63 batches, loss: 0.2047Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1841Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1618Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1540Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1461Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1473Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1426Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1412Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1361Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1400Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1407Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1498Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1492Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1456Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1473Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1435Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1446Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1470Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1494Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1478Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1501Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1488Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1502Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1480Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1464Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1445Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1460Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1487Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1472Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1448Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1444Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1447Epoch 2/15: [================              ] 34/63 batches, loss: 0.1437Epoch 2/15: [================              ] 35/63 batches, loss: 0.1440Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1455Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1433Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1421Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1426Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1445Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1429Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1416Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1413Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1439Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1433Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1447Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1434Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1416Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1405Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1428Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1438Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1434Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1432Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1430Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1446Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1435Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1433Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1431Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1426Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1412Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1422Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1421Epoch 2/15: [==============================] 63/63 batches, loss: 0.1432
[2025-05-02 11:38:26,685][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1432
[2025-05-02 11:38:26,889][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0723, Metrics: {'mse': 0.0723877102136612, 'rmse': 0.26904964265663167, 'r2': -0.11573541164398193}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0570Epoch 3/15: [                              ] 2/63 batches, loss: 0.0574Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0869Epoch 3/15: [=                             ] 4/63 batches, loss: 0.0853Epoch 3/15: [==                            ] 5/63 batches, loss: 0.0912Epoch 3/15: [==                            ] 6/63 batches, loss: 0.0995Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1019Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1315Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1444Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1466Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1465Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1411Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1440Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1461Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1413Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1392Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1348Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1338Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1318Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1308Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1303Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1286Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1295Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1268Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1277Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1268Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1261Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1271Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1246Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1232Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1262Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1253Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1235Epoch 3/15: [================              ] 34/63 batches, loss: 0.1233Epoch 3/15: [================              ] 35/63 batches, loss: 0.1224Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1209Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1227Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1219Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1218Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1216Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1210Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1199Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1195Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1201Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1192Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1192Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1195Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1195Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1199Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1203Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1215Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1221Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1213Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1215Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1217Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1227Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1214Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1219Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1236Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1229Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1219Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1216Epoch 3/15: [==============================] 63/63 batches, loss: 0.1200
[2025-05-02 11:38:29,233][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1200
[2025-05-02 11:38:29,441][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0656, Metrics: {'mse': 0.06531728059053421, 'rmse': 0.2555724566351668, 'r2': -0.00675654411315918}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1576Epoch 4/15: [                              ] 2/63 batches, loss: 0.1279Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1309Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1510Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1674Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1545Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1458Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1382Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1377Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1311Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1289Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1243Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1221Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1228Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1184Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1170Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1135Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1141Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1136Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1106Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1123Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1125Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1131Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1108Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1120Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1116Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1114Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1113Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1122Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1097Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1075Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1061Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1049Epoch 4/15: [================              ] 34/63 batches, loss: 0.1063Epoch 4/15: [================              ] 35/63 batches, loss: 0.1063Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1056Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1052Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1047Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1039Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1042Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1023Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1043Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1034Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1038Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1035Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1039Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1038Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1045Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1062Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1065Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1061Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1045Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1039Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1035Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1029Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1033Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1025Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1025Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1032Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1032Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1023Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1014Epoch 4/15: [==============================] 63/63 batches, loss: 0.1005
[2025-05-02 11:38:31,723][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1005
[2025-05-02 11:38:31,934][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0703, Metrics: {'mse': 0.07023677229881287, 'rmse': 0.26502221095374795, 'r2': -0.08258223533630371}
[2025-05-02 11:38:31,935][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.2067Epoch 5/15: [                              ] 2/63 batches, loss: 0.1459Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1201Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1182Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1070Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0964Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0893Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0900Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0890Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0901Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0874Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0900Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0884Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0880Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0861Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0870Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0857Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0846Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0859Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0853Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0848Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0841Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0824Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0812Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0815Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0828Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0835Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0845Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0846Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0870Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0860Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0851Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0847Epoch 5/15: [================              ] 34/63 batches, loss: 0.0859Epoch 5/15: [================              ] 35/63 batches, loss: 0.0853Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0851Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0854Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0852Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0848Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0848Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0851Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0843Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0858Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0854Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0863Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0870Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0866Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0858Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0850Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0845Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0853Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0848Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0848Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0847Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0841Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0842Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0846Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0843Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0846Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0844Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0843Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0835Epoch 5/15: [==============================] 63/63 batches, loss: 0.0841
[2025-05-02 11:38:33,883][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0841
[2025-05-02 11:38:34,092][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0764, Metrics: {'mse': 0.07635756582021713, 'rmse': 0.2763287278228906, 'r2': -0.1769239902496338}
[2025-05-02 11:38:34,093][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0762Epoch 6/15: [                              ] 2/63 batches, loss: 0.1219Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1247Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1065Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0998Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0970Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0945Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0857Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0859Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0825Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0825Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0831Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0836Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0815Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0819Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0814Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0797Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0787Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0820Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0815Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0822Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0816Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0810Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0814Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0847Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0863Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0873Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0875Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0862Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0861Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0857Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0844Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0839Epoch 6/15: [================              ] 34/63 batches, loss: 0.0838Epoch 6/15: [================              ] 35/63 batches, loss: 0.0836Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0821Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0822Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0823Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0840Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0843Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0848Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0846Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0842Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0837Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0845Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0837Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0836Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0829Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0822Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0815Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0813Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0811Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0811Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0805Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0801Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0797Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0798Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0801Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0809Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0811Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0811Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0810Epoch 6/15: [==============================] 63/63 batches, loss: 0.0805
[2025-05-02 11:38:36,026][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0805
[2025-05-02 11:38:36,236][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0605, Metrics: {'mse': 0.06030946597456932, 'rmse': 0.24557985661403364, 'r2': 0.07043057680130005}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0827Epoch 7/15: [                              ] 2/63 batches, loss: 0.0865Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0706Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0743Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0690Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0672Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0723Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0682Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0671Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0663Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0683Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0673Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0667Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0710Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0686Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0668Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0650Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0637Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0625Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0623Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0627Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0641Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0649Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0645Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0649Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0638Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0645Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0647Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0646Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0649Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0651Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0649Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0641Epoch 7/15: [================              ] 34/63 batches, loss: 0.0636Epoch 7/15: [================              ] 35/63 batches, loss: 0.0633Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0634Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0628Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0634Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0633Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0631Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0624Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0627Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0621Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0612Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0614Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0617Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0619Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0619Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0640Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0635Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0632Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0632Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0650Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0650Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0642Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0655Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0652Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0656Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0653Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0648Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0643Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0639Epoch 7/15: [==============================] 63/63 batches, loss: 0.0642
[2025-05-02 11:38:38,532][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0642
[2025-05-02 11:38:38,734][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0523, Metrics: {'mse': 0.05187778174877167, 'rmse': 0.2277669461286507, 'r2': 0.20039087533950806}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0340Epoch 8/15: [                              ] 2/63 batches, loss: 0.0446Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0481Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0597Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0637Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0597Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0585Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0604Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0607Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0578Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0566Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0608Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0600Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0589Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0619Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0629Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0621Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0630Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0619Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0631Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0661Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0650Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0635Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0628Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0625Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0614Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0612Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0612Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0617Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0637Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0639Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0638Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0645Epoch 8/15: [================              ] 34/63 batches, loss: 0.0657Epoch 8/15: [================              ] 35/63 batches, loss: 0.0650Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0652Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0652Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0653Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0646Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0660Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0664Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0668Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0674Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0694Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0684Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0682Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0680Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0674Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0670Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0665Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0662Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0667Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0660Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0660Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0652Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0653Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0655Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0652Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0648Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0647Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0641Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0646Epoch 8/15: [==============================] 63/63 batches, loss: 0.0642
[2025-05-02 11:38:41,098][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0642
[2025-05-02 11:38:41,323][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0550, Metrics: {'mse': 0.054604027420282364, 'rmse': 0.23367504663588357, 'r2': 0.15837037563323975}
[2025-05-02 11:38:41,323][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0949Epoch 9/15: [                              ] 2/63 batches, loss: 0.0635Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0679Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0681Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0628Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0633Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0679Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0656Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0611Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0630Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0629Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0601Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0578Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0596Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0587Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0575Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0575Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0640Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0631Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0624Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0611Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0618Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0616Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0623Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0609Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0612Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0623Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0615Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0613Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0624Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0629Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0648Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0643Epoch 9/15: [================              ] 34/63 batches, loss: 0.0636Epoch 9/15: [================              ] 35/63 batches, loss: 0.0623Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0620Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0618Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0627Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0633Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0636Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0634Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0640Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0639Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0635Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0638Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0643Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0640Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0645Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0645Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0644Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0646Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0644Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0645Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0642Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0646Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0645Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0637Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0640Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0636Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0644Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0644Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0641Epoch 9/15: [==============================] 63/63 batches, loss: 0.0635
[2025-05-02 11:38:43,265][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0635
[2025-05-02 11:38:43,483][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0639, Metrics: {'mse': 0.06359080970287323, 'rmse': 0.2521721826508095, 'r2': 0.01985412836074829}
[2025-05-02 11:38:43,484][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0706Epoch 10/15: [                              ] 2/63 batches, loss: 0.0785Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0724Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0725Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0733Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0696Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0695Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0686Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0697Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0710Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0676Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0676Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0666Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0656Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0655Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0642Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0628Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0629Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0609Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0620Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0613Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0605Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0607Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0598Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0591Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0586Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0588Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0580Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0589Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0596Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0588Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0582Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0576Epoch 10/15: [================              ] 34/63 batches, loss: 0.0581Epoch 10/15: [================              ] 35/63 batches, loss: 0.0587Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0582Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0578Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0575Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0586Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0578Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0581Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0590Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0591Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0600Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0595Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0593Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0603Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0606Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0607Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0610Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0602Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0609Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0612Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0621Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0616Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0613Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0609Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0608Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0602Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0603Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0602Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0601Epoch 10/15: [==============================] 63/63 batches, loss: 0.0599
[2025-05-02 11:38:45,423][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0599
[2025-05-02 11:38:45,641][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0552, Metrics: {'mse': 0.05462498962879181, 'rmse': 0.2337198956631459, 'r2': 0.15804719924926758}
[2025-05-02 11:38:45,641][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0554Epoch 11/15: [                              ] 2/63 batches, loss: 0.0581Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0672Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0597Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0553Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0554Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0564Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0549Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0540Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0551Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0534Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0548Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0543Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0545Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0531Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0522Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0513Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0520Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0518Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0524Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0526Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0517Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0529Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0543Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0532Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0548Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0544Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0543Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0533Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0530Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0524Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0549Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0546Epoch 11/15: [================              ] 34/63 batches, loss: 0.0542Epoch 11/15: [================              ] 35/63 batches, loss: 0.0546Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0543Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0538Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0537Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0533Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0532Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0539Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0535Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0531Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0526Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0531Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0532Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0526Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0524Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0520Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0531Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0529Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0526Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0521Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0515Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0512Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0517Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0516Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0515Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0517Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0516Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0511Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0511Epoch 11/15: [==============================] 63/63 batches, loss: 0.0514
[2025-05-02 11:38:47,597][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0514
[2025-05-02 11:38:47,815][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0583, Metrics: {'mse': 0.057868365198373795, 'rmse': 0.24055844445451047, 'r2': 0.10805600881576538}
[2025-05-02 11:38:47,816][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:38:47,816][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-02 11:38:47,816][src.training.lm_trainer][INFO] - Training completed in 26.40 seconds
[2025-05-02 11:38:47,816][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:38:50,302][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.022655367851257324, 'rmse': 0.15051700186775355, 'r2': 0.2619767189025879}
[2025-05-02 11:38:50,303][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05187778174877167, 'rmse': 0.2277669461286507, 'r2': 0.20039087533950806}
[2025-05-02 11:38:50,303][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07462602853775024, 'rmse': 0.27317765014318107, 'r2': -0.28651952743530273}
[2025-05-02 11:38:51,955][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar/ar/model.pt
[2025-05-02 11:38:51,956][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▃▁
wandb:     best_val_mse █▅▄▃▁
wandb:      best_val_r2 ▁▄▅▆█
wandb:    best_val_rmse █▆▄▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▃▂▄▅▅▄▅
wandb:       train_loss █▆▄▃▃▂▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▄▅▆▃▁▂▄▂▂
wandb:          val_mse █▅▄▅▆▃▁▂▃▂▂
wandb:           val_r2 ▁▄▅▄▃▆█▇▆▇▇
wandb:         val_rmse █▆▄▅▆▃▁▂▄▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05233
wandb:     best_val_mse 0.05188
wandb:      best_val_r2 0.20039
wandb:    best_val_rmse 0.22777
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.07463
wandb:    final_test_r2 -0.28652
wandb:  final_test_rmse 0.27318
wandb:  final_train_mse 0.02266
wandb:   final_train_r2 0.26198
wandb: final_train_rmse 0.15052
wandb:    final_val_mse 0.05188
wandb:     final_val_r2 0.20039
wandb:   final_val_rmse 0.22777
wandb:    learning_rate 2e-05
wandb:       train_loss 0.05143
wandb:       train_time 26.39657
wandb:         val_loss 0.05833
wandb:          val_mse 0.05787
wandb:           val_r2 0.10806
wandb:         val_rmse 0.24056
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113812-aqm9fphc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113812-aqm9fphc/logs
Experiment probe_layer11_complexity_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar/results.json
Running control probing experiments...
=======================
PROBING LAYER 1 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 4 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 6 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 9 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 11 (CONTROL EXPERIMENTS)
=======================
Running submetric probing experiments...
=======================
PROBING LAYER 1 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer1_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:39:02,960][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar
experiment_name: probe_layer1_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 11:39:02,961][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 11:39:02,961][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:39:02,961][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:39:02,961][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:39:02,965][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 11:39:02,965][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:39:02,965][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:39:04,554][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:39:06,809][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:39:06,809][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:39:06,866][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:06,908][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:07,024][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:39:07,031][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:39:07,031][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:39:07,032][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:39:07,055][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:07,093][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:07,106][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:39:07,107][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:39:07,107][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:39:07,108][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:39:07,127][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:07,158][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:39:07,171][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:39:07,172][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:39:07,173][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:39:07,174][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:39:07,175][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 11:39:07,175][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:39:07,175][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:39:07,176][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 11:39:07,176][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 11:39:07,176][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:39:07,177][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:39:07,177][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:39:07,177][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:39:07,178][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:39:07,178][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 11:39:07,178][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:39:11,197][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:39:11,198][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:39:11,198][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 11:39:11,198][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:39:11,202][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:39:11,203][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:39:11,203][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:39:11,203][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:39:11,204][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:39:11,205][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:39:11,205][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.2501Epoch 1/15: [                              ] 2/63 batches, loss: 0.2770Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2464Epoch 1/15: [=                             ] 4/63 batches, loss: 0.2656Epoch 1/15: [==                            ] 5/63 batches, loss: 0.2787Epoch 1/15: [==                            ] 6/63 batches, loss: 0.2660Epoch 1/15: [===                           ] 7/63 batches, loss: 0.2725Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2643Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2537Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2539Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2637Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2616Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2631Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2563Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2572Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2582Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2548Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2588Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2544Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2543Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2547Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2500Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2510Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2477Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2433Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2435Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2403Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2428Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2443Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2538Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2543Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2525Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2547Epoch 1/15: [================              ] 34/63 batches, loss: 0.2580Epoch 1/15: [================              ] 35/63 batches, loss: 0.2561Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2611Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2625Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2649Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2651Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2617Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2594Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2586Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2556Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2524Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2502Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2500Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2496Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2526Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2510Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2519Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2494Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2493Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2482Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2452Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2451Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2469Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2457Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2439Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2453Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2450Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2435Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2418Epoch 1/15: [==============================] 63/63 batches, loss: 0.2430
[2025-05-02 11:39:15,837][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2430
[2025-05-02 11:39:16,024][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1306, Metrics: {'mse': 0.13262265920639038, 'rmse': 0.3641739408667105, 'r2': -1.701899528503418}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2546Epoch 2/15: [                              ] 2/63 batches, loss: 0.2320Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2390Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2231Epoch 2/15: [==                            ] 5/63 batches, loss: 0.2117Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2021Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1963Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1841Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1884Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1917Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1908Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1871Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1909Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1966Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2004Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1999Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1970Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1936Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1978Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.2051Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.2018Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2049Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1998Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1952Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1899Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1874Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1854Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1838Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1835Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1816Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1825Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1802Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1815Epoch 2/15: [================              ] 34/63 batches, loss: 0.1779Epoch 2/15: [================              ] 35/63 batches, loss: 0.1785Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1767Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1762Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1760Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1740Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1731Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1724Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1730Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1723Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1726Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1721Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1731Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1720Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1717Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1701Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1688Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1671Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1693Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1700Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1691Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1683Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1686Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1680Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1672Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1656Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1665Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1656Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1646Epoch 2/15: [==============================] 63/63 batches, loss: 0.1669
[2025-05-02 11:39:18,344][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1669
[2025-05-02 11:39:18,529][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0901, Metrics: {'mse': 0.09151134639978409, 'rmse': 0.3025084236840093, 'r2': -0.8643455505371094}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1297Epoch 3/15: [                              ] 2/63 batches, loss: 0.1237Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1128Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1336Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1411Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1355Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1287Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1357Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1445Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1445Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1449Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1395Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1415Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1449Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1447Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1444Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1424Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1425Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1445Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1424Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1412Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1432Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1452Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1425Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1437Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1432Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1436Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1407Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1395Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1379Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1417Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1395Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1386Epoch 3/15: [================              ] 34/63 batches, loss: 0.1366Epoch 3/15: [================              ] 35/63 batches, loss: 0.1398Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1377Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1358Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1371Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1371Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1368Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1356Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1348Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1353Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1349Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1338Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1331Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1321Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1318Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1311Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1308Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1310Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1339Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1352Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1351Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1341Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1337Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1329Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1353Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1344Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1347Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1347Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1341Epoch 3/15: [==============================] 63/63 batches, loss: 0.1323
[2025-05-02 11:39:20,857][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1323
[2025-05-02 11:39:21,073][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0671, Metrics: {'mse': 0.06847388297319412, 'rmse': 0.2616751477943485, 'r2': -0.39500701427459717}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1106Epoch 4/15: [                              ] 2/63 batches, loss: 0.1302Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1411Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1197Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1151Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1181Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1142Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1119Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1210Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1260Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1282Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1265Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1289Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1214Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1176Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1161Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1133Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1177Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1176Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1171Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1174Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1179Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1197Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1226Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1212Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1195Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1183Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1156Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1137Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1138Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1136Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1155Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1152Epoch 4/15: [================              ] 34/63 batches, loss: 0.1169Epoch 4/15: [================              ] 35/63 batches, loss: 0.1160Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1139Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1157Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1171Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1166Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1171Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1173Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1177Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1172Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1162Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1153Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1145Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1140Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1150Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1149Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1142Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1138Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1125Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1117Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1122Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1116Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1106Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1098Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1103Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1104Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1098Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1098Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1103Epoch 4/15: [==============================] 63/63 batches, loss: 0.1101
[2025-05-02 11:39:23,353][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1101
[2025-05-02 11:39:23,569][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0588, Metrics: {'mse': 0.05978703871369362, 'rmse': 0.2445138824559735, 'r2': -0.2180314064025879}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1192Epoch 5/15: [                              ] 2/63 batches, loss: 0.1015Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0943Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0917Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0836Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0859Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0875Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0925Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0928Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0908Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0955Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0930Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0956Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0919Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0944Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0946Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0950Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0948Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0935Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0926Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0978Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0990Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0989Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0988Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1003Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0998Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0998Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0995Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0992Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0994Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0989Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0987Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0983Epoch 5/15: [================              ] 34/63 batches, loss: 0.0996Epoch 5/15: [================              ] 35/63 batches, loss: 0.1000Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0988Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0974Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0963Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0960Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0967Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0983Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0981Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0983Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0980Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0972Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0976Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0983Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0984Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0981Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0983Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0983Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0987Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0985Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0989Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0984Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0983Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0983Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0987Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0977Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0978Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0973Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0973Epoch 5/15: [==============================] 63/63 batches, loss: 0.0972
[2025-05-02 11:39:25,888][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0972
[2025-05-02 11:39:26,109][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0496, Metrics: {'mse': 0.05035823956131935, 'rmse': 0.2244064160431233, 'r2': -0.02594006061553955}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1106Epoch 6/15: [                              ] 2/63 batches, loss: 0.1018Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0882Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0825Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0907Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0946Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0899Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0853Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0924Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0907Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0859Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0850Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0877Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0879Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0870Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0844Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0860Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0847Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0843Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0834Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0838Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0862Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0875Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0890Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0904Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0881Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0893Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0908Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0897Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0884Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0895Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0883Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0884Epoch 6/15: [================              ] 34/63 batches, loss: 0.0887Epoch 6/15: [================              ] 35/63 batches, loss: 0.0899Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0885Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0896Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0894Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0893Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0894Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0888Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0892Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0904Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0901Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0931Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0918Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0916Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0913Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0912Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0901Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0894Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0897Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0893Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0892Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0898Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0898Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0900Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0895Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0888Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0887Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0878Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0871Epoch 6/15: [==============================] 63/63 batches, loss: 0.0860
[2025-05-02 11:39:28,422][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0860
[2025-05-02 11:39:28,646][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0435, Metrics: {'mse': 0.04396301880478859, 'rmse': 0.20967360063867982, 'r2': 0.10434871912002563}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0905Epoch 7/15: [                              ] 2/63 batches, loss: 0.0642Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0682Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0772Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0850Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0930Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0854Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0861Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0846Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0861Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0914Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0887Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0897Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0918Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0899Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0886Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0876Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0850Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0852Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0856Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0865Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0860Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0862Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0850Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0865Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0853Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0847Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0856Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0858Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0862Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0852Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0844Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0843Epoch 7/15: [================              ] 34/63 batches, loss: 0.0834Epoch 7/15: [================              ] 35/63 batches, loss: 0.0840Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0847Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0839Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0836Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0837Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0838Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0834Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0826Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0821Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0813Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0817Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0817Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0810Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0818Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0826Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0823Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0819Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0827Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0824Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0817Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0817Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0818Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0811Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0810Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0813Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0812Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0806Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0804Epoch 7/15: [==============================] 63/63 batches, loss: 0.0797
[2025-05-02 11:39:30,950][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0797
[2025-05-02 11:39:31,148][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0399, Metrics: {'mse': 0.04039166867733002, 'rmse': 0.20097678641407823, 'r2': 0.17710721492767334}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0412Epoch 8/15: [                              ] 2/63 batches, loss: 0.0426Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0480Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0459Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0443Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0429Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0474Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0549Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0573Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0580Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0590Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0614Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0645Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0657Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0690Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0685Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0673Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0666Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0660Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0663Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0682Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0685Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0697Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0700Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0702Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0694Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0703Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0714Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0720Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0723Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0726Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0736Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0740Epoch 8/15: [================              ] 34/63 batches, loss: 0.0739Epoch 8/15: [================              ] 35/63 batches, loss: 0.0736Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0730Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0738Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0732Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0734Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0742Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0739Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0730Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0731Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0724Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0726Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0723Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0744Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0748Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0744Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0737Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0737Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0742Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0739Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0735Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0730Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0735Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0734Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0743Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0735Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0732Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0740Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0735Epoch 8/15: [==============================] 63/63 batches, loss: 0.0729
[2025-05-02 11:39:33,525][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0729
[2025-05-02 11:39:33,744][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0356, Metrics: {'mse': 0.03600655123591423, 'rmse': 0.18975392284723452, 'r2': 0.26644450426101685}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0760Epoch 9/15: [                              ] 2/63 batches, loss: 0.0664Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0702Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0731Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0693Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0625Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0683Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0624Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0664Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0699Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0718Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0706Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0688Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0682Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0677Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0672Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0684Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0691Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0682Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0696Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0681Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0692Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0692Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0689Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0713Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0719Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0733Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0723Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0719Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0708Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0705Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0697Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0697Epoch 9/15: [================              ] 34/63 batches, loss: 0.0686Epoch 9/15: [================              ] 35/63 batches, loss: 0.0684Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0686Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0678Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0670Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0669Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0668Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0656Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0661Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0660Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0660Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0659Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0665Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0662Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0664Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0672Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0675Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0674Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0677Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0676Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0674Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0667Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0664Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0664Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0662Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0663Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0659Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0658Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0658Epoch 9/15: [==============================] 63/63 batches, loss: 0.0648
[2025-05-02 11:39:36,092][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0648
[2025-05-02 11:39:36,319][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0338, Metrics: {'mse': 0.033956028521060944, 'rmse': 0.18427161615685944, 'r2': 0.30821943283081055}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0760Epoch 10/15: [                              ] 2/63 batches, loss: 0.0788Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0615Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0520Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0479Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0493Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0478Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0479Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0494Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0529Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0541Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0585Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0578Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0572Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0563Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0552Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0629Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0621Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0634Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0644Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0647Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0635Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0633Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0628Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0634Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0630Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0629Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0615Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0625Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0623Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0616Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0614Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0612Epoch 10/15: [================              ] 34/63 batches, loss: 0.0608Epoch 10/15: [================              ] 35/63 batches, loss: 0.0622Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0612Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0609Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0613Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0606Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0599Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0598Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0597Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0590Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0588Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0587Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0587Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0592Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0599Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0591Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0592Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0591Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0592Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0595Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0593Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0592Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0594Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0594Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0594Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0589Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0591Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0589Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0585Epoch 10/15: [==============================] 63/63 batches, loss: 0.0592
[2025-05-02 11:39:38,639][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0592
[2025-05-02 11:39:38,873][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0333, Metrics: {'mse': 0.033294711261987686, 'rmse': 0.18246838428064102, 'r2': 0.3216923475265503}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0685Epoch 11/15: [                              ] 2/63 batches, loss: 0.0590Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0588Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0539Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0524Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0560Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0579Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0585Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0574Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0557Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0526Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0504Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0483Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0483Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0497Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0504Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0514Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0510Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0517Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0525Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0523Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0516Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0514Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0511Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0505Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0506Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0499Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0502Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0496Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0493Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0494Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0496Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0494Epoch 11/15: [================              ] 34/63 batches, loss: 0.0502Epoch 11/15: [================              ] 35/63 batches, loss: 0.0502Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0505Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0509Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0505Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0506Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0508Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0515Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0517Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0514Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0514Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0515Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0518Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0517Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0520Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0519Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0525Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0525Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0527Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0529Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0528Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0526Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0525Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0527Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0525Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0519Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0517Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0518Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0514Epoch 11/15: [==============================] 63/63 batches, loss: 0.0511
[2025-05-02 11:39:41,204][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0511
[2025-05-02 11:39:41,428][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0305, Metrics: {'mse': 0.030370382592082024, 'rmse': 0.1742710033025633, 'r2': 0.38126927614212036}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0824Epoch 12/15: [                              ] 2/63 batches, loss: 0.0640Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0700Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0687Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0661Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0618Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0589Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0622Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0626Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0599Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0589Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0588Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0565Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0572Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0567Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0551Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0546Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0560Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0548Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0540Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0531Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0524Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0537Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0532Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0531Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0527Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0522Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0516Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0510Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0502Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0495Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0489Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0486Epoch 12/15: [================              ] 34/63 batches, loss: 0.0487Epoch 12/15: [================              ] 35/63 batches, loss: 0.0486Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0486Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0481Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0482Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0479Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0475Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0476Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0473Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0487Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0490Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0484Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0483Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0477Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0472Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0484Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0491Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0490Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0491Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0498Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0498Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0498Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0501Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0500Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0502Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0503Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0499Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0501Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0498Epoch 12/15: [==============================] 63/63 batches, loss: 0.0493
[2025-05-02 11:39:43,784][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0493
[2025-05-02 11:39:44,012][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0329, Metrics: {'mse': 0.032610245048999786, 'rmse': 0.1805830696632433, 'r2': 0.33563685417175293}
[2025-05-02 11:39:44,013][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0524Epoch 13/15: [                              ] 2/63 batches, loss: 0.0436Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0426Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0424Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0453Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0476Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0453Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0459Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0452Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0462Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0464Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0475Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0512Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0522Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0533Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0530Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0529Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0522Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0525Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0520Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0535Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0536Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0531Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0534Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0535Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0527Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0526Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0523Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0523Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0520Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0514Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0510Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0507Epoch 13/15: [================              ] 34/63 batches, loss: 0.0502Epoch 13/15: [================              ] 35/63 batches, loss: 0.0503Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0501Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0509Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0510Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0510Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0508Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0508Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0508Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0506Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0511Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0513Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0509Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0514Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0512Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0506Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0503Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0499Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0500Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0501Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0499Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0497Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0495Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0497Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0496Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0494Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0490Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0495Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0503Epoch 13/15: [==============================] 63/63 batches, loss: 0.0511
[2025-05-02 11:39:45,962][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0511
[2025-05-02 11:39:46,195][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0293, Metrics: {'mse': 0.028873002156615257, 'rmse': 0.16992057602484537, 'r2': 0.4117751121520996}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0516Epoch 14/15: [                              ] 2/63 batches, loss: 0.0591Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0486Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0477Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0473Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0521Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0503Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0527Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0514Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0512Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0495Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0496Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0501Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0493Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0490Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0476Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0492Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0504Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0500Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0487Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0484Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0471Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0471Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0463Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0458Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0458Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0449Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0446Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0452Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0454Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0460Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0461Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0458Epoch 14/15: [================              ] 34/63 batches, loss: 0.0459Epoch 14/15: [================              ] 35/63 batches, loss: 0.0449Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0450Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0445Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0449Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0445Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0450Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0444Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0441Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0442Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0438Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0435Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0434Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0432Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0431Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0434Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0434Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0436Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0435Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0435Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0435Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0430Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0430Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0426Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0429Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0430Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0427Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0434Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0433Epoch 14/15: [==============================] 63/63 batches, loss: 0.0431
[2025-05-02 11:39:48,563][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0431
[2025-05-02 11:39:48,776][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0300, Metrics: {'mse': 0.029497183859348297, 'rmse': 0.17174744207512466, 'r2': 0.3990587592124939}
[2025-05-02 11:39:48,777][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0329Epoch 15/15: [                              ] 2/63 batches, loss: 0.0387Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0366Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0472Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0505Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0506Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0470Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0438Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0426Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0437Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0445Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0449Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0434Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0451Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0444Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0443Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0438Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0423Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0421Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0414Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0417Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0408Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0411Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0409Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0423Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0423Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0418Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0410Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0417Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0408Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0409Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0404Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0401Epoch 15/15: [================              ] 34/63 batches, loss: 0.0402Epoch 15/15: [================              ] 35/63 batches, loss: 0.0398Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0401Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0398Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0397Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0399Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0400Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0398Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0399Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0410Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0411Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0414Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0413Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0415Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0413Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0412Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0411Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0407Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0410Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0405Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0403Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0411Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0413Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0416Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0420Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0419Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0424Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0424Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0422Epoch 15/15: [==============================] 63/63 batches, loss: 0.0419
[2025-05-02 11:39:50,701][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0419
[2025-05-02 11:39:50,908][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0296, Metrics: {'mse': 0.02908812277019024, 'rmse': 0.17055240476226138, 'r2': 0.4073925018310547}
[2025-05-02 11:39:50,908][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
[2025-05-02 11:39:50,909][src.training.lm_trainer][INFO] - Training completed in 37.87 seconds
[2025-05-02 11:39:50,909][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:39:53,367][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.016027020290493965, 'rmse': 0.12659786842792403, 'r2': 0.23986351490020752}
[2025-05-02 11:39:53,368][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.028873002156615257, 'rmse': 0.16992057602484537, 'r2': 0.4117751121520996}
[2025-05-02 11:39:53,368][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04271293431520462, 'rmse': 0.20667107759724054, 'r2': 0.07777410745620728}
[2025-05-02 11:39:55,068][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar/ar/model.pt
[2025-05-02 11:39:55,070][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▃▂▂▂▁▁▁▁▁
wandb:     best_val_mse █▅▄▃▂▂▂▁▁▁▁▁
wandb:      best_val_r2 ▁▄▅▆▇▇▇█████
wandb:    best_val_rmse █▆▄▄▃▂▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▆▇▇▇▇▇██████
wandb:       train_loss █▅▄▃▃▃▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▄▃▂▂▂▁▁▁▁▁▁▁▁
wandb:          val_mse █▅▄▃▂▂▂▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁▄▅▆▇▇▇████████
wandb:         val_rmse █▆▄▄▃▂▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02931
wandb:     best_val_mse 0.02887
wandb:      best_val_r2 0.41178
wandb:    best_val_rmse 0.16992
wandb:            epoch 15
wandb:   final_test_mse 0.04271
wandb:    final_test_r2 0.07777
wandb:  final_test_rmse 0.20667
wandb:  final_train_mse 0.01603
wandb:   final_train_r2 0.23986
wandb: final_train_rmse 0.1266
wandb:    final_val_mse 0.02887
wandb:     final_val_r2 0.41178
wandb:   final_val_rmse 0.16992
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04195
wandb:       train_time 37.87428
wandb:         val_loss 0.02961
wandb:          val_mse 0.02909
wandb:           val_r2 0.40739
wandb:         val_rmse 0.17055
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113903-5ear0d17
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_113903-5ear0d17/logs
Experiment probe_layer1_avg_links_len_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar/results.json
=======================
PROBING LAYER 4 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer4_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:40:07,023][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar
experiment_name: probe_layer4_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 11:40:07,023][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 11:40:07,023][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:40:07,024][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:40:07,024][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:40:07,029][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 11:40:07,029][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:40:07,029][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:40:08,655][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:40:10,915][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:40:10,915][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:40:10,993][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,036][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,163][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:40:11,170][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:40:11,171][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:40:11,172][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:40:11,205][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,243][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,264][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:40:11,265][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:40:11,266][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:40:11,267][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:40:11,293][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:40:11,353][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:40:11,354][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:40:11,354][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:40:11,355][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:40:11,356][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:40:11,356][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:40:11,356][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:40:11,356][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:40:11,357][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 11:40:11,357][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:40:11,357][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:40:11,358][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 11:40:11,358][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:40:11,358][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:40:11,358][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:40:11,359][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 11:40:11,359][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:40:11,359][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 11:40:11,359][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:40:11,359][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:40:11,359][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:40:11,359][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 11:40:11,360][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:40:15,645][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:40:15,646][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:40:15,646][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 11:40:15,646][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:40:15,650][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:40:15,650][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:40:15,650][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:40:15,651][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:40:15,651][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:40:15,652][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:40:15,652][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.5001Epoch 1/15: [                              ] 2/63 batches, loss: 0.3515Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2800Epoch 1/15: [=                             ] 4/63 batches, loss: 0.3270Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3062Epoch 1/15: [==                            ] 6/63 batches, loss: 0.2922Epoch 1/15: [===                           ] 7/63 batches, loss: 0.2864Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2875Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2859Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2698Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2643Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2535Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2541Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2440Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2388Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2392Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2391Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2328Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2360Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2322Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2348Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2319Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2276Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2288Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2320Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2289Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2294Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2326Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2328Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2316Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2272Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2247Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2202Epoch 1/15: [================              ] 34/63 batches, loss: 0.2191Epoch 1/15: [================              ] 35/63 batches, loss: 0.2181Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2167Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2177Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2191Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2202Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2166Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2154Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2144Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2134Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2098Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2081Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2079Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2092Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2102Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2106Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2110Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2099Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2111Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2096Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2085Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2080Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2090Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2072Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2061Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2067Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2052Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2037Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2020Epoch 1/15: [==============================] 63/63 batches, loss: 0.1989
[2025-05-02 11:40:20,182][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1989
[2025-05-02 11:40:20,366][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0771, Metrics: {'mse': 0.07456333190202713, 'rmse': 0.273062871701788, 'r2': -0.5190664529800415}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1973Epoch 2/15: [                              ] 2/63 batches, loss: 0.1470Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1738Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1700Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1631Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1525Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1464Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1430Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1452Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1502Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1430Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1397Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1496Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1484Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1467Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1425Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1435Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1400Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1423Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1517Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1496Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1507Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1494Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1475Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1467Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1433Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1405Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1385Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1373Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1382Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1377Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1381Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1373Epoch 2/15: [================              ] 34/63 batches, loss: 0.1375Epoch 2/15: [================              ] 35/63 batches, loss: 0.1353Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1340Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1339Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1330Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1344Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1345Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1340Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1342Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1353Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1362Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1354Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1352Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1352Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1337Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1325Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1336Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1336Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1336Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1344Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1339Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1335Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1327Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1315Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1323Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1312Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1301Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1297Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1288Epoch 2/15: [==============================] 63/63 batches, loss: 0.1270
[2025-05-02 11:40:22,671][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1270
[2025-05-02 11:40:22,870][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0649, Metrics: {'mse': 0.06296280771493912, 'rmse': 0.25092390821709104, 'r2': -0.2827308177947998}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0892Epoch 3/15: [                              ] 2/63 batches, loss: 0.0907Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1082Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1156Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1111Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1082Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1180Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1159Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1139Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1147Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1131Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1117Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1109Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1166Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1169Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1183Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1139Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1146Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1156Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1131Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1146Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1149Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1152Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1133Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1122Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1127Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1111Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1098Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1096Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1078Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1108Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1115Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1106Epoch 3/15: [================              ] 34/63 batches, loss: 0.1113Epoch 3/15: [================              ] 35/63 batches, loss: 0.1104Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1100Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1088Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1087Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1089Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1091Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1107Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1099Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1111Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1106Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1092Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1086Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1098Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1106Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1109Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1125Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1136Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1165Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1171Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1178Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1174Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1171Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1161Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1159Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1149Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1154Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1155Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1150Epoch 3/15: [==============================] 63/63 batches, loss: 0.1133
[2025-05-02 11:40:25,193][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1133
[2025-05-02 11:40:25,394][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0574, Metrics: {'mse': 0.05582683905959129, 'rmse': 0.23627703879046583, 'r2': -0.1373509168624878}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0896Epoch 4/15: [                              ] 2/63 batches, loss: 0.0901Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0962Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1047Epoch 4/15: [==                            ] 5/63 batches, loss: 0.0998Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1018Epoch 4/15: [===                           ] 7/63 batches, loss: 0.0986Epoch 4/15: [===                           ] 8/63 batches, loss: 0.0971Epoch 4/15: [====                          ] 9/63 batches, loss: 0.0999Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1005Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1041Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1029Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1112Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1096Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1060Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1046Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1030Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1020Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1038Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1026Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1024Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1022Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1067Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1048Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1050Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1027Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1033Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1027Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1024Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1025Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1047Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1040Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1020Epoch 4/15: [================              ] 34/63 batches, loss: 0.1020Epoch 4/15: [================              ] 35/63 batches, loss: 0.1007Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1000Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1005Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1007Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1012Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1015Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1006Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1003Epoch 4/15: [====================          ] 43/63 batches, loss: 0.0997Epoch 4/15: [====================          ] 44/63 batches, loss: 0.0992Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.0996Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.0998Epoch 4/15: [======================        ] 47/63 batches, loss: 0.0986Epoch 4/15: [======================        ] 48/63 batches, loss: 0.0985Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1007Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1002Epoch 4/15: [========================      ] 51/63 batches, loss: 0.0990Epoch 4/15: [========================      ] 52/63 batches, loss: 0.0990Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.0981Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.0978Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.0970Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.0966Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.0965Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.0964Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0960Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0955Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0963Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0955Epoch 4/15: [==============================] 63/63 batches, loss: 0.0958
[2025-05-02 11:40:27,701][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0958
[2025-05-02 11:40:27,912][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0532, Metrics: {'mse': 0.051958117634058, 'rmse': 0.22794323335878605, 'r2': -0.05853414535522461}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1958Epoch 5/15: [                              ] 2/63 batches, loss: 0.1592Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1228Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1176Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1020Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0955Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0891Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0938Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0980Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0941Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0922Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0951Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1018Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1014Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1037Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1031Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1035Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1026Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1011Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1012Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0999Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0981Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0962Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0968Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0964Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0965Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0952Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0958Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0935Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0960Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0957Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0957Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0948Epoch 5/15: [================              ] 34/63 batches, loss: 0.0958Epoch 5/15: [================              ] 35/63 batches, loss: 0.0953Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0949Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0948Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0943Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0942Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0933Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0943Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0939Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0947Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0941Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0929Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0924Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0926Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0921Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0914Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0909Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0911Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0906Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0904Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0908Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0901Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0893Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0889Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0889Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0886Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0887Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0882Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0875Epoch 5/15: [==============================] 63/63 batches, loss: 0.0876
[2025-05-02 11:40:30,211][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0876
[2025-05-02 11:40:30,432][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0510, Metrics: {'mse': 0.05000246688723564, 'rmse': 0.22361231380949403, 'r2': -0.0186920166015625}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0944Epoch 6/15: [                              ] 2/63 batches, loss: 0.1160Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1068Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0933Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0910Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0845Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0877Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0802Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0838Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0842Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0786Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0860Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0821Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0840Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0837Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0815Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0807Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0829Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0852Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0838Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0822Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0816Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0822Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0828Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0821Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0828Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0825Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0827Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0819Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0815Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0801Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0793Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0784Epoch 6/15: [================              ] 34/63 batches, loss: 0.0778Epoch 6/15: [================              ] 35/63 batches, loss: 0.0787Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0773Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0776Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0784Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0786Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0782Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0783Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0781Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0779Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0775Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0781Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0773Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0773Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0767Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0766Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0756Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0755Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0752Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0753Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0756Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0751Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0753Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0757Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0755Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0759Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0761Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0757Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0755Epoch 6/15: [==============================] 63/63 batches, loss: 0.0747
[2025-05-02 11:40:32,737][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0747
[2025-05-02 11:40:32,955][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0469, Metrics: {'mse': 0.04593970626592636, 'rmse': 0.21433549931340437, 'r2': 0.06407797336578369}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0654Epoch 7/15: [                              ] 2/63 batches, loss: 0.0760Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0743Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0704Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0662Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0739Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0808Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0815Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0802Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0773Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0763Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0732Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0769Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0774Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0760Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0741Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0736Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0734Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0741Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0721Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0706Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0701Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0709Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0711Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0706Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0686Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0703Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0723Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0727Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0714Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0719Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0716Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0724Epoch 7/15: [================              ] 34/63 batches, loss: 0.0711Epoch 7/15: [================              ] 35/63 batches, loss: 0.0707Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0713Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0711Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0720Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0716Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0718Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0713Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0717Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0707Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0707Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0715Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0717Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0720Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0722Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0748Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0740Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0735Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0732Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0735Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0734Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0730Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0722Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0722Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0722Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0721Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0716Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0710Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0706Epoch 7/15: [==============================] 63/63 batches, loss: 0.0695
[2025-05-02 11:40:35,235][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0695
[2025-05-02 11:40:35,434][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0459, Metrics: {'mse': 0.045161936432123184, 'rmse': 0.21251337941909254, 'r2': 0.07992333173751831}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0527Epoch 8/15: [                              ] 2/63 batches, loss: 0.0535Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0504Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0541Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0502Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0479Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0494Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0559Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0560Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0547Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0535Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0539Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0547Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0572Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0573Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0569Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0559Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0574Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0580Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0583Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0606Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0610Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0599Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0608Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0608Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0596Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0587Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0611Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0624Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0627Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0637Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0640Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0644Epoch 8/15: [================              ] 34/63 batches, loss: 0.0648Epoch 8/15: [================              ] 35/63 batches, loss: 0.0658Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0657Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0653Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0648Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0638Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0641Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0641Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0634Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0639Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0649Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0644Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0636Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0636Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0634Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0628Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0623Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0621Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0624Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0619Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0619Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0613Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0609Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0614Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0613Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0608Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0605Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0607Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0605Epoch 8/15: [==============================] 63/63 batches, loss: 0.0623
[2025-05-02 11:40:37,821][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0623
[2025-05-02 11:40:38,036][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0450, Metrics: {'mse': 0.04427691921591759, 'rmse': 0.2104208145976001, 'r2': 0.0979536771774292}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1065Epoch 9/15: [                              ] 2/63 batches, loss: 0.0823Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0728Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0669Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0643Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0610Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0584Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0571Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0550Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0586Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0575Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0573Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0558Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0559Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0549Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0545Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0566Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0587Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0594Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0590Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0575Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0577Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0594Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0591Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0585Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0584Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0577Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0567Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0563Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0561Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0571Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0578Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0583Epoch 9/15: [================              ] 34/63 batches, loss: 0.0585Epoch 9/15: [================              ] 35/63 batches, loss: 0.0578Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0574Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0567Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0562Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0561Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0566Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0561Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0576Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0588Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0584Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0581Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0580Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0582Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0586Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0592Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0592Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0592Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0592Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0592Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0600Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0602Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0604Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0599Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0599Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0594Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0595Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0598Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0596Epoch 9/15: [==============================] 63/63 batches, loss: 0.0588
[2025-05-02 11:40:40,392][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0588
[2025-05-02 11:40:40,614][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0438, Metrics: {'mse': 0.043116144835948944, 'rmse': 0.20764427474878508, 'r2': 0.12160193920135498}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1493Epoch 10/15: [                              ] 2/63 batches, loss: 0.1033Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0873Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0731Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0721Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0689Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0644Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0599Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0615Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0631Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0613Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0596Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0591Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0583Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0603Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0583Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0624Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0623Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0629Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0631Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0628Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0608Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0600Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0593Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0595Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0605Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0601Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0590Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0597Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0595Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0587Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0582Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0574Epoch 10/15: [================              ] 34/63 batches, loss: 0.0572Epoch 10/15: [================              ] 35/63 batches, loss: 0.0572Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0565Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0558Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0562Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0558Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0553Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0547Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0546Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0547Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0548Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0546Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0543Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0551Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0561Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0558Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0552Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0553Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0555Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0562Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0559Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0555Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0553Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0549Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0548Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0542Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0541Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0546Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0546Epoch 10/15: [==============================] 63/63 batches, loss: 0.0538
[2025-05-02 11:40:42,965][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0538
[2025-05-02 11:40:43,194][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0446, Metrics: {'mse': 0.043847884982824326, 'rmse': 0.2093988657629843, 'r2': 0.10669434070587158}
[2025-05-02 11:40:43,195][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0594Epoch 11/15: [                              ] 2/63 batches, loss: 0.0520Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0480Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0470Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0440Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0463Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0454Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0470Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0455Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0479Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0457Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0472Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0469Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0469Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0492Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0499Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0490Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0486Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0498Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0509Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0518Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0508Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0512Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0502Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0497Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0489Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0493Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0486Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0486Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0476Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0472Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0472Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0478Epoch 11/15: [================              ] 34/63 batches, loss: 0.0494Epoch 11/15: [================              ] 35/63 batches, loss: 0.0496Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0506Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0512Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0510Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0511Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0505Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0503Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0501Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0507Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0508Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0506Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0507Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0508Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0512Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0510Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0516Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0511Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0506Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0507Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0503Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0502Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0504Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0505Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0501Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0496Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0497Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0494Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0490Epoch 11/15: [==============================] 63/63 batches, loss: 0.0491
[2025-05-02 11:40:45,143][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0491
[2025-05-02 11:40:45,372][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0425, Metrics: {'mse': 0.04156344756484032, 'rmse': 0.2038711543226268, 'r2': 0.1532348394393921}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0318Epoch 12/15: [                              ] 2/63 batches, loss: 0.0515Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0509Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0564Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0602Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0581Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0571Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0592Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0606Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0599Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0604Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0583Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0590Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0597Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0582Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0563Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0571Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0570Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0568Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0556Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0546Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0548Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0552Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0541Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0540Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0528Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0530Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0527Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0517Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0514Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0515Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0520Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0513Epoch 12/15: [================              ] 34/63 batches, loss: 0.0521Epoch 12/15: [================              ] 35/63 batches, loss: 0.0517Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0514Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0514Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0522Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0518Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0519Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0519Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0510Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0514Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0517Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0512Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0510Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0513Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0507Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0506Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0503Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0502Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0499Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0506Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0504Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0506Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0507Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0509Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0517Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0514Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0509Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0507Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0506Epoch 12/15: [==============================] 63/63 batches, loss: 0.0503
[2025-05-02 11:40:47,743][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0503
[2025-05-02 11:40:47,958][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0415, Metrics: {'mse': 0.040318552404642105, 'rmse': 0.200794801737102, 'r2': 0.17859679460525513}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0646Epoch 13/15: [                              ] 2/63 batches, loss: 0.0581Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0551Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0521Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0479Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0478Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0441Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0432Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0437Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0425Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0456Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0462Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0483Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0497Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0517Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0519Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0507Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0497Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0490Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0504Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0506Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0504Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0494Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0496Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0503Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0499Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0505Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0499Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0501Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0499Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0497Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0496Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0501Epoch 13/15: [================              ] 34/63 batches, loss: 0.0503Epoch 13/15: [================              ] 35/63 batches, loss: 0.0504Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0501Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0508Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0507Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0505Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0505Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0500Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0501Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0496Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0495Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0496Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0494Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0500Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0502Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0494Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0491Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0486Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0484Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0489Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0485Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0482Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0479Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0479Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0478Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0479Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0479Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0484Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0486Epoch 13/15: [==============================] 63/63 batches, loss: 0.0488
[2025-05-02 11:40:50,292][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0488
[2025-05-02 11:40:50,517][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0443, Metrics: {'mse': 0.04333150014281273, 'rmse': 0.20816219671883926, 'r2': 0.11721450090408325}
[2025-05-02 11:40:50,518][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0847Epoch 14/15: [                              ] 2/63 batches, loss: 0.0720Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0638Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0551Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0551Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0559Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0524Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0509Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0499Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0487Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0488Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0494Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0467Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0468Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0468Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0467Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0470Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0482Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0469Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0471Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0470Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0455Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0456Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0452Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0452Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0448Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0442Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0437Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0434Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0431Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0438Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0435Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0432Epoch 14/15: [================              ] 34/63 batches, loss: 0.0433Epoch 14/15: [================              ] 35/63 batches, loss: 0.0429Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0427Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0426Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0435Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0444Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0448Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0447Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0442Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0444Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0445Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0442Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0439Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0441Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0444Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0446Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0449Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0449Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0450Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0451Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0452Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0447Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0445Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0443Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0444Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0448Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0445Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0444Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0442Epoch 14/15: [==============================] 63/63 batches, loss: 0.0439
[2025-05-02 11:40:52,466][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0439
[2025-05-02 11:40:52,669][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0401, Metrics: {'mse': 0.03895459324121475, 'rmse': 0.19736918006926701, 'r2': 0.206384539604187}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0241Epoch 15/15: [                              ] 2/63 batches, loss: 0.0240Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0268Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0404Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0381Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0381Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0377Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0369Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0377Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0357Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0369Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0388Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0379Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0386Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0394Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0397Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0401Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0403Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0422Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0417Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0415Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0409Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0408Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0408Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0415Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0410Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0406Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0408Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0400Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0392Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0392Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0385Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0381Epoch 15/15: [================              ] 34/63 batches, loss: 0.0381Epoch 15/15: [================              ] 35/63 batches, loss: 0.0385Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0389Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0386Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0386Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0382Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0384Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0383Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0380Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0385Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0385Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0392Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0389Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0397Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0397Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0394Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0390Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0388Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0389Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0392Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0389Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0394Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0393Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0394Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0393Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0390Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0393Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0391Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0394Epoch 15/15: [==============================] 63/63 batches, loss: 0.0390
[2025-05-02 11:40:55,077][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0390
[2025-05-02 11:40:55,294][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0397, Metrics: {'mse': 0.03857932612299919, 'rmse': 0.19641620636546056, 'r2': 0.21402978897094727}
[2025-05-02 11:40:55,671][src.training.lm_trainer][INFO] - Training completed in 38.25 seconds
[2025-05-02 11:40:55,671][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:40:58,178][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.015390520915389061, 'rmse': 0.12405853826073021, 'r2': 0.2700517177581787}
[2025-05-02 11:40:58,178][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03857932612299919, 'rmse': 0.19641620636546056, 'r2': 0.21402978897094727}
[2025-05-02 11:40:58,178][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03287644684314728, 'rmse': 0.18131863346922533, 'r2': 0.29015618562698364}
[2025-05-02 11:40:59,893][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar/ar/model.pt
[2025-05-02 11:40:59,895][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▄▃▂▂▂▂▂▁▁▁
wandb:     best_val_mse █▆▄▄▃▂▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▃▅▅▆▇▇▇▇▇███
wandb:    best_val_rmse █▆▅▄▃▃▂▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▅▅▅▆▆▆▆▆▆▆▆
wandb:       train_loss █▅▄▃▃▃▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▄▃▂▂▂▂▂▂▁▂▁▁
wandb:          val_mse █▆▄▄▃▂▂▂▂▂▂▁▂▁▁
wandb:           val_r2 ▁▃▅▅▆▇▇▇▇▇▇█▇██
wandb:         val_rmse █▆▅▄▃▃▂▂▂▂▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03969
wandb:     best_val_mse 0.03858
wandb:      best_val_r2 0.21403
wandb:    best_val_rmse 0.19642
wandb:            epoch 15
wandb:   final_test_mse 0.03288
wandb:    final_test_r2 0.29016
wandb:  final_test_rmse 0.18132
wandb:  final_train_mse 0.01539
wandb:   final_train_r2 0.27005
wandb: final_train_rmse 0.12406
wandb:    final_val_mse 0.03858
wandb:     final_val_r2 0.21403
wandb:   final_val_rmse 0.19642
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03904
wandb:       train_time 38.24679
wandb:         val_loss 0.03969
wandb:          val_mse 0.03858
wandb:           val_r2 0.21403
wandb:         val_rmse 0.19642
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114007-bokkwxfy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114007-bokkwxfy/logs
Experiment probe_layer4_avg_links_len_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar/results.json
=======================
PROBING LAYER 6 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer6_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:41:11,490][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar
experiment_name: probe_layer6_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 11:41:11,490][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 11:41:11,490][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:41:11,490][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:41:11,490][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:41:11,495][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 11:41:11,495][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:41:11,495][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:41:12,894][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:41:15,157][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:41:15,157][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:41:15,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,222][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,310][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:41:15,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:41:15,318][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:41:15,318][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:41:15,336][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,364][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,376][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:41:15,377][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:41:15,377][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:41:15,378][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:41:15,396][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,425][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:41:15,437][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:41:15,438][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:41:15,438][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:41:15,439][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:41:15,439][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:41:15,440][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 11:41:15,440][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 11:41:15,440][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:41:15,441][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 11:41:15,441][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:41:15,441][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:41:15,442][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:41:15,442][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:41:15,442][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:41:15,443][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 11:41:15,443][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:41:19,311][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:41:19,312][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:41:19,312][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 11:41:19,312][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:41:19,316][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:41:19,317][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:41:19,317][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:41:19,317][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:41:19,317][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:41:19,318][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:41:19,318][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7782Epoch 1/15: [                              ] 2/63 batches, loss: 0.5990Epoch 1/15: [=                             ] 3/63 batches, loss: 0.4749Epoch 1/15: [=                             ] 4/63 batches, loss: 0.5157Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4591Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4192Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4146Epoch 1/15: [===                           ] 8/63 batches, loss: 0.3968Epoch 1/15: [====                          ] 9/63 batches, loss: 0.3776Epoch 1/15: [====                          ] 10/63 batches, loss: 0.3549Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.3442Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.3286Epoch 1/15: [======                        ] 13/63 batches, loss: 0.3216Epoch 1/15: [======                        ] 14/63 batches, loss: 0.3091Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.3011Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2957Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2922Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2872Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2854Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2806Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2814Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2765Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2718Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2701Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2679Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2676Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2687Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2681Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2655Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2633Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2590Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2562Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2523Epoch 1/15: [================              ] 34/63 batches, loss: 0.2507Epoch 1/15: [================              ] 35/63 batches, loss: 0.2482Epoch 1/15: [=================             ] 36/63 batches, loss: 0.2489Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2494Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2494Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2478Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2442Epoch 1/15: [===================           ] 41/63 batches, loss: 0.2413Epoch 1/15: [====================          ] 42/63 batches, loss: 0.2397Epoch 1/15: [====================          ] 43/63 batches, loss: 0.2379Epoch 1/15: [====================          ] 44/63 batches, loss: 0.2348Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.2327Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.2312Epoch 1/15: [======================        ] 47/63 batches, loss: 0.2336Epoch 1/15: [======================        ] 48/63 batches, loss: 0.2346Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.2327Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.2327Epoch 1/15: [========================      ] 51/63 batches, loss: 0.2329Epoch 1/15: [========================      ] 52/63 batches, loss: 0.2341Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.2317Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.2323Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.2300Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.2292Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.2280Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.2282Epoch 1/15: [============================  ] 59/63 batches, loss: 0.2290Epoch 1/15: [============================  ] 60/63 batches, loss: 0.2285Epoch 1/15: [============================= ] 61/63 batches, loss: 0.2271Epoch 1/15: [============================= ] 62/63 batches, loss: 0.2252Epoch 1/15: [==============================] 63/63 batches, loss: 0.2242
[2025-05-02 11:41:23,683][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2242
[2025-05-02 11:41:23,856][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0713, Metrics: {'mse': 0.07026985287666321, 'rmse': 0.2650846145604516, 'r2': -0.431596040725708}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1996Epoch 2/15: [                              ] 2/63 batches, loss: 0.1659Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1773Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1803Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1649Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1592Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1523Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1487Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1447Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1494Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1575Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1533Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1631Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1601Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1583Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1574Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1549Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1512Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1532Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1591Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1558Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1573Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1553Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1563Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1550Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1520Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1489Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1493Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1509Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1489Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1483Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1472Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1471Epoch 2/15: [================              ] 34/63 batches, loss: 0.1458Epoch 2/15: [================              ] 35/63 batches, loss: 0.1439Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1416Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1412Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1419Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1425Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1429Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1413Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1408Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1444Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1450Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1444Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1453Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1447Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1434Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1417Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1421Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1425Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1432Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1443Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1441Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1448Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1446Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1438Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1449Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1442Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1430Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1432Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1420Epoch 2/15: [==============================] 63/63 batches, loss: 0.1410
[2025-05-02 11:41:26,186][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1410
[2025-05-02 11:41:26,378][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0627, Metrics: {'mse': 0.06157650053501129, 'rmse': 0.24814612738265993, 'r2': -0.25448787212371826}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1194Epoch 3/15: [                              ] 2/63 batches, loss: 0.1236Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1294Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1335Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1248Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1236Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1275Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1365Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1326Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1369Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1347Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1353Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1328Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1412Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1381Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1369Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1335Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1354Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1338Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1340Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1370Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1374Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1371Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1341Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1337Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1329Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1301Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1333Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1327Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1322Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1372Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1381Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1377Epoch 3/15: [================              ] 34/63 batches, loss: 0.1374Epoch 3/15: [================              ] 35/63 batches, loss: 0.1348Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1329Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1321Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1311Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1303Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1314Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1320Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1303Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1305Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1301Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1282Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1285Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1287Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1281Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1281Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1294Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1302Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1320Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1317Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1316Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1303Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1295Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1281Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1285Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1284Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1282Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1272Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1275Epoch 3/15: [==============================] 63/63 batches, loss: 0.1261
[2025-05-02 11:41:28,706][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1261
[2025-05-02 11:41:28,909][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0534, Metrics: {'mse': 0.052474893629550934, 'rmse': 0.22907399160435243, 'r2': -0.06906235218048096}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0705Epoch 4/15: [                              ] 2/63 batches, loss: 0.0960Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0964Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1058Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1272Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1247Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1238Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1158Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1125Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1104Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1176Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1142Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1230Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1194Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1178Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1175Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1163Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1176Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1179Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1169Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1195Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1190Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1236Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1207Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1217Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1193Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1191Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1196Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1218Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1207Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1220Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1220Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1209Epoch 4/15: [================              ] 34/63 batches, loss: 0.1226Epoch 4/15: [================              ] 35/63 batches, loss: 0.1219Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1209Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1217Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1197Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1211Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1197Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1177Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1171Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1162Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1150Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1149Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1148Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1137Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1125Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1157Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1144Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1139Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1130Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1123Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1118Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1107Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1103Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1101Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1102Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1093Epoch 4/15: [============================  ] 60/63 batches, loss: 0.1086Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1096Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1094Epoch 4/15: [==============================] 63/63 batches, loss: 0.1096
[2025-05-02 11:41:31,191][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1096
[2025-05-02 11:41:31,400][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0458, Metrics: {'mse': 0.045105621218681335, 'rmse': 0.2123808400460864, 'r2': 0.0810706615447998}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1960Epoch 5/15: [                              ] 2/63 batches, loss: 0.1338Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1145Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0998Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1022Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0949Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0905Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1007Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1028Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0989Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0983Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0998Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0992Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0991Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1003Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1005Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1025Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1018Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1011Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1008Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0997Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0981Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0962Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0965Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0949Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0955Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0935Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0937Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0927Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0960Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0954Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0961Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0951Epoch 5/15: [================              ] 34/63 batches, loss: 0.0980Epoch 5/15: [================              ] 35/63 batches, loss: 0.0976Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0965Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0954Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0959Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0960Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0951Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0956Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0949Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0956Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0947Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0945Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0957Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0962Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0958Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0952Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0945Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0934Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0932Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0930Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0936Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0929Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0924Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0931Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0932Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0930Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0933Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0928Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0924Epoch 5/15: [==============================] 63/63 batches, loss: 0.0915
[2025-05-02 11:41:33,681][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0915
[2025-05-02 11:41:33,906][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0463, Metrics: {'mse': 0.04572180286049843, 'rmse': 0.21382657192336604, 'r2': 0.06851732730865479}
[2025-05-02 11:41:33,907][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1103Epoch 6/15: [                              ] 2/63 batches, loss: 0.1539Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1198Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1113Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1093Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1042Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1067Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0990Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0987Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0999Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0957Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1032Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0978Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0960Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0958Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0929Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0911Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0936Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0980Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0963Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0955Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0935Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0954Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0963Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0961Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0962Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0968Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0969Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0954Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0949Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0940Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0927Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0912Epoch 6/15: [================              ] 34/63 batches, loss: 0.0906Epoch 6/15: [================              ] 35/63 batches, loss: 0.0913Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0897Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0895Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0909Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0919Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0918Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0912Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0917Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0912Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0912Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0915Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0911Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0911Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0904Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0900Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0892Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0893Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0889Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0891Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0890Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0888Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0881Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0888Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0885Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0887Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0885Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0883Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0883Epoch 6/15: [==============================] 63/63 batches, loss: 0.0895
[2025-05-02 11:41:35,857][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0895
[2025-05-02 11:41:36,078][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0438, Metrics: {'mse': 0.04302722215652466, 'rmse': 0.20743004159601536, 'r2': 0.1234135627746582}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0860Epoch 7/15: [                              ] 2/63 batches, loss: 0.1072Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0918Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0900Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0800Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0841Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0919Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0883Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0909Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0877Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0876Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0824Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0828Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0845Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0808Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0786Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0788Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0800Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0816Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0812Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0781Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0773Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0786Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0774Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0765Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0747Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0754Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0774Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0777Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0766Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0777Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0781Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0803Epoch 7/15: [================              ] 34/63 batches, loss: 0.0790Epoch 7/15: [================              ] 35/63 batches, loss: 0.0785Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0793Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0788Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0791Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0785Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0786Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0777Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0775Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0769Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0773Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0779Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0769Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0772Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0769Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0790Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0784Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0780Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0773Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0772Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0767Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0764Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0766Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0762Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0766Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0763Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0760Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0753Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0749Epoch 7/15: [==============================] 63/63 batches, loss: 0.0743
[2025-05-02 11:41:38,403][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0743
[2025-05-02 11:41:38,613][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0423, Metrics: {'mse': 0.04154544323682785, 'rmse': 0.20382699339593824, 'r2': 0.15360158681869507}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0597Epoch 8/15: [                              ] 2/63 batches, loss: 0.0572Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0647Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0702Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0666Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0636Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0699Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0720Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0729Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0720Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0701Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0709Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0693Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0718Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0716Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0709Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0701Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0723Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0718Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0728Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0737Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0745Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0741Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0734Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0731Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0713Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0712Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0725Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0739Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0736Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0756Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0750Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0767Epoch 8/15: [================              ] 34/63 batches, loss: 0.0767Epoch 8/15: [================              ] 35/63 batches, loss: 0.0781Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0780Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0777Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0776Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0768Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0771Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0766Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0758Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0762Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0762Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0758Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0751Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0753Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0750Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0740Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0735Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0731Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0741Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0738Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0739Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0732Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0726Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0736Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0738Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0734Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0728Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0725Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0722Epoch 8/15: [==============================] 63/63 batches, loss: 0.0722
[2025-05-02 11:41:40,990][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0722
[2025-05-02 11:41:41,214][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0429, Metrics: {'mse': 0.042308833450078964, 'rmse': 0.2056911117430186, 'r2': 0.13804912567138672}
[2025-05-02 11:41:41,214][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1423Epoch 9/15: [                              ] 2/63 batches, loss: 0.0899Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0758Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0675Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0639Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0595Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0621Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0627Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0592Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0616Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0629Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0634Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0604Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0616Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0594Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0605Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0615Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0633Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0628Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0623Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0608Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0611Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0616Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0609Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0599Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0589Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0604Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0593Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0585Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0594Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0610Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0638Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0647Epoch 9/15: [================              ] 34/63 batches, loss: 0.0645Epoch 9/15: [================              ] 35/63 batches, loss: 0.0634Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0631Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0625Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0620Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0622Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0617Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0613Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0621Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0625Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0626Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0625Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0632Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0635Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0637Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0644Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0642Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0646Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0644Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0642Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0647Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0648Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0650Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0646Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0647Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0642Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0641Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0638Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0637Epoch 9/15: [==============================] 63/63 batches, loss: 0.0629
[2025-05-02 11:41:43,165][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0629
[2025-05-02 11:41:43,383][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0439, Metrics: {'mse': 0.04325103387236595, 'rmse': 0.20796882908831782, 'r2': 0.1188538670539856}
[2025-05-02 11:41:43,384][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1287Epoch 10/15: [                              ] 2/63 batches, loss: 0.0983Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0894Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0821Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0811Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0780Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0721Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0675Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0703Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0715Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0682Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0687Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0667Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0643Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0651Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0634Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0660Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0656Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0665Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0688Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0688Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0672Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0672Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0662Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0664Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0674Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0672Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0655Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0657Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0661Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0655Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0651Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0647Epoch 10/15: [================              ] 34/63 batches, loss: 0.0649Epoch 10/15: [================              ] 35/63 batches, loss: 0.0651Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0647Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0641Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0637Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0632Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0629Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0622Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0622Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0618Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0621Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0616Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0615Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0629Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0636Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0630Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0623Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0619Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0617Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0623Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0624Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0619Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0619Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0617Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0615Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0609Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0606Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0609Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0613Epoch 10/15: [==============================] 63/63 batches, loss: 0.0607
[2025-05-02 11:41:45,312][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0607
[2025-05-02 11:41:45,530][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0444, Metrics: {'mse': 0.043772101402282715, 'rmse': 0.20921783241942526, 'r2': 0.10823822021484375}
[2025-05-02 11:41:45,532][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0598Epoch 11/15: [                              ] 2/63 batches, loss: 0.0507Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0480Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0424Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0400Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0471Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0518Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0515Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0499Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0509Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0494Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0505Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0511Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0513Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0518Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0529Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0531Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0518Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0525Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0550Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0560Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0546Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0549Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0548Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0539Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0529Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0536Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0528Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0522Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0530Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0529Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0540Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0549Epoch 11/15: [================              ] 34/63 batches, loss: 0.0555Epoch 11/15: [================              ] 35/63 batches, loss: 0.0555Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0558Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0560Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0554Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0548Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0541Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0547Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0547Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0553Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0548Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0552Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0549Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0556Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0558Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0552Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0558Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0558Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0554Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0556Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0550Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0551Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0552Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0551Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0547Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0541Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0538Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0537Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0532Epoch 11/15: [==============================] 63/63 batches, loss: 0.0533
[2025-05-02 11:41:47,475][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0533
[2025-05-02 11:41:47,686][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0429, Metrics: {'mse': 0.04211604222655296, 'rmse': 0.20522193407760528, 'r2': 0.14197689294815063}
[2025-05-02 11:41:47,687][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:41:47,687][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-02 11:41:47,687][src.training.lm_trainer][INFO] - Training completed in 26.94 seconds
[2025-05-02 11:41:47,687][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:41:50,209][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02499011717736721, 'rmse': 0.15808262768997489, 'r2': -0.18524205684661865}
[2025-05-02 11:41:50,209][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04154544323682785, 'rmse': 0.20382699339593824, 'r2': 0.15360158681869507}
[2025-05-02 11:41:50,209][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0529649592936039, 'rmse': 0.23014117253026217, 'r2': -0.14358007907867432}
[2025-05-02 11:41:51,886][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar/ar/model.pt
[2025-05-02 11:41:51,888][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▂▁▁
wandb:     best_val_mse █▆▄▂▁▁
wandb:      best_val_r2 ▁▃▅▇██
wandb:    best_val_rmse █▆▄▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▅▅▅▆▅▅▅
wandb:       train_loss █▅▄▃▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▂▂▁▁▁▁▂▁
wandb:          val_mse █▆▄▂▂▁▁▁▁▂▁
wandb:           val_r2 ▁▃▅▇▇████▇█
wandb:         val_rmse █▆▄▂▂▁▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04229
wandb:     best_val_mse 0.04155
wandb:      best_val_r2 0.1536
wandb:    best_val_rmse 0.20383
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.05296
wandb:    final_test_r2 -0.14358
wandb:  final_test_rmse 0.23014
wandb:  final_train_mse 0.02499
wandb:   final_train_r2 -0.18524
wandb: final_train_rmse 0.15808
wandb:    final_val_mse 0.04155
wandb:     final_val_r2 0.1536
wandb:   final_val_rmse 0.20383
wandb:    learning_rate 2e-05
wandb:       train_loss 0.05329
wandb:       train_time 26.9406
wandb:         val_loss 0.04286
wandb:          val_mse 0.04212
wandb:           val_r2 0.14198
wandb:         val_rmse 0.20522
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114111-2d5vqngf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114111-2d5vqngf/logs
Experiment probe_layer6_avg_links_len_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar/results.json
=======================
PROBING LAYER 9 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer9_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:42:01,352][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar
experiment_name: probe_layer9_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 11:42:01,352][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 11:42:01,352][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:42:01,353][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:42:01,353][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:42:01,357][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 11:42:01,357][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:42:01,357][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:42:02,384][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:42:04,798][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:42:04,798][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:04,826][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:04,857][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:04,933][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:42:04,941][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:04,941][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:42:04,942][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:04,965][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:05,001][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:05,015][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:42:05,016][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:05,016][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:42:05,017][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:05,041][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:05,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:05,088][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:42:05,089][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:05,090][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:42:05,091][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:42:05,091][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:05,091][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:05,091][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:05,091][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:05,091][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 11:42:05,092][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:05,092][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:05,092][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 11:42:05,093][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:05,093][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:42:05,093][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 11:42:05,093][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:42:05,094][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 11:42:05,094][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:42:05,094][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:42:05,094][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:42:05,094][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 11:42:05,094][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:42:08,778][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:42:08,778][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:42:08,779][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 11:42:08,779][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:42:08,783][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:42:08,783][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:42:08,783][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:42:08,783][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:42:08,784][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:42:08,784][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:42:08,784][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.4940Epoch 1/15: [                              ] 2/63 batches, loss: 0.3370Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2748Epoch 1/15: [=                             ] 4/63 batches, loss: 0.3110Epoch 1/15: [==                            ] 5/63 batches, loss: 0.3032Epoch 1/15: [==                            ] 6/63 batches, loss: 0.2735Epoch 1/15: [===                           ] 7/63 batches, loss: 0.2762Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2618Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2693Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2544Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2448Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2388Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2373Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2309Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.2268Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2247Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2253Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2191Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.2169Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.2118Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.2136Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.2089Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.2043Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.2020Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.2045Epoch 1/15: [============                  ] 26/63 batches, loss: 0.2010Epoch 1/15: [============                  ] 27/63 batches, loss: 0.2052Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.2079Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.2092Epoch 1/15: [==============                ] 30/63 batches, loss: 0.2095Epoch 1/15: [==============                ] 31/63 batches, loss: 0.2066Epoch 1/15: [===============               ] 32/63 batches, loss: 0.2044Epoch 1/15: [===============               ] 33/63 batches, loss: 0.2010Epoch 1/15: [================              ] 34/63 batches, loss: 0.1998Epoch 1/15: [================              ] 35/63 batches, loss: 0.1981Epoch 1/15: [=================             ] 36/63 batches, loss: 0.1999Epoch 1/15: [=================             ] 37/63 batches, loss: 0.2022Epoch 1/15: [==================            ] 38/63 batches, loss: 0.2042Epoch 1/15: [==================            ] 39/63 batches, loss: 0.2039Epoch 1/15: [===================           ] 40/63 batches, loss: 0.2015Epoch 1/15: [===================           ] 41/63 batches, loss: 0.1995Epoch 1/15: [====================          ] 42/63 batches, loss: 0.1975Epoch 1/15: [====================          ] 43/63 batches, loss: 0.1961Epoch 1/15: [====================          ] 44/63 batches, loss: 0.1936Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.1914Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.1910Epoch 1/15: [======================        ] 47/63 batches, loss: 0.1933Epoch 1/15: [======================        ] 48/63 batches, loss: 0.1934Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.1931Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.1937Epoch 1/15: [========================      ] 51/63 batches, loss: 0.1931Epoch 1/15: [========================      ] 52/63 batches, loss: 0.1953Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.1951Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.1959Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.1936Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.1935Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.1925Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.1922Epoch 1/15: [============================  ] 59/63 batches, loss: 0.1948Epoch 1/15: [============================  ] 60/63 batches, loss: 0.1938Epoch 1/15: [============================= ] 61/63 batches, loss: 0.1931Epoch 1/15: [============================= ] 62/63 batches, loss: 0.1921Epoch 1/15: [==============================] 63/63 batches, loss: 0.1990
[2025-05-02 11:42:13,518][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1990
[2025-05-02 11:42:13,698][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0603, Metrics: {'mse': 0.06021980941295624, 'rmse': 0.2453972481772284, 'r2': -0.22684812545776367}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2728Epoch 2/15: [                              ] 2/63 batches, loss: 0.2157Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2176Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1802Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1633Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1621Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1611Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1564Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1507Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1449Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1488Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1457Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1560Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1515Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1509Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1499Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1480Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1463Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1488Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1543Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1520Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1530Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1523Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1528Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1502Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1480Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1453Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1455Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1505Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1489Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1470Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1466Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1470Epoch 2/15: [================              ] 34/63 batches, loss: 0.1457Epoch 2/15: [================              ] 35/63 batches, loss: 0.1450Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1463Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1453Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1436Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1452Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1460Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1442Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1437Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1441Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1447Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1435Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1448Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1448Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1439Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1427Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1444Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1456Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1465Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1484Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1486Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1483Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1473Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1478Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1479Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1470Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1455Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1465Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1459Epoch 2/15: [==============================] 63/63 batches, loss: 0.1459
[2025-05-02 11:42:16,040][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1459
[2025-05-02 11:42:16,234][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0543, Metrics: {'mse': 0.054004404693841934, 'rmse': 0.2323884779713528, 'r2': -0.10022270679473877}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0708Epoch 3/15: [                              ] 2/63 batches, loss: 0.0623Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0841Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1022Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1025Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1060Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1126Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1226Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1286Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1236Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1208Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1183Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1180Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1245Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1200Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1222Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1189Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1189Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1187Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1211Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1233Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1236Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1238Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1210Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1206Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1201Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1193Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1188Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1166Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1157Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1213Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1220Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1219Epoch 3/15: [================              ] 34/63 batches, loss: 0.1211Epoch 3/15: [================              ] 35/63 batches, loss: 0.1193Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1179Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1183Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1178Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1170Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1199Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1205Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1192Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1186Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1180Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1170Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1166Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1162Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1165Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1167Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1174Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1181Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1201Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1198Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1202Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1194Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1190Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1182Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1204Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1210Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1207Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1202Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1198Epoch 3/15: [==============================] 63/63 batches, loss: 0.1182
[2025-05-02 11:42:18,572][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1182
[2025-05-02 11:42:18,781][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0497, Metrics: {'mse': 0.049182962626218796, 'rmse': 0.22177232159631372, 'r2': -0.001996278762817383}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0506Epoch 4/15: [                              ] 2/63 batches, loss: 0.0750Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0811Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1077Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1294Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1253Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1186Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1119Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1125Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1059Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1096Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1079Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1142Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1128Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1073Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1062Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1064Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1066Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1065Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1044Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1064Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1055Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1090Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1060Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1069Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1066Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1080Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1067Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1068Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1055Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1057Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1051Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1045Epoch 4/15: [================              ] 34/63 batches, loss: 0.1050Epoch 4/15: [================              ] 35/63 batches, loss: 0.1042Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1044Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1044Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1049Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1047Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1035Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1020Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1022Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1015Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1011Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1004Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1021Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1006Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1001Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1023Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1028Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1029Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1019Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1013Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1009Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1001Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1005Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1006Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1003Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0999Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0995Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0991Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0983Epoch 4/15: [==============================] 63/63 batches, loss: 0.0989
[2025-05-02 11:42:21,062][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0989
[2025-05-02 11:42:21,273][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0406, Metrics: {'mse': 0.03975518047809601, 'rmse': 0.19938701180893406, 'r2': 0.19007426500320435}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1303Epoch 5/15: [                              ] 2/63 batches, loss: 0.0951Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0963Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0841Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0796Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0737Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0724Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0736Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0745Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0749Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0761Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0816Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0809Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0832Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0837Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0840Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0871Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0894Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0893Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0897Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0898Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0888Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0866Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0867Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0875Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0889Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0869Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0869Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0865Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0880Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0876Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0882Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0873Epoch 5/15: [================              ] 34/63 batches, loss: 0.0889Epoch 5/15: [================              ] 35/63 batches, loss: 0.0890Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0885Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0882Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0893Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0905Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0900Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0902Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0895Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0909Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0902Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0904Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0914Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0922Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0916Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0906Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0893Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0892Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0892Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0893Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0888Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0883Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0880Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0893Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0892Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0885Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0885Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0882Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0881Epoch 5/15: [==============================] 63/63 batches, loss: 0.0876
[2025-05-02 11:42:23,548][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0876
[2025-05-02 11:42:23,748][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0392, Metrics: {'mse': 0.03824518620967865, 'rmse': 0.19556376507338635, 'r2': 0.22083717584609985}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0531Epoch 6/15: [                              ] 2/63 batches, loss: 0.1165Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1083Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0919Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0979Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0924Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0947Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0863Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0886Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0874Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0862Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0920Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0877Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0870Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0852Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0853Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0841Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0876Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0914Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0893Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0920Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0907Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0900Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0900Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0893Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0896Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0895Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0904Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0890Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0895Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0887Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0872Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0860Epoch 6/15: [================              ] 34/63 batches, loss: 0.0859Epoch 6/15: [================              ] 35/63 batches, loss: 0.0873Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0865Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0869Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0896Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0900Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0902Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0897Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0901Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0898Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0895Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0895Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0881Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0879Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0870Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0872Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0867Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0863Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0867Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0868Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0867Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0860Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0855Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0855Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0857Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0860Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0866Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0864Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0865Epoch 6/15: [==============================] 63/63 batches, loss: 0.0870
[2025-05-02 11:42:26,069][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0870
[2025-05-02 11:42:26,295][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0379, Metrics: {'mse': 0.036801040172576904, 'rmse': 0.19183597205054348, 'r2': 0.25025850534439087}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0852Epoch 7/15: [                              ] 2/63 batches, loss: 0.0771Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0703Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0665Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0595Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0617Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0695Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0688Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0719Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0710Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0694Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0683Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0691Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0710Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0698Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0673Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0666Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0663Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0670Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0666Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0658Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0644Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0655Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0659Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0657Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0637Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0651Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0662Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0670Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0660Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0655Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0657Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0673Epoch 7/15: [================              ] 34/63 batches, loss: 0.0663Epoch 7/15: [================              ] 35/63 batches, loss: 0.0663Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0661Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0659Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0665Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0659Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0656Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0649Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0651Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0643Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0644Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0647Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0651Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0650Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0651Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0674Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0670Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0677Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0675Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0680Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0677Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0675Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0675Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0671Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0675Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0672Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0670Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0667Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0664Epoch 7/15: [==============================] 63/63 batches, loss: 0.0656
[2025-05-02 11:42:28,615][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0656
[2025-05-02 11:42:28,811][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0367, Metrics: {'mse': 0.035660650581121445, 'rmse': 0.18884027796294264, 'r2': 0.2734915018081665}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0295Epoch 8/15: [                              ] 2/63 batches, loss: 0.0543Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0570Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0657Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0643Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0642Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0624Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0646Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0634Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0623Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0624Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0613Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0609Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0619Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0635Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0639Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0623Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0632Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0630Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0647Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0669Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0667Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0661Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0661Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0659Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0639Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0655Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0660Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0673Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0677Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0697Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0696Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0726Epoch 8/15: [================              ] 34/63 batches, loss: 0.0724Epoch 8/15: [================              ] 35/63 batches, loss: 0.0732Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0730Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0723Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0724Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0711Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0715Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0711Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0717Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0720Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0723Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0715Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0711Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0715Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0710Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0699Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0691Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0691Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0694Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0689Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0686Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0681Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0678Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0678Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0689Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0687Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0686Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0684Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0688Epoch 8/15: [==============================] 63/63 batches, loss: 0.0706
[2025-05-02 11:42:31,186][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0706
[2025-05-02 11:42:31,402][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0360, Metrics: {'mse': 0.03496447950601578, 'rmse': 0.18698791272704174, 'r2': 0.2876744270324707}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1407Epoch 9/15: [                              ] 2/63 batches, loss: 0.0928Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0783Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0824Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0772Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0711Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0709Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0717Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0654Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0694Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0670Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0665Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0642Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0631Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0607Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0601Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0615Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0664Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0665Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0653Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0638Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0652Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0656Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0647Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0630Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0623Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0619Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0608Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0608Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0610Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0621Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0640Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0644Epoch 9/15: [================              ] 34/63 batches, loss: 0.0643Epoch 9/15: [================              ] 35/63 batches, loss: 0.0630Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0622Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0613Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0612Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0613Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0616Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0610Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0623Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0625Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0619Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0619Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0612Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0616Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0616Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0620Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0620Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0623Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0624Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0624Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0626Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0632Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0631Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0624Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0624Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0618Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0625Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0622Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0622Epoch 9/15: [==============================] 63/63 batches, loss: 0.0612
[2025-05-02 11:42:33,727][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0612
[2025-05-02 11:42:33,952][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0366, Metrics: {'mse': 0.035546042025089264, 'rmse': 0.18853658007158522, 'r2': 0.2758263945579529}
[2025-05-02 11:42:33,952][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0856Epoch 10/15: [                              ] 2/63 batches, loss: 0.0812Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0792Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0783Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0775Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0743Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0720Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0703Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0717Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0735Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0708Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0686Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0678Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0654Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0673Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0641Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0665Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0671Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0671Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0694Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0684Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0666Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0659Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0651Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0652Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0658Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0661Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0643Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0659Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0662Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0650Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0647Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0642Epoch 10/15: [================              ] 34/63 batches, loss: 0.0643Epoch 10/15: [================              ] 35/63 batches, loss: 0.0644Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0634Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0630Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0629Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0629Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0621Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0612Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0614Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0608Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0608Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0602Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0599Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0613Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0621Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0620Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0626Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0623Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0630Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0633Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0637Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0631Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0633Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0626Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0625Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0621Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0618Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0619Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0624Epoch 10/15: [==============================] 63/63 batches, loss: 0.0616
[2025-05-02 11:42:35,884][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0616
[2025-05-02 11:42:36,133][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0373, Metrics: {'mse': 0.03626381978392601, 'rmse': 0.1904306167188617, 'r2': 0.26120316982269287}
[2025-05-02 11:42:36,134][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0661Epoch 11/15: [                              ] 2/63 batches, loss: 0.0590Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0514Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0428Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0416Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0451Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0489Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0473Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0452Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0460Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0464Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0471Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0455Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0496Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0495Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0492Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0486Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0477Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0470Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0502Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0508Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0500Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0523Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0528Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0525Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0529Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0524Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0517Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0505Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0511Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0510Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0517Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0522Epoch 11/15: [================              ] 34/63 batches, loss: 0.0525Epoch 11/15: [================              ] 35/63 batches, loss: 0.0524Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0531Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0531Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0522Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0518Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0515Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0526Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0531Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0529Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0524Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0524Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0528Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0529Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0525Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0522Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0528Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0534Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0530Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0532Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0529Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0531Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0531Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0527Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0522Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0518Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0517Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0514Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0512Epoch 11/15: [==============================] 63/63 batches, loss: 0.0506
[2025-05-02 11:42:38,068][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0506
[2025-05-02 11:42:38,293][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0362, Metrics: {'mse': 0.03508596122264862, 'rmse': 0.1873124694798737, 'r2': 0.2851995825767517}
[2025-05-02 11:42:38,293][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0468Epoch 12/15: [                              ] 2/63 batches, loss: 0.0596Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0628Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0654Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0665Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0608Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0586Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0580Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0577Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0601Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0588Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0577Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0564Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0551Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0550Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0537Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0548Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0538Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0527Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0514Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0512Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0525Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0530Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0514Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0517Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0512Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0511Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0516Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0513Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0509Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0511Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0513Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0508Epoch 12/15: [================              ] 34/63 batches, loss: 0.0519Epoch 12/15: [================              ] 35/63 batches, loss: 0.0515Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0518Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0512Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0510Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0507Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0506Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0510Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0504Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0505Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0509Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0506Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0513Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0513Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0510Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0506Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0506Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0505Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0501Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0509Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0508Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0506Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0504Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0503Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0504Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0502Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0500Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0499Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0501Epoch 12/15: [==============================] 63/63 batches, loss: 0.0499
[2025-05-02 11:42:40,251][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0499
[2025-05-02 11:42:40,481][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0378, Metrics: {'mse': 0.036552611738443375, 'rmse': 0.19118737337607675, 'r2': 0.2553197145462036}
[2025-05-02 11:42:40,482][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-02 11:42:40,482][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-02 11:42:40,482][src.training.lm_trainer][INFO] - Training completed in 29.84 seconds
[2025-05-02 11:42:40,482][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:42:42,986][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.022053390741348267, 'rmse': 0.14850384083029053, 'r2': -0.04595768451690674}
[2025-05-02 11:42:42,986][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03496447950601578, 'rmse': 0.18698791272704174, 'r2': 0.2876744270324707}
[2025-05-02 11:42:42,987][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04390602558851242, 'rmse': 0.20953764718663903, 'r2': 0.05201375484466553}
[2025-05-02 11:42:44,634][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar/ar/model.pt
[2025-05-02 11:42:44,635][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▂▂▂▁▁
wandb:     best_val_mse █▆▅▂▂▂▁▁
wandb:      best_val_r2 ▁▃▄▇▇▇██
wandb:    best_val_rmse █▆▅▂▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▃▅▅▅▅▆▅▅▅
wandb:       train_loss █▆▄▃▃▃▂▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▂▂▂▁▁▁▁▁▁
wandb:          val_mse █▆▅▂▂▂▁▁▁▁▁▁
wandb:           val_r2 ▁▃▄▇▇▇██████
wandb:         val_rmse █▆▅▂▂▂▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03604
wandb:     best_val_mse 0.03496
wandb:      best_val_r2 0.28767
wandb:    best_val_rmse 0.18699
wandb: early_stop_epoch 12
wandb:            epoch 12
wandb:   final_test_mse 0.04391
wandb:    final_test_r2 0.05201
wandb:  final_test_rmse 0.20954
wandb:  final_train_mse 0.02205
wandb:   final_train_r2 -0.04596
wandb: final_train_rmse 0.1485
wandb:    final_val_mse 0.03496
wandb:     final_val_r2 0.28767
wandb:   final_val_rmse 0.18699
wandb:    learning_rate 2e-05
wandb:       train_loss 0.04989
wandb:       train_time 29.83678
wandb:         val_loss 0.03777
wandb:          val_mse 0.03655
wandb:           val_r2 0.25532
wandb:         val_rmse 0.19119
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114201-i7gstdn3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114201-i7gstdn3/logs
Experiment probe_layer9_avg_links_len_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar/results.json
=======================
PROBING LAYER 11 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer11_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=256" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 11:42:55,522][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar
experiment_name: probe_layer11_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 256
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 11:42:55,522][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 11:42:55,522][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:42:55,523][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 11:42:55,523][__main__][INFO] - Determined Task Type: regression
[2025-05-02 11:42:55,527][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 11:42:55,527][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 11:42:55,527][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 11:42:57,062][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 11:42:59,333][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 11:42:59,333][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:59,383][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,413][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,510][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 11:42:59,517][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:59,518][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 11:42:59,519][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:59,541][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,577][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,590][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 11:42:59,591][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:59,591][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 11:42:59,592][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 11:42:59,613][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 11:42:59,658][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 11:42:59,659][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 11:42:59,659][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 11:42:59,660][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 11:42:59,661][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:59,661][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:59,661][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:59,661][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:59,661][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 11:42:59,661][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:59,662][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 11:42:59,662][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 11:42:59,662][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 11:42:59,663][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 11:42:59,663][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 11:42:59,663][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 11:42:59,664][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 11:42:59,664][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 11:42:59,664][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 11:43:03,487][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 11:43:03,488][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 11:43:03,488][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 11:43:03,488][src.models.model_factory][INFO] - Using provided probe_hidden_size: 256
[2025-05-02 11:43:03,493][src.models.model_factory][INFO] - Model has 264,961 trainable parameters out of 394,386,433 total parameters
[2025-05-02 11:43:03,493][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 264,961 trainable parameters
[2025-05-02 11:43:03,493][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=256, depth=2, activation=silu, normalization=layer
[2025-05-02 11:43:03,493][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 256 hidden size
[2025-05-02 11:43:03,494][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 11:43:03,494][__main__][INFO] - Total parameters: 394,386,433
[2025-05-02 11:43:03,494][__main__][INFO] - Trainable parameters: 264,961 (0.07%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.3044Epoch 1/15: [                              ] 2/63 batches, loss: 0.2250Epoch 1/15: [=                             ] 3/63 batches, loss: 0.2314Epoch 1/15: [=                             ] 4/63 batches, loss: 0.2563Epoch 1/15: [==                            ] 5/63 batches, loss: 0.2537Epoch 1/15: [==                            ] 6/63 batches, loss: 0.2548Epoch 1/15: [===                           ] 7/63 batches, loss: 0.2510Epoch 1/15: [===                           ] 8/63 batches, loss: 0.2317Epoch 1/15: [====                          ] 9/63 batches, loss: 0.2483Epoch 1/15: [====                          ] 10/63 batches, loss: 0.2352Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.2269Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.2139Epoch 1/15: [======                        ] 13/63 batches, loss: 0.2128Epoch 1/15: [======                        ] 14/63 batches, loss: 0.2053Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.1997Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.2017Epoch 1/15: [========                      ] 17/63 batches, loss: 0.2024Epoch 1/15: [========                      ] 18/63 batches, loss: 0.2003Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.1996Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.1950Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.1947Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.1895Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.1885Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.1872Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.1885Epoch 1/15: [============                  ] 26/63 batches, loss: 0.1860Epoch 1/15: [============                  ] 27/63 batches, loss: 0.1865Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.1899Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.1909Epoch 1/15: [==============                ] 30/63 batches, loss: 0.1908Epoch 1/15: [==============                ] 31/63 batches, loss: 0.1886Epoch 1/15: [===============               ] 32/63 batches, loss: 0.1873Epoch 1/15: [===============               ] 33/63 batches, loss: 0.1840Epoch 1/15: [================              ] 34/63 batches, loss: 0.1828Epoch 1/15: [================              ] 35/63 batches, loss: 0.1813Epoch 1/15: [=================             ] 36/63 batches, loss: 0.1817Epoch 1/15: [=================             ] 37/63 batches, loss: 0.1835Epoch 1/15: [==================            ] 38/63 batches, loss: 0.1843Epoch 1/15: [==================            ] 39/63 batches, loss: 0.1840Epoch 1/15: [===================           ] 40/63 batches, loss: 0.1826Epoch 1/15: [===================           ] 41/63 batches, loss: 0.1819Epoch 1/15: [====================          ] 42/63 batches, loss: 0.1802Epoch 1/15: [====================          ] 43/63 batches, loss: 0.1790Epoch 1/15: [====================          ] 44/63 batches, loss: 0.1777Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.1762Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.1773Epoch 1/15: [======================        ] 47/63 batches, loss: 0.1779Epoch 1/15: [======================        ] 48/63 batches, loss: 0.1781Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.1774Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.1777Epoch 1/15: [========================      ] 51/63 batches, loss: 0.1764Epoch 1/15: [========================      ] 52/63 batches, loss: 0.1772Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.1762Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.1771Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.1751Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.1750Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.1736Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.1737Epoch 1/15: [============================  ] 59/63 batches, loss: 0.1748Epoch 1/15: [============================  ] 60/63 batches, loss: 0.1735Epoch 1/15: [============================= ] 61/63 batches, loss: 0.1737Epoch 1/15: [============================= ] 62/63 batches, loss: 0.1738Epoch 1/15: [==============================] 63/63 batches, loss: 0.1753
[2025-05-02 11:43:07,819][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.1753
[2025-05-02 11:43:08,010][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0670, Metrics: {'mse': 0.06575343012809753, 'rmse': 0.25642431656942666, 'r2': -0.3395836353302002}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1615Epoch 2/15: [                              ] 2/63 batches, loss: 0.1732Epoch 2/15: [=                             ] 3/63 batches, loss: 0.1689Epoch 2/15: [=                             ] 4/63 batches, loss: 0.1453Epoch 2/15: [==                            ] 5/63 batches, loss: 0.1318Epoch 2/15: [==                            ] 6/63 batches, loss: 0.1305Epoch 2/15: [===                           ] 7/63 batches, loss: 0.1321Epoch 2/15: [===                           ] 8/63 batches, loss: 0.1293Epoch 2/15: [====                          ] 9/63 batches, loss: 0.1262Epoch 2/15: [====                          ] 10/63 batches, loss: 0.1234Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.1288Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.1298Epoch 2/15: [======                        ] 13/63 batches, loss: 0.1423Epoch 2/15: [======                        ] 14/63 batches, loss: 0.1398Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.1364Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.1376Epoch 2/15: [========                      ] 17/63 batches, loss: 0.1345Epoch 2/15: [========                      ] 18/63 batches, loss: 0.1338Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.1376Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1425Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1412Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1441Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1415Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1414Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1396Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1375Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1356Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1363Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1407Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1386Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1361Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1367Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1383Epoch 2/15: [================              ] 34/63 batches, loss: 0.1381Epoch 2/15: [================              ] 35/63 batches, loss: 0.1374Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1386Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1363Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1352Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1357Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1374Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1356Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1349Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1348Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1365Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1355Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1364Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1359Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1340Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1329Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1357Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1363Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1362Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1355Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1353Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1365Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1355Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1353Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1349Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1347Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1331Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1339Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1343Epoch 2/15: [==============================] 63/63 batches, loss: 0.1348
[2025-05-02 11:43:10,321][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1348
[2025-05-02 11:43:10,509][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0631, Metrics: {'mse': 0.061582859605550766, 'rmse': 0.24815894020879192, 'r2': -0.2546173334121704}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.0774Epoch 3/15: [                              ] 2/63 batches, loss: 0.0546Epoch 3/15: [=                             ] 3/63 batches, loss: 0.0823Epoch 3/15: [=                             ] 4/63 batches, loss: 0.0870Epoch 3/15: [==                            ] 5/63 batches, loss: 0.0916Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1041Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1009Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1284Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1372Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1365Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1367Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1301Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1293Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1350Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1297Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1275Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1228Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1227Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1203Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1200Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1215Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1206Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1202Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1167Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1166Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1160Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1159Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1174Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1150Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1141Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1173Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1166Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1166Epoch 3/15: [================              ] 34/63 batches, loss: 0.1164Epoch 3/15: [================              ] 35/63 batches, loss: 0.1151Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1136Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1144Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1135Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1127Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1134Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1134Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1129Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1121Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1127Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1122Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1123Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1126Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1126Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1130Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1130Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1138Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1141Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1133Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1137Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1132Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1131Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1118Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1123Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1136Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1129Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1121Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1119Epoch 3/15: [==============================] 63/63 batches, loss: 0.1101
[2025-05-02 11:43:12,853][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1101
[2025-05-02 11:43:13,066][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0654, Metrics: {'mse': 0.06384167075157166, 'rmse': 0.2526690933841566, 'r2': -0.30063581466674805}
[2025-05-02 11:43:13,067][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1202Epoch 4/15: [                              ] 2/63 batches, loss: 0.1190Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1192Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1355Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1434Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1361Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1258Epoch 4/15: [===                           ] 8/63 batches, loss: 0.1226Epoch 4/15: [====                          ] 9/63 batches, loss: 0.1194Epoch 4/15: [====                          ] 10/63 batches, loss: 0.1122Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1147Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1109Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1108Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1104Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1061Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1046Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1038Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1041Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1041Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1017Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1045Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1030Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1043Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1022Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1037Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1026Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1028Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1027Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1030Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1013Epoch 4/15: [==============                ] 31/63 batches, loss: 0.0997Epoch 4/15: [===============               ] 32/63 batches, loss: 0.0985Epoch 4/15: [===============               ] 33/63 batches, loss: 0.0967Epoch 4/15: [================              ] 34/63 batches, loss: 0.0974Epoch 4/15: [================              ] 35/63 batches, loss: 0.0970Epoch 4/15: [=================             ] 36/63 batches, loss: 0.0968Epoch 4/15: [=================             ] 37/63 batches, loss: 0.0976Epoch 4/15: [==================            ] 38/63 batches, loss: 0.0973Epoch 4/15: [==================            ] 39/63 batches, loss: 0.0974Epoch 4/15: [===================           ] 40/63 batches, loss: 0.0970Epoch 4/15: [===================           ] 41/63 batches, loss: 0.0951Epoch 4/15: [====================          ] 42/63 batches, loss: 0.0962Epoch 4/15: [====================          ] 43/63 batches, loss: 0.0959Epoch 4/15: [====================          ] 44/63 batches, loss: 0.0959Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.0956Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.0966Epoch 4/15: [======================        ] 47/63 batches, loss: 0.0961Epoch 4/15: [======================        ] 48/63 batches, loss: 0.0961Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.0986Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.0987Epoch 4/15: [========================      ] 51/63 batches, loss: 0.0987Epoch 4/15: [========================      ] 52/63 batches, loss: 0.0976Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.0968Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.0968Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.0959Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.0962Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.0954Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.0954Epoch 4/15: [============================  ] 59/63 batches, loss: 0.0954Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0949Epoch 4/15: [============================= ] 61/63 batches, loss: 0.0944Epoch 4/15: [============================= ] 62/63 batches, loss: 0.0936Epoch 4/15: [==============================] 63/63 batches, loss: 0.0929
[2025-05-02 11:43:14,971][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0929
[2025-05-02 11:43:15,179][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0483, Metrics: {'mse': 0.047313109040260315, 'rmse': 0.21751576733712966, 'r2': 0.036097824573516846}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1596Epoch 5/15: [                              ] 2/63 batches, loss: 0.1190Epoch 5/15: [=                             ] 3/63 batches, loss: 0.0987Epoch 5/15: [=                             ] 4/63 batches, loss: 0.0909Epoch 5/15: [==                            ] 5/63 batches, loss: 0.0833Epoch 5/15: [==                            ] 6/63 batches, loss: 0.0756Epoch 5/15: [===                           ] 7/63 batches, loss: 0.0697Epoch 5/15: [===                           ] 8/63 batches, loss: 0.0702Epoch 5/15: [====                          ] 9/63 batches, loss: 0.0724Epoch 5/15: [====                          ] 10/63 batches, loss: 0.0753Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.0757Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0787Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0775Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0782Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0777Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0782Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0789Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0801Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0810Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0806Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0803Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0796Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0787Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0771Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0772Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0798Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0790Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0792Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0787Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0808Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0797Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0792Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0789Epoch 5/15: [================              ] 34/63 batches, loss: 0.0797Epoch 5/15: [================              ] 35/63 batches, loss: 0.0796Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0786Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0782Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0790Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0801Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0801Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0802Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0790Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0799Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0794Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0799Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0801Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0803Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0795Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0789Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0783Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0789Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0787Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0785Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0786Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0784Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0786Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0793Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0794Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0787Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0789Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0789Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0783Epoch 5/15: [==============================] 63/63 batches, loss: 0.0782
[2025-05-02 11:43:17,440][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0782
[2025-05-02 11:43:17,651][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0474, Metrics: {'mse': 0.04636644199490547, 'rmse': 0.21532868363250046, 'r2': 0.055384159088134766}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0514Epoch 6/15: [                              ] 2/63 batches, loss: 0.0832Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0811Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0722Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0720Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0713Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0720Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0664Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0685Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0677Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0722Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0774Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0784Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0777Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0765Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0756Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0736Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0739Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0752Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0745Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0741Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0753Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0749Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0769Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0796Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0809Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0832Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0847Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0833Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0825Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0820Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0806Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0801Epoch 6/15: [================              ] 34/63 batches, loss: 0.0810Epoch 6/15: [================              ] 35/63 batches, loss: 0.0815Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0804Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0813Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0819Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0839Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0833Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0831Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0830Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0829Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0818Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0819Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0806Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0805Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0800Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0795Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0787Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0784Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0788Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0786Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0785Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0781Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0781Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0777Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0780Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0784Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0783Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0787Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0786Epoch 6/15: [==============================] 63/63 batches, loss: 0.0779
[2025-05-02 11:43:19,942][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0779
[2025-05-02 11:43:20,170][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0477, Metrics: {'mse': 0.04629848152399063, 'rmse': 0.21517081940632804, 'r2': 0.056768715381622314}
[2025-05-02 11:43:20,170][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0653Epoch 7/15: [                              ] 2/63 batches, loss: 0.0758Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0662Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0674Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0630Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0649Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0698Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0666Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0676Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0698Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0717Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0690Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0685Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0721Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0704Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0678Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0659Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0650Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0651Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0647Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0642Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0651Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0662Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0665Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0677Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0654Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0666Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0669Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0671Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0667Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0661Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0657Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0658Epoch 7/15: [================              ] 34/63 batches, loss: 0.0650Epoch 7/15: [================              ] 35/63 batches, loss: 0.0642Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0649Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0642Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0644Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0641Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0636Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0628Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0627Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0622Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0616Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0615Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0614Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0617Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0616Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0642Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0636Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0633Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0631Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0639Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0636Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0632Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0640Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0637Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0644Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0640Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0639Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0635Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0628Epoch 7/15: [==============================] 63/63 batches, loss: 0.0627
[2025-05-02 11:43:22,114][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0627
[2025-05-02 11:43:22,310][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0466, Metrics: {'mse': 0.04511883109807968, 'rmse': 0.21241193727773325, 'r2': 0.08080154657363892}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0305Epoch 8/15: [                              ] 2/63 batches, loss: 0.0454Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0490Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0619Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0618Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0632Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0597Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0622Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0614Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0587Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0589Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0639Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0637Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0630Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0641Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0642Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0625Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0639Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0627Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0642Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.0657Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.0651Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.0642Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.0644Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.0641Epoch 8/15: [============                  ] 26/63 batches, loss: 0.0622Epoch 8/15: [============                  ] 27/63 batches, loss: 0.0619Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.0630Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.0642Epoch 8/15: [==============                ] 30/63 batches, loss: 0.0644Epoch 8/15: [==============                ] 31/63 batches, loss: 0.0644Epoch 8/15: [===============               ] 32/63 batches, loss: 0.0642Epoch 8/15: [===============               ] 33/63 batches, loss: 0.0646Epoch 8/15: [================              ] 34/63 batches, loss: 0.0650Epoch 8/15: [================              ] 35/63 batches, loss: 0.0647Epoch 8/15: [=================             ] 36/63 batches, loss: 0.0648Epoch 8/15: [=================             ] 37/63 batches, loss: 0.0642Epoch 8/15: [==================            ] 38/63 batches, loss: 0.0641Epoch 8/15: [==================            ] 39/63 batches, loss: 0.0631Epoch 8/15: [===================           ] 40/63 batches, loss: 0.0644Epoch 8/15: [===================           ] 41/63 batches, loss: 0.0646Epoch 8/15: [====================          ] 42/63 batches, loss: 0.0643Epoch 8/15: [====================          ] 43/63 batches, loss: 0.0648Epoch 8/15: [====================          ] 44/63 batches, loss: 0.0656Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.0649Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.0646Epoch 8/15: [======================        ] 47/63 batches, loss: 0.0645Epoch 8/15: [======================        ] 48/63 batches, loss: 0.0642Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.0634Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.0631Epoch 8/15: [========================      ] 51/63 batches, loss: 0.0628Epoch 8/15: [========================      ] 52/63 batches, loss: 0.0631Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.0625Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.0625Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.0619Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.0618Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.0620Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.0619Epoch 8/15: [============================  ] 59/63 batches, loss: 0.0616Epoch 8/15: [============================  ] 60/63 batches, loss: 0.0619Epoch 8/15: [============================= ] 61/63 batches, loss: 0.0614Epoch 8/15: [============================= ] 62/63 batches, loss: 0.0614Epoch 8/15: [==============================] 63/63 batches, loss: 0.0627
[2025-05-02 11:43:24,689][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0627
[2025-05-02 11:43:24,913][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0430, Metrics: {'mse': 0.04168294742703438, 'rmse': 0.2041640208926009, 'r2': 0.15080022811889648}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1015Epoch 9/15: [                              ] 2/63 batches, loss: 0.0688Epoch 9/15: [=                             ] 3/63 batches, loss: 0.0734Epoch 9/15: [=                             ] 4/63 batches, loss: 0.0691Epoch 9/15: [==                            ] 5/63 batches, loss: 0.0640Epoch 9/15: [==                            ] 6/63 batches, loss: 0.0622Epoch 9/15: [===                           ] 7/63 batches, loss: 0.0650Epoch 9/15: [===                           ] 8/63 batches, loss: 0.0633Epoch 9/15: [====                          ] 9/63 batches, loss: 0.0590Epoch 9/15: [====                          ] 10/63 batches, loss: 0.0656Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.0634Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.0614Epoch 9/15: [======                        ] 13/63 batches, loss: 0.0592Epoch 9/15: [======                        ] 14/63 batches, loss: 0.0583Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.0558Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.0544Epoch 9/15: [========                      ] 17/63 batches, loss: 0.0557Epoch 9/15: [========                      ] 18/63 batches, loss: 0.0634Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.0623Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.0614Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.0608Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.0606Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.0609Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.0603Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.0592Epoch 9/15: [============                  ] 26/63 batches, loss: 0.0592Epoch 9/15: [============                  ] 27/63 batches, loss: 0.0596Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.0587Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.0588Epoch 9/15: [==============                ] 30/63 batches, loss: 0.0594Epoch 9/15: [==============                ] 31/63 batches, loss: 0.0602Epoch 9/15: [===============               ] 32/63 batches, loss: 0.0605Epoch 9/15: [===============               ] 33/63 batches, loss: 0.0603Epoch 9/15: [================              ] 34/63 batches, loss: 0.0596Epoch 9/15: [================              ] 35/63 batches, loss: 0.0584Epoch 9/15: [=================             ] 36/63 batches, loss: 0.0579Epoch 9/15: [=================             ] 37/63 batches, loss: 0.0573Epoch 9/15: [==================            ] 38/63 batches, loss: 0.0576Epoch 9/15: [==================            ] 39/63 batches, loss: 0.0581Epoch 9/15: [===================           ] 40/63 batches, loss: 0.0587Epoch 9/15: [===================           ] 41/63 batches, loss: 0.0581Epoch 9/15: [====================          ] 42/63 batches, loss: 0.0589Epoch 9/15: [====================          ] 43/63 batches, loss: 0.0591Epoch 9/15: [====================          ] 44/63 batches, loss: 0.0586Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.0589Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.0596Epoch 9/15: [======================        ] 47/63 batches, loss: 0.0591Epoch 9/15: [======================        ] 48/63 batches, loss: 0.0592Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.0591Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.0591Epoch 9/15: [========================      ] 51/63 batches, loss: 0.0595Epoch 9/15: [========================      ] 52/63 batches, loss: 0.0593Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.0592Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.0598Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.0600Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.0599Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.0592Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.0597Epoch 9/15: [============================  ] 59/63 batches, loss: 0.0594Epoch 9/15: [============================  ] 60/63 batches, loss: 0.0602Epoch 9/15: [============================= ] 61/63 batches, loss: 0.0603Epoch 9/15: [============================= ] 62/63 batches, loss: 0.0603Epoch 9/15: [==============================] 63/63 batches, loss: 0.0594
[2025-05-02 11:43:27,253][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0594
[2025-05-02 11:43:27,471][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0416, Metrics: {'mse': 0.04045359790325165, 'rmse': 0.20113079799784928, 'r2': 0.17584550380706787}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.0717Epoch 10/15: [                              ] 2/63 batches, loss: 0.0769Epoch 10/15: [=                             ] 3/63 batches, loss: 0.0690Epoch 10/15: [=                             ] 4/63 batches, loss: 0.0660Epoch 10/15: [==                            ] 5/63 batches, loss: 0.0727Epoch 10/15: [==                            ] 6/63 batches, loss: 0.0685Epoch 10/15: [===                           ] 7/63 batches, loss: 0.0669Epoch 10/15: [===                           ] 8/63 batches, loss: 0.0669Epoch 10/15: [====                          ] 9/63 batches, loss: 0.0667Epoch 10/15: [====                          ] 10/63 batches, loss: 0.0672Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.0649Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.0622Epoch 10/15: [======                        ] 13/63 batches, loss: 0.0611Epoch 10/15: [======                        ] 14/63 batches, loss: 0.0587Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.0583Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.0571Epoch 10/15: [========                      ] 17/63 batches, loss: 0.0587Epoch 10/15: [========                      ] 18/63 batches, loss: 0.0586Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.0581Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.0581Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.0587Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.0570Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.0575Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.0569Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.0574Epoch 10/15: [============                  ] 26/63 batches, loss: 0.0578Epoch 10/15: [============                  ] 27/63 batches, loss: 0.0580Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.0571Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.0588Epoch 10/15: [==============                ] 30/63 batches, loss: 0.0599Epoch 10/15: [==============                ] 31/63 batches, loss: 0.0589Epoch 10/15: [===============               ] 32/63 batches, loss: 0.0580Epoch 10/15: [===============               ] 33/63 batches, loss: 0.0582Epoch 10/15: [================              ] 34/63 batches, loss: 0.0587Epoch 10/15: [================              ] 35/63 batches, loss: 0.0594Epoch 10/15: [=================             ] 36/63 batches, loss: 0.0586Epoch 10/15: [=================             ] 37/63 batches, loss: 0.0580Epoch 10/15: [==================            ] 38/63 batches, loss: 0.0580Epoch 10/15: [==================            ] 39/63 batches, loss: 0.0589Epoch 10/15: [===================           ] 40/63 batches, loss: 0.0579Epoch 10/15: [===================           ] 41/63 batches, loss: 0.0577Epoch 10/15: [====================          ] 42/63 batches, loss: 0.0584Epoch 10/15: [====================          ] 43/63 batches, loss: 0.0581Epoch 10/15: [====================          ] 44/63 batches, loss: 0.0584Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.0582Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.0577Epoch 10/15: [======================        ] 47/63 batches, loss: 0.0591Epoch 10/15: [======================        ] 48/63 batches, loss: 0.0599Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.0598Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.0603Epoch 10/15: [========================      ] 51/63 batches, loss: 0.0596Epoch 10/15: [========================      ] 52/63 batches, loss: 0.0597Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.0599Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.0607Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.0602Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.0601Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.0597Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.0595Epoch 10/15: [============================  ] 59/63 batches, loss: 0.0591Epoch 10/15: [============================  ] 60/63 batches, loss: 0.0587Epoch 10/15: [============================= ] 61/63 batches, loss: 0.0587Epoch 10/15: [============================= ] 62/63 batches, loss: 0.0586Epoch 10/15: [==============================] 63/63 batches, loss: 0.0580
[2025-05-02 11:43:29,810][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0580
[2025-05-02 11:43:30,035][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0432, Metrics: {'mse': 0.04175861179828644, 'rmse': 0.20434923977907635, 'r2': 0.14925867319107056}
[2025-05-02 11:43:30,036][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.0558Epoch 11/15: [                              ] 2/63 batches, loss: 0.0523Epoch 11/15: [=                             ] 3/63 batches, loss: 0.0533Epoch 11/15: [=                             ] 4/63 batches, loss: 0.0490Epoch 11/15: [==                            ] 5/63 batches, loss: 0.0442Epoch 11/15: [==                            ] 6/63 batches, loss: 0.0455Epoch 11/15: [===                           ] 7/63 batches, loss: 0.0492Epoch 11/15: [===                           ] 8/63 batches, loss: 0.0480Epoch 11/15: [====                          ] 9/63 batches, loss: 0.0461Epoch 11/15: [====                          ] 10/63 batches, loss: 0.0462Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.0459Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.0463Epoch 11/15: [======                        ] 13/63 batches, loss: 0.0449Epoch 11/15: [======                        ] 14/63 batches, loss: 0.0461Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.0468Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.0465Epoch 11/15: [========                      ] 17/63 batches, loss: 0.0459Epoch 11/15: [========                      ] 18/63 batches, loss: 0.0460Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.0455Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.0475Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.0475Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.0462Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.0477Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.0488Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.0478Epoch 11/15: [============                  ] 26/63 batches, loss: 0.0483Epoch 11/15: [============                  ] 27/63 batches, loss: 0.0474Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.0474Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.0470Epoch 11/15: [==============                ] 30/63 batches, loss: 0.0474Epoch 11/15: [==============                ] 31/63 batches, loss: 0.0473Epoch 11/15: [===============               ] 32/63 batches, loss: 0.0490Epoch 11/15: [===============               ] 33/63 batches, loss: 0.0492Epoch 11/15: [================              ] 34/63 batches, loss: 0.0495Epoch 11/15: [================              ] 35/63 batches, loss: 0.0497Epoch 11/15: [=================             ] 36/63 batches, loss: 0.0505Epoch 11/15: [=================             ] 37/63 batches, loss: 0.0505Epoch 11/15: [==================            ] 38/63 batches, loss: 0.0499Epoch 11/15: [==================            ] 39/63 batches, loss: 0.0497Epoch 11/15: [===================           ] 40/63 batches, loss: 0.0498Epoch 11/15: [===================           ] 41/63 batches, loss: 0.0505Epoch 11/15: [====================          ] 42/63 batches, loss: 0.0502Epoch 11/15: [====================          ] 43/63 batches, loss: 0.0505Epoch 11/15: [====================          ] 44/63 batches, loss: 0.0500Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.0501Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.0502Epoch 11/15: [======================        ] 47/63 batches, loss: 0.0502Epoch 11/15: [======================        ] 48/63 batches, loss: 0.0501Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.0495Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.0504Epoch 11/15: [========================      ] 51/63 batches, loss: 0.0499Epoch 11/15: [========================      ] 52/63 batches, loss: 0.0496Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.0494Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.0491Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.0489Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.0489Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.0486Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.0486Epoch 11/15: [============================  ] 59/63 batches, loss: 0.0483Epoch 11/15: [============================  ] 60/63 batches, loss: 0.0487Epoch 11/15: [============================= ] 61/63 batches, loss: 0.0484Epoch 11/15: [============================= ] 62/63 batches, loss: 0.0482Epoch 11/15: [==============================] 63/63 batches, loss: 0.0474
[2025-05-02 11:43:31,979][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0474
[2025-05-02 11:43:32,200][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0414, Metrics: {'mse': 0.03995735943317413, 'rmse': 0.1998933701581274, 'r2': 0.18595528602600098}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0383Epoch 12/15: [                              ] 2/63 batches, loss: 0.0466Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0480Epoch 12/15: [=                             ] 4/63 batches, loss: 0.0482Epoch 12/15: [==                            ] 5/63 batches, loss: 0.0495Epoch 12/15: [==                            ] 6/63 batches, loss: 0.0462Epoch 12/15: [===                           ] 7/63 batches, loss: 0.0454Epoch 12/15: [===                           ] 8/63 batches, loss: 0.0463Epoch 12/15: [====                          ] 9/63 batches, loss: 0.0467Epoch 12/15: [====                          ] 10/63 batches, loss: 0.0495Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.0481Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.0470Epoch 12/15: [======                        ] 13/63 batches, loss: 0.0452Epoch 12/15: [======                        ] 14/63 batches, loss: 0.0455Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.0470Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.0459Epoch 12/15: [========                      ] 17/63 batches, loss: 0.0463Epoch 12/15: [========                      ] 18/63 batches, loss: 0.0467Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.0467Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.0466Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.0467Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.0473Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.0472Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.0465Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.0465Epoch 12/15: [============                  ] 26/63 batches, loss: 0.0463Epoch 12/15: [============                  ] 27/63 batches, loss: 0.0467Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.0477Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.0469Epoch 12/15: [==============                ] 30/63 batches, loss: 0.0468Epoch 12/15: [==============                ] 31/63 batches, loss: 0.0467Epoch 12/15: [===============               ] 32/63 batches, loss: 0.0472Epoch 12/15: [===============               ] 33/63 batches, loss: 0.0475Epoch 12/15: [================              ] 34/63 batches, loss: 0.0485Epoch 12/15: [================              ] 35/63 batches, loss: 0.0486Epoch 12/15: [=================             ] 36/63 batches, loss: 0.0487Epoch 12/15: [=================             ] 37/63 batches, loss: 0.0482Epoch 12/15: [==================            ] 38/63 batches, loss: 0.0481Epoch 12/15: [==================            ] 39/63 batches, loss: 0.0483Epoch 12/15: [===================           ] 40/63 batches, loss: 0.0486Epoch 12/15: [===================           ] 41/63 batches, loss: 0.0489Epoch 12/15: [====================          ] 42/63 batches, loss: 0.0488Epoch 12/15: [====================          ] 43/63 batches, loss: 0.0489Epoch 12/15: [====================          ] 44/63 batches, loss: 0.0488Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.0486Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.0489Epoch 12/15: [======================        ] 47/63 batches, loss: 0.0488Epoch 12/15: [======================        ] 48/63 batches, loss: 0.0487Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.0481Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.0480Epoch 12/15: [========================      ] 51/63 batches, loss: 0.0479Epoch 12/15: [========================      ] 52/63 batches, loss: 0.0480Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.0487Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.0485Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.0481Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.0476Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.0475Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.0478Epoch 12/15: [============================  ] 59/63 batches, loss: 0.0477Epoch 12/15: [============================  ] 60/63 batches, loss: 0.0473Epoch 12/15: [============================= ] 61/63 batches, loss: 0.0472Epoch 12/15: [============================= ] 62/63 batches, loss: 0.0480Epoch 12/15: [==============================] 63/63 batches, loss: 0.0475
[2025-05-02 11:43:34,543][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0475
[2025-05-02 11:43:34,762][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0412, Metrics: {'mse': 0.039793483912944794, 'rmse': 0.1994830416675683, 'r2': 0.1892939805984497}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0363Epoch 13/15: [                              ] 2/63 batches, loss: 0.0433Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0406Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0448Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0437Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0423Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0405Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0428Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0446Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0505Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0512Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0510Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0500Epoch 13/15: [======                        ] 14/63 batches, loss: 0.0491Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.0489Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.0476Epoch 13/15: [========                      ] 17/63 batches, loss: 0.0458Epoch 13/15: [========                      ] 18/63 batches, loss: 0.0449Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.0463Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.0462Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.0457Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.0458Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.0448Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.0441Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.0457Epoch 13/15: [============                  ] 26/63 batches, loss: 0.0459Epoch 13/15: [============                  ] 27/63 batches, loss: 0.0465Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.0458Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.0461Epoch 13/15: [==============                ] 30/63 batches, loss: 0.0454Epoch 13/15: [==============                ] 31/63 batches, loss: 0.0453Epoch 13/15: [===============               ] 32/63 batches, loss: 0.0456Epoch 13/15: [===============               ] 33/63 batches, loss: 0.0455Epoch 13/15: [================              ] 34/63 batches, loss: 0.0452Epoch 13/15: [================              ] 35/63 batches, loss: 0.0462Epoch 13/15: [=================             ] 36/63 batches, loss: 0.0464Epoch 13/15: [=================             ] 37/63 batches, loss: 0.0470Epoch 13/15: [==================            ] 38/63 batches, loss: 0.0468Epoch 13/15: [==================            ] 39/63 batches, loss: 0.0464Epoch 13/15: [===================           ] 40/63 batches, loss: 0.0457Epoch 13/15: [===================           ] 41/63 batches, loss: 0.0459Epoch 13/15: [====================          ] 42/63 batches, loss: 0.0466Epoch 13/15: [====================          ] 43/63 batches, loss: 0.0461Epoch 13/15: [====================          ] 44/63 batches, loss: 0.0460Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.0466Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.0463Epoch 13/15: [======================        ] 47/63 batches, loss: 0.0463Epoch 13/15: [======================        ] 48/63 batches, loss: 0.0467Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.0463Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.0465Epoch 13/15: [========================      ] 51/63 batches, loss: 0.0461Epoch 13/15: [========================      ] 52/63 batches, loss: 0.0460Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.0463Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.0458Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.0457Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.0454Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.0461Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.0464Epoch 13/15: [============================  ] 59/63 batches, loss: 0.0461Epoch 13/15: [============================  ] 60/63 batches, loss: 0.0460Epoch 13/15: [============================= ] 61/63 batches, loss: 0.0466Epoch 13/15: [============================= ] 62/63 batches, loss: 0.0464Epoch 13/15: [==============================] 63/63 batches, loss: 0.0466
[2025-05-02 11:43:37,063][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0466
[2025-05-02 11:43:37,298][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0409, Metrics: {'mse': 0.039566900581121445, 'rmse': 0.19891430461663998, 'r2': 0.19391006231307983}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0628Epoch 14/15: [                              ] 2/63 batches, loss: 0.0756Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0712Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0594Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0589Epoch 14/15: [==                            ] 6/63 batches, loss: 0.0545Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0528Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0506Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0502Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0493Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0478Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0473Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0451Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0464Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0455Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0440Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0438Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0431Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0420Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0416Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0423Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0414Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0418Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0412Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0418Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0415Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0415Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0411Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0409Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0403Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0402Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0399Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0396Epoch 14/15: [================              ] 34/63 batches, loss: 0.0396Epoch 14/15: [================              ] 35/63 batches, loss: 0.0398Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0402Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0404Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0416Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0414Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0429Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0428Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0423Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0425Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0423Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0423Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0424Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0426Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0426Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0431Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0433Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0431Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0431Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0426Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0426Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0428Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0424Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0424Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0431Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0431Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0428Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0430Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0426Epoch 14/15: [==============================] 63/63 batches, loss: 0.0424
[2025-05-02 11:43:39,646][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0424
[2025-05-02 11:43:39,836][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0392, Metrics: {'mse': 0.03761577978730202, 'rmse': 0.19394787904821753, 'r2': 0.23365998268127441}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0342Epoch 15/15: [                              ] 2/63 batches, loss: 0.0284Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0347Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0500Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0494Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0511Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0524Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0488Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0513Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0504Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0490Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0495Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0515Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0513Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0506Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0509Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0489Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0475Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0486Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0474Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0466Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0468Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0459Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0452Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0445Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0442Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0436Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0438Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0428Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0422Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0424Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0416Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0417Epoch 15/15: [================              ] 34/63 batches, loss: 0.0414Epoch 15/15: [================              ] 35/63 batches, loss: 0.0411Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0411Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0413Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0411Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0404Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0412Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0412Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0409Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0419Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0418Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0418Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0414Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0417Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0415Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0415Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0410Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0406Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0402Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0399Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0395Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0393Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0393Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0391Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0391Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0390Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0390Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0391Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0389Epoch 15/15: [==============================] 63/63 batches, loss: 0.0385
[2025-05-02 11:43:42,240][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0385
[2025-05-02 11:43:42,456][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0375, Metrics: {'mse': 0.036020319908857346, 'rmse': 0.1897901997176286, 'r2': 0.26616400480270386}
[2025-05-02 11:43:42,855][src.training.lm_trainer][INFO] - Training completed in 37.83 seconds
[2025-05-02 11:43:42,855][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 11:43:45,379][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01822594553232193, 'rmse': 0.13500350192614238, 'r2': 0.13557201623916626}
[2025-05-02 11:43:45,380][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.036020319908857346, 'rmse': 0.1897901997176286, 'r2': 0.26616400480270386}
[2025-05-02 11:43:45,380][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04958401620388031, 'rmse': 0.2226746869401197, 'r2': -0.07058119773864746}
[2025-05-02 11:43:47,069][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar/ar/model.pt
[2025-05-02 11:43:47,071][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▂▂▂▂▂▁▁
wandb:     best_val_mse █▇▄▃▃▂▂▂▂▂▁▁
wandb:      best_val_r2 ▁▂▅▆▆▇▇▇▇▇██
wandb:    best_val_rmse █▇▄▄▃▃▂▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▁▄▅▄▅▅▅▅▅▅▅▆
wandb:       train_loss █▆▅▄▃▃▂▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇█▄▃▃▃▂▂▂▂▂▂▁▁
wandb:          val_mse █▇█▄▃▃▃▂▂▂▂▂▂▁▁
wandb:           val_r2 ▁▂▁▅▆▆▆▇▇▇▇▇▇██
wandb:         val_rmse █▇█▄▄▄▃▃▂▃▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03749
wandb:     best_val_mse 0.03602
wandb:      best_val_r2 0.26616
wandb:    best_val_rmse 0.18979
wandb:            epoch 15
wandb:   final_test_mse 0.04958
wandb:    final_test_r2 -0.07058
wandb:  final_test_rmse 0.22267
wandb:  final_train_mse 0.01823
wandb:   final_train_r2 0.13557
wandb: final_train_rmse 0.135
wandb:    final_val_mse 0.03602
wandb:     final_val_r2 0.26616
wandb:   final_val_rmse 0.18979
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03853
wandb:       train_time 37.83193
wandb:         val_loss 0.03749
wandb:          val_mse 0.03602
wandb:           val_r2 0.26616
wandb:         val_rmse 0.18979
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114255-37lwi33n
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_114255-37lwi33n/logs
Experiment probe_layer11_avg_links_len_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar/results.json
Running control submetric probing experiments...
=======================
PROBING LAYER 1 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 4 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 6 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 9 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 11 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Some experiments failed. See /scratch/leuven/371/vsc37132/probe_output/failed_experiments.log for details.
Failed experiments (15):
probe_layer1_question_type_ar
probe_layer1_complexity_ar
probe_layer4_question_type_ar
probe_layer4_complexity_ar
probe_layer6_question_type_ar
probe_layer6_complexity_ar
probe_layer9_question_type_ar
probe_layer9_complexity_ar
probe_layer11_question_type_ar
probe_layer11_complexity_ar
probe_layer1_avg_links_len_ar
probe_layer4_avg_links_len_ar
probe_layer6_avg_links_len_ar
probe_layer9_avg_links_len_ar
probe_layer11_avg_links_len_ar
==============================================
Layer-wise probing experiments completed!
==============================================
Total planned experiments: 15
Successfully completed: 0
Failed experiments: 15
Success rate: 0%
Results available in: /scratch/leuven/371/vsc37132/probe_output
Layer summary: /scratch/leuven/371/vsc37132/probe_output/layer_performance_summary.json
==============================================
