{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604a83f1",
   "metadata": {},
   "source": [
    "## Result Analysis and Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8911bf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84571c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.linewidth'] = 1.0\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 16\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "model_colors = {\n",
    "    \"Dummy\": \"#E1E1E1\",\n",
    "    \"LogisticRegression\": \"#6EB5FF\",\n",
    "    \"Ridge\": \"#6EB5FF\",\n",
    "    \"XGBRegressor\": \"#3D85C6\",\n",
    "    \"XGBClassifier\": \"#3D85C6\",\n",
    "    \"glot500\": \"#073763\"\n",
    "}\n",
    "\n",
    "language_colors = {\n",
    "    \"ar\": \"#C44E52\",  # Arabic - red\n",
    "    \"en\": \"#4C72B0\",  # English - blue\n",
    "    \"fi\": \"#55A868\",  # Finnish - green\n",
    "    \"id\": \"#8172B3\",  # Indonesian - purple\n",
    "    \"ja\": \"#CCB974\",  # Japanese - yellow\n",
    "    \"ko\": \"#64B5CD\",  # Korean - teal\n",
    "    \"ru\": \"#DB8E00\"   # Russian - orange\n",
    "}\n",
    "\n",
    "cmap_blues = LinearSegmentedColormap.from_list(\n",
    "    \"custom_blues\", [\"#FFFFFF\", \"#EBF1F9\", \"#D6E3F3\", \"#C1D6ED\", \"#ADCAE7\", \"#98BDE0\", \"#83B0DA\", \"#6EA4D4\", \"#5997CE\", \"#448BC8\", \"#2F7EC2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 880 experiment results from /home/robin/Research/qtype-eval/scripts/visualization/combined_results/combined_results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load the experiment results\n",
    "RESULTS_FILE = '/home/robin/Research/qtype-eval/scripts/visualization/combined_results/combined_results.json'\n",
    "\n",
    "with open(RESULTS_FILE, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(results)} experiment results from {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframe with 1318 rows\n",
      "\n",
      "Dataframe columns: ['experiment_key', 'source', 'model', 'task', 'control_index', 'submetric', 'language', 'score']\n",
      "\n",
      "Unique sources: ['tfidf' 'glot500']\n",
      "Unique models: ['Ridge_complexity_control1_avg_max_depth'\n",
      " 'XGBRegressor_complexity_control1_n_tokens'\n",
      " 'XGBRegressor_complexity_avg_max_depth'\n",
      " 'XGBRegressor_complexity_control2'\n",
      " 'XGBRegressor_complexity_control2_avg_max_depth'\n",
      " 'XGBRegressor_complexity_control1_avg_subordinate_chain_len'\n",
      " 'DummyClassifier_question_type' 'XGBRegressor_complexity_control1'\n",
      " 'XGBRegressor_complexity_control1_avg_verb_edges'\n",
      " 'XGBRegressor_complexity_control2_lexical_density'\n",
      " 'Ridge_complexity_control1_avg_subordinate_chain_len'\n",
      " 'XGBRegressor_complexity_control3_avg_max_depth'\n",
      " 'Ridge_complexity_avg_subordinate_chain_len' 'all'\n",
      " 'LogisticRegression_question_type_control1' 'Ridge_complexity'\n",
      " 'Ridge_complexity_control1_avg_links_len'\n",
      " 'XGBRegressor_complexity_control3_avg_links_len'\n",
      " 'Ridge_complexity_control3_avg_verb_edges'\n",
      " 'Ridge_complexity_control2_avg_links_len'\n",
      " 'XGBRegressor_complexity_avg_links_len'\n",
      " 'XGBClassifier_question_type_control1'\n",
      " 'XGBRegressor_complexity_avg_verb_edges'\n",
      " 'Ridge_complexity_control3_lexical_density'\n",
      " 'Ridge_complexity_control3_avg_max_depth'\n",
      " 'Ridge_complexity_lexical_density' 'LogisticRegression_question_type'\n",
      " 'DummyRegressor_complexity_control3' 'XGBRegressor_complexity'\n",
      " 'Ridge_complexity_control2' 'Ridge_complexity_avg_max_depth'\n",
      " 'XGBRegressor_complexity_avg_subordinate_chain_len'\n",
      " 'XGBRegressor_complexity_control2_n_tokens'\n",
      " 'XGBRegressor_complexity_control3_avg_verb_edges'\n",
      " 'XGBRegressor_complexity_control3_lexical_density'\n",
      " 'DummyRegressor_complexity_control2'\n",
      " 'Ridge_complexity_control3_avg_subordinate_chain_len'\n",
      " 'XGBRegressor_complexity_control1_lexical_density'\n",
      " 'DummyRegressor_complexity_control1' 'Ridge_complexity_control3'\n",
      " 'Ridge_complexity_control3_avg_links_len'\n",
      " 'Ridge_complexity_control1_lexical_density'\n",
      " 'XGBClassifier_question_type_control2'\n",
      " 'XGBRegressor_complexity_control3_n_tokens'\n",
      " 'Ridge_complexity_control2_lexical_density' 'XGBClassifier_question_type'\n",
      " 'Ridge_complexity_avg_links_len' 'Ridge_complexity_control1'\n",
      " 'XGBRegressor_complexity_control3_avg_subordinate_chain_len'\n",
      " 'Ridge_complexity_control2_avg_verb_edges'\n",
      " 'XGBClassifier_question_type_control3'\n",
      " 'DummyClassifier_question_type_control3'\n",
      " 'LogisticRegression_question_type_control2'\n",
      " 'XGBRegressor_complexity_lexical_density' 'DummyRegressor_complexity'\n",
      " 'Ridge_complexity_control2_avg_subordinate_chain_len'\n",
      " 'Ridge_complexity_control1_avg_verb_edges'\n",
      " 'Ridge_complexity_control1_n_tokens' 'XGBRegressor_complexity_n_tokens'\n",
      " 'XGBRegressor_complexity_control2_avg_subordinate_chain_len'\n",
      " 'Ridge_complexity_n_tokens'\n",
      " 'XGBRegressor_complexity_control1_avg_max_depth'\n",
      " 'Ridge_complexity_control2_n_tokens' 'Ridge_complexity_avg_verb_edges'\n",
      " 'XGBRegressor_complexity_control3'\n",
      " 'XGBRegressor_complexity_control1_avg_links_len'\n",
      " 'LogisticRegression_question_type_control3'\n",
      " 'XGBRegressor_complexity_control2_avg_verb_edges'\n",
      " 'Ridge_complexity_control2_avg_max_depth'\n",
      " 'Ridge_complexity_control3_n_tokens'\n",
      " 'DummyClassifier_question_type_control1'\n",
      " 'XGBRegressor_complexity_control2_avg_links_len'\n",
      " 'DummyClassifier_question_type_control2' 'layer' 'lm_probe' 'lm']\n",
      "Unique tasks: ['complexity' 'question_type' 'all' 'mapping' 'probe']\n",
      "Unique languages: ['ar' 'en' 'fi' 'id' 'ja' 'ko' 'ru']\n",
      "Unique submetrics: ['avg_max_depth' 'n_tokens' 'avg_subordinate_chain_len' 'avg_verb_edges'\n",
      " 'lexical_density' 'avg_links_len']\n",
      "Unique control indices: ['1' '2' '3']\n",
      "\n",
      "Sample of the dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_key</th>\n",
       "      <th>source</th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>control_index</th>\n",
       "      <th>submetric</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf_Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_max_depth</td>\n",
       "      <td>ar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf_Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_max_depth</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_max_depth</td>\n",
       "      <td>fi</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_max_depth</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Ridge_complexity_control1_avg_max_depth</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_max_depth</td>\n",
       "      <td>ja</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>glot500_lm_probe_single_submetric_control3_avg...</td>\n",
       "      <td>glot500</td>\n",
       "      <td>lm_probe</td>\n",
       "      <td>complexity</td>\n",
       "      <td>3</td>\n",
       "      <td>avg_verb_edges</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>glot500_lm_probe_single_submetric_control2_avg...</td>\n",
       "      <td>glot500</td>\n",
       "      <td>lm_probe</td>\n",
       "      <td>complexity</td>\n",
       "      <td>2</td>\n",
       "      <td>avg_verb_edges</td>\n",
       "      <td>id</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>glot500_lm_probe_single_submetric_control1_avg...</td>\n",
       "      <td>glot500</td>\n",
       "      <td>lm_probe</td>\n",
       "      <td>complexity</td>\n",
       "      <td>1</td>\n",
       "      <td>avg_verb_edges</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>glot500_lm_probe_single_submetric_control3_avg...</td>\n",
       "      <td>glot500</td>\n",
       "      <td>lm_probe</td>\n",
       "      <td>complexity</td>\n",
       "      <td>3</td>\n",
       "      <td>avg_verb_edges</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>glot500_lm_probe_single_submetric_control2_avg...</td>\n",
       "      <td>glot500</td>\n",
       "      <td>lm_probe</td>\n",
       "      <td>complexity</td>\n",
       "      <td>2</td>\n",
       "      <td>avg_verb_edges</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1318 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         experiment_key   source  \\\n",
       "0         tfidf_Ridge_complexity_control1_avg_max_depth    tfidf   \n",
       "1         tfidf_Ridge_complexity_control1_avg_max_depth    tfidf   \n",
       "2         tfidf_Ridge_complexity_control1_avg_max_depth    tfidf   \n",
       "3         tfidf_Ridge_complexity_control1_avg_max_depth    tfidf   \n",
       "4         tfidf_Ridge_complexity_control1_avg_max_depth    tfidf   \n",
       "...                                                 ...      ...   \n",
       "1313  glot500_lm_probe_single_submetric_control3_avg...  glot500   \n",
       "1314  glot500_lm_probe_single_submetric_control2_avg...  glot500   \n",
       "1315  glot500_lm_probe_single_submetric_control1_avg...  glot500   \n",
       "1316  glot500_lm_probe_single_submetric_control3_avg...  glot500   \n",
       "1317  glot500_lm_probe_single_submetric_control2_avg...  glot500   \n",
       "\n",
       "                                        model        task control_index  \\\n",
       "0     Ridge_complexity_control1_avg_max_depth  complexity             1   \n",
       "1     Ridge_complexity_control1_avg_max_depth  complexity             1   \n",
       "2     Ridge_complexity_control1_avg_max_depth  complexity             1   \n",
       "3     Ridge_complexity_control1_avg_max_depth  complexity             1   \n",
       "4     Ridge_complexity_control1_avg_max_depth  complexity             1   \n",
       "...                                       ...         ...           ...   \n",
       "1313                                 lm_probe  complexity             3   \n",
       "1314                                 lm_probe  complexity             2   \n",
       "1315                                 lm_probe  complexity             1   \n",
       "1316                                 lm_probe  complexity             3   \n",
       "1317                                 lm_probe  complexity             2   \n",
       "\n",
       "           submetric language  score  \n",
       "0      avg_max_depth       ar    NaN  \n",
       "1      avg_max_depth       en    NaN  \n",
       "2      avg_max_depth       fi    NaN  \n",
       "3      avg_max_depth       id    NaN  \n",
       "4      avg_max_depth       ja    NaN  \n",
       "...              ...      ...    ...  \n",
       "1313  avg_verb_edges       id    NaN  \n",
       "1314  avg_verb_edges       id    NaN  \n",
       "1315  avg_verb_edges       ru    NaN  \n",
       "1316  avg_verb_edges       ru    NaN  \n",
       "1317  avg_verb_edges       ru    NaN  \n",
       "\n",
       "[1318 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_structured_dataframe(results):\n",
    "    data_rows = []\n",
    "    \n",
    "    for exp_key, exp_data in results.items():\n",
    "        parts = exp_key.split('_')\n",
    "        source = parts[0]  # e.g., tfidf or glot500\n",
    "        \n",
    "        # Handle different experiment naming patterns\n",
    "        if 'single_submetric' in exp_key:\n",
    "            # Extract model, task, control index, submetric and language\n",
    "            model = 'lm_probe'\n",
    "            task = 'complexity'  # Single submetric experiments are for complexity\n",
    "            \n",
    "            # Find control index if present\n",
    "            control_idx = None\n",
    "            for part in parts:\n",
    "                if part.startswith('control') and part[7:].isdigit():\n",
    "                    control_idx = part[7:]\n",
    "                    break\n",
    "            \n",
    "            # Extract submetric\n",
    "            submetric = None\n",
    "            for sm in ['avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens']:\n",
    "                if sm in exp_key:\n",
    "                    submetric = sm\n",
    "                    break\n",
    "            \n",
    "            # Extract language\n",
    "            language = None\n",
    "            for lang in ['ar', 'en', 'fi', 'id', 'ja', 'ko', 'ru']:\n",
    "                if f\"_{lang}\" in exp_key or exp_key.endswith(f\"_{lang}\"):\n",
    "                    language = lang\n",
    "                    break\n",
    "        \n",
    "        elif 'question_type' in exp_key:\n",
    "            idx = exp_key.index('question_type')\n",
    "            if 'lm_probe' in exp_key:\n",
    "                model = 'lm_probe'\n",
    "            else:\n",
    "                model = '_'.join(parts[1:idx])\n",
    "            task = 'question_type'\n",
    "            \n",
    "            # Find control index if present\n",
    "            control_idx = None\n",
    "            for part in parts:\n",
    "                if part.startswith('control') and part[7:].isdigit():\n",
    "                    control_idx = part[7:]\n",
    "                    break\n",
    "            \n",
    "            # Extract language if present\n",
    "            language = None\n",
    "            for lang in ['ar', 'en', 'fi', 'id', 'ja', 'ko', 'ru']:\n",
    "                if f\"_{lang}\" in exp_key or exp_key.endswith(f\"_{lang}\"):\n",
    "                    language = lang\n",
    "                    break\n",
    "            \n",
    "            submetric = None  # Question type task doesn't have submetrics\n",
    "        \n",
    "        elif 'complexity' in exp_key:\n",
    "            idx = exp_key.index('complexity')\n",
    "            if 'lm_probe' in exp_key:\n",
    "                model = 'lm_probe'\n",
    "            else:\n",
    "                model = '_'.join(parts[1:idx])\n",
    "            task = 'complexity'\n",
    "            \n",
    "            # Find control index if present\n",
    "            control_idx = None\n",
    "            for part in parts:\n",
    "                if part.startswith('control') and part[7:].isdigit():\n",
    "                    control_idx = part[7:]\n",
    "                    break\n",
    "            \n",
    "            # Look for specific submetrics in traditional complexity experiments\n",
    "            submetric = None\n",
    "            for sm in ['avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens']:\n",
    "                if sm in exp_key:\n",
    "                    submetric = sm\n",
    "                    break\n",
    "                    \n",
    "            # Extract language if present\n",
    "            language = None\n",
    "            for lang in ['ar', 'en', 'fi', 'id', 'ja', 'ko', 'ru']:\n",
    "                if f\"_{lang}\" in exp_key or exp_key.endswith(f\"_{lang}\"):\n",
    "                    language = lang\n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            # Default parsing for other experiment types\n",
    "            if len(parts) >= 3:\n",
    "                model = parts[1]\n",
    "                task = parts[2] if len(parts) > 2 else None\n",
    "            else:\n",
    "                model = None\n",
    "                task = None\n",
    "            \n",
    "            control_idx = None\n",
    "            submetric = None\n",
    "            language = None\n",
    "        \n",
    "        # Extract scores from the experiment data\n",
    "        if isinstance(exp_data, dict):\n",
    "            score = exp_data.get('score')\n",
    "            scores_by_lang = exp_data.get('scores', {})\n",
    "            languages_in_exp = exp_data.get('languages', [])\n",
    "        else:\n",
    "            score = exp_data\n",
    "            scores_by_lang = {}\n",
    "            languages_in_exp = []\n",
    "        \n",
    "        # Create a base row with common metadata\n",
    "        base_row = {\n",
    "            'experiment_key': exp_key,\n",
    "            'source': source,\n",
    "            'model': model,\n",
    "            'task': task,\n",
    "            'control_index': control_idx,\n",
    "            'submetric': submetric,\n",
    "        }\n",
    "        \n",
    "        # If we have language-specific scores and no specific language was extracted\n",
    "        if language is None and scores_by_lang:\n",
    "            for lang, lang_score in scores_by_lang.items():\n",
    "                lang_row = base_row.copy()\n",
    "                lang_row['language'] = lang\n",
    "                lang_row['score'] = lang_score\n",
    "                data_rows.append(lang_row)\n",
    "        elif language is None and languages_in_exp:\n",
    "            # If we have a list of languages but no language-specific scores\n",
    "            for lang in languages_in_exp:\n",
    "                lang_row = base_row.copy()\n",
    "                lang_row['language'] = lang\n",
    "                lang_row['score'] = score  # Use the same score for all languages\n",
    "                data_rows.append(lang_row)\n",
    "        else:\n",
    "            # Either we have a specific language or no language information\n",
    "            row = base_row.copy()\n",
    "            row['language'] = language\n",
    "            row['score'] = score\n",
    "            data_rows.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Clean up - handle NaN and None values appropriately\n",
    "    df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataframe\n",
    "df = create_structured_dataframe(results)\n",
    "\n",
    "# Display basic information about the dataframe\n",
    "print(f\"Created dataframe with {len(df)} rows\")\n",
    "print(\"\\nDataframe columns:\", df.columns.tolist())\n",
    "print(\"\\nUnique sources:\", df['source'].unique())\n",
    "print(\"Unique models:\", df['model'].unique())\n",
    "print(\"Unique tasks:\", df['task'].unique())\n",
    "print(\"Unique languages:\", df['language'].dropna().unique())\n",
    "print(\"Unique submetrics:\", df['submetric'].dropna().unique())\n",
    "print(\"Unique control indices:\", df['control_index'].dropna().unique())\n",
    "\n",
    "# Show a sample of the dataframe\n",
    "print(\"\\nSample of the dataframe:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Main function to run the parser on JSON data from uploaded files\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to parse and analyze experiment results.\n",
    "    \n",
    "    This should be run in a Jupyter notebook cell after uploading your JSON data files.\n",
    "    \"\"\"\n",
    "    # Extract JSON data from the uploaded files\n",
    "    json_chunks = []\n",
    "    \n",
    "    try:\n",
    "        # Assuming you have the file content available\n",
    "        # For document 3 (paste.txt)\n",
    "        paste_txt_content = await window.fs.readFile('paste.txt', { encoding: 'utf8' })\n",
    "        json_chunks.append(\"{\" + paste_txt_content + \"}\")\n",
    "        print(\"Successfully loaded paste.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading paste.txt: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # For document 4 (paste-2.txt)\n",
    "        paste2_txt_content = await window.fs.readFile('paste-2.txt', { encoding: 'utf8' })\n",
    "        json_chunks.append(\"{\" + paste2_txt_content + \"}\")\n",
    "        print(\"Successfully loaded paste-2.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading paste-2.txt: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # For document 5 (paste-3.txt)\n",
    "        paste3_txt_content = await window.fs.readFile('paste-3.txt', { encoding: 'utf8' })\n",
    "        json_chunks.append(\"{\" + paste3_txt_content + \"}\")\n",
    "        print(\"Successfully loaded paste-3.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading paste-3.txt: {e}\")\n",
    "    \n",
    "    # Check if we have any data\n",
    "    if not json_chunks:\n",
    "        print(\"No JSON data loaded. Please upload your experiment results files.\")\n",
    "        return\n",
    "    \n",
    "    # Parse all chunks\n",
    "    results_df, overview = parse_multiple_json_chunks(json_chunks)\n",
    "    \n",
    "    # Display overview\n",
    "    print(\"\\nOverview of Experiments:\")\n",
    "    display(overview)\n",
    "    \n",
    "    # Display key metrics by experiment type\n",
    "    print(\"\\nPerformance Metrics by Experiment Type:\")\n",
    "    metrics_cols = [col for col in results_df.columns if col.startswith('test_')]\n",
    "    if metrics_cols:\n",
    "        display(results_df.groupby(['source', 'task', 'experiment_type', 'is_control'])[metrics_cols].mean().reset_index())\n",
    "    \n",
    "    # Plot results for regression tasks\n",
    "    regression_df = results_df[results_df['task_type'] == 'regression']\n",
    "    if len(regression_df) > 0:\n",
    "        print(\"\\nRegression Task Performance:\")\n",
    "        if 'test_r2' in regression_df.columns:\n",
    "            plot_experiment_results(regression_df, metric='test_r2', task='complexity')\n",
    "    \n",
    "    # Plot results for classification tasks\n",
    "    classification_df = results_df[results_df['task_type'] == 'classification']\n",
    "    if len(classification_df) > 0:\n",
    "        print(\"\\nClassification Task Performance:\")\n",
    "        if 'test_accuracy' in classification_df.columns:\n",
    "            plot_experiment_results(classification_df, metric='test_accuracy', task='question_type')\n",
    "    \n",
    "    # Check for cross-lingual experiments\n",
    "    cross_lingual_df = results_df[results_df['experiment_type'] == 'cross_lingual']\n",
    "    if len(cross_lingual_df) > 0:\n",
    "        print(\"\\nCross-Lingual Performance:\")\n",
    "        plot_cross_lingual_heatmap(results_df, task='complexity', metric='test_r2')\n",
    "        if 'question_type' in cross_lingual_df['task'].values:\n",
    "            plot_cross_lingual_heatmap(results_df, task='question_type', metric='test_accuracy')\n",
    "    \n",
    "    # Analyze per-language metrics for tfidf models\n",
    "    if 'tfidf' in results_df['source'].values:\n",
    "        print(\"\\nAnalyzing TFIDF Per-Language Performance:\")\n",
    "        analyze_per_language_metrics(results_df)\n",
    "    \n",
    "    return results_df, overview\n",
    "\n",
    "# Function to generate experiment overview\n",
    "def generate_experiment_overview(results_df):\n",
    "    \"\"\"Generate an overview of experiments by source, task, and type.\"\"\"\n",
    "    # Convert to pandas DataFrame if not already\n",
    "    if not isinstance(results_df, pd.DataFrame):\n",
    "        results_df = pd.DataFrame(results_df)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    overview = []\n",
    "    \n",
    "    # Group by source, task, and experiment type\n",
    "    grouped = results_df.groupby(['source', 'task', 'experiment_type', 'is_control'])\n",
    "    \n",
    "    for (source, task, exp_type, is_control), group in grouped:\n",
    "        control_status = 'control' if is_control else 'non_control'\n",
    "        languages = []\n",
    "        \n",
    "        # Get unique languages\n",
    "        if 'language' in group.columns:\n",
    "            languages = group['language'].dropna().unique().tolist()\n",
    "        elif 'train_language' in group.columns:\n",
    "            train_langs = group['train_language'].dropna().unique().tolist()\n",
    "            eval_langs = group['eval_language'].dropna().unique().tolist()\n",
    "            languages = list(set(train_langs + eval_langs))\n",
    "        elif 'languages' in group.columns:\n",
    "            # Handle comma-separated language strings\n",
    "            all_langs = []\n",
    "            for lang_str in group['languages'].dropna():\n",
    "                all_langs.extend(lang_str.split(','))\n",
    "            languages = list(set(all_langs))\n",
    "        \n",
    "        # Get unique submetrics\n",
    "        submetrics = group['submetric'].dropna().unique().tolist() if 'submetric' in group.columns else []\n",
    "        \n",
    "        overview.append({\n",
    "            'source': source,\n",
    "            'task': task,\n",
    "            'experiment_type': exp_type,\n",
    "            'control_status': control_status,\n",
    "            'count': len(group),\n",
    "            'languages': ', '.join(languages) if languages else '',\n",
    "            'submetrics': ', '.join(submetrics) if submetrics else '',\n",
    "            'models': ', '.join(group['model'].unique().tolist())\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(overview)\n",
    "\n",
    "# Function to handle parsing data from multiple chunks\n",
    "def parse_multiple_json_chunks(json_chunks):\n",
    "    \"\"\"\n",
    "    Parse ML experiment results from multiple JSON chunks.\n",
    "    \n",
    "    Args:\n",
    "        json_chunks: List of JSON strings containing experiment results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (results_df, overview_df)\n",
    "    \"\"\"\n",
    "    # Combine all chunks into a single JSON object\n",
    "    combined_data = {}\n",
    "    \n",
    "    for chunk in json_chunks:\n",
    "        try:\n",
    "            # Parse the chunk\n",
    "            chunk_data = json.loads(chunk)\n",
    "            \n",
    "            # Add to combined data\n",
    "            combined_data.update(chunk_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON chunk: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Process the combined data\n",
    "    results = parse_experiment_results(combined_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Generate overview\n",
    "    overview_df = generate_experiment_overview(results_df)\n",
    "    \n",
    "    return results_df, overview_df\n",
    "\n",
    "# Function to visualize results\n",
    "def plot_experiment_results(df, metric='test_r2', task=None, by='source', is_control=None, submetric=None):\n",
    "    \"\"\"\n",
    "    Plot experiment results based on specified filters.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with experiment results\n",
    "        metric: Metric to plot (default: test_r2)\n",
    "        task: Filter by task (default: None to show all)\n",
    "        by: Group by column (default: 'source')\n",
    "        is_control: Filter by control status (default: None to show all)\n",
    "        submetric: Filter by submetric (default: None to show all)\n",
    "    \"\"\"\n",
    "    # Filter data based on parameters\n",
    "    plot_df = df.copy()\n",
    "    \n",
    "    if task:\n",
    "        plot_df = plot_df[plot_df['task'] == task]\n",
    "    \n",
    "    if is_control is not None:\n",
    "        plot_df = plot_df[plot_df['is_control'] == is_control]\n",
    "    \n",
    "    if submetric:\n",
    "        plot_df = plot_df[plot_df['submetric'] == submetric]\n",
    "    \n",
    "    # Check if we have data after filtering\n",
    "    if len(plot_df) == 0:\n",
    "        print(\"No data available with the current filters.\")\n",
    "        return\n",
    "    \n",
    "    # Check if the metric exists in the DataFrame\n",
    "    if metric not in plot_df.columns:\n",
    "        print(f\"Metric '{metric}' not found in data. Available metrics: {[c for c in df.columns if c.startswith('test_') or c.startswith('val_')]}\")\n",
    "        return\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a grouped boxplot\n",
    "    sns.boxplot(x=by, y=metric, data=plot_df)\n",
    "    \n",
    "    # Add individual points\n",
    "    sns.stripplot(x=by, y=metric, data=plot_df, color='black', size=4, alpha=0.5)\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(f'{metric} by {by}' + (f' for {task}' if task else '') + \n",
    "              (f' (control: {is_control})' if is_control is not None else '') +\n",
    "              (f' - {submetric}' if submetric else ''))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to create a cross-lingual performance heatmap\n",
    "def plot_cross_lingual_heatmap(df, task='complexity', metric='test_r2'):\n",
    "    \"\"\"Create a heatmap of cross-lingual performance\"\"\"\n",
    "    # Filter for cross-lingual experiments\n",
    "    cross_df = df[(df['experiment_type'] == 'cross_lingual') & (df['task'] == task)]\n",
    "    \n",
    "    if len(cross_df) == 0:\n",
    "        print(f\"No cross-lingual data found for task: {task}\")\n",
    "        return\n",
    "    \n",
    "    # Check if we have the necessary columns\n",
    "    if 'train_language' not in cross_df.columns or 'eval_language' not in cross_df.columns:\n",
    "        print(\"Missing language columns for cross-lingual analysis\")\n",
    "        return\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    heatmap_data = cross_df.pivot_table(\n",
    "        index='train_language', \n",
    "        columns='eval_language', \n",
    "        values=metric,\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=.5)\n",
    "    plt.title(f'Cross-Lingual Performance: {task} - {metric}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return heatmap_data\n",
    "\n",
    "# Function to analyze per-language metrics from tfidf models\n",
    "def analyze_per_language_metrics(df):\n",
    "    \"\"\"Extract and analyze per-language metrics from tfidf models\"\"\"\n",
    "    # Filter for tfidf models\n",
    "    tfidf_df = df[df['source'] == 'tfidf']\n",
    "    \n",
    "    # Check if we have any tfidf models\n",
    "    if len(tfidf_df) == 0:\n",
    "        print(\"No tfidf models found in the data\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame to store per-language metrics\n",
    "    per_lang_metrics = []\n",
    "    \n",
    "    for _, row in tfidf_df.iterrows():\n",
    "        if 'per_language_metrics' in row and isinstance(row['per_language_metrics'], dict):\n",
    "            test_metrics = row['per_language_metrics'].get('test', {})\n",
    "            \n",
    "            # Extract metrics for each language\n",
    "            for lang, metrics in test_metrics.items():\n",
    "                per_lang_metrics.append({\n",
    "                    'experiment_id': row['experiment_id'],\n",
    "                    'model': row['model'],\n",
    "                    'task': row['task'],\n",
    "                    'is_control': row['is_control'],\n",
    "                    'control_index': row['control_index'],\n",
    "                    'submetric': row['submetric'],\n",
    "                    'language': lang,\n",
    "                    'test_mse': metrics.get('mse'),\n",
    "                    'test_rmse': metrics.get('rmse'),\n",
    "                    'test_mae': metrics.get('mae'),\n",
    "                    'test_r2': metrics.get('r2')\n",
    "                })\n",
    "    \n",
    "    if len(per_lang_metrics) > 0:\n",
    "        per_lang_df = pd.DataFrame(per_lang_metrics)\n",
    "        print(\"\\nPer-Language Performance for TFIDF Models:\")\n",
    "        display(per_lang_df.groupby(['language', 'task', 'is_control'])['test_r2'].mean().reset_index())\n",
    "        \n",
    "        # Create a visualization\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        sns.barplot(x='language', y='test_r2', hue='is_control', data=per_lang_df)\n",
    "        plt.title('Per-Language Performance (TFIDF Models)')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return per_lang_df\n",
    "    else:\n",
    "        print(\"No per-language metrics found in the data\")\n",
    "        return None\n",
    "\n",
    "# Function to parse experiment results\n",
    "def parse_experiment_results(json_data):\n",
    "    \"\"\"\n",
    "    Parse ML experiment results from JSON data.\n",
    "    \n",
    "    Args:\n",
    "        json_data: Dictionary containing experiment results\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with parsed results\n",
    "    \"\"\"\n",
    "    # Initialize the results array\n",
    "    results = []\n",
    "    \n",
    "    # Process each experiment in the JSON data\n",
    "    for key, value in json_data.items():\n",
    "        try:\n",
    "            # Skip any potential empty or invalid entries\n",
    "            if not value or not isinstance(value, dict):\n",
    "                continue\n",
    "            \n",
    "            # Parse the experiment key to extract metadata\n",
    "            key_parts = key.split('_')\n",
    "            source = key_parts[0]\n",
    "            \n",
    "            # Create a results object with common fields\n",
    "            result_obj = {\n",
    "                'experiment_id': key,\n",
    "                'source': source\n",
    "            }\n",
    "            \n",
    "            # Handle the two main experiment types: glot500 and tfidf\n",
    "            if source == 'glot500':\n",
    "                # Extract model name (usually 'lm_probe')\n",
    "                result_obj['model'] = '_'.join(key_parts[1:3])\n",
    "                \n",
    "                # Handle the experiment_type\n",
    "                if 'experiment_type' in value and value['experiment_type'] == 'cross_lingual' or \\\n",
    "                   ('train_language' in value and 'eval_language' in value):\n",
    "                    result_obj['experiment_type'] = 'cross_lingual'\n",
    "                    result_obj['train_language'] = value.get('train_language')\n",
    "                    result_obj['eval_language'] = value.get('eval_language')\n",
    "                else:\n",
    "                    result_obj['experiment_type'] = 'monolingual'\n",
    "                    # Handle ko and other nested structures\n",
    "                    if 'language' in value:\n",
    "                        result_obj['language'] = value['language']\n",
    "                    elif 'ko' in value and isinstance(value['ko'], dict):\n",
    "                        result_obj['language'] = 'ko'\n",
    "                        # Use the nested metrics for ko\n",
    "                        if 'test_metrics' in value['ko']:\n",
    "                            value['test_metrics'] = value['ko']['test_metrics']\n",
    "                            value['train_metrics'] = value['ko']['train_metrics']\n",
    "                            value['val_metrics'] = value['ko']['val_metrics']\n",
    "                    else:\n",
    "                        # Try to extract language from key\n",
    "                        lang_candidates = [part for part in key_parts if len(part) == 2]\n",
    "                        if lang_candidates:\n",
    "                            result_obj['language'] = lang_candidates[0]\n",
    "                        else:\n",
    "                            result_obj['language'] = 'unknown'\n",
    "                \n",
    "                # Extract task details\n",
    "                result_obj['task'] = value.get('task')\n",
    "                result_obj['task_type'] = value.get('task_type', '')  # Classification or regression\n",
    "                \n",
    "                # Handle control info\n",
    "                result_obj['is_control'] = value.get('is_control', False)\n",
    "                result_obj['control_index'] = value.get('control_index')\n",
    "                \n",
    "                # Handle submetric\n",
    "                result_obj['submetric'] = value.get('submetric', '')\n",
    "                \n",
    "                # Handle layer info \n",
    "                result_obj['layer'] = value.get('layer')\n",
    "                \n",
    "                # Extract metrics based on task type\n",
    "                result_obj['train_time'] = value.get('train_time')\n",
    "                \n",
    "                # Add test metrics\n",
    "                if 'test_metrics' in value:\n",
    "                    if value.get('task_type') == 'classification':\n",
    "                        result_obj['test_loss'] = value['test_metrics'].get('loss')\n",
    "                        result_obj['test_accuracy'] = value['test_metrics'].get('accuracy')\n",
    "                        result_obj['test_f1'] = value['test_metrics'].get('f1')\n",
    "                    else:  # regression\n",
    "                        result_obj['test_loss'] = value['test_metrics'].get('loss')\n",
    "                        result_obj['test_mse'] = value['test_metrics'].get('mse')\n",
    "                        result_obj['test_rmse'] = value['test_metrics'].get('rmse')\n",
    "                        result_obj['test_r2'] = value['test_metrics'].get('r2')\n",
    "                \n",
    "                # Add val metrics\n",
    "                if 'val_metrics' in value:\n",
    "                    if value.get('task_type') == 'classification':\n",
    "                        result_obj['val_loss'] = value['val_metrics'].get('loss')\n",
    "                        result_obj['val_accuracy'] = value['val_metrics'].get('accuracy')\n",
    "                        result_obj['val_f1'] = value['val_metrics'].get('f1')\n",
    "                    else:  # regression\n",
    "                        result_obj['val_loss'] = value['val_metrics'].get('loss')\n",
    "                        result_obj['val_mse'] = value['val_metrics'].get('mse')\n",
    "                        result_obj['val_rmse'] = value['val_metrics'].get('rmse')\n",
    "                        result_obj['val_r2'] = value['val_metrics'].get('r2')\n",
    "            \n",
    "            elif source == 'tfidf':\n",
    "                # Extract model name (e.g., Ridge, XGBRegressor)\n",
    "                result_obj['model'] = key_parts[1]\n",
    "                \n",
    "                # Set experiment type (always monolingual for tfidf)\n",
    "                result_obj['experiment_type'] = 'monolingual'\n",
    "                \n",
    "                # Extract task\n",
    "                result_obj['task'] = key_parts[2]\n",
    "                result_obj['task_type'] = 'regression'  # TFIDF models are regression\n",
    "                \n",
    "                # Handle control info\n",
    "                control_parts = [part for part in key_parts if part.startswith('control')]\n",
    "                if control_parts:\n",
    "                    result_obj['is_control'] = True\n",
    "                    result_obj['control_index'] = int(control_parts[0].replace('control', ''))\n",
    "                else:\n",
    "                    result_obj['is_control'] = False\n",
    "                    result_obj['control_index'] = None\n",
    "                \n",
    "                # Handle submetric (if present)\n",
    "                submetric_candidates = [\n",
    "                    'avg_max_depth', 'n_tokens', 'avg_subordinate_chain_len', \n",
    "                    'avg_verb_edges', 'lexical_density'\n",
    "                ]\n",
    "                detected_submetrics = [sm for sm in submetric_candidates if sm in key or \n",
    "                                     (isinstance(value.get('submetric'), str) and sm in value.get('submetric', ''))]\n",
    "                \n",
    "                if detected_submetrics:\n",
    "                    result_obj['submetric'] = detected_submetrics[0]\n",
    "                elif 'submetric' in value and value['submetric']:\n",
    "                    result_obj['submetric'] = value['submetric']\n",
    "                else:\n",
    "                    result_obj['submetric'] = ''\n",
    "                \n",
    "                # Extract languages\n",
    "                if 'languages' in value:\n",
    "                    result_obj['languages'] = ','.join(value['languages'])\n",
    "                \n",
    "                # Extract metrics\n",
    "                result_obj['train_time'] = value.get('training_time')\n",
    "                \n",
    "                # Add test metrics\n",
    "                if 'test_metrics' in value:\n",
    "                    result_obj['test_mse'] = value['test_metrics'].get('mse')\n",
    "                    result_obj['test_rmse'] = value['test_metrics'].get('rmse')\n",
    "                    result_obj['test_mae'] = value['test_metrics'].get('mae')\n",
    "                    result_obj['test_r2'] = value['test_metrics'].get('r2')\n",
    "                \n",
    "                # Add validation metrics\n",
    "                if 'val_metrics' in value:\n",
    "                    result_obj['val_mse'] = value['val_metrics'].get('mse')\n",
    "                    result_obj['val_rmse'] = value['val_metrics'].get('rmse')\n",
    "                    result_obj['val_mae'] = value['val_metrics'].get('mae')\n",
    "                    result_obj['val_r2'] = value['val_metrics'].get('r2')\n",
    "                \n",
    "                # Store per-language metrics reference\n",
    "                if 'per_language_metrics' in value:\n",
    "                    result_obj['per_language_metrics'] = value['per_language_metrics']\n",
    "            \n",
    "            # Add the processed result to the results array\n",
    "            results.append(result_obj)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing experiment {key}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading paste.txt: name 'window' is not defined\n",
      "Error loading paste-2.txt: name 'window' is not defined\n",
      "Error loading paste-3.txt: name 'window' is not defined\n",
      "No JSON data loaded. Please upload your experiment results files.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Option 1: Run the main function to automatically process uploaded files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results_df, overview = \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Option 2: Manually process JSON strings or files\u001b[39;00m\n\u001b[32m      5\u001b[39m json_chunks = []\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option 1: Run the main function to automatically process uploaded files\n",
    "results_df, overview = await main()\n",
    "\n",
    "# Option 2: Manually process JSON strings or files\n",
    "json_chunks = []\n",
    "\n",
    "# Example: Manually specify JSON content as strings\n",
    "json_chunks.append(\"\"\"\n",
    "{\n",
    "  \"glot500_lm_probe_single_submetric_control2_avg_subordinate_chain_len_fi_layer12\": {\n",
    "    \"train_time\": 80.71137046813965,\n",
    "    \"train_metrics\": {\n",
    "      \"loss\": 0.013749146613602837,\n",
    "      \"mse\": 0.013802728615701199,\n",
    "      \"rmse\": 0.11748501443035703,\n",
    "      \"r2\": 0.008942842483520508\n",
    "    },\n",
    "    \"val_metrics\": {\n",
    "      \"loss\": 0.05693912319839001,\n",
    "      \"mse\": 0.05693540349602699,\n",
    "      \"rmse\": 0.23861140688581298,\n",
    "      \"r2\": -0.1697772741317749\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "      \"loss\": 0.054117132776549885,\n",
    "      \"mse\": 0.054204776883125305,\n",
    "      \"rmse\": 0.232819193545389,\n",
    "      \"r2\": -0.22100341320037842\n",
    "    },\n",
    "    \"language\": \"fi\",\n",
    "    \"task\": \"single_submetric\",\n",
    "    \"task_type\": \"regression\",\n",
    "    \"model_type\": \"lm_probe\",\n",
    "    \"is_control\": true,\n",
    "    \"control_index\": 2,\n",
    "    \"submetric\": \"avg_subordinate_chain_len\",\n",
    "    \"source\": \"glot500\",\n",
    "    \"layer\": 12\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Parse the manually provided JSON chunks\n",
    "manual_results_df, manual_overview = parse_multiple_json_chunks(json_chunks)\n",
    "\n",
    "# Display the overview\n",
    "print(\"Overview of Experiments:\")\n",
    "display(manual_overview)\n",
    "\n",
    "# Additional analysis examples:\n",
    "\n",
    "# 1. Compare performance across different tasks\n",
    "if 'task' in manual_results_df.columns and 'test_r2' in manual_results_df.columns:\n",
    "    task_performance = manual_results_df.groupby('task')['test_r2'].mean().reset_index()\n",
    "    display(task_performance)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='task', y='test_r2', data=task_performance)\n",
    "    plt.title('Performance by Task')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Compare control vs non-control experiments\n",
    "if 'is_control' in manual_results_df.columns and 'test_r2' in manual_results_df.columns:\n",
    "    control_comparison = manual_results_df.groupby(['task', 'is_control'])['test_r2'].mean().reset_index()\n",
    "    display(control_comparison)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='task', y='test_r2', hue='is_control', data=control_comparison)\n",
    "    plt.title('Control vs Non-Control Performance')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# 3. Find the best performing model for each task\n",
    "if len(manual_results_df) > 0:\n",
    "    best_models = manual_results_df.loc[manual_results_df.groupby('task')['test_r2'].idxmax()]\n",
    "    print(\"Best Performing Models by Task:\")\n",
    "    display(best_models[['task', 'model', 'source', 'test_r2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a34e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
