{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Generating TF-IDF Vectors\n",
    "##### Prepare datasets, normalize features, create destroyed (sub)sets and extract Text2Text TF-IDF features for our questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q text2text -q wandb -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import text2text as t2t\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "SEED = 69\n",
    "np.random.seed(SEED)\n",
    "\n",
    "LANGUAGES = ['en', 'fi', 'id', 'ko', 'ja', 'ru', 'ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rokii-ku-leuven/MAIthesis/runs/a4p2tpma?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73f958ffe720>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"MAIthesis\",name=\"data-preparation\",tags=[\"data-prep\", \"tfidf\", \"normalization\", \"data-randomization\"],job_type=\"data-processing\",dir=\"/home/robin/Research/qtype-eval/scripts/experiments/baselines/\")   #tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944fdc25e6544641b91830ca0a058340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346e5c2ffedb4ed79af0f93a8fa32bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d5df91a4a344bdbf3fe865c8901e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77dd11ba6844883be23f2ea7b7da1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('rokokot/question-type-and-complexity-v2')\n",
    "train_data = dataset['train']\n",
    "train = train_data.to_pandas()\n",
    "dev_data = dataset['validation']\n",
    "dev = dev_data.to_pandas()\n",
    "test_data = dataset['test']\n",
    "test = test_data.to_pandas()\n",
    "\n",
    "#wandb.log({\"train_data_rows\": len(train), \"dev_data_rows\": len(dev), \"test_data_rows\": len(test), \"data_columns\": len(train.columns)})      #tracking\n",
    "#print({\"train_data_rows\": len(train), \"dev_data_rows\": len(dev), \"test_data_rows\": len(test), \"data_columns\": len(train.columns)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>avg_links_len</th>\n",
       "      <th>avg_max_depth</th>\n",
       "      <th>avg_subordinate_chain_len</th>\n",
       "      <th>avg_verb_edges</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>question_type</th>\n",
       "      <th>complexity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finnish_polar_360</td>\n",
       "      <td>Onko Tampereen rantatunneli Suomen pisin maant...</td>\n",
       "      <td>fi</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1</td>\n",
       "      <td>1.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>russian_content_3904</td>\n",
       "      <td>В каком фильме снимался Дзюн Фукуяма?</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0</td>\n",
       "      <td>1.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finnish_content_10111</td>\n",
       "      <td>Kuka oli Mary Jane Watsonin lempisukulainen pe...</td>\n",
       "      <td>fi</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0</td>\n",
       "      <td>1.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finnish_content_13146</td>\n",
       "      <td>Milloin HMS Castleton tilattiin?</td>\n",
       "      <td>fi</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0</td>\n",
       "      <td>1.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>korean_content_4335</td>\n",
       "      <td>6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?</td>\n",
       "      <td>ko</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.400</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0</td>\n",
       "      <td>2.471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               unique_id                                               text  \\\n",
       "0      finnish_polar_360  Onko Tampereen rantatunneli Suomen pisin maant...   \n",
       "1   russian_content_3904              В каком фильме снимался Дзюн Фукуяма?   \n",
       "2  finnish_content_10111  Kuka oli Mary Jane Watsonin lempisukulainen pe...   \n",
       "3  finnish_content_13146                   Milloin HMS Castleton tilattiin?   \n",
       "4    korean_content_4335                    6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?   \n",
       "\n",
       "  language  avg_links_len  avg_max_depth  avg_subordinate_chain_len  \\\n",
       "0       fi          0.228          0.250                        0.0   \n",
       "1       ru          0.045          0.125                        0.0   \n",
       "2       fi          0.296          0.333                        0.0   \n",
       "3       fi          0.173          0.167                        0.0   \n",
       "4       ko          0.143          0.300                        0.5   \n",
       "\n",
       "   avg_verb_edges  lexical_density  n_tokens  question_type  complexity_score  \n",
       "0           0.000            0.750     0.231              1             1.459  \n",
       "1           0.333            0.667     0.073              0             1.243  \n",
       "2           0.000            0.531     0.294              0             1.455  \n",
       "3           0.333            1.000     0.059              0             1.732  \n",
       "4           0.400            1.000     0.128              0             2.471  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"questions:\")\n",
    "for i in range(3):\n",
    "    print(f\"{train['text'][i][:100]}... (lang id: {train['language'][i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Normalize total complexity scores\n",
    "\n",
    "def normalize_complexity_scores(df): \n",
    "    df['lang_norm_complexity_score'] = 0.0\n",
    "    for language, group in df.groupby('language'):\n",
    "        min_score = group['complexity_score'].min()\n",
    "        max_score = group['complexity_score'].max()\n",
    "        if min_score == max_score:\n",
    "            df.loc[df['language'] == language, 'lang_norm_complexity_score'] = 0.5\n",
    "        else:\n",
    "            normalized_scores = (group['complexity_score'] - min_score) / (max_score - min_score)\n",
    "            df.loc[df['language'] == language, 'lang_norm_complexity_score'] = normalized_scores.values\n",
    "    return df\n",
    "\n",
    "train_df = normalize_complexity_scores(train)\n",
    "dev_df = normalize_complexity_scores(dev)\n",
    "test_df = normalize_complexity_scores(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Complexity Score Distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=train_df, x='lang_norm_complexity_score', label='Train', fill=True, alpha=0.3)\n",
    "sns.kdeplot(data=test_df, x='lang_norm_complexity_score', label='Test', fill=True, alpha=0.3)\n",
    "sns.kdeplot(data=dev_df, x='lang_norm_complexity_score', label='Dev', fill=True, alpha=0.3)\n",
    "plt.title('Normalized Complexity Score Distribution Across Splits')\n",
    "plt.xlabel('Normalized Complexity Score')\n",
    "plt.legend()\n",
    "wandb.log({\"complexity_distribution/all_splits\": wandb.Image(plt)})     #tracking\n",
    "plt.close()\n",
    "\n",
    "complexity_stats = {\n",
    "    \"complexity_stats/train_mean\": train_df['lang_norm_complexity_score'].mean(),\n",
    "    \"complexity_stats/dev_mean\": dev_df['lang_norm_complexity_score'].mean(),\n",
    "    \"complexity_stats/test_mean\": test_df['lang_norm_complexity_score'].mean(),\n",
    "    \"complexity_stats/train_median\": train_df['lang_norm_complexity_score'].median(),\n",
    "    \"complexity_stats/dev_median\": dev_df['lang_norm_complexity_score'].median(),\n",
    "    \"complexity_stats/test_median\": test_df['lang_norm_complexity_score'].median(),}\n",
    "wandb.log(complexity_stats)     #tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================== Question type distributions \n",
    "def plot_type_dist(train_df, test_df, dev_df):\n",
    "  train_types = train_df['question_type'].value_counts().reset_index()\n",
    "  train_types.columns = ['question_type', 'count']\n",
    "  train_types['split'] = 'Train'\n",
    "\n",
    "  test_types = test_df['question_type'].value_counts().reset_index()\n",
    "  test_types.columns = ['question_type', 'count']\n",
    "  test_types['split'] = 'Test'\n",
    "\n",
    "  dev_types = dev_df['question_type'].value_counts().reset_index()\n",
    "  dev_types.columns = ['question_type', 'count']\n",
    "  dev_types['split'] = 'Dev'\n",
    "\n",
    "  all_types = pd.concat([train_types, test_types, dev_types])\n",
    "\n",
    "  for split, group in all_types.groupby('split'):\n",
    "      total = group['count'].sum()\n",
    "      all_types.loc[all_types['split'] == split, 'percentage'] = all_types.loc[all_types['split'] == split, 'count'] / total * 100\n",
    "\n",
    "  plt.figure(figsize=(15, 8))\n",
    "  chart = sns.barplot(data=all_types, x='question_type', y='count', hue='split')\n",
    "  plt.title('Question Type Distribution Across Splits')\n",
    "  plt.xlabel('Question Type')\n",
    "  plt.ylabel('Count')\n",
    "  plt.xticks(rotation=45, ha='right')\n",
    "  plt.tight_layout()\n",
    "  wandb.log({\"question_type_distribution/counts\": wandb.Image(plt)})      #tracking\n",
    "  plt.close()\n",
    "\n",
    "  plt.figure(figsize=(15, 8))\n",
    "  chart = sns.barplot(data=all_types, x='question_type', y='percentage', hue='split')\n",
    "  plt.title('Question Type Percentage Distribution Across Splits')\n",
    "  plt.xlabel('Question Type')\n",
    "  plt.ylabel('Percentage (%)')\n",
    "  plt.xticks(rotation=45, ha='right')\n",
    "  plt.tight_layout()\n",
    "  wandb.log({\"question_type_distribution/percentages\": wandb.Image(plt)})     #tracking\n",
    "  plt.close()\n",
    "\n",
    "  return all_types\n",
    "\n",
    "question_type_stats = plot_type_dist(train_df, dev_df, test_df)\n",
    "\n",
    "wandb.log({\"question_type_stats\": wandb.Table(dataframe=question_type_stats)})      #tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_language_dist(train_df, dev_df, test_df):\n",
    "    train_langs = train_df['language'].value_counts().reset_index()\n",
    "    train_langs.columns = ['language', 'count']\n",
    "    train_langs['split'] = 'Train'\n",
    "    dev_langs = dev_df['language'].value_counts().reset_index()\n",
    "    dev_langs.columns = ['language', 'count']\n",
    "    dev_langs['split'] = 'Dev'\n",
    "    test_langs = test_df['language'].value_counts().reset_index()\n",
    "    test_langs.columns = ['language', 'count']\n",
    "    test_langs['split'] = 'Test'\n",
    "    all_langs = pd.concat([train_langs, dev_langs, test_langs])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=all_langs, x='language', y='count', hue='split')\n",
    "    plt.title('Language Distribution Across Splits')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    wandb.log({\"language_distribution\": wandb.Image(plt)})          #tracking\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return all_langs\n",
    "\n",
    "language_stats = plot_language_dist(train_df, dev_df, test_df)\n",
    "\n",
    "wandb.log({\"language_stats\": wandb.Table(dataframe=language_stats)})            #tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4f718448884aa8ae69987eca16fdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106be4c1a3924ff78d1c4d545c3d733d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15528dc08efe4aaf8166f0980e55f767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF matrix shape: (7460, 1)\n",
      "Dev TF-IDF matrix shape: (441, 1)\n",
      "Test TF-IDF matrix shape: (719, 1)\n"
     ]
    }
   ],
   "source": [
    "tfidfer = t2t.Tfidfer()\n",
    "indexer = t2t.Indexer()\n",
    "\n",
    "\n",
    "def extract_tfidf_vectors(questions, languages):\n",
    "    vectors = []\n",
    "    for i, (question, lang) in enumerate(tqdm(zip(questions, languages), total=len(questions))):\n",
    "        vector = tfidfer.transform([question], src_lang=lang, output='matrix')[0]\n",
    "        vectors.append(vector)\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "X_train = extract_tfidf_vectors(train_df['text'].tolist(), train_df['language'].tolist())\n",
    "X_dev = extract_tfidf_vectors(dev_df['text'].tolist(), dev_df['language'].tolist())\n",
    "X_test = extract_tfidf_vectors(test_df['text'].tolist(), test_df['language'].tolist())\n",
    "\n",
    "print(f\"Training TF-IDF matrix shape: {X_train.shape}\")\n",
    "print(f\"Dev TF-IDF matrix shape: {X_dev.shape}\")\n",
    "print(f\"Test TF-IDF matrix shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/robin/Research/qtype-eval/scripts/baselines/vectors/tfidf_vectors_train.pkl', 'wb') as v: \n",
    "    pickle.dump(X_train, v)\n",
    "with open('/home/robin/Research/qtype-eval/scripts/baselines/vectors/tfidf_vectors_dev.pkl', 'wb') as v:\n",
    "    pickle.dump(X_dev, v)\n",
    "with open('/home/robin/Research/qtype-eval/scripts/baselines/vectors/tfidf_vectors_test.pkl', 'wb') as v:\n",
    "    pickle.dump(X_test, v)\n",
    "with open('/home/robin/Research/qtype-eval/scripts/baselines/vectors/idf_values.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidfer.idf, f)\n",
    "    \n",
    "tokenizer = t2t.Tokenizer()\n",
    "vocab = tokenizer.__class__.tokenizer.get_vocab()\n",
    "token_to_index = {token: idx for token, idx in vocab.items()}\n",
    "    \n",
    "with open('/home/robin/Research/qtype-eval/scripts/baselines/vectors/token_to_index_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(token_to_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating destroyed sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: tydi_train_control_question_type_seed1.csv\n",
      "Generated: tydi_train_control_complexity_seed1.csv\n",
      "Generated: tydi_train_control_avg_links_len_seed1.csv\n",
      "Generated: tydi_train_control_avg_max_depth_seed1.csv\n",
      "Generated: tydi_train_control_avg_subordinate_chain_len_seed1.csv\n",
      "Generated: tydi_train_control_avg_verb_edges_seed1.csv\n",
      "Generated: tydi_train_control_lexical_density_seed1.csv\n",
      "Generated: tydi_train_control_n_tokens_seed1.csv\n",
      "Generated: tydi_train_control_question_type_seed2.csv\n",
      "Generated: tydi_train_control_complexity_seed2.csv\n",
      "Generated: tydi_train_control_avg_links_len_seed2.csv\n",
      "Generated: tydi_train_control_avg_max_depth_seed2.csv\n",
      "Generated: tydi_train_control_avg_subordinate_chain_len_seed2.csv\n",
      "Generated: tydi_train_control_avg_verb_edges_seed2.csv\n",
      "Generated: tydi_train_control_lexical_density_seed2.csv\n",
      "Generated: tydi_train_control_n_tokens_seed2.csv\n",
      "Generated: tydi_train_control_question_type_seed3.csv\n",
      "Generated: tydi_train_control_complexity_seed3.csv\n",
      "Generated: tydi_train_control_avg_links_len_seed3.csv\n",
      "Generated: tydi_train_control_avg_max_depth_seed3.csv\n",
      "Generated: tydi_train_control_avg_subordinate_chain_len_seed3.csv\n",
      "Generated: tydi_train_control_avg_verb_edges_seed3.csv\n",
      "Generated: tydi_train_control_lexical_density_seed3.csv\n",
      "Generated: tydi_train_control_n_tokens_seed3.csv\n",
      "Generated all files for Hugging Face dataset upload\n"
     ]
    }
   ],
   "source": [
    "# ==================== Generate control datasets with consistent formet\n",
    "def generate_control_files(train_df, output_dir, n_seeds=3):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    train_df.to_csv(os.path.join(output_dir, \"tydi_train_base.csv\"), index=False)\n",
    "    \n",
    "    metrics = {\n",
    "        'question_type': 'question_type',\n",
    "        'complexity_score': 'complexity',\n",
    "        'avg_links_len': 'avg_links_len',\n",
    "        'avg_max_depth': 'avg_max_depth',\n",
    "        'avg_subordinate_chain_len': 'avg_subordinate_chain_len',\n",
    "        'avg_verb_edges': 'avg_verb_edges',\n",
    "        'lexical_density': 'lexical_density',\n",
    "        'n_tokens': 'n_tokens'\n",
    "    }\n",
    "    \n",
    "    for seed in range(1, n_seeds+1):\n",
    "        for feature, file_prefix in metrics.items():\n",
    "            control_df = train_df.copy()\n",
    "            \n",
    "            for lang in LANGUAGES:\n",
    "                lang_mask = control_df['language'] == lang\n",
    "                if lang_mask.sum() > 0:\n",
    "                    lang_indices = control_df[lang_mask].index\n",
    "                    np.random.seed(seed)\n",
    "                    \n",
    "                    if feature == 'question_type':\n",
    "                        shuffled_values = np.random.permutation(control_df.loc[lang_indices, feature].values)\n",
    "                    elif feature == 'complexity_score':\n",
    "                        shuffled_values = np.random.permutation(control_df.loc[lang_indices, 'lang_norm_complexity_score'].values)\n",
    "                        control_df.loc[lang_indices, 'lang_norm_complexity_score'] = shuffled_values\n",
    "                        continue\n",
    "                    else:\n",
    "                        shuffled_values = np.random.permutation(control_df.loc[lang_indices, feature].values)\n",
    "                    \n",
    "                    control_df.loc[lang_indices, feature] = shuffled_values\n",
    "            \n",
    "            filename = f\"tydi_train_control_{file_prefix}_seed{seed}.csv\"\n",
    "            control_df.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "            print(f\"Generated: {filename}\")\n",
    "\n",
    "output_dir = \"/home/robin/Research/qtype-eval/scripts/data/huggingface_upload_final\"\n",
    "generate_control_files(train_df, output_dir, n_seeds=3)\n",
    "\n",
    "dev_df.to_csv(os.path.join(output_dir, \"dev_base.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, \"ud_test_base.csv\"), index=False)\n",
    "print(\"Generated all files for Hugging Face dataset upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all destroyed sets with descriptive filenames\n"
     ]
    }
   ],
   "source": [
    "# ==================== Save destroyed sets\n",
    "base_output_dir = '/home/robin/Research/qtype-eval/data/destroyed'\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "features = {'question_type': 'destroyed_types','complexity_score': 'destroyed_complexity','avg_links_len': 'destroyed_avg_links_len','avg_max_depth': 'destroyed_avg_max_depth','avg_subordinate_chain_len': 'destroyed_subordinate_chain','avg_verb_edges': 'destroyed_verb_edges','lexical_density': 'destroyed_lexical_density','n_tokens': 'destroyed_tokens'}\n",
    "\n",
    "for seed in range(1, 4):\n",
    "    seed_key = f'within_lang_shuffle_{seed}'\n",
    "    \n",
    "    type_dir = os.path.join(base_output_dir, 'destroyed_types')\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "    \n",
    "    destroyed_sets['types'][seed_key].to_csv(os.path.join(type_dir, f'question_type_destroyed_seed_{seed}.csv'),index=False)\n",
    "    \n",
    "    complexity_dir = os.path.join(base_output_dir, 'destroyed_complexity')\n",
    "    os.makedirs(complexity_dir, exist_ok=True)\n",
    "    \n",
    "    destroyed_sets['complexity'][seed_key].to_csv(os.path.join(complexity_dir, f'complexity_score_destroyed_seed_{seed}.csv'),index=False)\n",
    "\n",
    "submetrics = ['avg_links_len','avg_max_depth','avg_subordinate_chain_len','avg_verb_edges', 'lexical_density', 'n_tokens']\n",
    "\n",
    "for metric in submetrics:\n",
    "    metric_dir = os.path.join(base_output_dir, f'destroyed_{metric}')\n",
    "    os.makedirs(metric_dir, exist_ok=True)\n",
    "    \n",
    "    for seed in range(1, 4):\n",
    "        seed_key = f'within_lang_shuffle_{seed}'        \n",
    "        destroyed_sets['submetrics'][metric][seed_key].to_csv(os.path.join(metric_dir, f'{metric}_destroyed_seed_{seed}.csv'),index=False)\n",
    "\n",
    "print(\"Saved all destroyed sets with descriptive filenames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Comparison of destroyed and original scores\n",
    "for seed in range(1, 4):\n",
    "    seed_key = f'within_lang_shuffle_{seed}'\n",
    "    destroyed_df = destroyed_sets['complexity'][seed_key]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(data=train_df,x='lang_norm_complexity_score',label='Original',fill=True,alpha=0.4,color='blue')\n",
    "    \n",
    "    sns.kdeplot(data=destroyed_df,x='complexity_score_destroyed',label=f'Destroyed (Seed {seed})',fill=True,alpha=0.4,color='yellow')\n",
    "    \n",
    "    plt.title(f'Original vs. Control Complexity Score Distribution (Seed {seed})')\n",
    "    plt.xlabel('Normalized Complexity Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({f\"control_label_combined/seed_{seed}\": wandb.Image(plt)})           #tracking\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>complexity_stats/dev_mean</td><td>▁</td></tr><tr><td>complexity_stats/dev_median</td><td>▁</td></tr><tr><td>complexity_stats/test_mean</td><td>▁</td></tr><tr><td>complexity_stats/test_median</td><td>▁</td></tr><tr><td>complexity_stats/train_mean</td><td>▁</td></tr><tr><td>complexity_stats/train_median</td><td>▁</td></tr><tr><td>data_columns</td><td>▁</td></tr><tr><td>dev_data_rows</td><td>▁</td></tr><tr><td>test_data_rows</td><td>▁</td></tr><tr><td>train_data_rows</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>complexity_stats/dev_mean</td><td>0.44659</td></tr><tr><td>complexity_stats/dev_median</td><td>0.43849</td></tr><tr><td>complexity_stats/test_mean</td><td>0.42937</td></tr><tr><td>complexity_stats/test_median</td><td>0.40675</td></tr><tr><td>complexity_stats/train_mean</td><td>0.38545</td></tr><tr><td>complexity_stats/train_median</td><td>0.37212</td></tr><tr><td>data_columns</td><td>11</td></tr><tr><td>dev_data_rows</td><td>441</td></tr><tr><td>test_data_rows</td><td>719</td></tr><tr><td>train_data_rows</td><td>7460</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">data-preparation</strong> at: <a href='https://wandb.ai/rokii-ku-leuven/MAIthesis/runs/a4p2tpma' target=\"_blank\">https://wandb.ai/rokii-ku-leuven/MAIthesis/runs/a4p2tpma</a><br> View project at: <a href='https://wandb.ai/rokii-ku-leuven/MAIthesis' target=\"_blank\">https://wandb.ai/rokii-ku-leuven/MAIthesis</a><br>Synced 5 W&B file(s), 12 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./baselines/wandb/wandb/run-20250327_195535-a4p2tpma/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtype-eval-pAepV5Z2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
