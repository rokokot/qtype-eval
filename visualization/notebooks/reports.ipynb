{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1089d17",
   "metadata": {},
   "source": [
    "## Generating Reports and Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe321c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed1bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/bauwenst/fiject.git\n",
      "  Cloning https://github.com/bauwenst/fiject.git to /tmp/pip-req-build-w4_8_ali\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/bauwenst/fiject.git /tmp/pip-req-build-w4_8_ali\n",
      "  Resolved https://github.com/bauwenst/fiject.git to commit 2588362e72c5750176778b9ca80027f07617cbf1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.5.3 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from fiject==2025.1.1) (3.9.4)\n",
      "Requirement already satisfied: seaborn>=0.12.1 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from fiject==2025.1.1) (0.13.2)\n",
      "Collecting natsort (from fiject==2025.1.1)\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from matplotlib>=3.5.3->fiject==2025.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from seaborn>=0.12.1->fiject==2025.1.1) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.12.1->fiject==2025.1.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from pandas>=1.2->seaborn>=0.12.1->fiject==2025.1.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.3->fiject==2025.1.1) (1.17.0)\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: fiject\n",
      "  Building wheel for fiject (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fiject: filename=fiject-2025.1.1-py3-none-any.whl size=57197 sha256=b009fb8691ebfedfc4a49a30cccdf269b7d15fde613f71b9e25cdc5a01895216\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hgapurat/wheels/29/0b/72/111b9c34627e7b9ccd2f01373cdf2aa4c4f66e071f3b0f6225\n",
      "Successfully built fiject\n",
      "Installing collected packages: natsort, fiject\n",
      "Successfully installed fiject-2025.1.1 natsort-8.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/bauwenst/fiject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from fiject import LineGraph, CacheMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/home/robin/Research/qtype-eval/visualization/notebooks/FINAL_MERGED_RESULTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preloaded data for visual 'question_type_probe_accuracy' sparing you 0m0s of computation time.\n",
      "Writing figure question_type_probe_accuracy ...\n"
     ]
    }
   ],
   "source": [
    "def create_simple_visualization():\n",
    "    g_question = LineGraph(\"question_type_probe_accuracy\", caching=CacheMode.IF_MISSING)\n",
    "    if g_question.needs_computation:\n",
    "        \n",
    "        layers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        \n",
    "        ar_values =[0.8571428571428571,0.8571428571428571,0.7662337662337663,0.7792207792207793,0.7272727272727273,0.7662337662337663,0.7662337662337663,0.7662337662337663,0.7792207792207793,0.7142857142857143,0.6753246753246753,0.6233766233766234]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Arabic\", layer, ar_values[i])\n",
    "\n",
    "        en_values = [0.9363636363636364,0.8545454545454545,0.9090909090909091,0.9545454545454546,0.9727272727272728,0.9636363636363636,0.9363636363636364,0.9454545454545454,0.9545454545454546,0.9545454545454546,0.9272727272727272,0.9]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"English\", layer, en_values[i])\n",
    "        \n",
    "        fi_values = [0.8909090909090909,0.9181818181818182,0.8363636363636363,0.8818181818181818,0.9454545454545454,0.8909090909090909,0.9363636363636364,0.9,0.9090909090909091,0.9,0.8727272727272727,0.8636363636363636]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Finnish\", layer, fi_values[i])\n",
    "        \n",
    "        id_values = [0.6909090909090909,0.7,0.7090909090909091,0.6727272727272727,0.7909090909090909,0.8090909090909091,0.8090909090909091,0.8,0.7545454545454545,0.7818181818181819,0.7090909090909091,0.6272727272727273]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Indonesian\", layer, id_values[i])\n",
    "\n",
    "        ja_values = [0.75,0.6847826086956522,0.6739130434782609,0.7282608695652174,0.6304347826086957,0.7065217391304348,0.717391304347826,0.6521739130434783,0.7934782608695652,0.8260869565217391,0.7934782608695652,0.7717391304347826]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Japanese\", layer, ja_values[i])\n",
    "\n",
    "        ko_values = [0.6363636363636364,0.6818181818181818,0.6636363636363637,0.6545454545454545,0.7181818181818181,0.7181818181818181,0.7454545454545455,0.7545454545454545,0.7636363636363637,0.7636363636363637,0.7363636363636363,0.7]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Korean\", layer, ko_values[i])\n",
    "\n",
    "        ru_values = [0.9090909090909091,0.9,0.9545454545454546,0.9636363636363636,0.9727272727272728,0.9454545454545454,0.9545454545454546,0.9636363636363636,0.9545454545454546,0.9636363636363636,0.9727272727272728,0.9636363636363636]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Russian\", layer, ru_values[i])\n",
    "    \n",
    "    g_question.commit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_simple_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preloaded data for visual 'complexity_probe_mse' sparing you 0m0s of computation time.\n",
      "Writing figure complexity_probe_mse ...\n"
     ]
    }
   ],
   "source": [
    "def complexity_probe_line():\n",
    "    g_question = LineGraph(\"complexity_probe_mse\", caching=CacheMode.IF_MISSING)\n",
    "    if g_question.needs_computation:\n",
    "        \n",
    "        layers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "        \n",
    "        ar_values =[0.03399290516972542,0.03338051959872246,0.030697615817189217,0.030198050662875175,0.034776024520397186,0.03472450003027916,0.03763638064265251,0.031766075640916824,0.03561026602983475,0.0420287624001503,0.040672700852155685,0.05731217935681343]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Arabic\", layer, ar_values[i])\n",
    "\n",
    "        en_values = [0.0165211483836174,0.018173784017562866,0.022300826385617256,0.020360849797725677,0.01990492269396782,0.01724899373948574,0.032355424016714096,0.019637851044535637,0.0230465866625309,0.03620355203747749,0.03820689022541046,0.043550752103328705]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"English\", layer, en_values[i])\n",
    "        \n",
    "        fi_values = [0.02535470761358738,0.03404341638088226,0.03400088846683502,0.030467664822936058,0.030116546899080276,0.02697366289794445,0.030296334996819496,0.028861360624432564,0.03142784163355827,0.04039514437317848,0.031944818794727325,0.04165467247366905]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Finnish\", layer, fi_values[i])\n",
    "        \n",
    "        id_values = [0.024353371933102608,0.025125887244939804,0.023815622553229332,0.02371056005358696,0.027490098029375076,0.028783269226551056,0.029681118205189705,0.027115244418382645,0.028400041162967682,0.03267641365528107,0.03282563388347626,0.0426039919257164]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Indonesian\", layer, id_values[i])\n",
    "\n",
    "        ja_values = [0.018212871626019478,0.027955954894423485,0.019171690568327904,0.01669086143374443,0.016645682975649834,0.018545590341091156,0.021489085629582405,0.02461579069495201,0.038797203451395035,0.05011143535375595,0.030812587589025497,0.05790027976036072]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Japanese\", layer, ja_values[i])\n",
    "\n",
    "        ko_values = [0.02085908316075802,0.04128776490688324,0.019171690568327904,0.03518727794289589,0.03677823767066002,0.03256898745894432,0.03059077076613903,0.029142329469323158,0.029354188591241837,0.02352721430361271,0.02644418366253376,0.03508226200938225]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Korean\", layer, ko_values[i])\n",
    "\n",
    "        ru_values = [0.045390259474515915,0.056273140013217926,0.060910020023584366,0.05462970212101936,0.05645507201552391,0.03991546109318733,0.054057203233242035,0.05920558050274849,0.05551239475607872,0.0673564225435257,0.06186512112617493,0.04694022238254547]\n",
    "        for i, layer in enumerate(layers):\n",
    "            g_question.add(\"Russian\", layer, ru_values[i])\n",
    "    \n",
    "    g_question.commit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  complexity_probe_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df46ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv('/home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e6a6e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of ordering:\n",
      "      layer_index  control_index     value\n",
      "1               1            NaN  0.857143\n",
      "757             1            1.0  0.714286\n",
      "762             1            2.0  0.714286\n",
      "767             1            3.0  0.714286\n",
      "64              2            NaN  0.857143\n",
      "946             2            1.0  0.714286\n",
      "951             2            2.0  0.714286\n",
      "956             2            3.0  0.714286\n",
      "127             3            NaN  0.766234\n",
      "1135            3            1.0  0.714286\n",
      "1140            3            2.0  0.714286\n",
      "1145            3            3.0  0.714286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is loaded as base_df\n",
    "# Filter for accuracy and mse metrics\n",
    "filtered_df = base_df[base_df['metric'].isin(['accuracy', 'mse'])]\n",
    "\n",
    "# Create a proper ordering for control_index\n",
    "# This approach ensures \"None\" values come first, followed by numeric values\n",
    "def control_index_to_sort_value(val):\n",
    "    if val == \"None\" or pd.isna(val):\n",
    "        return -1  # Make None come first\n",
    "    try:\n",
    "        return float(val)  # Convert numeric strings to actual numbers\n",
    "    except:\n",
    "        return val  # Keep other values as-is\n",
    "\n",
    "# Apply the conversion function\n",
    "filtered_df = filtered_df.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "filtered_df['control_index_sort'] = filtered_df['control_index'].apply(control_index_to_sort_value)\n",
    "\n",
    "# Sort the DataFrame - crucial change: layer_index comes before control_index in the sorting\n",
    "sorted_df = filtered_df.sort_values(by=['language', 'task', 'layer_index', 'control_index_sort'])\n",
    "\n",
    "# Remove the temporary sorting column\n",
    "final_df = sorted_df.drop(columns=['control_index_sort'])\n",
    "\n",
    "# Save to CSV with desired format\n",
    "final_df.to_csv('/home/robin/Research/qtype-eval/visualization/notebooks/organized_probe_data.csv', index=False)\n",
    "\n",
    "# Quick verification of the ordering\n",
    "ar_question_type_verify = final_df[\n",
    "    (final_df['language'] == 'ar') & \n",
    "    (final_df['task'] == 'question_type') & \n",
    "    (final_df['metric'] == 'accuracy')\n",
    "].head(12)  # First 12 rows should cover layers 1-3 with all control indices\n",
    "\n",
    "print(\"Verification of ordering:\")\n",
    "print(ar_question_type_verify[['layer_index', 'control_index', 'value']])\n",
    "\n",
    "# The output should show layer 1 with all control indices, then layer 2 with all control indices, etc.\n",
    "# For each layer, \"None\" should appear first, followed by control indices 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78305d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11088 rows from /home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv\n",
      "After filtering for mse and accuracy: 2688 rows\n",
      "Processing 336 rows for task 'question_type'\n",
      "Processing 336 rows for task 'complexity'\n",
      "Processing 2016 rows for task 'single_submetric'\n",
      "\n",
      "Task distribution in result DataFrame:\n",
      "task\n",
      "single_submetric    1008\n",
      "question_type        168\n",
      "complexity           168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processed 1344 rows and saved to /home/robin/Research/qtype-eval/visualization/notebooks/processed_probe.csv\n",
      "\n",
      "Task 'complexity' (168 rows) sample:\n",
      "    language        task  layer_index metric control_index     value\n",
      "168       ar  complexity            1    mse           NaN  0.033993\n",
      "\n",
      "Task 'question_type' (168 rows) sample:\n",
      "  language           task  layer_index    metric control_index     value\n",
      "0       ar  question_type            1  accuracy           NaN  0.857143\n",
      "\n",
      "Task 'single_submetric' (1008 rows) sample:\n",
      "    language              task  layer_index metric control_index     value\n",
      "336       ar  single_submetric            1    mse           NaN  0.054192\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "input_file = '/home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df)} rows from {input_file}\")\n",
    "\n",
    "# Filter for only MSE and accuracy metrics\n",
    "df = df[df['metric'].isin(['mse', 'accuracy'])]\n",
    "print(f\"After filtering for mse and accuracy: {len(df)} rows\")\n",
    "\n",
    "# Convert control_index to numeric, handling errors\n",
    "df['control_index'] = pd.to_numeric(df['control_index'], errors='coerce')\n",
    "\n",
    "# Initialize result list\n",
    "result_rows = []\n",
    "\n",
    "# Process all rows, avoiding groupby issues\n",
    "for task in df['task'].unique():\n",
    "    task_df = df[df['task'] == task]\n",
    "    print(f\"Processing {len(task_df)} rows for task '{task}'\")\n",
    "    \n",
    "    # Determine which columns to use for grouping based on task\n",
    "    if task in ['question_type', 'complexity']:\n",
    "        # These tasks don't use submetric, so exclude it\n",
    "        keys = ['experiment_type', 'language', 'layer_index', 'metric']\n",
    "    else:\n",
    "        # For single_submetric, include submetric\n",
    "        keys = ['experiment_type', 'language', 'submetric', 'layer_index', 'metric']\n",
    "    \n",
    "    # Identify unique combinations of key columns\n",
    "    unique_combos = task_df[keys].drop_duplicates()\n",
    "    \n",
    "    # Process each unique combination\n",
    "    for _, combo in unique_combos.iterrows():\n",
    "        # Build filter condition for this combination\n",
    "        condition = True\n",
    "        for key in keys:\n",
    "            condition = condition & (task_df[key] == combo[key])\n",
    "        \n",
    "        # Get rows for this combination\n",
    "        combo_rows = task_df[condition]\n",
    "        \n",
    "        # Split into control and non-control rows\n",
    "        non_control_rows = combo_rows[combo_rows['control_index'].isna()]\n",
    "        control_rows = combo_rows[combo_rows['control_index'].notna()]\n",
    "        \n",
    "        # Add non-control rows to result\n",
    "        result_rows.extend(non_control_rows.to_dict('records'))\n",
    "        \n",
    "        # Process control rows if any\n",
    "        if not control_rows.empty:\n",
    "            mean_value = control_rows['value'].mean()\n",
    "            \n",
    "            # Create mean control row\n",
    "            mean_row = control_rows.iloc[0].copy()\n",
    "            mean_row['control_index'] = 'mean_control'\n",
    "            mean_row['value'] = mean_value\n",
    "            \n",
    "            result_rows.append(mean_row)\n",
    "\n",
    "# Create result DataFrame\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "\n",
    "# Check task distribution in result\n",
    "print(\"\\nTask distribution in result DataFrame:\")\n",
    "result_task_counts = result_df['task'].value_counts()\n",
    "print(result_task_counts)\n",
    "\n",
    "# Sort the DataFrame\n",
    "result_df = result_df.sort_values(['language', 'task', 'layer_index'])\n",
    "\n",
    "# Save to CSV\n",
    "output_file = '/home/robin/Research/qtype-eval/visualization/notebooks/processed_probe.csv'\n",
    "result_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nProcessed {len(result_df)} rows and saved to {output_file}\")\n",
    "\n",
    "# Show sample rows from each task in the result\n",
    "for task in result_df['task'].unique():\n",
    "    task_rows = result_df[result_df['task'] == task]\n",
    "    print(f\"\\nTask '{task}' ({len(task_rows)} rows) sample:\")\n",
    "    print(task_rows.head(1)[['language', 'task', 'layer_index', 'metric', 'control_index', 'value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0c6b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 441 rows from /home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv\n",
      "After removing lm_probe rows: 217 rows remaining\n",
      "Processed data saved to /home/robin/Research/qtype-eval/visualization/notebooks/processed_base.csv\n",
      "\n",
      "Task distribution in result:\n",
      "task\n",
      "single_submetric    168\n",
      "complexity           28\n",
      "question_type        21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Language distribution in result:\n",
      "language\n",
      "ar    31\n",
      "en    31\n",
      "fi    31\n",
      "id    31\n",
      "ja    31\n",
      "ko    31\n",
      "ru    31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows from the processed data:\n",
      "    language           task  layer    metric      real  control_mean\n",
      "63        ar     complexity    NaN       mse  0.058921      0.058921\n",
      "133       ar     complexity    NaN       mse  0.042423      0.058553\n",
      "168       ar     complexity    NaN       mse  0.026290      0.065327\n",
      "196       ar     complexity    NaN       mse  0.038349      0.064952\n",
      "35        ar  question_type    NaN  accuracy  0.714286      0.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2204838/4242315108.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['layer'] = pd.to_numeric(df_filtered['layer'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "input_file = '/home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv'  # Change this to your input file path\n",
    "output_file = '/home/robin/Research/qtype-eval/visualization/notebooks/processed_base.csv'  # Change this to your output file path\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df)} rows from {input_file}\")\n",
    "\n",
    "# Step 1: Remove all rows with model_type 'lm_probe'\n",
    "df_filtered = df[df['model_type'] != 'lm_probe']\n",
    "print(f\"After removing lm_probe rows: {len(df_filtered)} rows remaining\")\n",
    "\n",
    "# Step 2: Convert layer to numeric if it's not already\n",
    "if df_filtered['layer'].dtype == 'object':\n",
    "    df_filtered['layer'] = pd.to_numeric(df_filtered['layer'], errors='coerce')\n",
    "\n",
    "# Step 3: Sort the DataFrame by language, task, submetric, and layer\n",
    "# This is the same logic used in the previous script\n",
    "df_sorted = df_filtered.sort_values(['language', 'task', 'submetric', 'layer'])\n",
    "\n",
    "# Step 4: Save the processed DataFrame\n",
    "df_sorted.to_csv(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Step 5: Print some verification info\n",
    "print(\"\\nTask distribution in result:\")\n",
    "print(df_sorted['task'].value_counts())\n",
    "\n",
    "print(\"\\nLanguage distribution in result:\")\n",
    "print(df_sorted['language'].value_counts())\n",
    "\n",
    "print(\"\\nSample rows from the processed data:\")\n",
    "print(df_sorted.head(5)[['language', 'task', 'layer', 'metric', 'real', 'control_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e809a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 441 rows from /home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv\n",
      "After filtering: 168 rows\n",
      "Processing 126 rows for task 'single_submetric'\n",
      "Processing 21 rows for task 'question_type'\n",
      "Processing 21 rows for task 'complexity'\n",
      "Processed data saved to /home/robin/Research/qtype-eval/visualization/notebooks/processed_base.csv\n",
      "\n",
      "Task distribution in result:\n",
      "task\n",
      "single_submetric    252\n",
      "complexity           42\n",
      "question_type        42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows from each task:\n",
      "\n",
      "Task: complexity\n",
      "       model_type language        task   submetric control_index  layer_index  \\\n",
      "0  DummyRegressor       ar  complexity  complexity          None          NaN   \n",
      "1  DummyRegressor       ar  complexity  complexity  mean_control          NaN   \n",
      "\n",
      "  metric     value  \n",
      "0    mse  0.058921  \n",
      "1    mse  0.058921  \n",
      "\n",
      "Task: question_type\n",
      "        model_type language           task submetric control_index  \\\n",
      "0  DummyClassifier       ar  question_type   overall          None   \n",
      "1  DummyClassifier       ar  question_type   overall  mean_control   \n",
      "\n",
      "   layer_index    metric     value  \n",
      "0          NaN  accuracy  0.714286  \n",
      "1          NaN  accuracy  0.714286  \n",
      "\n",
      "Task: single_submetric\n",
      "      model_type language              task      submetric control_index  \\\n",
      "14  XGBRegressor       ar  single_submetric  avg_links_len          None   \n",
      "15  XGBRegressor       ar  single_submetric  avg_links_len  mean_control   \n",
      "\n",
      "    layer_index metric     value  \n",
      "14          NaN    mse  0.045155  \n",
      "15          NaN    mse  0.057564  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = '/home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv'  # Change this to your input file path\n",
    "output_file = '/home/robin/Research/qtype-eval/visualization/notebooks/processed_base.csv'  # Change this to your output file path\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df)} rows from {input_file}\")\n",
    "\n",
    "# Step 1: Apply filters\n",
    "# Remove rows with model_type 'lm_probe'\n",
    "df = df[df['model_type'] != 'lm_probe']\n",
    "# Keep only the test split\n",
    "df = df[df['split'] == 'test']\n",
    "print(f\"After filtering: {len(df)} rows\")\n",
    "\n",
    "# Step 2: Create a new DataFrame with the desired column structure\n",
    "result_dfs = []\n",
    "\n",
    "# Process each task separately to handle submetric appropriately\n",
    "for task in df['task'].unique():\n",
    "    task_df = df[df['task'] == task]\n",
    "    print(f\"Processing {len(task_df)} rows for task '{task}'\")\n",
    "    \n",
    "    task_rows = []\n",
    "    for _, row in task_df.iterrows():\n",
    "        # Create a row for the \"real\" value\n",
    "        real_row = {\n",
    "            'model_type': row['model_type'],\n",
    "            'language': row['language'],\n",
    "            'task': row['task'],\n",
    "            'submetric': row['submetric'] if pd.notna(row['submetric']) else None,\n",
    "            'control_index': None,\n",
    "            'layer_index': row['layer'],\n",
    "            'metric': row['metric'],\n",
    "            'value': row['real']\n",
    "        }\n",
    "        task_rows.append(real_row)\n",
    "        \n",
    "        # Create a row for the \"mean_control\" value\n",
    "        control_row = {\n",
    "            'model_type': row['model_type'],\n",
    "            'language': row['language'],\n",
    "            'task': row['task'],\n",
    "            'submetric': row['submetric'] if pd.notna(row['submetric']) else None,\n",
    "            'control_index': 'mean_control',\n",
    "            'layer_index': row['layer'],\n",
    "            'metric': row['metric'],\n",
    "            'value': row['control_mean']\n",
    "        }\n",
    "        task_rows.append(control_row)\n",
    "    \n",
    "    # Create DataFrame for this task\n",
    "    task_result_df = pd.DataFrame(task_rows)\n",
    "    \n",
    "    # Sort based on task-specific criteria\n",
    "    if task in ['question_type', 'complexity']:\n",
    "        # These tasks don't use submetric\n",
    "        task_result_df = task_result_df.sort_values(['language', 'layer_index'])\n",
    "    else:\n",
    "        # For single_submetric, include submetric in sorting\n",
    "        task_result_df = task_result_df.sort_values(['language', 'submetric', 'layer_index'])\n",
    "    \n",
    "    result_dfs.append(task_result_df)\n",
    "\n",
    "# Combine all task DataFrames\n",
    "result_df = pd.concat(result_dfs)\n",
    "\n",
    "# Ensure layer_index is numeric\n",
    "result_df['layer_index'] = pd.to_numeric(result_df['layer_index'], errors='coerce')\n",
    "\n",
    "# Final sort to ensure tasks are grouped together\n",
    "result_df = result_df.sort_values(['language', 'task', 'submetric', 'layer_index'])\n",
    "\n",
    "# Ensure columns are in the specified order\n",
    "result_df = result_df[[\n",
    "    'model_type', 'language', 'task', 'submetric', \n",
    "    'control_index', 'layer_index', 'metric', 'value'\n",
    "]]\n",
    "\n",
    "# Save the processed DataFrame\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Display task distribution\n",
    "print(\"\\nTask distribution in result:\")\n",
    "print(result_df['task'].value_counts())\n",
    "\n",
    "# Display sample rows from each task\n",
    "print(\"\\nSample rows from each task:\")\n",
    "for task in result_df['task'].unique():\n",
    "    task_sample = result_df[result_df['task'] == task].head(2)\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(task_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
