{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604a83f1",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8911bf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfadcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/Research/qtype-eval/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c4d90",
   "metadata": {},
   "source": [
    "### dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c4c099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  Questions  Polar (%)  Content (%)  Avg. Complexity\n",
      "    Arabic        995       49.9         50.1             1.50\n",
      "   English       1192       50.0         50.0             1.60\n",
      "   Finnish       1195       50.0         50.0             1.37\n",
      "Indonesian        954       47.9         52.1             1.86\n",
      "  Japanese       1191       50.0         50.0             1.60\n",
      "    Korean        739       46.1         53.9             1.97\n",
      "   Russian       1194       50.0         50.0             1.76\n",
      "    Arabic         44       45.5         54.5             1.73\n",
      "   English         72       50.0         50.0             1.74\n",
      "   Finnish         63       47.6         52.4             1.64\n",
      "Indonesian         72       50.0         50.0             2.01\n",
      "  Japanese         46       52.2         47.8             1.71\n",
      "    Korean         72       50.0         50.0             2.05\n",
      "   Russian         72       50.0         50.0             1.83\n",
      "    Arabic         77       28.6         71.4             2.04\n",
      "   English        110       50.0         50.0             1.61\n",
      "   Finnish        110       50.0         50.0             1.58\n",
      "Indonesian        110       50.0         50.0             1.91\n",
      "  Japanese         92       59.8         40.2             2.38\n",
      "    Korean        110       50.0         50.0             2.00\n",
      "   Russian        110       50.0         50.0             1.74\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rokokot/question-type-and-complexity\", name=\"base\", split=\"train\")\n",
    "lang_map = {'ar': 'Arabic', 'en': 'English', 'fi': 'Finnish','id': 'Indonesian', 'ja': 'Japanese','ko': 'Korean', 'ru': 'Russian'}\n",
    "\n",
    "splits = ['train', 'validation', 'test']\n",
    "all_results = []\n",
    "\n",
    "for split in splits:\n",
    "  dataset = load_dataset('rokokot/question-type-and-complexity', name='base', split=split)\n",
    "  \n",
    "  for lang in list(lang_map.keys()):\n",
    "    lang_data = dataset.filter(lambda x: x['language'] == lang)\n",
    "\n",
    "    n_questions = len(lang_data)\n",
    "\n",
    "    question_types = lang_data['question_type']\n",
    "\n",
    "    polar_count = sum(1 for qt in question_types if qt == 1)\n",
    "    content_count = sum(1 for qt in question_types if qt == 0)\n",
    "\n",
    "    polar_pct = round((polar_count / n_questions) * 100, 1)\n",
    "    content_pct = round((content_count / n_questions) * 100, 1)\n",
    "\n",
    "    avg_complexity = round(np.mean(lang_data['complexity_score']), 2)\n",
    "\n",
    "    all_results.append({'Language': lang_map[lang],'Questions': n_questions,'Polar (%)': polar_pct,'Content (%)': content_pct,'Avg. Complexity': avg_complexity})\n",
    "stats_df = pd.DataFrame(all_results)\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  Dataset %  Polar %  Content %  Avg. Complexity\n",
      "    Arabic       12.9     48.3       51.7             1.55\n",
      "   English       15.9     50.0       50.0             1.61\n",
      "   Finnish       15.9     49.9       50.1             1.40\n",
      "Indonesian       13.2     48.2       51.8             1.88\n",
      "  Japanese       15.4     50.8       49.2             1.66\n",
      "    Korean       10.7     46.9       53.1             1.98\n",
      "   Russian       16.0     50.0       50.0             1.76\n"
     ]
    }
   ],
   "source": [
    "def analyze_averages():\n",
    "    splits = [\"train\", \"validation\", \"test\"]\n",
    "    lang_map = {\n",
    "        'ar': 'Arabic', 'en': 'English', 'fi': 'Finnish',\n",
    "        'id': 'Indonesian', 'ja': 'Japanese',\n",
    "        'ko': 'Korean', 'ru': 'Russian'\n",
    "    }\n",
    "    \n",
    "    combined_stats = {lang: {'Questions': 0, 'Polar': 0, 'Content': 0, 'Complexity': []} \n",
    "                     for lang in lang_map.values()}\n",
    "    \n",
    "    total_questions = 0\n",
    "\n",
    "    for split in splits:\n",
    "        try:\n",
    "            dataset = load_dataset(\"rokokot/question-type-and-complexity\", name=\"base\", split=split)\n",
    "            \n",
    "            for lang_code, lang_name in lang_map.items():\n",
    "                lang_data = dataset.filter(lambda x: x['language'] == lang_code)\n",
    "                \n",
    "                if len(lang_data) == 0:\n",
    "                    print(f\"No data for {lang_name} in {split} split\")\n",
    "                    continue\n",
    "                \n",
    "                combined_stats[lang_name]['Questions'] += len(lang_data)\n",
    "                total_questions += len(lang_data)\n",
    "                \n",
    "                question_types = lang_data['question_type']\n",
    "                polar_count = sum(1 for qt in question_types if qt == 1)\n",
    "                combined_stats[lang_name]['Polar'] += polar_count\n",
    "                combined_stats[lang_name]['Content'] += (len(lang_data) - polar_count)\n",
    "                \n",
    "                combined_stats[lang_name]['Complexity'].extend(lang_data['complexity_score'])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {split} split: {e}\")\n",
    "    \n",
    "    results = []\n",
    "    for lang_name, stats in combined_stats.items():\n",
    "        if stats['Questions'] > 0:\n",
    "            polar_pct = round((stats['Polar'] / stats['Questions']) * 100, 1)\n",
    "            content_pct = round((stats['Content'] / stats['Questions']) * 100, 1)\n",
    "            avg_complexity = round(np.mean(stats['Complexity']), 2) if stats['Complexity'] else 0\n",
    "            dataset_pct = round((stats['Questions'] / total_questions) * 100, 1)\n",
    "\n",
    "            results.append({'Language': lang_name,'Dataset %': dataset_pct,'Polar %': polar_pct,'Content %': content_pct,'Avg. Complexity': avg_complexity\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(results)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "stats_df = analyze_averages()\n",
    "print(stats_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a47ab784",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv\")\n",
    "probe_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv\")\n",
    "finetune_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/finetune_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d7468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'language', 'real', 'control1', 'control2', 'control3', 'control_mean', \n",
    "    'selectivity', 'normalized_selectivity', 'task', 'model_type', 'metric', 'submetric'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_filtered = original_df[original_df['model_type'] != 'lm_probe'].copy()\n",
    "original_filtered = original_filtered[original_filtered['split'] == 'test'].copy()\n",
    "finetune_real = finetune_df[finetune_df['control_index'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e2ba2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>language</th>\n",
       "      <th>task</th>\n",
       "      <th>submetric</th>\n",
       "      <th>control_index</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>finetune</td>\n",
       "      <td>fi</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.011284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>finetune</td>\n",
       "      <td>fi</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.011379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>finetune</td>\n",
       "      <td>fi</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.106674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>finetune</td>\n",
       "      <td>fi</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.176997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>finetune</td>\n",
       "      <td>id</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>avg_links_len</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.031529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>finetune</td>\n",
       "      <td>ru</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>lexical_density</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r2</td>\n",
       "      <td>-0.067467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>finetune</td>\n",
       "      <td>ru</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.005862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>finetune</td>\n",
       "      <td>ru</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mse</td>\n",
       "      <td>0.005938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>finetune</td>\n",
       "      <td>ru</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.077056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>finetune</td>\n",
       "      <td>ru</td>\n",
       "      <td>single_submetric</td>\n",
       "      <td>n_tokens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.478031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_type language              task        submetric  \\\n",
       "320        finetune       fi  single_submetric         n_tokens   \n",
       "321        finetune       fi  single_submetric         n_tokens   \n",
       "322        finetune       fi  single_submetric         n_tokens   \n",
       "323        finetune       fi  single_submetric         n_tokens   \n",
       "324        finetune       id  single_submetric    avg_links_len   \n",
       "..              ...      ...               ...              ...   \n",
       "415        finetune       ru  single_submetric  lexical_density   \n",
       "416        finetune       ru  single_submetric         n_tokens   \n",
       "417        finetune       ru  single_submetric         n_tokens   \n",
       "418        finetune       ru  single_submetric         n_tokens   \n",
       "419        finetune       ru  single_submetric         n_tokens   \n",
       "\n",
       "     control_index metric     value  \n",
       "320            NaN   loss  0.011284  \n",
       "321            NaN    mse  0.011379  \n",
       "322            NaN   rmse  0.106674  \n",
       "323            NaN     r2  0.176997  \n",
       "324            NaN   loss  0.031529  \n",
       "..             ...    ...       ...  \n",
       "415            NaN     r2 -0.067467  \n",
       "416            NaN   loss  0.005862  \n",
       "417            NaN    mse  0.005938  \n",
       "418            NaN   rmse  0.077056  \n",
       "419            NaN     r2  0.478031  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_real.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bb1ed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 231 rows from finetune data where control_index is None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selected {len(finetune_real)} rows from finetune data where control_index is None\")\n",
    "\n",
    "finetune_processed = pd.DataFrame()\n",
    "finetune_processed['language'] = finetune_real['language']\n",
    "finetune_processed['real'] = finetune_real['value']\n",
    "finetune_processed['task'] = finetune_real['task']\n",
    "finetune_processed['metric'] = finetune_real['metric']\n",
    "finetune_processed['submetric'] = finetune_real['submetric']\n",
    "finetune_processed['model_type'] = 'finetune'\n",
    "finetune_processed['control1'] = np.nan\n",
    "finetune_processed['control2'] = np.nan\n",
    "finetune_processed['control3'] = np.nan\n",
    "finetune_processed['control_mean'] = np.nan\n",
    "finetune_processed['selectivity'] = np.nan\n",
    "finetune_processed['normalized_selectivity'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f1b0243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>real</th>\n",
       "      <th>task</th>\n",
       "      <th>metric</th>\n",
       "      <th>submetric</th>\n",
       "      <th>model_type</th>\n",
       "      <th>control1</th>\n",
       "      <th>control2</th>\n",
       "      <th>control3</th>\n",
       "      <th>control_mean</th>\n",
       "      <th>selectivity</th>\n",
       "      <th>normalized_selectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.042425</td>\n",
       "      <td>complexity</td>\n",
       "      <td>loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finetune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>complexity</td>\n",
       "      <td>mse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finetune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.205289</td>\n",
       "      <td>complexity</td>\n",
       "      <td>rmse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finetune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.273465</td>\n",
       "      <td>complexity</td>\n",
       "      <td>r2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finetune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.742846</td>\n",
       "      <td>question_type</td>\n",
       "      <td>loss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finetune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language      real           task metric submetric model_type  control1  \\\n",
       "0       ar  0.042425     complexity   loss       NaN   finetune       NaN   \n",
       "1       ar  0.042143     complexity    mse       NaN   finetune       NaN   \n",
       "2       ar  0.205289     complexity   rmse       NaN   finetune       NaN   \n",
       "3       ar  0.273465     complexity     r2       NaN   finetune       NaN   \n",
       "4       ar  0.742846  question_type   loss       NaN   finetune       NaN   \n",
       "\n",
       "   control2  control3  control_mean  selectivity  normalized_selectivity  \n",
       "0       NaN       NaN           NaN          NaN                     NaN  \n",
       "1       NaN       NaN           NaN          NaN                     NaN  \n",
       "2       NaN       NaN           NaN          NaN                     NaN  \n",
       "3       NaN       NaN           NaN          NaN                     NaN  \n",
       "4       NaN       NaN           NaN          NaN                     NaN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabe519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d3c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9940490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d1882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f04a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa4a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5461e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2411"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05909118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e132f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38276288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19394261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering original data...\n",
      "Processing finetune data...\n",
      "Selected 231 rows from finetune data where control_index is None\n",
      "Processing probe data with improved control extraction...\n",
      "Found 2772 unique parameter combinations in probe data\n",
      "Created 2016 processed probe rows\n",
      "\n",
      "Task breakdown in processed probe data:\n",
      "task\n",
      "single_submetric    2016\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "Successfully created /home/robin/Research/qtype-eval/visualization/notebooks/merged_ml_results.csv\n",
      "Total rows: 2415\n",
      "  - Original data: 168\n",
      "  - Probe data: 2016\n",
      "  - Finetune data: 231\n",
      "\n",
      "Layer value counts:\n",
      "layer\n",
      "None       231\n",
      "None       168\n",
      "layer1     168\n",
      "layer2     168\n",
      "layer3     168\n",
      "layer4     168\n",
      "layer5     168\n",
      "layer6     168\n",
      "layer7     168\n",
      "layer8     168\n",
      "layer9     168\n",
      "layer10    168\n",
      "layer11    168\n",
      "layer12    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking layer values by model_type:\n",
      "\n",
      "DummyRegressor:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "XGBRegressor:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DummyClassifier:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ridge:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LogisticRegression:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "XGBClassifier:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "probe:\n",
      "layer\n",
      "layer1     168\n",
      "layer2     168\n",
      "layer3     168\n",
      "layer4     168\n",
      "layer5     168\n",
      "layer6     168\n",
      "layer7     168\n",
      "layer8     168\n",
      "layer9     168\n",
      "layer10    168\n",
      "layer11    168\n",
      "layer12    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "finetune:\n",
      "layer\n",
      "None    231\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv\")\n",
    "probe_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv\")\n",
    "finetune_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/finetune_base.csv\")\n",
    "\n",
    "target_columns = [\n",
    "    'language', 'real', 'control1', 'control2', 'control3', 'control_mean', \n",
    "    'selectivity', 'normalized_selectivity', 'task', 'model_type', 'metric', 'submetric', 'split', 'layer'\n",
    "]\n",
    "\n",
    "print(\"Filtering original data...\")\n",
    "original_filtered = original_df[original_df['model_type'] != 'lm_probe'].copy()\n",
    "original_filtered = original_filtered[original_filtered['split'] == 'test'].copy()\n",
    "\n",
    "original_filtered['layer'] = None\n",
    "\n",
    "print(\"Processing finetune data...\")\n",
    "finetune_real = finetune_df[finetune_df['control_index'].isna()].copy()\n",
    "print(f\"Selected {len(finetune_real)} rows from finetune data where control_index is None\")\n",
    "\n",
    "finetune_processed = pd.DataFrame()\n",
    "finetune_processed['language'] = finetune_real['language']\n",
    "finetune_processed['real'] = finetune_real['value']\n",
    "finetune_processed['task'] = finetune_real['task']\n",
    "finetune_processed['metric'] = finetune_real['metric']\n",
    "finetune_processed['submetric'] = finetune_real['submetric']\n",
    "finetune_processed['model_type'] = 'finetune'\n",
    "finetune_processed['split'] = 'test'\n",
    "finetune_processed['layer'] = 'None'  # Explicitly set to None for finetune experiments\n",
    "finetune_processed['control1'] = 'None'\n",
    "finetune_processed['control2'] = 'None'\n",
    "finetune_processed['control3'] = 'None'\n",
    "finetune_processed['control_mean'] = 'None'\n",
    "finetune_processed['selectivity'] = 'None'\n",
    "finetune_processed['normalized_selectivity'] = 'None'\n",
    "\n",
    "# Step 3: Process probe data - IMPROVED APPROACH with proper layer handling\n",
    "print(\"Processing probe data with improved control extraction...\")\n",
    "\n",
    "# Get all unique combinations of parameters \n",
    "unique_combinations = probe_df.drop_duplicates(\n",
    "    subset=['language', 'task', 'submetric', 'metric', 'layer_index']\n",
    ").copy()\n",
    "\n",
    "print(f\"Found {len(unique_combinations)} unique parameter combinations in probe data\")\n",
    "\n",
    "# Process each unique combination\n",
    "results = []\n",
    "for _, row in unique_combinations.iterrows():\n",
    "    language = row['language']\n",
    "    task = row['task']\n",
    "    submetric = row['submetric']\n",
    "    metric = row['metric']\n",
    "    layer_idx = row['layer_index']\n",
    "    \n",
    "    # Get all rows matching this combination\n",
    "    matching_rows = probe_df[\n",
    "        (probe_df['language'] == language) & \n",
    "        (probe_df['task'] == task) & \n",
    "        (probe_df['submetric'] == submetric) & \n",
    "        (probe_df['metric'] == metric) & \n",
    "        (probe_df['layer_index'] == layer_idx)\n",
    "    ]\n",
    "    \n",
    "    # Skip if no matching rows\n",
    "    if len(matching_rows) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create a new row for this combination\n",
    "    result_row = {\n",
    "        'language': language,\n",
    "        'task': task,\n",
    "        'submetric': submetric,\n",
    "        'metric': metric,\n",
    "        'model_type': 'probe',\n",
    "        'split': 'test',\n",
    "        'layer': f'layer{layer_idx}'  # Properly include layer information for probe experiments\n",
    "    }\n",
    "    \n",
    "    # Find real value (NaN control_index)\n",
    "    real_rows = matching_rows[matching_rows['control_index'].isna()]\n",
    "    if len(real_rows) > 0:\n",
    "        result_row['real'] = real_rows.iloc[0]['value']\n",
    "    else:\n",
    "        # Skip if no real value found\n",
    "        print(f\"Warning: No real value found for {language}/{task}/{metric}\")\n",
    "        continue\n",
    "    \n",
    "    # Find control values\n",
    "    for control_idx in [1.0, 2.0, 3.0]:\n",
    "        control_rows = matching_rows[matching_rows['control_index'] == control_idx]\n",
    "        if len(control_rows) > 0:\n",
    "            result_row[f'control{int(control_idx)}'] = control_rows.iloc[0]['value']\n",
    "    \n",
    "    # Calculate control_mean if we have at least one control\n",
    "    controls = []\n",
    "    for control_key in ['control1', 'control2', 'control3']:\n",
    "        if control_key in result_row and pd.notna(result_row.get(control_key)):\n",
    "            try:\n",
    "                controls.append(float(result_row[control_key]))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    if controls:\n",
    "        result_row['control_mean'] = sum(controls) / len(controls)\n",
    "        \n",
    "        if pd.notna(result_row.get('real')):\n",
    "            try:\n",
    "                real_val = float(result_row['real'])\n",
    "                control_mean = float(result_row['control_mean'])\n",
    "                result_row['selectivity'] = abs(real_val - control_mean)\n",
    "                \n",
    "                # Calculate normalized_selectivity\n",
    "                if control_mean != 0:\n",
    "                    result_row['normalized_selectivity'] = (abs(real_val - control_mean) / control_mean) * 100\n",
    "                else:\n",
    "                    result_row['normalized_selectivity'] = 0\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # Add to our results\n",
    "    results.append(result_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "probe_processed = pd.DataFrame(results)\n",
    "print(f\"Created {len(probe_processed)} processed probe rows\")\n",
    "\n",
    "# Print a diagnosis of tasks in the probe data for debugging\n",
    "print(\"\\nTask breakdown in processed probe data:\")\n",
    "task_counts = probe_processed['task'].value_counts()\n",
    "print(task_counts)\n",
    "\n",
    "# Step 4: Merge all datasets\n",
    "print(\"\\nMerging datasets...\")\n",
    "\n",
    "# Ensure all DataFrames have all required columns\n",
    "for df in [original_filtered, probe_processed, finetune_processed]:\n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "# Combine all processed data\n",
    "merged_df = pd.concat(\n",
    "    [\n",
    "        original_filtered[target_columns],\n",
    "        probe_processed[target_columns],\n",
    "        finetune_processed[target_columns]\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Fill NaN values in the 'split' column with 'test'\n",
    "merged_df['split'] = merged_df['split'].fillna('test')\n",
    "\n",
    "# Save the result\n",
    "output_file = '/home/robin/Research/qtype-eval/visualization/notebooks/merged_ml_results.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully created {output_file}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"  - Original data: {len(original_filtered)}\")\n",
    "print(f\"  - Probe data: {len(probe_processed)}\")\n",
    "print(f\"  - Finetune data: {len(finetune_processed)}\")\n",
    "\n",
    "# Display some diagnostics about layer values\n",
    "print(\"\\nLayer value counts:\")\n",
    "layer_counts = merged_df['layer'].value_counts(dropna=False)\n",
    "print(layer_counts)\n",
    "\n",
    "# Check each model type for proper layer handling\n",
    "print(\"\\nChecking layer values by model_type:\")\n",
    "for model_type in merged_df['model_type'].unique():\n",
    "    model_data = merged_df[merged_df['model_type'] == model_type]\n",
    "    layer_values = model_data['layer'].value_counts(dropna=False)\n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(layer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0df6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering original data...\n",
      "Processing finetune data...\n",
      "Selected 231 rows from finetune data where control_index is None\n",
      "Processing probe data...\n",
      "Found 2772 unique parameter combinations in probe data\n",
      "Created 2016 processed probe rows\n",
      "\n",
      "Task breakdown in processed probe data:\n",
      "task\n",
      "single_submetric    2016\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "Successfully created /home/robin/Research/qtype-eval/visualization/notebooks/merged_ml_results.csv\n",
      "Total rows: 2415\n",
      "  - Original data: 168\n",
      "  - Probe data: 2016\n",
      "  - Finetune data: 231\n",
      "\n",
      "Layer value counts:\n",
      "layer\n",
      "None       399\n",
      "layer1     168\n",
      "layer2     168\n",
      "layer3     168\n",
      "layer4     168\n",
      "layer5     168\n",
      "layer6     168\n",
      "layer7     168\n",
      "layer8     168\n",
      "layer9     168\n",
      "layer10    168\n",
      "layer11    168\n",
      "layer12    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking layer values by model_type:\n",
      "\n",
      "DummyRegressor:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "XGBRegressor:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DummyClassifier:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ridge:\n",
      "layer\n",
      "None    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LogisticRegression:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "XGBClassifier:\n",
      "layer\n",
      "None    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "probe:\n",
      "layer\n",
      "layer1     168\n",
      "layer2     168\n",
      "layer3     168\n",
      "layer4     168\n",
      "layer5     168\n",
      "layer6     168\n",
      "layer7     168\n",
      "layer8     168\n",
      "layer9     168\n",
      "layer10    168\n",
      "layer11    168\n",
      "layer12    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "finetune:\n",
      "layer\n",
      "None    231\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1835995/2965936545.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "original_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/original_base.csv\")\n",
    "probe_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/probe_base.csv\")\n",
    "finetune_df = pd.read_csv(\"/home/robin/Research/qtype-eval/visualization/base_csv_files/finetune_base.csv\")\n",
    "\n",
    "# Define the target columns and their order for the final dataset\n",
    "target_columns = [\n",
    "    'language', 'task', 'submetric', 'model_type', 'layer', 'metric', \n",
    "    'real', 'control1', 'control2', 'control3', 'control_mean', 'split'\n",
    "]\n",
    "\n",
    "print(\"Filtering original data...\")\n",
    "original_filtered = original_df[original_df['model_type'] != 'lm_probe'].copy()\n",
    "original_filtered = original_filtered[original_filtered['split'] == 'test'].copy()\n",
    "\n",
    "# Set layer to None for original data\n",
    "original_filtered['layer'] = None\n",
    "\n",
    "print(\"Processing finetune data...\")\n",
    "# Process finetune data (keeping this as it is in the original script)\n",
    "finetune_real = finetune_df[finetune_df['control_index'].isna()].copy()\n",
    "print(f\"Selected {len(finetune_real)} rows from finetune data where control_index is None\")\n",
    "\n",
    "finetune_processed = pd.DataFrame()\n",
    "finetune_processed['language'] = finetune_real['language']\n",
    "finetune_processed['real'] = finetune_real['value']\n",
    "finetune_processed['task'] = finetune_real['task']\n",
    "finetune_processed['metric'] = finetune_real['metric']\n",
    "finetune_processed['submetric'] = finetune_real['submetric'] \n",
    "finetune_processed['model_type'] = 'finetune'\n",
    "finetune_processed['split'] = 'test'\n",
    "finetune_processed['layer'] = 'None'  # Set to None for finetune experiments\n",
    "finetune_processed['control1'] = 'None'\n",
    "finetune_processed['control2'] = 'None'\n",
    "finetune_processed['control3'] = 'None'\n",
    "finetune_processed['control_mean'] = 'None'\n",
    "\n",
    "# Process probe data without selectivity calculations\n",
    "print(\"Processing probe data...\")\n",
    "\n",
    "# Get all unique combinations of parameters\n",
    "unique_combinations = probe_df.drop_duplicates(\n",
    "    subset=['language', 'task', 'submetric', 'metric', 'layer_index']\n",
    ").copy()\n",
    "\n",
    "print(f\"Found {len(unique_combinations)} unique parameter combinations in probe data\")\n",
    "\n",
    "# Process each unique combination\n",
    "results = []\n",
    "for _, row in unique_combinations.iterrows():\n",
    "    language = row['language']\n",
    "    task = row['task']\n",
    "    submetric = row['submetric']\n",
    "    metric = row['metric']\n",
    "    layer_idx = row['layer_index']\n",
    "    \n",
    "    # Get all rows matching this combination\n",
    "    matching_rows = probe_df[\n",
    "        (probe_df['language'] == language) & \n",
    "        (probe_df['task'] == task) & \n",
    "        (probe_df['submetric'] == submetric) & \n",
    "        (probe_df['metric'] == metric) & \n",
    "        (probe_df['layer_index'] == layer_idx)\n",
    "    ]\n",
    "    \n",
    "    # Skip if no matching rows\n",
    "    if len(matching_rows) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create a new row for this combination\n",
    "    result_row = {\n",
    "        'language': language,\n",
    "        'task': task,\n",
    "        'submetric': submetric,\n",
    "        'metric': metric,\n",
    "        'model_type': 'probe',\n",
    "        'split': 'test',\n",
    "        'layer': f'layer{layer_idx}'  # Include layer information for probe experiments\n",
    "    }\n",
    "    \n",
    "    # Find real value (NaN control_index)\n",
    "    real_rows = matching_rows[matching_rows['control_index'].isna()]\n",
    "    if len(real_rows) > 0:\n",
    "        result_row['real'] = real_rows.iloc[0]['value']\n",
    "    else:\n",
    "        # Skip if no real value found\n",
    "        print(f\"Warning: No real value found for {language}/{task}/{metric}\")\n",
    "        continue\n",
    "    \n",
    "    # Find control values\n",
    "    for control_idx in [1.0, 2.0, 3.0]:\n",
    "        control_rows = matching_rows[matching_rows['control_index'] == control_idx]\n",
    "        if len(control_rows) > 0:\n",
    "            result_row[f'control{int(control_idx)}'] = control_rows.iloc[0]['value']\n",
    "    \n",
    "    # Calculate control_mean if we have at least one control\n",
    "    controls = []\n",
    "    for control_key in ['control1', 'control2', 'control3']:\n",
    "        if control_key in result_row and pd.notna(result_row.get(control_key)):\n",
    "            try:\n",
    "                controls.append(float(result_row[control_key]))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    if controls:\n",
    "        result_row['control_mean'] = sum(controls) / len(controls)\n",
    "    \n",
    "    # Add to our results\n",
    "    results.append(result_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "probe_processed = pd.DataFrame(results)\n",
    "print(f\"Created {len(probe_processed)} processed probe rows\")\n",
    "\n",
    "# Print a diagnosis of tasks in the probe data for debugging\n",
    "print(\"\\nTask breakdown in processed probe data:\")\n",
    "task_counts = probe_processed['task'].value_counts()\n",
    "print(task_counts)\n",
    "\n",
    "# Merge all datasets\n",
    "print(\"\\nMerging datasets...\")\n",
    "\n",
    "# Ensure all DataFrames have all required columns\n",
    "for df in [original_filtered, probe_processed, finetune_processed]:\n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "# Combine all processed data\n",
    "merged_df = pd.concat(\n",
    "    [\n",
    "        original_filtered[target_columns],\n",
    "        probe_processed[target_columns],\n",
    "        finetune_processed[target_columns]\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Fill NaN values in the 'split' column with 'test'\n",
    "merged_df['split'] = merged_df['split'].fillna('test')\n",
    "\n",
    "# Save the result\n",
    "output_file = '/home/robin/Research/qtype-eval/visualization/notebooks/merged_ml_results.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully created {output_file}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"  - Original data: {len(original_filtered)}\")\n",
    "print(f\"  - Probe data: {len(probe_processed)}\")\n",
    "print(f\"  - Finetune data: {len(finetune_processed)}\")\n",
    "\n",
    "# Display some diagnostics about layer values\n",
    "print(\"\\nLayer value counts:\")\n",
    "layer_counts = merged_df['layer'].value_counts(dropna=False)\n",
    "print(layer_counts)\n",
    "\n",
    "# Check each model type for proper layer handling\n",
    "print(\"\\nChecking layer values by model_type:\")\n",
    "for model_type in merged_df['model_type'].unique():\n",
    "    model_data = merged_df[merged_df['model_type'] == model_type]\n",
    "    layer_values = model_data['layer'].value_counts(dropna=False)\n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(layer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "604d5ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. language\n",
      "2. real\n",
      "3. control1\n",
      "4. control2\n",
      "5. control3\n",
      "6. control_mean\n",
      "7. selectivity\n",
      "8. normalized_selectivity\n",
      "9. task\n",
      "10. model_type\n",
      "11. metric\n",
      "12. submetric\n",
      "13. split\n",
      "14. layer\n",
      "\n",
      "\n",
      "=== Unique values in 'language' (7 values) ===\n",
      "  ar: 345 occurrences\n",
      "  en: 345 occurrences\n",
      "  fi: 345 occurrences\n",
      "  id: 345 occurrences\n",
      "  ja: 345 occurrences\n",
      "  ko: 345 occurrences\n",
      "  ru: 345 occurrences\n",
      "\n",
      "\n",
      "=== Unique values in 'task' (3 values) ===\n",
      "  complexity: 49 occurrences\n",
      "  question_type: 56 occurrences\n",
      "  single_submetric: 2310 occurrences\n",
      "\n",
      "\n",
      "=== Unique values in 'model_type' (8 values) ===\n",
      "  DummyClassifier: 7 occurrences\n",
      "  DummyRegressor: 49 occurrences\n",
      "  LogisticRegression: 7 occurrences\n",
      "  Ridge: 49 occurrences\n",
      "  XGBClassifier: 7 occurrences\n",
      "  XGBRegressor: 49 occurrences\n",
      "  finetune: 231 occurrences\n",
      "  probe: 2016 occurrences\n",
      "\n",
      "\n",
      "=== Unique values in 'metric' (8 values) ===\n",
      "  accuracy: 28 occurrences\n",
      "  f1: 7 occurrences\n",
      "  loss: 560 occurrences\n",
      "  mse: 700 occurrences\n",
      "  precision: 7 occurrences\n",
      "  r2: 553 occurrences\n",
      "  recall: 7 occurrences\n",
      "  rmse: 553 occurrences\n",
      "\n",
      "\n",
      "=== Unique values in 'submetric' (8 values) ===\n",
      "  avg_links_len: 385 occurrences\n",
      "  avg_max_depth: 385 occurrences\n",
      "  avg_subordinate_chain_len: 385 occurrences\n",
      "  avg_verb_edges: 385 occurrences\n",
      "  complexity: 21 occurrences\n",
      "  lexical_density: 385 occurrences\n",
      "  n_tokens: 385 occurrences\n",
      "  overall: 21 occurrences\n",
      "  None/NaN: 63 occurrences\n",
      "\n",
      "\n",
      "=== Unique values in 'layer' (12 values) ===\n",
      "  layer1: 168 occurrences\n",
      "  layer10: 168 occurrences\n",
      "  layer11: 168 occurrences\n",
      "  layer12: 168 occurrences\n",
      "  layer2: 168 occurrences\n",
      "  layer3: 168 occurrences\n",
      "  layer4: 168 occurrences\n",
      "  layer5: 168 occurrences\n",
      "  layer6: 168 occurrences\n",
      "  layer7: 168 occurrences\n",
      "  layer8: 168 occurrences\n",
      "  layer9: 168 occurrences\n",
      "  None/NaN: 399 occurrences\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/robin/Research/qtype-eval/visualization/notebooks/merged_ml_results.csv')\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to display unique values for selected columns\n",
    "def show_unique_values(dataframe, column_name):\n",
    "    \"\"\"Display all unique values in a specified column\"\"\"\n",
    "    unique_vals = dataframe[column_name].dropna().unique()\n",
    "    print(f\"=== Unique values in '{column_name}' ({len(unique_vals)} values) ===\")\n",
    "    \n",
    "    # Sort values for better readability\n",
    "    sorted_vals = sorted(unique_vals)\n",
    "    \n",
    "    # Display count of each value\n",
    "    value_counts = dataframe[column_name].value_counts().to_dict()\n",
    "    \n",
    "    for val in sorted_vals:\n",
    "        count = value_counts.get(val, 0)\n",
    "        print(f\"  {val}: {count} occurrences\")\n",
    "    \n",
    "    if dataframe[column_name].isna().sum() > 0:\n",
    "        print(f\"  None/NaN: {dataframe[column_name].isna().sum()} occurrences\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Show unique values for categorical columns\n",
    "categorical_columns = ['language', 'task', 'model_type', 'metric', 'submetric', 'layer']\n",
    "for col in categorical_columns:\n",
    "    show_unique_values(df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2aeb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English probe rows: 288\n",
      "Question type classification rows: 56\n",
      "\n",
      "=== Model Type Comparison for Question Type Task ===\n",
      "                         mean        std  count\n",
      "model_type                                     \n",
      "DummyClassifier      0.000000   0.000000      7\n",
      "LogisticRegression  54.447740  29.593497      7\n",
      "XGBClassifier       50.941154  25.927299      7\n",
      "finetune                  NaN        NaN      0\n",
      "\n",
      "=== Layer-wise Performance for English Question Type ===\n",
      "Empty DataFrame\n",
      "Columns: [layer, normalized_selectivity]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
