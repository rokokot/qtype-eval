SLURM_JOB_ID: 64466379
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 18:23:09 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Running experiment: probe_layer2_complexity_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:23:53,209][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ru
experiment_name: probe_layer2_complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:23:53,209][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:23:53,209][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:23:53,209][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:23:53,214][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:23:53,215][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:23:56,758][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:23:59,279][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:23:59,280][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:23:59,604][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:23:59,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:24:00,108][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:24:00,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:24:00,117][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:24:00,119][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:24:00,246][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:24:00,330][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:24:00,369][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:24:00,370][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:24:00,370][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:24:00,371][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:24:00,453][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:24:00,544][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:24:00,584][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:24:00,585][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:24:00,585][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:24:00,606][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:24:00,607][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:24:00,607][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:24:00,607][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:24:00,607][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:24:00,607][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:24:00,607][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:24:00,607][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:24:00,608][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:24:00,608][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:24:00,608][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:24:00,609][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:24:00,609][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:24:00,609][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:24:00,610][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:24:00,610][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:24:00,610][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:24:09,059][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:24:09,060][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:24:09,061][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:24:09,061][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:24:09,063][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:24:09,064][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:24:09,064][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:24:09,064][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:24:09,064][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:24:09,065][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:24:09,065][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3108Epoch 1/15: [                              ] 2/75 batches, loss: 0.4121Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3396Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3880Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3905Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3659Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4198Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4138Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4521Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4386Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4244Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4231Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4036Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3916Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3873Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3805Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3700Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3771Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3839Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3779Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3797Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3823Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3733Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3717Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3643Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3612Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3610Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3565Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3577Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3607Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3593Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3535Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3504Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3507Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3472Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3493Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3446Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3444Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3419Epoch 1/15: [================              ] 40/75 batches, loss: 0.3392Epoch 1/15: [================              ] 41/75 batches, loss: 0.3364Epoch 1/15: [================              ] 42/75 batches, loss: 0.3336Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3324Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3289Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3280Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3236Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3216Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3191Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3159Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3144Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3108Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3080Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3054Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3049Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3032Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3040Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3068Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3050Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3029Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3003Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2978Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2970Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2944Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2924Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2906Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2887Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2862Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2829Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2820Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2808Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2785Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2778Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2755Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2742Epoch 1/15: [==============================] 75/75 batches, loss: 0.2723
[2025-05-07 18:24:16,454][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2723
[2025-05-07 18:24:16,787][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0934, Metrics: {'mse': 0.09961844980716705, 'rmse': 0.31562390563321885, 'r2': -1.1414501667022705}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1017Epoch 2/15: [                              ] 2/75 batches, loss: 0.1567Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1298Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1767Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1574Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1528Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1575Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1533Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1470Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1499Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1522Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1521Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1562Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1528Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1478Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1445Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1438Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1488Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1468Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1461Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1462Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1426Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1453Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1449Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1448Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1457Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1462Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1446Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1446Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1409Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1397Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1384Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1411Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1409Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1415Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1399Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1393Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1396Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1402Epoch 2/15: [================              ] 40/75 batches, loss: 0.1385Epoch 2/15: [================              ] 41/75 batches, loss: 0.1377Epoch 2/15: [================              ] 42/75 batches, loss: 0.1386Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1394Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1377Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1371Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1364Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1364Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1357Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1352Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1337Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1339Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1343Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1344Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1337Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1330Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1338Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1331Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1320Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1317Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1307Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1304Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1298Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1286Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1284Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1279Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1271Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1266Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1259Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1252Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1244Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1236Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1231Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1230Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1226Epoch 2/15: [==============================] 75/75 batches, loss: 0.1220
[2025-05-07 18:24:19,519][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1220
[2025-05-07 18:24:19,899][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1159, Metrics: {'mse': 0.12409377098083496, 'rmse': 0.35226945791657127, 'r2': -1.6675844192504883}
[2025-05-07 18:24:19,900][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0969Epoch 3/15: [                              ] 2/75 batches, loss: 0.1452Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1349Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1236Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1104Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1105Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1133Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1044Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1053Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1053Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1020Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1010Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1016Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1019Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0999Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0967Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0962Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0964Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0950Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0919Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0903Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0900Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0903Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0930Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0934Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0922Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0908Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0896Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0916Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0913Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0921Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0927Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0915Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0908Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0914Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0901Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0898Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0902Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0899Epoch 3/15: [================              ] 40/75 batches, loss: 0.0898Epoch 3/15: [================              ] 41/75 batches, loss: 0.0914Epoch 3/15: [================              ] 42/75 batches, loss: 0.0918Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0913Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0905Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0899Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0906Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0902Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0902Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0893Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0902Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0892Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0885Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0892Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0884Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0878Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0872Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0874Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0866Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0863Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0859Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0864Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0864Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0855Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0852Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0853Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0853Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0851Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0852Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0853Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0850Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0846Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0845Epoch 3/15: [==============================] 75/75 batches, loss: 0.0848
[2025-05-07 18:24:22,222][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0848
[2025-05-07 18:24:22,535][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0650, Metrics: {'mse': 0.06826744228601456, 'rmse': 0.2612803901673728, 'r2': -0.4675126075744629}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0403Epoch 4/15: [                              ] 2/75 batches, loss: 0.0602Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0496Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0561Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0568Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0595Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0640Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0644Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0679Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0647Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0656Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0651Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0626Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0638Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0652Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0663Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0642Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0635Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0639Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0644Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0649Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0669Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0681Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0678Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0703Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0713Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0714Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0717Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0727Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0719Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0718Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0706Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0700Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0701Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0701Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0695Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0700Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0717Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0707Epoch 4/15: [================              ] 40/75 batches, loss: 0.0703Epoch 4/15: [================              ] 41/75 batches, loss: 0.0704Epoch 4/15: [================              ] 42/75 batches, loss: 0.0702Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0712Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0709Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0715Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0720Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0720Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0712Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0711Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0705Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0718Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0729Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0728Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0732Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0733Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0732Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0730Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0721Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0721Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0719Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0720Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0715Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0717Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0711Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0713Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0708Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0711Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0711Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0712Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0709Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0704Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0707Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0711Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0715Epoch 4/15: [==============================] 75/75 batches, loss: 0.0724
[2025-05-07 18:24:25,336][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0724
[2025-05-07 18:24:25,660][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0599, Metrics: {'mse': 0.06261800974607468, 'rmse': 0.25023590818680413, 'r2': -0.34606945514678955}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0374Epoch 5/15: [                              ] 2/75 batches, loss: 0.0475Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0815Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0901Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0824Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0767Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0791Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0736Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0734Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0724Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0721Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0730Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0726Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0716Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0704Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0703Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0702Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0707Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0694Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0690Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0670Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0677Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0678Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0673Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0668Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0669Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0660Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0654Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0662Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0661Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0659Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0654Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0653Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0649Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0648Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0641Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0646Epoch 5/15: [================              ] 40/75 batches, loss: 0.0641Epoch 5/15: [================              ] 41/75 batches, loss: 0.0646Epoch 5/15: [================              ] 42/75 batches, loss: 0.0645Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0642Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0639Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0638Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0646Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0641Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0642Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0630Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0621Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0622Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0616Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0619Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0613Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0617Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0617Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0612Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0611Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0606Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0602Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0605Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0608Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0608Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0605Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0602Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0598Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0596Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0601Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0597Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0595Epoch 5/15: [==============================] 75/75 batches, loss: 0.0594
[2025-05-07 18:24:28,296][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0594
[2025-05-07 18:24:28,545][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0595, Metrics: {'mse': 0.062031492590904236, 'rmse': 0.24906122257570373, 'r2': -0.3334614038467407}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0225Epoch 6/15: [                              ] 2/75 batches, loss: 0.0286Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0375Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0444Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0463Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0472Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0451Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0432Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0432Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0423Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0435Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0424Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0422Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0414Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0430Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0422Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0433Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0440Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0433Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0439Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0461Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0483Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0503Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0519Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0525Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0523Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0517Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0516Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0518Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0510Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0509Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0515Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0515Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0517Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0520Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0511Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0521Epoch 6/15: [================              ] 40/75 batches, loss: 0.0520Epoch 6/15: [================              ] 41/75 batches, loss: 0.0523Epoch 6/15: [================              ] 42/75 batches, loss: 0.0524Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0541Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0540Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0544Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0542Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0550Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0551Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0548Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0551Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0551Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0545Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0538Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0537Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0532Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0531Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0531Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0533Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0531Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0529Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0528Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0526Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0523Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0521Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0519Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0518Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0518Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0517Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0513Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0510Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0508Epoch 6/15: [==============================] 75/75 batches, loss: 0.0506
[2025-05-07 18:24:31,237][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0506
[2025-05-07 18:24:31,557][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0681, Metrics: {'mse': 0.07136891782283783, 'rmse': 0.26714961692437034, 'r2': -0.5341835021972656}
[2025-05-07 18:24:31,557][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0688Epoch 7/15: [                              ] 2/75 batches, loss: 0.0623Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0519Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0488Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0438Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0429Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0418Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0408Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0403Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0400Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0419Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0435Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0431Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0419Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0429Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0431Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0424Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0416Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0428Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0429Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0426Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0421Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0436Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0448Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0445Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0445Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0437Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0438Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0439Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0434Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0435Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0440Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0442Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0446Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0441Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0435Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0435Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0428Epoch 7/15: [================              ] 40/75 batches, loss: 0.0430Epoch 7/15: [================              ] 41/75 batches, loss: 0.0432Epoch 7/15: [================              ] 42/75 batches, loss: 0.0442Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0440Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0437Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0433Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0435Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0435Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0431Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0438Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0436Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0438Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0442Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0442Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0447Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0447Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0444Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0444Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0446Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0444Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0444Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0440Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0438Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0436Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0436Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0435Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0435Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0435Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0433Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0431Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0435Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0434Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0433Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0432Epoch 7/15: [==============================] 75/75 batches, loss: 0.0432
[2025-05-07 18:24:33,890][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0432
[2025-05-07 18:24:34,137][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0692, Metrics: {'mse': 0.07269065827131271, 'rmse': 0.26961205142076405, 'r2': -0.562596321105957}
[2025-05-07 18:24:34,137][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0409Epoch 8/15: [                              ] 2/75 batches, loss: 0.0515Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0504Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0526Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0493Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0491Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0483Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0448Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0423Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0475Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0460Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0446Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0442Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0424Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0420Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0418Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0417Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0419Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0424Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0417Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0415Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0409Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0407Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0409Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0408Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0396Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0400Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0403Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0407Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0415Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0411Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0406Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0415Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0414Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0407Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0406Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0409Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0408Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0413Epoch 8/15: [================              ] 40/75 batches, loss: 0.0418Epoch 8/15: [================              ] 41/75 batches, loss: 0.0419Epoch 8/15: [================              ] 42/75 batches, loss: 0.0415Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0413Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0416Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0415Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0417Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0415Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0413Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0410Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0413Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0411Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0419Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0415Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0417Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0418Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0422Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0420Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0418Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0416Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0413Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0411Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0411Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0415Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0413Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0410Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0408Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0407Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0408Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0405Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0403Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0401Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0398Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0397Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0398Epoch 8/15: [==============================] 75/75 batches, loss: 0.0398
[2025-05-07 18:24:36,573][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0398
[2025-05-07 18:24:36,917][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0650, Metrics: {'mse': 0.0681576207280159, 'rmse': 0.26107014522540856, 'r2': -0.4651517868041992}
[2025-05-07 18:24:36,917][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0491Epoch 9/15: [                              ] 2/75 batches, loss: 0.0489Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0401Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0400Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0358Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0375Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0374Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0368Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0359Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0340Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0359Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0360Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0363Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0359Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0343Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0348Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0342Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0346Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0350Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0352Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0350Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0355Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0352Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0358Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0358Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0357Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0362Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0360Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0359Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0361Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0372Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0373Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0367Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0373Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0370Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0373Epoch 9/15: [================              ] 40/75 batches, loss: 0.0376Epoch 9/15: [================              ] 41/75 batches, loss: 0.0373Epoch 9/15: [================              ] 42/75 batches, loss: 0.0373Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0374Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0371Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0368Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0367Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0364Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0363Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0361Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0361Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0359Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0356Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0356Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0354Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0355Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0358Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0359Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0358Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0363Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0366Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0368Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0367Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0366Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0364Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0366Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0364Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0364Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0361Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0362Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0363Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0365Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0365Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0363Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0361Epoch 9/15: [==============================] 75/75 batches, loss: 0.0360
[2025-05-07 18:24:39,239][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0360
[2025-05-07 18:24:39,625][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0570, Metrics: {'mse': 0.05893178656697273, 'rmse': 0.24275870029099417, 'r2': -0.26682841777801514}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0666Epoch 10/15: [                              ] 2/75 batches, loss: 0.0616Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0506Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0508Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0529Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0502Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0483Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0456Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0446Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0440Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0434Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0425Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0414Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0406Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0394Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0388Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0386Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0381Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0375Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0374Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0393Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0391Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0386Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0392Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0395Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0390Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0395Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0389Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0387Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0388Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0388Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0394Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0391Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0399Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0398Epoch 10/15: [================              ] 40/75 batches, loss: 0.0394Epoch 10/15: [================              ] 41/75 batches, loss: 0.0395Epoch 10/15: [================              ] 42/75 batches, loss: 0.0392Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0398Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0393Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0389Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0385Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0382Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0381Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0378Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0373Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0370Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0369Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0369Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0368Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0366Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0365Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0366Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0366Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0365Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0362Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0361Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0359Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0357Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0358Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0357Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0357Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0360Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0358Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0356Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0355Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0355Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0355Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0357Epoch 10/15: [==============================] 75/75 batches, loss: 0.0358
[2025-05-07 18:24:42,389][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0358
[2025-05-07 18:24:42,640][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0758, Metrics: {'mse': 0.07967965304851532, 'rmse': 0.2822758456696487, 'r2': -0.712835431098938}
[2025-05-07 18:24:42,641][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0342Epoch 11/15: [                              ] 2/75 batches, loss: 0.0270Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0285Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0329Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0358Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0340Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0378Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0365Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0355Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0346Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0348Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0349Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0336Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0346Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0340Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0334Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0332Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0336Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0336Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0345Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0343Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0333Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0348Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0343Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0348Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0348Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0346Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0350Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0351Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0347Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0346Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0342Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0342Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0345Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0339Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0337Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0334Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0332Epoch 11/15: [================              ] 40/75 batches, loss: 0.0334Epoch 11/15: [================              ] 41/75 batches, loss: 0.0337Epoch 11/15: [================              ] 42/75 batches, loss: 0.0337Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0335Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0335Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0337Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0338Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0337Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0333Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0333Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0329Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0325Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0326Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0323Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0323Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0322Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0322Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0322Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0320Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0317Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0316Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0315Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0315Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0317Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0318Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0318Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0315Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0315Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0315Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0314Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0319Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0317Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0316Epoch 11/15: [==============================] 75/75 batches, loss: 0.0316
[2025-05-07 18:24:45,014][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0316
[2025-05-07 18:24:45,293][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0603, Metrics: {'mse': 0.06241382658481598, 'rmse': 0.24982759372178243, 'r2': -0.3416801691055298}
[2025-05-07 18:24:45,294][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0411Epoch 12/15: [                              ] 2/75 batches, loss: 0.0321Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0386Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0335Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0335Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0315Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0333Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0333Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0345Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0327Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0325Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0316Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0300Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0309Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0310Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0313Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0306Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0309Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0305Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0300Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0296Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0291Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0292Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0292Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0290Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0297Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0300Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0296Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0300Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0301Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0302Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0302Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0301Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0298Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0292Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0295Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0295Epoch 12/15: [================              ] 40/75 batches, loss: 0.0297Epoch 12/15: [================              ] 41/75 batches, loss: 0.0296Epoch 12/15: [================              ] 42/75 batches, loss: 0.0302Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0299Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0295Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0297Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0300Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0300Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0303Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0308Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0308Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0311Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0314Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0314Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0316Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0320Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0320Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0318Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0319Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0318Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0319Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0319Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0321Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0321Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0319Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0320Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0321Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0320Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0320Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0318Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0317Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0315Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0315Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0315Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0314Epoch 12/15: [==============================] 75/75 batches, loss: 0.0312
[2025-05-07 18:24:47,591][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0312
[2025-05-07 18:24:47,943][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0570, Metrics: {'mse': 0.05863787233829498, 'rmse': 0.2421525806971608, 'r2': -0.2605102062225342}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0203Epoch 13/15: [                              ] 2/75 batches, loss: 0.0354Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0324Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0327Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0334Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0355Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0334Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0324Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0335Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0311Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0304Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0302Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0299Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0299Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0293Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0285Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0281Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0273Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0277Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0275Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0273Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0266Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0266Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0283Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0286Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0276Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0274Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0272Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0268Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0270Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0269Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0263Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0269Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0267Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0269Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0279Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0279Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0283Epoch 13/15: [================              ] 40/75 batches, loss: 0.0286Epoch 13/15: [================              ] 41/75 batches, loss: 0.0289Epoch 13/15: [================              ] 42/75 batches, loss: 0.0288Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0286Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0289Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0288Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0287Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0290Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0289Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0290Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0292Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0291Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0292Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0292Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0295Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0294Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0293Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0297Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0296Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0293Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0296Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0297Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0296Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0295Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0296Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0297Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0293Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0293Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0293Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0293Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0293Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0292Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0290Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0295Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0295Epoch 13/15: [==============================] 75/75 batches, loss: 0.0295
[2025-05-07 18:24:50,711][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0295
[2025-05-07 18:24:51,066][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0661, Metrics: {'mse': 0.06901881098747253, 'rmse': 0.26271431439392967, 'r2': -0.4836643934249878}
[2025-05-07 18:24:51,067][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0124Epoch 14/15: [                              ] 2/75 batches, loss: 0.0197Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0232Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0235Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0231Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0218Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0207Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0216Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0213Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0219Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0245Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0259Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0248Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0243Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0243Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0244Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0245Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0247Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0248Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0249Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0254Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0262Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0259Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0254Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0251Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0252Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0250Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0256Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0264Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0261Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0259Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0268Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0264Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0270Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0268Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0267Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0269Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0268Epoch 14/15: [================              ] 40/75 batches, loss: 0.0266Epoch 14/15: [================              ] 41/75 batches, loss: 0.0266Epoch 14/15: [================              ] 42/75 batches, loss: 0.0265Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0266Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0273Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0272Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0274Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0276Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0276Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0274Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0276Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0274Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0274Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0274Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0273Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0274Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0271Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0275Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0274Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0276Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0275Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0274Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0274Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0273Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0275Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0274Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0274Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0275Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0277Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0278Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0277Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0279Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0278Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0276Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0275Epoch 14/15: [==============================] 75/75 batches, loss: 0.0275
[2025-05-07 18:24:53,449][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0275
[2025-05-07 18:24:53,848][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0528, Metrics: {'mse': 0.05404801666736603, 'rmse': 0.23248229323405692, 'r2': -0.1618443727493286}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0385Epoch 15/15: [                              ] 2/75 batches, loss: 0.0334Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0292Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0294Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0271Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0290Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0295Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0284Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0295Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0294Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0308Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0308Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0305Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0295Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0290Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0284Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0291Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0294Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0297Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0290Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0296Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0294Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0294Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0300Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0294Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0288Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0287Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0291Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0289Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0287Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0286Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0286Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0282Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0285Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0280Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0280Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0282Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0277Epoch 15/15: [================              ] 40/75 batches, loss: 0.0275Epoch 15/15: [================              ] 41/75 batches, loss: 0.0273Epoch 15/15: [================              ] 42/75 batches, loss: 0.0273Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0270Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0270Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0269Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0273Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0271Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0271Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0271Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0268Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0273Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0273Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0278Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0278Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0276Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0279Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0278Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0276Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0277Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0278Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0277Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0278Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0277Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0277Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0277Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0280Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0279Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0277Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0278Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0276Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0275Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0274Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0277Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0276Epoch 15/15: [==============================] 75/75 batches, loss: 0.0275
[2025-05-07 18:24:56,655][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0275
[2025-05-07 18:24:57,043][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0593, Metrics: {'mse': 0.06154636666178703, 'rmse': 0.24808540195220483, 'r2': -0.3230327367782593}
[2025-05-07 18:24:57,044][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 18:24:57,044][src.training.lm_trainer][INFO] - Training completed in 43.69 seconds
[2025-05-07 18:24:57,044][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:25:00,158][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.01594678685069084, 'rmse': 0.12628058778248874, 'r2': 0.20014894008636475}
[2025-05-07 18:25:00,159][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05404801666736603, 'rmse': 0.23248229323405692, 'r2': -0.1618443727493286}
[2025-05-07 18:25:00,159][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.056273140013217926, 'rmse': 0.2372196029277891, 'r2': -0.42337608337402344}
[2025-05-07 18:25:01,805][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ru/ru/model.pt
[2025-05-07 18:25:01,806][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▂▂▁
wandb:     best_val_mse █▃▂▂▂▂▁
wandb:      best_val_r2 ▁▆▇▇▇▇█
wandb:    best_val_rmse █▃▂▂▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▆▆▆▆▆▆▆▅▆▆▆▇
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▂▂▂▃▃▂▁▄▂▁▂▁▂
wandb:          val_mse ▆█▂▂▂▃▃▂▁▄▂▁▂▁▂
wandb:           val_r2 ▃▁▇▇▇▆▆▇█▅▇█▇█▇
wandb:         val_rmse ▆█▃▂▂▃▃▃▂▄▂▂▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05282
wandb:     best_val_mse 0.05405
wandb:      best_val_r2 -0.16184
wandb:    best_val_rmse 0.23248
wandb:            epoch 15
wandb:   final_test_mse 0.05627
wandb:    final_test_r2 -0.42338
wandb:  final_test_rmse 0.23722
wandb:  final_train_mse 0.01595
wandb:   final_train_r2 0.20015
wandb: final_train_rmse 0.12628
wandb:    final_val_mse 0.05405
wandb:     final_val_r2 -0.16184
wandb:   final_val_rmse 0.23248
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02754
wandb:       train_time 43.6925
wandb:         val_loss 0.05934
wandb:          val_mse 0.06155
wandb:           val_r2 -0.32303
wandb:         val_rmse 0.24809
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182353-39srv257
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182353-39srv257/logs
Experiment probe_layer2_complexity_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6
=======================
Running experiment: probe_layer6_complexity_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ru"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:25:36,198][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ru
experiment_name: probe_layer6_complexity_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:25:36,198][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:25:36,198][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:25:36,198][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:25:36,203][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:25:36,203][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:25:39,674][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:25:42,001][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:25:42,002][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:25:42,274][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:42,355][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:42,696][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:25:42,705][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:25:42,706][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:25:42,708][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:25:42,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:42,938][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:42,979][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:25:42,981][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:25:42,981][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:25:42,983][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:25:43,107][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:43,257][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:25:43,295][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:25:43,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:25:43,297][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:25:43,300][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:25:43,300][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:25:43,300][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:25:43,300][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:25:43,300][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:25:43,301][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:25:43,301][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:25:43,301][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:25:43,301][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:25:43,302][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:25:43,302][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:25:43,302][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:25:43,302][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:25:43,303][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:25:43,303][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:25:43,303][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:25:43,303][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:25:43,303][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:25:43,303][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:25:43,304][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:25:51,730][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:25:51,731][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:25:51,731][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:25:51,731][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:25:51,734][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:25:51,735][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:25:51,735][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:25:51,735][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:25:51,735][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:25:51,736][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:25:51,736][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.1758Epoch 1/15: [                              ] 2/75 batches, loss: 0.4533Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3805Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3932Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4057Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3772Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3779Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3905Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3916Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3904Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3845Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3819Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3769Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3658Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3661Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3759Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3714Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3625Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3664Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3533Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3543Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3573Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3505Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3606Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3639Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3566Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3595Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3571Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3567Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3547Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3480Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3454Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3426Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3414Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3392Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3386Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3355Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3332Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3307Epoch 1/15: [================              ] 40/75 batches, loss: 0.3275Epoch 1/15: [================              ] 41/75 batches, loss: 0.3241Epoch 1/15: [================              ] 42/75 batches, loss: 0.3207Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3184Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3166Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3154Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3145Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3123Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3099Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3070Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3043Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3006Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3000Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2973Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2977Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2965Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2967Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2977Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2975Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2945Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2916Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2902Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2891Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2878Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2862Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2863Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2843Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2817Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2787Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2780Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2762Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2741Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2722Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2694Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2683Epoch 1/15: [==============================] 75/75 batches, loss: 0.2672
[2025-05-07 18:25:58,806][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2672
[2025-05-07 18:25:59,145][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0628, Metrics: {'mse': 0.06631624698638916, 'rmse': 0.2575194108924396, 'r2': -0.42556869983673096}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.0899Epoch 2/15: [                              ] 2/75 batches, loss: 0.1043Epoch 2/15: [=                             ] 3/75 batches, loss: 0.0866Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1468Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1314Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1289Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1368Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1372Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1335Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1305Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1337Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1335Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1322Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1325Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1291Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1276Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1277Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1281Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1259Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1275Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1279Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1302Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1324Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1322Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1340Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1369Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1377Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1350Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1352Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1341Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1332Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1323Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1354Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1356Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1351Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1325Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1315Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1320Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1333Epoch 2/15: [================              ] 40/75 batches, loss: 0.1322Epoch 2/15: [================              ] 41/75 batches, loss: 0.1335Epoch 2/15: [================              ] 42/75 batches, loss: 0.1331Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1332Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1321Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1321Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1331Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1330Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1325Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1315Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1300Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1301Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1311Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1322Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1316Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1305Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1344Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1331Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1319Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1324Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1311Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1305Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1305Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1307Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1315Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1306Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1302Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1307Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1303Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1293Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1290Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1282Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1275Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1273Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1268Epoch 2/15: [==============================] 75/75 batches, loss: 0.1256
[2025-05-07 18:26:01,870][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1256
[2025-05-07 18:26:02,202][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0975, Metrics: {'mse': 0.1036398634314537, 'rmse': 0.3219314576605612, 'r2': -1.2278964519500732}
[2025-05-07 18:26:02,203][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1970Epoch 3/15: [                              ] 2/75 batches, loss: 0.1777Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1482Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1296Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1108Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1165Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1151Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1050Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1047Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1008Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0992Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0956Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0951Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0973Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0959Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0947Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0929Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0907Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0890Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0877Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0856Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0867Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0878Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0918Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0941Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0929Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0915Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0902Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0913Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0901Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0899Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0914Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0917Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0912Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0902Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0895Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0890Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0886Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0891Epoch 3/15: [================              ] 40/75 batches, loss: 0.0881Epoch 3/15: [================              ] 41/75 batches, loss: 0.0889Epoch 3/15: [================              ] 42/75 batches, loss: 0.0887Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0884Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0877Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0868Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0861Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0861Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0865Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0866Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0855Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0857Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0853Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0844Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0849Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0845Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0835Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0828Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0828Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0827Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0822Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0816Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0817Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0818Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0821Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0816Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0815Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0816Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0818Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0815Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0816Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0812Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0808Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0804Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0805Epoch 3/15: [==============================] 75/75 batches, loss: 0.0813
[2025-05-07 18:26:04,508][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0813
[2025-05-07 18:26:04,807][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0531, Metrics: {'mse': 0.05561848357319832, 'rmse': 0.23583571309960313, 'r2': -0.19560396671295166}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1168Epoch 4/15: [                              ] 2/75 batches, loss: 0.0923Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0865Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0850Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0859Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0872Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0872Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0865Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0903Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0866Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0847Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0839Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0801Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0782Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0836Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0837Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0823Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0798Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0805Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0819Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0816Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0833Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0833Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0823Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0825Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0829Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0816Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0813Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0813Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0799Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0791Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0783Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0774Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0765Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0765Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0771Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0773Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0787Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0775Epoch 4/15: [================              ] 40/75 batches, loss: 0.0769Epoch 4/15: [================              ] 41/75 batches, loss: 0.0760Epoch 4/15: [================              ] 42/75 batches, loss: 0.0757Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0758Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0750Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0749Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0747Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0753Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0743Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0743Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0738Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0742Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0749Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0748Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0744Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0744Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0747Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0746Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0737Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0730Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0725Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0732Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0728Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0725Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0722Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0724Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0720Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0722Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0723Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0724Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0722Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0719Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0719Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0722Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0722Epoch 4/15: [==============================] 75/75 batches, loss: 0.0731
[2025-05-07 18:26:07,500][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0731
[2025-05-07 18:26:07,857][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0522, Metrics: {'mse': 0.0538792759180069, 'rmse': 0.2321190985636617, 'r2': -0.1582169532775879}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0934Epoch 5/15: [                              ] 2/75 batches, loss: 0.0797Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0942Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0888Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0837Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0786Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0828Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0772Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0765Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0724Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0713Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0720Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0707Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0741Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0719Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0703Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0709Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0709Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0713Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0704Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0695Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0680Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0687Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0680Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0678Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0672Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0671Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0682Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0669Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0659Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0661Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0657Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0649Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0646Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0641Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0633Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0629Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0629Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0639Epoch 5/15: [================              ] 40/75 batches, loss: 0.0638Epoch 5/15: [================              ] 41/75 batches, loss: 0.0638Epoch 5/15: [================              ] 42/75 batches, loss: 0.0639Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0637Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0631Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0627Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0629Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0639Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0638Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0637Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0633Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0635Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0626Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0622Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0616Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0617Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0615Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0613Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0607Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0608Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0603Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0598Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0598Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0596Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0594Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0597Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0598Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0595Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0591Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0586Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0582Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0580Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0585Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0581Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0578Epoch 5/15: [==============================] 75/75 batches, loss: 0.0576
[2025-05-07 18:26:10,537][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0576
[2025-05-07 18:26:10,886][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0471, Metrics: {'mse': 0.04843052849173546, 'rmse': 0.22006937199832116, 'r2': -0.04108786582946777}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0307Epoch 6/15: [                              ] 2/75 batches, loss: 0.0304Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0364Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0335Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0344Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0370Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0443Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0417Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0421Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0416Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0398Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0403Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0401Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0401Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0388Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0392Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0382Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0399Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0405Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0398Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0414Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0438Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0452Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0460Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0466Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0479Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0486Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0487Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0483Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0487Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0490Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0484Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0479Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0486Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0482Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0475Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0479Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0471Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0479Epoch 6/15: [================              ] 40/75 batches, loss: 0.0482Epoch 6/15: [================              ] 41/75 batches, loss: 0.0485Epoch 6/15: [================              ] 42/75 batches, loss: 0.0491Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0508Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0510Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0508Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0506Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0505Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0505Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0509Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0514Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0512Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0511Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0509Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0507Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0502Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0501Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0497Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0495Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0495Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0498Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0497Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0493Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0492Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0490Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0487Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0484Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0485Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0486Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0489Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0488Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0486Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0486Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0486Epoch 6/15: [==============================] 75/75 batches, loss: 0.0487
[2025-05-07 18:26:13,560][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0487
[2025-05-07 18:26:13,846][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0501, Metrics: {'mse': 0.05170311778783798, 'rmse': 0.2273831959222976, 'r2': -0.11143720149993896}
[2025-05-07 18:26:13,847][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0434Epoch 7/15: [                              ] 2/75 batches, loss: 0.0550Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0471Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0509Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0491Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0459Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0440Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0423Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0423Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0416Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0420Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0430Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0427Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0425Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0417Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0422Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0420Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0414Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0402Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0414Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0411Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0402Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0403Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0404Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0411Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0412Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0409Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0410Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0416Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0412Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0403Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0402Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0402Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0406Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0410Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0407Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0402Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0402Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0397Epoch 7/15: [================              ] 40/75 batches, loss: 0.0400Epoch 7/15: [================              ] 41/75 batches, loss: 0.0408Epoch 7/15: [================              ] 42/75 batches, loss: 0.0412Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0412Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0411Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0408Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0412Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0418Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0416Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0420Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0418Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0418Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0423Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0420Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0424Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0425Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0428Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0431Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0428Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0430Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0428Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0427Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0424Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0422Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0422Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0420Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0419Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0416Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0415Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0419Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0416Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0417Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0416Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0412Epoch 7/15: [==============================] 75/75 batches, loss: 0.0412
[2025-05-07 18:26:16,179][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0412
[2025-05-07 18:26:16,495][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0584, Metrics: {'mse': 0.06083056703209877, 'rmse': 0.2466385351726262, 'r2': -0.3076455593109131}
[2025-05-07 18:26:16,496][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0259Epoch 8/15: [                              ] 2/75 batches, loss: 0.0325Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0355Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0412Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0372Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0426Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0404Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0373Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0361Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0411Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0398Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0384Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0372Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0363Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0356Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0370Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0364Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0374Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0385Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0384Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0382Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0382Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0376Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0385Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0387Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0383Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0385Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0389Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0395Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0405Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0397Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0394Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0395Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0393Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0387Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0385Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0389Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0392Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0394Epoch 8/15: [================              ] 40/75 batches, loss: 0.0400Epoch 8/15: [================              ] 41/75 batches, loss: 0.0400Epoch 8/15: [================              ] 42/75 batches, loss: 0.0401Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0402Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0404Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0404Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0408Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0406Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0404Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0402Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0401Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0402Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0407Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0404Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0404Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0405Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0406Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0404Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0401Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0399Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0399Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0397Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0398Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0398Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0396Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0394Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0394Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0393Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0394Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0393Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0393Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0390Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0389Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0387Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0386Epoch 8/15: [==============================] 75/75 batches, loss: 0.0390
[2025-05-07 18:26:18,841][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0390
[2025-05-07 18:26:19,168][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0617, Metrics: {'mse': 0.06455828249454498, 'rmse': 0.2540832196240928, 'r2': -0.38777852058410645}
[2025-05-07 18:26:19,168][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0379Epoch 9/15: [                              ] 2/75 batches, loss: 0.0367Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0332Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0337Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0312Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0364Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0363Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0361Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0351Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0340Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0341Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0338Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0339Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0343Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0333Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0333Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0324Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0332Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0325Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0340Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0336Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0338Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0345Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0341Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0346Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0345Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0349Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0351Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0361Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0365Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0359Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0362Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0371Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0369Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0363Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0366Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0363Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0363Epoch 9/15: [================              ] 40/75 batches, loss: 0.0373Epoch 9/15: [================              ] 41/75 batches, loss: 0.0371Epoch 9/15: [================              ] 42/75 batches, loss: 0.0370Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0369Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0365Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0363Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0362Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0359Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0356Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0356Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0356Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0356Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0355Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0354Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0353Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0353Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0353Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0353Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0354Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0358Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0359Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0360Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0358Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0357Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0354Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0354Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0356Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0356Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0354Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0356Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0356Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0356Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0355Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0354Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0353Epoch 9/15: [==============================] 75/75 batches, loss: 0.0351
[2025-05-07 18:26:21,719][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0351
[2025-05-07 18:26:22,148][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0471, Metrics: {'mse': 0.048262935131788254, 'rmse': 0.21968826807954095, 'r2': -0.03748524188995361}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0439Epoch 10/15: [                              ] 2/75 batches, loss: 0.0463Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0383Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0373Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0413Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0414Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0414Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0411Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0409Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0388Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0384Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0393Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0389Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0388Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0372Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0367Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0359Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0350Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0349Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0347Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0350Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0346Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0360Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0365Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0363Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0370Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0368Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0376Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0374Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0374Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0368Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0377Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0379Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0379Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0377Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0372Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0382Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0378Epoch 10/15: [================              ] 40/75 batches, loss: 0.0374Epoch 10/15: [================              ] 41/75 batches, loss: 0.0373Epoch 10/15: [================              ] 42/75 batches, loss: 0.0371Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0379Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0375Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0370Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0368Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0364Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0369Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0365Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0360Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0357Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0354Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0353Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0352Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0354Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0351Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0350Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0354Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0353Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0352Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0352Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0351Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0347Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0348Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0347Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0347Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0348Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0347Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0345Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0344Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0343Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0342Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0343Epoch 10/15: [==============================] 75/75 batches, loss: 0.0344
[2025-05-07 18:26:24,841][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0344
[2025-05-07 18:26:25,212][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0541, Metrics: {'mse': 0.05618817359209061, 'rmse': 0.23704044716480477, 'r2': -0.20785033702850342}
[2025-05-07 18:26:25,212][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0264Epoch 11/15: [                              ] 2/75 batches, loss: 0.0235Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0220Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0289Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0312Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0334Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0329Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0309Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0336Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0323Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0325Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0322Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0313Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0331Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0322Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0318Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0314Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0312Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0314Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0324Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0327Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0324Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0341Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0341Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0346Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0350Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0343Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0346Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0345Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0340Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0340Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0335Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0331Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0334Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0331Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0328Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0325Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0322Epoch 11/15: [================              ] 40/75 batches, loss: 0.0322Epoch 11/15: [================              ] 41/75 batches, loss: 0.0323Epoch 11/15: [================              ] 42/75 batches, loss: 0.0323Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0320Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0319Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0318Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0321Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0320Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0318Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0318Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0314Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0312Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0315Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0313Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0311Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0310Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0309Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0309Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0308Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0307Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0306Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0304Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0309Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0308Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0307Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0309Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0310Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0310Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0307Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0309Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0310Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0310Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0313Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0311Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0309Epoch 11/15: [==============================] 75/75 batches, loss: 0.0309
[2025-05-07 18:26:27,540][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0309
[2025-05-07 18:26:27,848][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0473, Metrics: {'mse': 0.04833817854523659, 'rmse': 0.2198594517987266, 'r2': -0.03910267353057861}
[2025-05-07 18:26:27,848][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0290Epoch 12/15: [                              ] 2/75 batches, loss: 0.0301Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0349Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0316Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0305Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0304Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0291Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0297Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0294Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0293Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0289Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0292Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0279Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0272Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0272Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0268Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0265Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0257Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0263Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0262Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0267Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0266Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0266Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0264Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0263Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0268Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0268Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0271Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0276Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0276Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0281Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0280Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0279Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0276Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0275Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0274Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0272Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0278Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0276Epoch 12/15: [================              ] 40/75 batches, loss: 0.0278Epoch 12/15: [================              ] 41/75 batches, loss: 0.0278Epoch 12/15: [================              ] 42/75 batches, loss: 0.0281Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0278Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0276Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0278Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0283Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0285Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0285Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0287Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0289Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0291Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0293Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0296Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0298Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0303Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0302Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0302Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0303Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0301Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0303Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0302Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0303Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0302Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0301Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0302Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0302Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0302Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0302Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0300Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0298Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0296Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0295Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0295Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0293Epoch 12/15: [==============================] 75/75 batches, loss: 0.0291
[2025-05-07 18:26:30,173][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0291
[2025-05-07 18:26:30,474][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0449, Metrics: {'mse': 0.0456935353577137, 'rmse': 0.213760462569002, 'r2': 0.017747879028320312}
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0365Epoch 13/15: [                              ] 2/75 batches, loss: 0.0356Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0359Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0314Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0329Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0365Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0337Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0336Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0345Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0327Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0331Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0327Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0317Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0310Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0328Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0313Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0317Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0307Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0306Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0305Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0301Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0295Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0297Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0310Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0310Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0302Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0297Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0290Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0287Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0286Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0287Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0286Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0280Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0288Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0286Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0286Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0290Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0291Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0292Epoch 13/15: [================              ] 40/75 batches, loss: 0.0292Epoch 13/15: [================              ] 41/75 batches, loss: 0.0292Epoch 13/15: [================              ] 42/75 batches, loss: 0.0293Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0291Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0296Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0296Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0294Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0296Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0294Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0294Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0292Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0292Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0293Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0292Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0293Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0293Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0290Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0294Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0292Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0290Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0292Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0294Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0294Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0294Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0295Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0297Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0294Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0294Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0296Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0297Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0295Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0295Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0293Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0298Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0298Epoch 13/15: [==============================] 75/75 batches, loss: 0.0297
[2025-05-07 18:26:33,214][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0297
[2025-05-07 18:26:33,519][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0506, Metrics: {'mse': 0.05210861191153526, 'rmse': 0.22827310816549387, 'r2': -0.12015390396118164}
[2025-05-07 18:26:33,519][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0141Epoch 14/15: [                              ] 2/75 batches, loss: 0.0197Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0193Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0194Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0181Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0183Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0179Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0183Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0192Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0195Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0219Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0240Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0231Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0228Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0228Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0236Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0234Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0234Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0231Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0233Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0230Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0235Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0237Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0237Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0233Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0229Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0235Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0236Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0242Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0250Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0254Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0249Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0259Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0256Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0260Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0258Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0259Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0258Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0256Epoch 14/15: [================              ] 40/75 batches, loss: 0.0254Epoch 14/15: [================              ] 41/75 batches, loss: 0.0254Epoch 14/15: [================              ] 42/75 batches, loss: 0.0254Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0255Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0258Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0258Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0261Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0261Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0262Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0260Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0259Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0258Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0257Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0256Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0255Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0256Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0254Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0257Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0256Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0256Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0256Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0255Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0258Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0258Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0259Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0258Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0258Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0259Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0260Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0262Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0262Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0265Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0266Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0265Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0266Epoch 14/15: [==============================] 75/75 batches, loss: 0.0264
[2025-05-07 18:26:35,822][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0264
[2025-05-07 18:26:36,215][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0384, Metrics: {'mse': 0.038245148956775665, 'rmse': 0.1955636698284619, 'r2': 0.17786234617233276}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0334Epoch 15/15: [                              ] 2/75 batches, loss: 0.0268Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0248Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0233Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0231Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0251Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0263Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0265Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0259Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0260Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0261Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0271Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0264Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0266Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0261Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0263Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0262Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0265Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0270Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0275Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0273Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0281Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0279Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0278Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0283Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0274Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0275Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0276Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0272Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0271Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0271Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0272Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0268Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0272Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0268Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0266Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0269Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0267Epoch 15/15: [================              ] 40/75 batches, loss: 0.0267Epoch 15/15: [================              ] 41/75 batches, loss: 0.0266Epoch 15/15: [================              ] 42/75 batches, loss: 0.0265Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0262Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0262Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0262Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0266Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0264Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0266Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0265Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0263Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0268Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0272Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0277Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0276Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0274Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0275Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0275Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0276Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0276Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0276Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0277Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0281Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0280Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0279Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0278Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0281Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0281Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0278Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0278Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0277Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0276Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0276Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0276Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0275Epoch 15/15: [==============================] 75/75 batches, loss: 0.0273
[2025-05-07 18:26:38,985][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0273
[2025-05-07 18:26:39,289][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0426, Metrics: {'mse': 0.04359547421336174, 'rmse': 0.20879529260345345, 'r2': 0.06284898519515991}
[2025-05-07 18:26:39,290][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 18:26:39,290][src.training.lm_trainer][INFO] - Training completed in 43.67 seconds
[2025-05-07 18:26:39,290][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:26:42,423][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.013217497617006302, 'rmse': 0.1149673763160937, 'r2': 0.33704328536987305}
[2025-05-07 18:26:42,423][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.038245148956775665, 'rmse': 0.1955636698284619, 'r2': 0.17786234617233276}
[2025-05-07 18:26:42,423][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03991546109318733, 'rmse': 0.19978854094563916, 'r2': -0.009624004364013672}
[2025-05-07 18:26:44,066][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ru/ru/model.pt
[2025-05-07 18:26:44,067][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▄▃▃▁
wandb:     best_val_mse █▅▅▄▃▃▁
wandb:      best_val_r2 ▁▄▄▅▆▆█
wandb:    best_val_rmse █▆▅▄▄▃▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▆▆▆▆▅▅▆▆▆▇▆▇
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄█▃▃▂▂▃▄▂▃▂▂▂▁▁
wandb:          val_mse ▄█▃▃▂▂▃▄▂▃▂▂▂▁▂
wandb:           val_r2 ▅▁▆▆▇▇▆▅▇▆▇▇▇█▇
wandb:         val_rmse ▄█▃▃▂▃▄▄▂▃▂▂▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03839
wandb:     best_val_mse 0.03825
wandb:      best_val_r2 0.17786
wandb:    best_val_rmse 0.19556
wandb:            epoch 15
wandb:   final_test_mse 0.03992
wandb:    final_test_r2 -0.00962
wandb:  final_test_rmse 0.19979
wandb:  final_train_mse 0.01322
wandb:   final_train_r2 0.33704
wandb: final_train_rmse 0.11497
wandb:    final_val_mse 0.03825
wandb:     final_val_r2 0.17786
wandb:   final_val_rmse 0.19556
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0273
wandb:       train_time 43.6731
wandb:         val_loss 0.04258
wandb:          val_mse 0.0436
wandb:           val_r2 0.06285
wandb:         val_rmse 0.2088
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182536-3n0ef3y7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182536-3n0ef3y7/logs
Experiment probe_layer6_complexity_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer6/ja/ja/results.json for layer 6
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_complexity_control1_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:27:31,196][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ru
experiment_name: probe_layer2_complexity_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:27:31,196][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:27:31,196][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:27:31,196][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:27:31,201][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:27:31,201][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:27:37,721][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:27:40,143][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:27:40,143][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:27:40,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:27:40,677][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:27:41,175][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:27:41,184][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:27:41,185][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:27:41,193][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:27:41,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:27:41,525][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:27:41,561][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:27:41,562][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:27:41,562][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:27:41,564][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:27:41,766][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:27:41,960][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:27:41,994][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:27:41,996][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:27:41,996][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:27:41,999][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:27:41,999][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:27:41,999][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:27:41,999][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:27:41,999][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:27:42,000][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:27:42,000][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Sample label: 0.639226496219635
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:27:42,000][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:27:42,001][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:27:42,001][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:27:42,001][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:27:42,001][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:27:42,001][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:27:42,002][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:27:42,002][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:27:42,002][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:27:42,002][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:27:42,002][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:27:42,002][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:27:42,002][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:27:52,731][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:27:52,732][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:27:52,732][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:27:52,732][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:27:52,735][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:27:52,735][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:27:52,735][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:27:52,736][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:27:52,736][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:27:52,736][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:27:52,737][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2491Epoch 1/15: [                              ] 2/75 batches, loss: 0.4550Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3707Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4225Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4364Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4056Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4624Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4496Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4843Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4711Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4423Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4348Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4128Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4014Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3913Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3908Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3774Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3778Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3738Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3663Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3649Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3709Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3650Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3632Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3583Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3543Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3547Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3505Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3517Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3530Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3520Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3468Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3429Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3443Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3431Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3461Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3397Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3395Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3364Epoch 1/15: [================              ] 40/75 batches, loss: 0.3322Epoch 1/15: [================              ] 41/75 batches, loss: 0.3281Epoch 1/15: [================              ] 42/75 batches, loss: 0.3256Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3231Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3204Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3184Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3133Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3123Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3096Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3060Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3057Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3015Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2994Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2969Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2958Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2945Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2939Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2944Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2924Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2910Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2883Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2858Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2844Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2818Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2798Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2794Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2769Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2748Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2718Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2717Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2703Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2690Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2702Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2683Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2666Epoch 1/15: [==============================] 75/75 batches, loss: 0.2654
[2025-05-07 18:28:01,074][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2654
[2025-05-07 18:28:01,460][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0872, Metrics: {'mse': 0.09114568680524826, 'rmse': 0.3019034395386185, 'r2': -0.959315299987793}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1483Epoch 2/15: [                              ] 2/75 batches, loss: 0.1846Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1477Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1928Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1760Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1691Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1660Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1700Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1639Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1660Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1600Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1639Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1627Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1544Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1514Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1491Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1457Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1509Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1515Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1515Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1499Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1489Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1508Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1503Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1494Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1513Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1512Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1516Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1552Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1528Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1530Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1507Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1515Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1515Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1519Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1502Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1488Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1498Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1514Epoch 2/15: [================              ] 40/75 batches, loss: 0.1505Epoch 2/15: [================              ] 41/75 batches, loss: 0.1502Epoch 2/15: [================              ] 42/75 batches, loss: 0.1501Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1489Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1461Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1460Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1451Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1440Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1442Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1434Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1424Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1422Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1418Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1421Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1413Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1412Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1404Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1398Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1398Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1396Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1388Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1386Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1381Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1372Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1366Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1361Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1354Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1354Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1351Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1344Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1337Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1327Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1322Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1318Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1312Epoch 2/15: [==============================] 75/75 batches, loss: 0.1307
[2025-05-07 18:28:04,301][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1307
[2025-05-07 18:28:04,606][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1149, Metrics: {'mse': 0.12211452424526215, 'rmse': 0.3494488864558909, 'r2': -1.625037670135498}
[2025-05-07 18:28:04,606][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1169Epoch 3/15: [                              ] 2/75 batches, loss: 0.1786Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1686Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1421Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1293Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1231Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1275Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1202Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1155Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1143Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1137Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1101Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1125Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1096Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1078Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1036Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1009Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1002Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0985Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0959Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0943Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0950Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0947Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0953Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0939Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0936Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0933Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0923Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0933Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0939Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0953Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0946Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0938Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0938Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0935Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0935Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0928Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0930Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0922Epoch 3/15: [================              ] 40/75 batches, loss: 0.0916Epoch 3/15: [================              ] 41/75 batches, loss: 0.0921Epoch 3/15: [================              ] 42/75 batches, loss: 0.0928Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0921Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0927Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0920Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0934Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0937Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0942Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0946Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0937Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0933Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0924Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0917Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0919Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0912Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0913Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0911Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0909Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0901Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0894Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0886Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0892Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0896Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0898Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0889Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0886Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0880Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0876Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0876Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0875Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0875Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0873Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0869Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0864Epoch 3/15: [==============================] 75/75 batches, loss: 0.0877
[2025-05-07 18:28:06,957][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0877
[2025-05-07 18:28:07,250][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0738, Metrics: {'mse': 0.0762481540441513, 'rmse': 0.276130682909653, 'r2': -0.6390701532363892}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0732Epoch 4/15: [                              ] 2/75 batches, loss: 0.0764Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0635Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0652Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0618Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0648Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0733Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0744Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0797Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0808Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0828Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0845Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0817Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0804Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0790Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0790Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0775Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0770Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0782Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0771Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0772Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0790Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0790Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0794Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0803Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0800Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0797Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0800Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0801Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0798Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0805Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0792Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0784Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0774Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0770Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0769Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0760Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0772Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0767Epoch 4/15: [================              ] 40/75 batches, loss: 0.0777Epoch 4/15: [================              ] 41/75 batches, loss: 0.0780Epoch 4/15: [================              ] 42/75 batches, loss: 0.0777Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0779Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0778Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0779Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0776Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0770Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0764Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0765Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0756Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0765Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0770Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0762Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0764Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0760Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0758Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0758Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0757Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0770Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0773Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0773Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0766Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0766Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0763Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0762Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0759Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0756Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0755Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0757Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0751Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0747Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0740Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0741Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0743Epoch 4/15: [==============================] 75/75 batches, loss: 0.0740
[2025-05-07 18:28:09,969][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0740
[2025-05-07 18:28:10,300][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0732, Metrics: {'mse': 0.07492542266845703, 'rmse': 0.2737250859319566, 'r2': -0.6106359958648682}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0533Epoch 5/15: [                              ] 2/75 batches, loss: 0.0685Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0639Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0762Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0678Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0656Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0737Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0692Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0661Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0663Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0650Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0629Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0607Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0619Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0635Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0638Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0656Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0667Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0666Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0664Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0654Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0638Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0647Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0642Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0633Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0629Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0622Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0639Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0635Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0630Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0628Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0627Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0621Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0624Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0624Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0618Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0615Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0611Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0616Epoch 5/15: [================              ] 40/75 batches, loss: 0.0607Epoch 5/15: [================              ] 41/75 batches, loss: 0.0612Epoch 5/15: [================              ] 42/75 batches, loss: 0.0616Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0624Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0626Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0626Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0636Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0642Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0637Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0643Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0643Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0637Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0633Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0632Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0629Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0626Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0624Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0619Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0616Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0613Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0610Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0607Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0606Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0603Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0601Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0601Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0600Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0594Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0592Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0588Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0590Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0589Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0590Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0586Epoch 5/15: [==============================] 75/75 batches, loss: 0.0584
[2025-05-07 18:28:13,029][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0584
[2025-05-07 18:28:13,354][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0831, Metrics: {'mse': 0.08623600006103516, 'rmse': 0.29365966706552526, 'r2': -0.8537740707397461}
[2025-05-07 18:28:13,355][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0491Epoch 6/15: [                              ] 2/75 batches, loss: 0.0503Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0513Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0440Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0565Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0634Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0600Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0614Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0592Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0567Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0551Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0571Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0567Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0549Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0540Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0563Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0548Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0568Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0564Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0561Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0553Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0564Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0573Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0585Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0586Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0584Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0576Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0570Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0571Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0571Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0561Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0554Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0551Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0551Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0550Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0548Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0543Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0541Epoch 6/15: [================              ] 40/75 batches, loss: 0.0538Epoch 6/15: [================              ] 41/75 batches, loss: 0.0534Epoch 6/15: [================              ] 42/75 batches, loss: 0.0536Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0535Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0539Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0559Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0556Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0561Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0558Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0562Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0560Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0554Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0549Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0547Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0540Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0538Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0543Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0544Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0547Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0549Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0545Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0550Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0548Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0545Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0548Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0544Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0544Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0543Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0540Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0539Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0537Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0534Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0528Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0528Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0526Epoch 6/15: [==============================] 75/75 batches, loss: 0.0522
[2025-05-07 18:28:15,631][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0522
[2025-05-07 18:28:15,968][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0784, Metrics: {'mse': 0.08041766285896301, 'rmse': 0.28358008191507916, 'r2': -0.7287001609802246}
[2025-05-07 18:28:15,968][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0466Epoch 7/15: [                              ] 2/75 batches, loss: 0.0553Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0522Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0503Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0470Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0461Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0514Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0509Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0530Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0526Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0524Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0538Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0542Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0534Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0514Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0508Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0523Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0508Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0505Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0505Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0510Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0500Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0498Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0497Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0499Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0498Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0496Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0499Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0503Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0504Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0506Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0496Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0498Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0502Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0495Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0490Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0490Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0492Epoch 7/15: [================              ] 40/75 batches, loss: 0.0495Epoch 7/15: [================              ] 41/75 batches, loss: 0.0490Epoch 7/15: [================              ] 42/75 batches, loss: 0.0491Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0496Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0493Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0495Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0502Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0501Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0497Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0499Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0495Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0492Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0490Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0484Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0481Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0481Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0477Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0474Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0476Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0472Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0470Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0472Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0468Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0471Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0470Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0467Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0464Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0463Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0460Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0464Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0462Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0462Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0462Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0461Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0460Epoch 7/15: [==============================] 75/75 batches, loss: 0.0458
[2025-05-07 18:28:18,275][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0458
[2025-05-07 18:28:18,590][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0841, Metrics: {'mse': 0.08681513369083405, 'rmse': 0.2946440796806107, 'r2': -0.8662233352661133}
[2025-05-07 18:28:18,591][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0480Epoch 8/15: [                              ] 2/75 batches, loss: 0.0455Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0408Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0449Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0405Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0438Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0430Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0416Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0439Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0454Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0446Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0463Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0472Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0459Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0446Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0444Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0445Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0456Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0457Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0471Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0471Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0471Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0460Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0448Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0458Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0450Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0449Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0467Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0467Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0466Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0464Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0456Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0453Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0454Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0454Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0450Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0451Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0450Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0450Epoch 8/15: [================              ] 40/75 batches, loss: 0.0462Epoch 8/15: [================              ] 41/75 batches, loss: 0.0460Epoch 8/15: [================              ] 42/75 batches, loss: 0.0458Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0453Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0449Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0448Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0445Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0442Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0443Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0439Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0435Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0432Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0432Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0428Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0424Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0421Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0425Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0424Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0424Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0422Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0421Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0420Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0416Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0418Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0419Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0419Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0420Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0421Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0421Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0421Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0420Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0420Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0419Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0421Epoch 8/15: [==============================] 75/75 batches, loss: 0.0422
[2025-05-07 18:28:20,932][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0422
[2025-05-07 18:28:21,303][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0809, Metrics: {'mse': 0.08359237760305405, 'rmse': 0.2891234642900054, 'r2': -0.796945333480835}
[2025-05-07 18:28:21,303][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:28:21,303][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 18:28:21,304][src.training.lm_trainer][INFO] - Training completed in 23.30 seconds
[2025-05-07 18:28:21,304][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:28:24,370][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021709762513637543, 'rmse': 0.14734233103096184, 'r2': -0.08890759944915771}
[2025-05-07 18:28:24,370][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07492542266845703, 'rmse': 0.2737250859319566, 'r2': -0.6106359958648682}
[2025-05-07 18:28:24,370][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07351811975240707, 'rmse': 0.2711422500319843, 'r2': -0.8595715761184692}
[2025-05-07 18:28:26,030][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ru/ru/model.pt
[2025-05-07 18:28:26,031][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▁▅▅▄▅▄
wandb:       train_loss █▄▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▃█▁▁▃▂▃▂
wandb:          val_mse ▃█▁▁▃▂▃▂
wandb:           val_r2 ▆▁██▆▇▆▇
wandb:         val_rmse ▄█▁▁▃▂▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07316
wandb:     best_val_mse 0.07493
wandb:      best_val_r2 -0.61064
wandb:    best_val_rmse 0.27373
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.07352
wandb:    final_test_r2 -0.85957
wandb:  final_test_rmse 0.27114
wandb:  final_train_mse 0.02171
wandb:   final_train_r2 -0.08891
wandb: final_train_rmse 0.14734
wandb:    final_val_mse 0.07493
wandb:     final_val_r2 -0.61064
wandb:   final_val_rmse 0.27373
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04218
wandb:       train_time 23.29756
wandb:         val_loss 0.08095
wandb:          val_mse 0.08359
wandb:           val_r2 -0.79695
wandb:         val_rmse 0.28912
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182731-j5f1if4f
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182731-j5f1if4f/logs
Experiment probe_layer2_complexity_control1_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_complexity_control2_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control2_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:29:01,290][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ru
experiment_name: probe_layer2_complexity_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:29:01,291][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:29:01,291][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:29:01,291][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:29:01,295][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:29:01,295][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:29:05,179][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:29:07,696][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:29:07,696][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:29:07,984][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:29:08,135][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:29:08,483][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:29:08,492][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:29:08,493][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:29:08,497][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:29:08,571][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:29:08,695][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:29:08,732][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:29:08,733][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:29:08,734][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:29:08,736][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:29:08,855][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:29:09,001][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:29:09,030][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:29:09,031][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:29:09,031][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:29:09,034][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:29:09,035][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:29:09,035][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:29:09,035][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:29:09,035][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:29:09,035][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:29:09,036][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Sample label: 0.28674033284187317
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:29:09,036][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:29:09,036][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:29:09,036][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:29:09,037][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:29:09,037][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:29:09,037][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:29:09,038][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:29:09,038][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:29:09,038][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:29:18,343][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:29:18,344][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:29:18,344][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:29:18,344][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:29:18,347][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:29:18,347][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:29:18,347][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:29:18,347][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:29:18,348][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:29:18,348][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:29:18,349][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2886Epoch 1/15: [                              ] 2/75 batches, loss: 0.4299Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3649Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4076Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4164Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3973Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4532Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4499Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4857Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4702Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4427Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4384Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4187Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4056Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3916Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3857Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3721Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3814Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3813Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3759Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3779Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3822Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3738Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3727Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3676Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3653Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3643Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3591Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3574Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3559Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3544Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3514Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3480Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3489Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3472Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3496Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3437Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3436Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3400Epoch 1/15: [================              ] 40/75 batches, loss: 0.3368Epoch 1/15: [================              ] 41/75 batches, loss: 0.3334Epoch 1/15: [================              ] 42/75 batches, loss: 0.3312Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3290Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3254Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3223Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3176Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3156Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3126Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3103Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3105Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3082Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3054Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3040Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3021Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3001Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2994Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3009Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3008Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2994Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2971Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2952Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2936Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2906Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2890Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2878Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2854Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2826Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2793Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2792Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2774Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2748Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2738Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2720Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2696Epoch 1/15: [==============================] 75/75 batches, loss: 0.2682
[2025-05-07 18:29:28,597][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2682
[2025-05-07 18:29:29,041][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0967, Metrics: {'mse': 0.1014472097158432, 'rmse': 0.3185077859579624, 'r2': -1.1807622909545898}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2140Epoch 2/15: [                              ] 2/75 batches, loss: 0.2150Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1704Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1997Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1782Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1706Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1716Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1631Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1559Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1613Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1574Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1581Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1612Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1565Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1515Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1484Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1443Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1469Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1457Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1463Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1466Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1467Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1511Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1507Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1500Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1507Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1500Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1501Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1544Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1516Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1502Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1500Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1510Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1505Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1510Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1489Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1480Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1487Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1490Epoch 2/15: [================              ] 40/75 batches, loss: 0.1477Epoch 2/15: [================              ] 41/75 batches, loss: 0.1455Epoch 2/15: [================              ] 42/75 batches, loss: 0.1456Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1452Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1447Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1436Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1427Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1421Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1413Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1410Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1400Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1401Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1396Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1399Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1389Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1381Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1384Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1372Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1374Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1372Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1362Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1365Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1358Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1351Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1348Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1341Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1340Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1340Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1331Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1322Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1314Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1308Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1302Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1299Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1299Epoch 2/15: [==============================] 75/75 batches, loss: 0.1296
[2025-05-07 18:29:32,160][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1296
[2025-05-07 18:29:32,509][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1157, Metrics: {'mse': 0.12306475639343262, 'rmse': 0.3508058671023514, 'r2': -1.6454641819000244}
[2025-05-07 18:29:32,509][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1075Epoch 3/15: [                              ] 2/75 batches, loss: 0.1202Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1141Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1006Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1006Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1050Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1073Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1010Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1047Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1065Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1030Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1007Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1019Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1035Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1012Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0976Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0976Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0966Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0963Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0933Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0908Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0908Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0915Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0938Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0928Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0914Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0909Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0907Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0921Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0920Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0935Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0930Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0920Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0917Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0925Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0911Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0909Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0907Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0895Epoch 3/15: [================              ] 40/75 batches, loss: 0.0898Epoch 3/15: [================              ] 41/75 batches, loss: 0.0906Epoch 3/15: [================              ] 42/75 batches, loss: 0.0911Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0910Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0915Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0910Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0912Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0914Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0935Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0939Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0938Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0930Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0919Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0921Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0913Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0914Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0911Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0911Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0904Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0909Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0903Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0911Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0915Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0916Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0906Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0903Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0901Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0896Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0900Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0896Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0897Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0892Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0887Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0886Epoch 3/15: [==============================] 75/75 batches, loss: 0.0891
[2025-05-07 18:29:34,980][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0891
[2025-05-07 18:29:35,304][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0760, Metrics: {'mse': 0.07877297699451447, 'rmse': 0.2806652400895317, 'r2': -0.6933449506759644}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0611Epoch 4/15: [                              ] 2/75 batches, loss: 0.0669Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0596Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0590Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0572Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0599Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0641Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0623Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0678Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0667Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0703Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0729Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0742Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0737Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0744Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0749Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0730Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0704Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0714Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0714Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0728Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0748Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0761Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0772Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0785Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0779Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0772Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0788Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0804Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0789Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0797Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0783Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0776Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0772Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0764Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0768Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0763Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0761Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0752Epoch 4/15: [================              ] 40/75 batches, loss: 0.0744Epoch 4/15: [================              ] 41/75 batches, loss: 0.0749Epoch 4/15: [================              ] 42/75 batches, loss: 0.0751Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0764Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0762Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0769Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0768Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0765Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0761Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0756Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0747Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0758Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0757Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0756Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0752Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0752Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0748Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0743Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0742Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0744Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0737Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0755Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0751Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0751Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0746Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0743Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0738Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0735Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0732Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0729Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0726Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0722Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0723Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0719Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0726Epoch 4/15: [==============================] 75/75 batches, loss: 0.0730
[2025-05-07 18:29:38,135][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0730
[2025-05-07 18:29:38,517][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0734, Metrics: {'mse': 0.07512568682432175, 'rmse': 0.27409065439069924, 'r2': -0.6149410009384155}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0382Epoch 5/15: [                              ] 2/75 batches, loss: 0.0513Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0556Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0664Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0627Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0629Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0639Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0653Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0617Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0580Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0604Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0598Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0593Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0590Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0587Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0617Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0624Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0626Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0644Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0633Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0617Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0604Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0612Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0609Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0619Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0615Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0618Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0622Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0620Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0615Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0621Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0628Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0624Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0630Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0627Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0619Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0613Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0600Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0595Epoch 5/15: [================              ] 40/75 batches, loss: 0.0597Epoch 5/15: [================              ] 41/75 batches, loss: 0.0603Epoch 5/15: [================              ] 42/75 batches, loss: 0.0601Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0609Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0608Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0611Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0613Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0613Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0615Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0612Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0614Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0620Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0618Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0617Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0614Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0614Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0610Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0609Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0609Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0605Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0605Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0605Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0604Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0601Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0598Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0597Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0602Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0599Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0594Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0592Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0588Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0586Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0585Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0585Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0580Epoch 5/15: [==============================] 75/75 batches, loss: 0.0580
[2025-05-07 18:29:41,343][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0580
[2025-05-07 18:29:41,694][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0791, Metrics: {'mse': 0.08192329853773117, 'rmse': 0.2862224633702449, 'r2': -0.7610659599304199}
[2025-05-07 18:29:41,695][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0512Epoch 6/15: [                              ] 2/75 batches, loss: 0.0466Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0507Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0454Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0509Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0605Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0605Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0627Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0616Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0599Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0596Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0582Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0553Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0543Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0538Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0534Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0523Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0525Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0511Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0511Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0519Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0532Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0525Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0540Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0544Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0568Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0565Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0565Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0571Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0576Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0587Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0579Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0575Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0578Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0583Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0587Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0576Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0587Epoch 6/15: [================              ] 40/75 batches, loss: 0.0582Epoch 6/15: [================              ] 41/75 batches, loss: 0.0576Epoch 6/15: [================              ] 42/75 batches, loss: 0.0576Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0576Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0579Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0593Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0588Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0587Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0583Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0588Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0589Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0584Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0582Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0576Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0568Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0565Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0566Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0566Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0564Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0568Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0571Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0575Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0569Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0572Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0571Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0575Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0577Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0581Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0579Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0578Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0576Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0571Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0566Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0560Epoch 6/15: [==============================] 75/75 batches, loss: 0.0555
[2025-05-07 18:29:44,119][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0555
[2025-05-07 18:29:44,629][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0827, Metrics: {'mse': 0.08559901267290115, 'rmse': 0.29257308945441507, 'r2': -0.840080976486206}
[2025-05-07 18:29:44,629][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0665Epoch 7/15: [                              ] 2/75 batches, loss: 0.0623Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0498Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0478Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0428Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0408Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0434Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0436Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0413Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0400Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0388Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0444Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0466Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0479Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0465Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0472Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0483Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0499Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0491Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0482Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0488Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0489Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0494Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0497Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0494Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0489Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0494Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0489Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0495Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0490Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0486Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0486Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0484Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0485Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0486Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0482Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0480Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0483Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0477Epoch 7/15: [================              ] 40/75 batches, loss: 0.0479Epoch 7/15: [================              ] 41/75 batches, loss: 0.0484Epoch 7/15: [================              ] 42/75 batches, loss: 0.0487Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0486Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0486Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0486Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0485Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0485Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0484Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0486Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0481Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0483Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0482Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0490Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0493Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0497Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0494Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0492Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0493Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0492Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0488Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0488Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0488Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0488Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0485Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0482Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0484Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0481Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0479Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0477Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0473Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0472Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0470Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0470Epoch 7/15: [==============================] 75/75 batches, loss: 0.0472
[2025-05-07 18:29:46,998][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0472
[2025-05-07 18:29:47,462][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0823, Metrics: {'mse': 0.08501054346561432, 'rmse': 0.2915656760759303, 'r2': -0.8274309635162354}
[2025-05-07 18:29:47,463][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0660Epoch 8/15: [                              ] 2/75 batches, loss: 0.0612Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0553Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0543Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0517Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0577Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0553Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0524Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0539Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0545Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0533Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0513Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0513Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0506Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0514Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0503Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0500Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0499Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0485Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0483Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0474Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0468Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0468Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0467Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0474Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0465Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0461Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0462Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0465Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0466Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0462Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0458Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0461Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0455Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0451Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0445Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0443Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0445Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0449Epoch 8/15: [================              ] 40/75 batches, loss: 0.0455Epoch 8/15: [================              ] 41/75 batches, loss: 0.0450Epoch 8/15: [================              ] 42/75 batches, loss: 0.0450Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0451Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0454Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0451Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0454Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0457Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0459Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0463Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0462Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0458Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0460Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0454Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0450Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0446Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0444Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0441Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0439Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0438Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0442Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0438Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0436Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0436Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0440Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0439Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0437Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0435Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0434Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0433Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0432Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0429Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0427Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0431Epoch 8/15: [==============================] 75/75 batches, loss: 0.0431
[2025-05-07 18:29:49,886][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0431
[2025-05-07 18:29:50,264][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0792, Metrics: {'mse': 0.08145219087600708, 'rmse': 0.2853983021603441, 'r2': -0.7509387731552124}
[2025-05-07 18:29:50,265][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:29:50,265][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 18:29:50,265][src.training.lm_trainer][INFO] - Training completed in 24.76 seconds
[2025-05-07 18:29:50,265][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:29:53,511][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02207065001130104, 'rmse': 0.14856193998228834, 'r2': -0.10700857639312744}
[2025-05-07 18:29:53,511][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07512568682432175, 'rmse': 0.27409065439069924, 'r2': -0.6149410009384155}
[2025-05-07 18:29:53,512][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07397358864545822, 'rmse': 0.27198086080726014, 'r2': -0.8710922002792358}
[2025-05-07 18:29:55,170][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ru/ru/model.pt
[2025-05-07 18:29:55,171][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁
wandb:     best_val_mse █▂▁
wandb:      best_val_r2 ▁▇█
wandb:    best_val_rmse █▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▅▅▅▄▄
wandb:       train_loss █▄▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▁▁▂▃▂▂
wandb:          val_mse ▅█▂▁▂▃▂▂
wandb:           val_r2 ▄▁▇█▇▆▇▇
wandb:         val_rmse ▅█▂▁▂▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07338
wandb:     best_val_mse 0.07513
wandb:      best_val_r2 -0.61494
wandb:    best_val_rmse 0.27409
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.07397
wandb:    final_test_r2 -0.87109
wandb:  final_test_rmse 0.27198
wandb:  final_train_mse 0.02207
wandb:   final_train_r2 -0.10701
wandb: final_train_rmse 0.14856
wandb:    final_val_mse 0.07513
wandb:     final_val_r2 -0.61494
wandb:   final_val_rmse 0.27409
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04307
wandb:       train_time 24.76059
wandb:         val_loss 0.0792
wandb:          val_mse 0.08145
wandb:           val_r2 -0.75094
wandb:         val_rmse 0.2854
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182901-bvl5a3pl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_182901-bvl5a3pl/logs
Experiment probe_layer2_complexity_control2_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_complexity_control3_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control3_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:30:43,847][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ru
experiment_name: probe_layer2_complexity_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:30:43,847][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:30:43,847][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:30:43,847][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:30:43,851][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:30:43,851][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:30:47,320][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:30:49,806][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:30:49,806][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:30:49,967][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:30:50,060][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:30:50,426][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:30:50,434][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:30:50,435][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:30:50,438][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:30:50,550][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:30:50,636][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:30:50,692][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:30:50,694][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:30:50,694][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:30:50,716][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:30:50,827][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:30:50,933][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:30:50,956][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:30:50,958][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:30:50,958][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:30:50,960][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:30:50,960][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:30:50,960][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:30:50,960][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:30:50,960][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:30:50,961][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:30:50,961][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Sample label: 0.4091160297393799
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:30:50,961][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:30:50,962][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:30:50,962][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:30:50,962][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:30:50,962][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:30:50,963][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:30:50,963][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:30:50,963][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:30:50,963][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:30:50,963][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:30:50,963][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:30:50,963][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:30:50,964][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:30:58,061][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:30:58,062][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:30:58,062][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:30:58,062][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:30:58,065][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:30:58,066][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:30:58,066][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:30:58,066][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:30:58,066][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:30:58,067][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:30:58,067][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2528Epoch 1/15: [                              ] 2/75 batches, loss: 0.4115Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3424Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4108Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4393Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4094Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4584Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4474Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4867Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4681Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4475Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4382Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4180Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4032Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3937Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3904Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3789Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3866Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3854Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3775Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3772Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3849Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3791Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3738Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3686Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3631Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3618Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3593Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3613Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3655Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3630Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3584Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3551Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3553Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3544Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3571Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3502Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3484Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3466Epoch 1/15: [================              ] 40/75 batches, loss: 0.3444Epoch 1/15: [================              ] 41/75 batches, loss: 0.3430Epoch 1/15: [================              ] 42/75 batches, loss: 0.3403Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3365Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3343Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3315Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3275Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3258Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3247Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3200Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3194Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3167Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3144Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3118Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3101Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3094Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3076Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3092Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3073Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3057Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3034Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3016Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2994Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2971Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2953Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2944Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2920Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2887Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2853Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2855Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2841Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2822Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2814Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2792Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2776Epoch 1/15: [==============================] 75/75 batches, loss: 0.2751
[2025-05-07 18:31:05,253][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2751
[2025-05-07 18:31:05,577][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0925, Metrics: {'mse': 0.09618321061134338, 'rmse': 0.3101341816236053, 'r2': -1.0676043033599854}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1127Epoch 2/15: [                              ] 2/75 batches, loss: 0.1427Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1343Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1594Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1435Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1373Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1442Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1433Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1361Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1417Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1406Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1454Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1460Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1438Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1390Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1376Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1367Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1415Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1432Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1395Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1394Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1410Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1442Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1439Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1437Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1445Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1435Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1416Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1436Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1410Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1398Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1386Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1394Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1396Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1406Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1393Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1400Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1419Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1427Epoch 2/15: [================              ] 40/75 batches, loss: 0.1409Epoch 2/15: [================              ] 41/75 batches, loss: 0.1405Epoch 2/15: [================              ] 42/75 batches, loss: 0.1407Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1395Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1379Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1366Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1367Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1353Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1354Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1353Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1341Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1335Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1327Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1335Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1333Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1323Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1323Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1318Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1312Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1312Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1302Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1299Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1289Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1293Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1294Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1284Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1274Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1267Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1266Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1259Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1250Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1243Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1238Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1242Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1240Epoch 2/15: [==============================] 75/75 batches, loss: 0.1231
[2025-05-07 18:31:08,281][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1231
[2025-05-07 18:31:08,535][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0997, Metrics: {'mse': 0.10479399561882019, 'rmse': 0.32371900719423347, 'r2': -1.252706527709961}
[2025-05-07 18:31:08,536][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1278Epoch 3/15: [                              ] 2/75 batches, loss: 0.1326Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1357Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1194Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1125Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1143Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1241Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1171Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1116Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1104Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1078Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1058Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1105Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1076Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1078Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1045Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1034Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1060Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1053Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1039Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1028Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1011Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1023Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1033Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1034Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1027Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1010Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0993Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0988Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0991Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0992Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0987Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0975Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0975Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0969Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0957Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0956Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0949Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0942Epoch 3/15: [================              ] 40/75 batches, loss: 0.0945Epoch 3/15: [================              ] 41/75 batches, loss: 0.0951Epoch 3/15: [================              ] 42/75 batches, loss: 0.0947Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0946Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0949Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0946Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0963Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0970Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0974Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0975Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0966Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0963Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0949Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0942Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0950Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0939Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0936Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0930Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0934Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0926Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0919Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0909Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0916Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0913Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0921Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0914Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0906Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0905Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0906Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0909Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0904Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0910Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0906Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0903Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0899Epoch 3/15: [==============================] 75/75 batches, loss: 0.0910
[2025-05-07 18:31:10,845][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0910
[2025-05-07 18:31:11,139][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0801, Metrics: {'mse': 0.08247590065002441, 'rmse': 0.2871861776792616, 'r2': -0.7729450464248657}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0397Epoch 4/15: [                              ] 2/75 batches, loss: 0.0596Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0619Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0681Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0696Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0685Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0701Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0684Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0689Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0681Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0699Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0676Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0670Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0650Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0640Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0669Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0653Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0660Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0670Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0697Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0698Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0721Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0742Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0740Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0767Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0767Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0767Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0760Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0763Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0764Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0765Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0750Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0756Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0750Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0752Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0749Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0741Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0757Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0749Epoch 4/15: [================              ] 40/75 batches, loss: 0.0742Epoch 4/15: [================              ] 41/75 batches, loss: 0.0752Epoch 4/15: [================              ] 42/75 batches, loss: 0.0748Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0757Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0749Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0756Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0755Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0755Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0754Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0751Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0744Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0755Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0755Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0751Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0754Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0751Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0750Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0749Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0753Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0753Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0752Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0754Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0745Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0748Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0746Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0746Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0741Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0736Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0733Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0732Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0729Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0723Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0720Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0720Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0722Epoch 4/15: [==============================] 75/75 batches, loss: 0.0720
[2025-05-07 18:31:13,900][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0720
[2025-05-07 18:31:14,271][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0835, Metrics: {'mse': 0.08614856749773026, 'rmse': 0.29351076214975536, 'r2': -0.8518944978713989}
[2025-05-07 18:31:14,271][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0478Epoch 5/15: [                              ] 2/75 batches, loss: 0.0515Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0712Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0709Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0670Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0658Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0742Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0716Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0687Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0662Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0646Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0648Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0649Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0669Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0682Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0690Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0729Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0727Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0717Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0703Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0691Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0704Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0695Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0711Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0701Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0699Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0692Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0688Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0678Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0668Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0672Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0672Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0669Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0684Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0674Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0667Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0666Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0659Epoch 5/15: [================              ] 40/75 batches, loss: 0.0656Epoch 5/15: [================              ] 41/75 batches, loss: 0.0655Epoch 5/15: [================              ] 42/75 batches, loss: 0.0659Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0656Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0652Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0649Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0645Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0640Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0639Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0636Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0643Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0639Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0635Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0631Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0625Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0626Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0625Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0624Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0624Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0619Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0614Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0614Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0615Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0611Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0612Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0610Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0617Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0614Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0616Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0611Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0605Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0605Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0607Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0604Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0603Epoch 5/15: [==============================] 75/75 batches, loss: 0.0608
[2025-05-07 18:31:16,570][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0608
[2025-05-07 18:31:16,863][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0733, Metrics: {'mse': 0.07443014532327652, 'rmse': 0.2728188874020208, 'r2': -0.5999892950057983}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0289Epoch 6/15: [                              ] 2/75 batches, loss: 0.0480Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0492Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0516Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0528Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0588Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0579Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0591Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0559Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0553Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0565Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0569Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0548Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0530Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0534Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0537Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0517Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0538Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0556Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0566Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0565Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0561Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0557Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0553Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0556Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0551Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0548Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0541Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0536Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0542Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0561Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0554Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0548Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0545Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0544Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0547Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0545Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0545Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0551Epoch 6/15: [================              ] 40/75 batches, loss: 0.0543Epoch 6/15: [================              ] 41/75 batches, loss: 0.0543Epoch 6/15: [================              ] 42/75 batches, loss: 0.0547Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0545Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0550Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0557Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0554Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0559Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0562Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0560Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0559Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0555Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0552Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0553Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0547Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0541Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0546Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0549Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0544Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0544Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0542Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0546Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0546Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0544Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0542Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0540Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0541Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0540Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0541Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0538Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0540Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0543Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0541Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0538Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0538Epoch 6/15: [==============================] 75/75 batches, loss: 0.0537
[2025-05-07 18:31:19,549][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0537
[2025-05-07 18:31:19,838][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0781, Metrics: {'mse': 0.07994643598794937, 'rmse': 0.2827480079292326, 'r2': -0.718570351600647}
[2025-05-07 18:31:19,839][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0496Epoch 7/15: [                              ] 2/75 batches, loss: 0.0607Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0509Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0578Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0589Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0529Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0499Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0496Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0496Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0488Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0485Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0484Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0478Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0491Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0478Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0499Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0491Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0488Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0480Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0492Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0494Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0506Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0503Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0517Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0514Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0517Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0525Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0520Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0525Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0522Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0521Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0521Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0517Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0522Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0526Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0520Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0519Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0524Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0516Epoch 7/15: [================              ] 40/75 batches, loss: 0.0518Epoch 7/15: [================              ] 41/75 batches, loss: 0.0519Epoch 7/15: [================              ] 42/75 batches, loss: 0.0525Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0530Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0536Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0534Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0541Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0543Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0544Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0545Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0542Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0537Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0533Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0526Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0523Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0523Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0519Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0518Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0518Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0517Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0518Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0513Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0510Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0509Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0506Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0503Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0504Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0504Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0501Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0498Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0497Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0493Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0490Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0490Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0488Epoch 7/15: [==============================] 75/75 batches, loss: 0.0486
[2025-05-07 18:31:22,236][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0486
[2025-05-07 18:31:22,586][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0844, Metrics: {'mse': 0.08707311749458313, 'rmse': 0.29508154380540835, 'r2': -0.8717690706253052}
[2025-05-07 18:31:22,587][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0525Epoch 8/15: [                              ] 2/75 batches, loss: 0.0450Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0476Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0512Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0494Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0515Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0505Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0506Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0484Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0487Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0458Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0460Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0462Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0447Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0455Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0445Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0443Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0439Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0428Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0428Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0422Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0421Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0423Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0428Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0433Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0428Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0426Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0426Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0424Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0424Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0419Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0416Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0417Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0416Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0410Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0406Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0405Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0403Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0412Epoch 8/15: [================              ] 40/75 batches, loss: 0.0412Epoch 8/15: [================              ] 41/75 batches, loss: 0.0414Epoch 8/15: [================              ] 42/75 batches, loss: 0.0417Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0416Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0414Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0412Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0420Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0421Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0419Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0420Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0424Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0423Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0428Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0427Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0427Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0426Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0425Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0426Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0423Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0421Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0420Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0425Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0428Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0431Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0428Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0425Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0427Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0425Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0423Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0426Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0425Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 8/15: [==============================] 75/75 batches, loss: 0.0421
[2025-05-07 18:31:24,964][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0421
[2025-05-07 18:31:25,326][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0841, Metrics: {'mse': 0.08678824454545975, 'rmse': 0.2945984462712927, 'r2': -0.8656454086303711}
[2025-05-07 18:31:25,326][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0191Epoch 9/15: [                              ] 2/75 batches, loss: 0.0256Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0322Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0321Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0315Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0304Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0323Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0365Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0367Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0360Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0355Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0349Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0347Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0348Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0342Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0344Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0346Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0342Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0368Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0364Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0363Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0359Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0354Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0362Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0370Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0366Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0365Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0368Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0369Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0369Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0371Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0369Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0369Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0377Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0378Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0372Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0382Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0378Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0392Epoch 9/15: [================              ] 40/75 batches, loss: 0.0391Epoch 9/15: [================              ] 41/75 batches, loss: 0.0388Epoch 9/15: [================              ] 42/75 batches, loss: 0.0388Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0387Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0381Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0381Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0377Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0374Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0373Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0374Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0374Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0377Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0378Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0387Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0384Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0385Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0386Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0386Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0384Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0388Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0389Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0393Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0393Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0394Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0396Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0394Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0396Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0394Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0391Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0393Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0392Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0390Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0394Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0393Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0391Epoch 9/15: [==============================] 75/75 batches, loss: 0.0389
[2025-05-07 18:31:27,876][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0389
[2025-05-07 18:31:28,239][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0733, Metrics: {'mse': 0.07427295297384262, 'rmse': 0.272530645935173, 'r2': -0.5966100692749023}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0452Epoch 10/15: [                              ] 2/75 batches, loss: 0.0448Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0424Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0397Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0420Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0382Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0361Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0380Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0389Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0402Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0399Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0387Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0382Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0384Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0375Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0397Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0382Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0377Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0376Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0372Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0367Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0372Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0365Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0359Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0354Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0350Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0356Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0358Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0356Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0364Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0365Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0364Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0358Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0356Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0358Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0365Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0374Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0372Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0375Epoch 10/15: [================              ] 40/75 batches, loss: 0.0375Epoch 10/15: [================              ] 41/75 batches, loss: 0.0375Epoch 10/15: [================              ] 42/75 batches, loss: 0.0372Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0370Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0366Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0362Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0366Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0372Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0371Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0370Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0371Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0373Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0371Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0369Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0366Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0366Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0365Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0362Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0359Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0358Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0359Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0359Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0357Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0358Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0354Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0355Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0355Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0355Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0363Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0364Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0362Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0363Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0365Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0363Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0363Epoch 10/15: [==============================] 75/75 batches, loss: 0.0364
[2025-05-07 18:31:31,046][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0364
[2025-05-07 18:31:31,446][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0841, Metrics: {'mse': 0.08661869913339615, 'rmse': 0.29431054879734797, 'r2': -0.8620008230209351}
[2025-05-07 18:31:31,446][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0393Epoch 11/15: [                              ] 2/75 batches, loss: 0.0282Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0301Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0306Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0290Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0298Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0305Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0315Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0311Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0304Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0315Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0322Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0336Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0322Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0326Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0328Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0327Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0337Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0340Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0338Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0339Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0342Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0336Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0331Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0326Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0328Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0339Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0334Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0340Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0334Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0335Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0330Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0341Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0335Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0337Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0338Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0338Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0336Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0334Epoch 11/15: [================              ] 40/75 batches, loss: 0.0343Epoch 11/15: [================              ] 41/75 batches, loss: 0.0347Epoch 11/15: [================              ] 42/75 batches, loss: 0.0343Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0339Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0341Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0343Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0344Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0348Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0352Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0354Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0352Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0351Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0352Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0351Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0352Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0354Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0354Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0351Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0350Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0350Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0351Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0350Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0348Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0346Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0352Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0350Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0349Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0347Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0344Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0344Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0346Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0344Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0345Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0344Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0347Epoch 11/15: [==============================] 75/75 batches, loss: 0.0348
[2025-05-07 18:31:34,049][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0348
[2025-05-07 18:31:34,535][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0658, Metrics: {'mse': 0.06539115309715271, 'rmse': 0.255716939402052, 'r2': -0.40568244457244873}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0515Epoch 12/15: [                              ] 2/75 batches, loss: 0.0489Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0453Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0505Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0433Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0388Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0364Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0351Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0334Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0348Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0353Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0363Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0375Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0363Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0371Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0371Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0366Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0365Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0358Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0365Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0362Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0358Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0354Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0351Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0357Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0363Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0363Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0369Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0376Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0379Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0373Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0373Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0369Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0367Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0367Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0368Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0364Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0359Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0363Epoch 12/15: [================              ] 40/75 batches, loss: 0.0358Epoch 12/15: [================              ] 41/75 batches, loss: 0.0354Epoch 12/15: [================              ] 42/75 batches, loss: 0.0359Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0361Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0359Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0359Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0355Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0353Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0354Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0353Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0350Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0354Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0355Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0352Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0351Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0350Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0352Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0352Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0351Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0348Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0348Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0349Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0347Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0345Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0345Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0345Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0343Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0340Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0340Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0342Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0340Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0340Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0337Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0338Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0338Epoch 12/15: [==============================] 75/75 batches, loss: 0.0335
[2025-05-07 18:31:37,500][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0335
[2025-05-07 18:31:38,426][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0732, Metrics: {'mse': 0.07397966086864471, 'rmse': 0.27199202353864116, 'r2': -0.5903054475784302}
[2025-05-07 18:31:38,427][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0424Epoch 13/15: [                              ] 2/75 batches, loss: 0.0457Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0459Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0472Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0421Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0389Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0351Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0334Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0336Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0372Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0363Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0354Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0339Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0340Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0351Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0348Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0342Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0344Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0336Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0333Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0338Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0332Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0325Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0335Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0334Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0336Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0331Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0336Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0333Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0334Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0332Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0336Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0334Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0343Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0344Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0344Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0340Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0335Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0331Epoch 13/15: [================              ] 40/75 batches, loss: 0.0329Epoch 13/15: [================              ] 41/75 batches, loss: 0.0332Epoch 13/15: [================              ] 42/75 batches, loss: 0.0330Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0327Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0325Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0328Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0326Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0330Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0333Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0332Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0329Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0327Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0334Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0334Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0338Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0340Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0339Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0346Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0347Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0345Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0345Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0342Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0344Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0342Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0341Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0339Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0337Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0337Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0334Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0336Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0336Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0337Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0335Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0334Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0332Epoch 13/15: [==============================] 75/75 batches, loss: 0.0331
[2025-05-07 18:31:41,022][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0331
[2025-05-07 18:31:41,424][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0833, Metrics: {'mse': 0.08559662103652954, 'rmse': 0.29256900217987813, 'r2': -0.8400295972824097}
[2025-05-07 18:31:41,424][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0329Epoch 14/15: [                              ] 2/75 batches, loss: 0.0276Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0312Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0315Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0351Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0352Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0319Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0317Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0314Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0304Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0303Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0317Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0320Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0325Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0322Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0313Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0314Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0314Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0314Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0314Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0314Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0316Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0316Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0318Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0312Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0310Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0312Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0311Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0308Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0304Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0302Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0302Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0302Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0303Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0308Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0307Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0308Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0305Epoch 14/15: [================              ] 40/75 batches, loss: 0.0302Epoch 14/15: [================              ] 41/75 batches, loss: 0.0304Epoch 14/15: [================              ] 42/75 batches, loss: 0.0302Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0301Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0304Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0305Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0302Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0301Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0309Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0313Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0310Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0311Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0312Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0311Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0309Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0309Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0306Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0309Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0307Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0305Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0307Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0307Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0306Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0304Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0303Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0303Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0307Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0307Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0308Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0308Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0309Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0313Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0311Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0312Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0311Epoch 14/15: [==============================] 75/75 batches, loss: 0.0312
[2025-05-07 18:31:43,876][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0312
[2025-05-07 18:31:44,331][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0736, Metrics: {'mse': 0.07429318875074387, 'rmse': 0.2725677690974189, 'r2': -0.5970451831817627}
[2025-05-07 18:31:44,332][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0481Epoch 15/15: [                              ] 2/75 batches, loss: 0.0357Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0380Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0371Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0347Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0311Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0327Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0299Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0309Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0305Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0295Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0301Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0308Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0301Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0297Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0293Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0300Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0299Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0295Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0303Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0301Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0298Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0296Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0288Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0287Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0298Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0297Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0299Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0299Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0297Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0297Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0298Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0298Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0297Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0294Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0294Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0290Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0296Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0294Epoch 15/15: [================              ] 40/75 batches, loss: 0.0293Epoch 15/15: [================              ] 41/75 batches, loss: 0.0297Epoch 15/15: [================              ] 42/75 batches, loss: 0.0295Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0297Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0296Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0297Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0295Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0294Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0295Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0297Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0299Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0303Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0304Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0306Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0304Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0307Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0305Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0302Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0301Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0306Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0303Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0301Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0298Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0297Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0298Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0296Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0297Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0296Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0295Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0298Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0297Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0298Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0296Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0295Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0294Epoch 15/15: [==============================] 75/75 batches, loss: 0.0291
[2025-05-07 18:31:46,852][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0291
[2025-05-07 18:31:47,322][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0704, Metrics: {'mse': 0.07096485048532486, 'rmse': 0.26639228683527016, 'r2': -0.525497555732727}
[2025-05-07 18:31:47,323][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:31:47,323][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-07 18:31:47,323][src.training.lm_trainer][INFO] - Training completed in 45.43 seconds
[2025-05-07 18:31:47,323][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:31:50,692][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.019935205578804016, 'rmse': 0.141192087521943, 'r2': 9.989738464355469e-05}
[2025-05-07 18:31:50,692][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06539115309715271, 'rmse': 0.255716939402052, 'r2': -0.40568244457244873}
[2025-05-07 18:31:50,693][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06294624507427216, 'rmse': 0.2508909027331843, 'r2': -0.5921659469604492}
[2025-05-07 18:31:52,354][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ru/ru/model.pt
[2025-05-07 18:31:52,355][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▃▁
wandb:     best_val_mse █▅▃▃▁
wandb:      best_val_r2 ▁▄▆▆█
wandb:    best_val_rmse █▅▃▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▃▄▄▃▃▄▃▅▄▃▄
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▄▅▃▄▅▅▃▅▁▃▅▃▂
wandb:          val_mse ▆█▄▅▃▄▅▅▃▅▁▃▅▃▂
wandb:           val_r2 ▃▁▅▄▆▅▄▄▆▄█▆▄▆▇
wandb:         val_rmse ▇█▄▅▃▄▅▅▃▅▁▃▅▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06577
wandb:     best_val_mse 0.06539
wandb:      best_val_r2 -0.40568
wandb:    best_val_rmse 0.25572
wandb: early_stop_epoch 15
wandb:            epoch 15
wandb:   final_test_mse 0.06295
wandb:    final_test_r2 -0.59217
wandb:  final_test_rmse 0.25089
wandb:  final_train_mse 0.01994
wandb:   final_train_r2 0.0001
wandb: final_train_rmse 0.14119
wandb:    final_val_mse 0.06539
wandb:     final_val_r2 -0.40568
wandb:   final_val_rmse 0.25572
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02913
wandb:       train_time 45.43047
wandb:         val_loss 0.07044
wandb:          val_mse 0.07096
wandb:           val_r2 -0.5255
wandb:         val_rmse 0.26639
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183043-sy6hswms
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183043-sy6hswms/logs
Experiment probe_layer2_complexity_control3_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_complexity_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:32:29,124][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja
experiment_name: probe_layer2_complexity_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:32:29,124][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:32:29,124][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:32:29,124][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:32:29,129][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:32:29,130][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:32:32,830][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:32:35,156][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:32:35,156][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:32:35,312][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:32:35,404][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:32:35,681][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:32:35,696][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:32:35,697][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:32:35,699][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:32:35,752][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:32:35,826][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:32:35,848][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:32:35,849][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:32:35,849][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:32:35,852][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:32:35,916][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:32:36,031][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:32:36,061][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:32:36,062][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:32:36,062][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:32:36,074][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:32:36,075][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:32:36,075][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:32:36,075][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:32:36,075][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:32:36,075][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:32:36,075][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:32:36,075][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Sample label: 0.5826417803764343
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:32:36,076][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:32:36,076][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:32:36,076][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:32:36,077][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:32:36,077][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:32:36,077][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:32:36,078][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:32:36,078][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:32:36,078][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:32:43,410][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:32:43,411][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:32:43,411][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:32:43,411][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:32:43,414][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:32:43,415][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:32:43,415][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:32:43,415][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:32:43,415][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:32:43,416][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:32:43,416][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4611Epoch 1/15: [                              ] 2/75 batches, loss: 0.5168Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4971Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4820Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4501Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4213Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4048Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4202Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3981Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3889Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3764Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3864Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3682Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3841Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3834Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3989Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3949Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4127Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4067Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4091Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4001Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4028Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3907Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3805Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3813Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3789Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3728Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3676Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3669Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3615Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3559Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3511Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3518Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3502Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3486Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3516Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3485Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3452Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3449Epoch 1/15: [================              ] 40/75 batches, loss: 0.3412Epoch 1/15: [================              ] 41/75 batches, loss: 0.3375Epoch 1/15: [================              ] 42/75 batches, loss: 0.3370Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3343Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3345Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3336Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3288Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3248Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3208Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3182Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3155Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3144Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3171Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3132Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3152Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3151Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3120Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3088Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3071Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3052Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3047Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3041Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3036Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3050Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3050Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3036Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3021Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3018Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2999Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2987Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2960Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2947Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2941Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2919Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2913Epoch 1/15: [==============================] 75/75 batches, loss: 0.2891
[2025-05-07 18:32:51,150][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2891
[2025-05-07 18:32:51,382][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0808, Metrics: {'mse': 0.08104854077100754, 'rmse': 0.2846902540850451, 'r2': -0.3209230899810791}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1608Epoch 2/15: [                              ] 2/75 batches, loss: 0.2141Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2375Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2447Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2356Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2223Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2340Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2207Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2347Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2376Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2251Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2242Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2222Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2169Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2166Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2128Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2103Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2104Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2069Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2033Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2002Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2001Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1960Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1940Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1892Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1916Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1925Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1928Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1935Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1904Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1877Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1886Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1877Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1863Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1842Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1823Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1840Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1818Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1792Epoch 2/15: [================              ] 40/75 batches, loss: 0.1790Epoch 2/15: [================              ] 41/75 batches, loss: 0.1777Epoch 2/15: [================              ] 42/75 batches, loss: 0.1771Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1755Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1771Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1764Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1746Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1738Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1721Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1721Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1712Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1710Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1706Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1701Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1696Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1681Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1690Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1680Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1675Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1662Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1651Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1649Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1660Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1657Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1645Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1626Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1611Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1635Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1635Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1622Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1618Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1614Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1609Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1600Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1587Epoch 2/15: [==============================] 75/75 batches, loss: 0.1598
[2025-05-07 18:32:54,105][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1598
[2025-05-07 18:32:54,363][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0872, Metrics: {'mse': 0.08705375343561172, 'rmse': 0.29504873061176135, 'r2': -0.4187955856323242}
[2025-05-07 18:32:54,364][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1935Epoch 3/15: [                              ] 2/75 batches, loss: 0.1772Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1512Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1368Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1318Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1385Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1301Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1316Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1329Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1269Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1245Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1207Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1273Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1218Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1215Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1212Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1169Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1179Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1181Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1200Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1233Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1219Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1234Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1217Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1196Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1200Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1239Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1245Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1234Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1230Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1237Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1282Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1286Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1278Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1269Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1267Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1297Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1278Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1272Epoch 3/15: [================              ] 40/75 batches, loss: 0.1257Epoch 3/15: [================              ] 41/75 batches, loss: 0.1257Epoch 3/15: [================              ] 42/75 batches, loss: 0.1256Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1245Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1231Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1228Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1216Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1212Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1197Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1188Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1185Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1180Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1180Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1173Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1168Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1160Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1149Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1142Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1154Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1152Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1146Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1149Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1147Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1141Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1137Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1153Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1145Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1141Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1144Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1141Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1136Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1129Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1131Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1135Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1133Epoch 3/15: [==============================] 75/75 batches, loss: 0.1139
[2025-05-07 18:32:56,748][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1139
[2025-05-07 18:32:57,008][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0747, Metrics: {'mse': 0.07468772679567337, 'rmse': 0.273290553798834, 'r2': -0.21725499629974365}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1173Epoch 4/15: [                              ] 2/75 batches, loss: 0.1120Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1367Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1318Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1153Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1151Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1142Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1173Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1140Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1083Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1151Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1126Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1108Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1080Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1031Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1031Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1039Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1034Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1066Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1057Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1027Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1029Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1013Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1025Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1018Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1009Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1025Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1009Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1008Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1006Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1010Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1010Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1008Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0997Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0996Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0998Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0988Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0984Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0978Epoch 4/15: [================              ] 40/75 batches, loss: 0.0983Epoch 4/15: [================              ] 41/75 batches, loss: 0.0976Epoch 4/15: [================              ] 42/75 batches, loss: 0.0983Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0991Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0984Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0978Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0984Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0984Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0993Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0980Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0973Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0972Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0968Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0972Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0963Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0959Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0963Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0957Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0962Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0956Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0951Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0948Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0940Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0938Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0935Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0933Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0937Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0940Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0941Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0946Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0942Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0938Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0941Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0940Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0954Epoch 4/15: [==============================] 75/75 batches, loss: 0.0956
[2025-05-07 18:32:59,683][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0956
[2025-05-07 18:32:59,920][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0740, Metrics: {'mse': 0.07396923750638962, 'rmse': 0.2719728617093801, 'r2': -0.20554518699645996}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1075Epoch 5/15: [                              ] 2/75 batches, loss: 0.1029Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0835Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0857Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0864Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0939Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0927Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0873Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0892Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0901Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1010Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0998Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0965Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0971Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0959Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0939Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0922Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0908Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0899Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0885Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0895Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0923Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0909Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0928Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0920Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0926Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0913Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0923Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0912Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0904Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0895Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0893Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0889Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0902Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0889Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0888Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0884Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0874Epoch 5/15: [================              ] 40/75 batches, loss: 0.0866Epoch 5/15: [================              ] 41/75 batches, loss: 0.0867Epoch 5/15: [================              ] 42/75 batches, loss: 0.0885Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0879Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0872Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0865Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0869Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0867Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0864Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0862Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0865Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0860Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0858Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0863Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0864Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0867Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0865Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0857Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0855Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0855Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0851Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0855Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0864Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0862Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0857Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0869Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0870Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0865Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0861Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0859Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0861Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0852Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0849Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0850Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0853Epoch 5/15: [==============================] 75/75 batches, loss: 0.0865
[2025-05-07 18:33:02,597][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0865
[2025-05-07 18:33:02,842][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0748, Metrics: {'mse': 0.07469876855611801, 'rmse': 0.27331075455627063, 'r2': -0.21743500232696533}
[2025-05-07 18:33:02,842][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0732Epoch 6/15: [                              ] 2/75 batches, loss: 0.0881Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0838Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0800Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0901Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0891Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0899Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0856Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0848Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0850Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0829Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0848Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0830Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0842Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0822Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0824Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0862Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0869Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0859Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0861Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0871Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0864Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0844Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0843Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0866Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0869Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0854Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0860Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0843Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0841Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0835Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0836Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0820Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0814Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0807Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0817Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0814Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0819Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0819Epoch 6/15: [================              ] 40/75 batches, loss: 0.0819Epoch 6/15: [================              ] 41/75 batches, loss: 0.0828Epoch 6/15: [================              ] 42/75 batches, loss: 0.0825Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0824Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0830Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0824Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0820Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0812Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0812Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0816Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0832Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0829Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0818Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0819Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0813Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0808Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0804Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0803Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0806Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0808Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0802Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0800Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0797Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0795Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0790Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0795Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0794Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0790Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0789Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0785Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0781Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0779Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0777Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0774Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0770Epoch 6/15: [==============================] 75/75 batches, loss: 0.0773
[2025-05-07 18:33:05,132][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0773
[2025-05-07 18:33:05,331][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0645, Metrics: {'mse': 0.06458133459091187, 'rmse': 0.2541285788550982, 'r2': -0.05254173278808594}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0480Epoch 7/15: [                              ] 2/75 batches, loss: 0.0487Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0466Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0485Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0606Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0618Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0618Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0632Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0661Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0685Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0663Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0636Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0655Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0674Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0654Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0632Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0616Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0630Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0628Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0644Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0639Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0625Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0634Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0639Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0686Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0693Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0699Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0688Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0696Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0706Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0700Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0699Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0700Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0703Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0707Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0715Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0716Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0714Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0713Epoch 7/15: [================              ] 40/75 batches, loss: 0.0722Epoch 7/15: [================              ] 41/75 batches, loss: 0.0718Epoch 7/15: [================              ] 42/75 batches, loss: 0.0716Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0717Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0719Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0718Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0709Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0704Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0699Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0694Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0703Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0702Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0695Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0695Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0692Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0690Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0694Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0697Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0692Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0694Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0700Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0698Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0696Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0695Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0693Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0701Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0699Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0696Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0694Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0691Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0690Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0691Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0690Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0690Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0687Epoch 7/15: [==============================] 75/75 batches, loss: 0.0685
[2025-05-07 18:33:08,092][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0685
[2025-05-07 18:33:08,397][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0696, Metrics: {'mse': 0.06955654919147491, 'rmse': 0.2637357563764817, 'r2': -0.1336275339126587}
[2025-05-07 18:33:08,398][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0592Epoch 8/15: [                              ] 2/75 batches, loss: 0.0644Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0633Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0696Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0664Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0686Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0637Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0618Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0612Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0626Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0633Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0618Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0635Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0634Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0644Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0632Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0623Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0616Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0621Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0621Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0619Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0618Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0635Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0640Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0647Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0635Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0651Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0654Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0656Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0648Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0641Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0648Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0644Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0662Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0662Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0658Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0657Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0658Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0654Epoch 8/15: [================              ] 40/75 batches, loss: 0.0650Epoch 8/15: [================              ] 41/75 batches, loss: 0.0649Epoch 8/15: [================              ] 42/75 batches, loss: 0.0646Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0646Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0656Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0658Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0654Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0650Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0654Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0648Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0652Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0647Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0640Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0641Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0643Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0641Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0642Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0640Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0645Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0641Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0641Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0644Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0643Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0641Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0647Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0645Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0648Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0648Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0646Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0643Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0641Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0646Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0643Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0650Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0651Epoch 8/15: [==============================] 75/75 batches, loss: 0.0648
[2025-05-07 18:33:10,775][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0648
[2025-05-07 18:33:10,990][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0699, Metrics: {'mse': 0.06987729668617249, 'rmse': 0.2643431419314155, 'r2': -0.13885498046875}
[2025-05-07 18:33:10,990][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0418Epoch 9/15: [                              ] 2/75 batches, loss: 0.0685Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0574Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0572Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0667Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0606Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0583Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0570Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0570Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0579Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0631Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0658Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0666Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0661Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0642Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0643Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0634Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0630Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0647Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0639Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0632Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0626Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0629Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0614Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0601Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0599Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0600Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0608Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0617Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0607Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0609Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0615Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0611Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0611Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0604Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0608Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0621Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0618Epoch 9/15: [================              ] 40/75 batches, loss: 0.0613Epoch 9/15: [================              ] 41/75 batches, loss: 0.0612Epoch 9/15: [================              ] 42/75 batches, loss: 0.0615Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0627Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0623Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0619Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0624Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0625Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0629Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0629Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0627Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0621Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0632Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0635Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0628Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0623Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0622Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0623Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0624Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0622Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0619Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0621Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0620Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0623Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0621Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0621Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0623Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0625Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0623Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0620Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0623Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0626Epoch 9/15: [==============================] 75/75 batches, loss: 0.0621
[2025-05-07 18:33:13,286][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0621
[2025-05-07 18:33:13,562][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0734, Metrics: {'mse': 0.07334759831428528, 'rmse': 0.2708276173404132, 'r2': -0.1954137086868286}
[2025-05-07 18:33:13,563][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0394Epoch 10/15: [                              ] 2/75 batches, loss: 0.0560Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0700Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0634Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0589Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0596Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0556Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0580Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0561Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0550Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0532Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0558Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0586Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0600Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0606Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0603Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0608Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0633Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0629Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0633Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0646Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0643Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0645Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0641Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0636Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0629Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0629Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0637Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0639Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0641Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0632Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0635Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0634Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0630Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0630Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0635Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0637Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0632Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 10/15: [================              ] 40/75 batches, loss: 0.0628Epoch 10/15: [================              ] 41/75 batches, loss: 0.0631Epoch 10/15: [================              ] 42/75 batches, loss: 0.0633Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0638Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0642Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0649Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0647Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0647Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0651Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0647Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0649Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0645Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0648Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0644Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0642Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0642Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0648Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0651Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0651Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0645Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0640Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0638Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0642Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0643Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0639Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0639Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0636Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0641Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0641Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0647Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0644Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0640Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0640Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0644Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0641Epoch 10/15: [==============================] 75/75 batches, loss: 0.0638
[2025-05-07 18:33:15,871][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0638
[2025-05-07 18:33:16,189][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0733, Metrics: {'mse': 0.07322414219379425, 'rmse': 0.2705995975492097, 'r2': -0.19340157508850098}
[2025-05-07 18:33:16,190][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:33:16,190][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 18:33:16,190][src.training.lm_trainer][INFO] - Training completed in 28.21 seconds
[2025-05-07 18:33:16,190][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:33:19,262][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04023658484220505, 'rmse': 0.2005905901138063, 'r2': -0.004274129867553711}
[2025-05-07 18:33:19,262][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06458133459091187, 'rmse': 0.2541285788550982, 'r2': -0.05254173278808594}
[2025-05-07 18:33:19,262][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06769626587629318, 'rmse': 0.26018506082458537, 'r2': -0.30038702487945557}
[2025-05-07 18:33:20,939][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja/ja/model.pt
[2025-05-07 18:33:20,941][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▁
wandb:     best_val_mse █▅▅▁
wandb:      best_val_r2 ▁▄▄█
wandb:    best_val_rmse █▅▅▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▃▃▄▄▄▃
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▄▄▄▁▃▃▄▄
wandb:          val_mse ▆█▄▄▄▁▃▃▄▄
wandb:           val_r2 ▃▁▅▅▅█▆▆▅▅
wandb:         val_rmse ▆█▄▄▄▁▃▃▄▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06451
wandb:     best_val_mse 0.06458
wandb:      best_val_r2 -0.05254
wandb:    best_val_rmse 0.25413
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.0677
wandb:    final_test_r2 -0.30039
wandb:  final_test_rmse 0.26019
wandb:  final_train_mse 0.04024
wandb:   final_train_r2 -0.00427
wandb: final_train_rmse 0.20059
wandb:    final_val_mse 0.06458
wandb:     final_val_r2 -0.05254
wandb:   final_val_rmse 0.25413
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06376
wandb:       train_time 28.21253
wandb:         val_loss 0.07326
wandb:          val_mse 0.07322
wandb:           val_r2 -0.1934
wandb:         val_rmse 0.2706
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183229-46gfmgsb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183229-46gfmgsb/logs
Experiment probe_layer2_complexity_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_complexity_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:34:09,363][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja
experiment_name: probe_layer2_complexity_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:34:09,363][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:34:09,363][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:34:09,363][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:34:09,368][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:34:09,368][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:34:15,319][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:34:17,636][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:34:17,636][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:34:17,936][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:34:18,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:34:18,519][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:34:18,528][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:34:18,528][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:34:18,532][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:34:18,672][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:34:18,890][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:34:18,957][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:34:18,958][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:34:18,958][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:34:18,964][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:34:19,170][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:34:19,526][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:34:19,580][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:34:19,581][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:34:19,581][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:34:19,585][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:34:19,586][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:34:19,586][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:34:19,586][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:34:19,587][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:34:19,587][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Sample label: 0.5349239110946655
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:34:19,587][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:34:19,588][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:34:19,588][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:34:19,588][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:34:19,588][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:34:19,589][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:34:19,589][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:34:19,589][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:34:19,589][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:34:19,589][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:34:19,589][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:34:19,589][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:34:19,590][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:34:27,456][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:34:27,457][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:34:27,457][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:34:27,457][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:34:27,460][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:34:27,460][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:34:27,461][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:34:27,461][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:34:27,461][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:34:27,462][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:34:27,462][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4810Epoch 1/15: [                              ] 2/75 batches, loss: 0.4236Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4286Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4242Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4098Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3812Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3801Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4045Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3843Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3705Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3645Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3781Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3628Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3772Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3773Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3946Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3954Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4068Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3983Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3949Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3906Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3894Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3775Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3683Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3666Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3601Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3550Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3487Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3479Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3419Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3366Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3322Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3325Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3331Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3342Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3363Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3327Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3293Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3287Epoch 1/15: [================              ] 40/75 batches, loss: 0.3248Epoch 1/15: [================              ] 41/75 batches, loss: 0.3218Epoch 1/15: [================              ] 42/75 batches, loss: 0.3224Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3231Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3269Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3262Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3230Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3199Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3177Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3155Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3140Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3140Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3177Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3137Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3127Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3120Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3084Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3050Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3029Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3005Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2990Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2974Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2966Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2973Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2960Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2958Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2947Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2956Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2936Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2924Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2907Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2913Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2922Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2902Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2895Epoch 1/15: [==============================] 75/75 batches, loss: 0.2863
[2025-05-07 18:34:34,198][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2863
[2025-05-07 18:34:34,458][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0793, Metrics: {'mse': 0.07949675619602203, 'rmse': 0.2819516912451884, 'r2': -0.2956322431564331}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2466Epoch 2/15: [                              ] 2/75 batches, loss: 0.2215Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2116Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2405Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2223Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2059Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2161Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2141Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2201Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2257Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2143Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2060Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2073Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2009Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1994Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1985Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1944Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1904Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1850Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1798Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1763Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1744Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1747Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1737Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1712Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1726Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1764Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1744Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1759Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1751Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1736Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1742Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1719Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1713Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1696Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1673Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1685Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1661Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1646Epoch 2/15: [================              ] 40/75 batches, loss: 0.1633Epoch 2/15: [================              ] 41/75 batches, loss: 0.1616Epoch 2/15: [================              ] 42/75 batches, loss: 0.1625Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1625Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1665Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1634Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1613Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1590Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1585Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1585Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1582Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1587Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1581Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1580Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1587Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1581Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1582Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1570Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1552Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1546Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1545Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1536Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1543Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1540Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1534Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1522Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1515Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1523Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1528Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1522Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1531Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1522Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1517Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1509Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1513Epoch 2/15: [==============================] 75/75 batches, loss: 0.1513
[2025-05-07 18:34:37,193][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1513
[2025-05-07 18:34:37,447][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0813, Metrics: {'mse': 0.08120986819267273, 'rmse': 0.2849734517330917, 'r2': -0.32355237007141113}
[2025-05-07 18:34:37,447][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1239Epoch 3/15: [                              ] 2/75 batches, loss: 0.1193Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1081Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1039Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1327Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1390Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1372Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1364Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1392Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1350Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1364Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1385Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1372Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1325Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1305Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1290Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1247Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1262Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1227Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1252Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1298Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1275Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1274Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1263Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1236Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1243Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1265Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1289Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1289Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1267Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1278Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1322Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1323Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1321Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1317Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1303Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1302Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1301Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1281Epoch 3/15: [================              ] 40/75 batches, loss: 0.1270Epoch 3/15: [================              ] 41/75 batches, loss: 0.1262Epoch 3/15: [================              ] 42/75 batches, loss: 0.1249Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1237Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1229Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1227Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1221Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1224Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1215Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1207Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1204Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1201Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1206Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1202Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1192Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1187Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1181Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1184Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1188Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1180Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1169Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1166Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1159Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1150Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1152Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1157Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1148Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1141Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1140Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1139Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1141Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1134Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1134Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1132Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1130Epoch 3/15: [==============================] 75/75 batches, loss: 0.1127
[2025-05-07 18:34:39,859][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1127
[2025-05-07 18:34:40,477][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0781, Metrics: {'mse': 0.07814638316631317, 'rmse': 0.2795467459411988, 'r2': -0.2736239433288574}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1566Epoch 4/15: [                              ] 2/75 batches, loss: 0.1286Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1353Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1387Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1200Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1083Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1084Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1023Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1006Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0993Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1015Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1000Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0999Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0980Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0960Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1008Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1002Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0987Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0999Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1030Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1009Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1006Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1008Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1013Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0997Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0988Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0991Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0998Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1007Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1010Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0998Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0992Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0995Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0989Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0995Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0985Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0985Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0978Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0977Epoch 4/15: [================              ] 40/75 batches, loss: 0.0990Epoch 4/15: [================              ] 41/75 batches, loss: 0.0982Epoch 4/15: [================              ] 42/75 batches, loss: 0.0987Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0998Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0988Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0985Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0980Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0981Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0996Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0994Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0980Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0971Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0966Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0984Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0979Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0973Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0984Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0976Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0993Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0992Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0994Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1005Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1001Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1015Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1015Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1010Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1006Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1005Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1004Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1008Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1007Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1004Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1003Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1002Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1009Epoch 4/15: [==============================] 75/75 batches, loss: 0.1003
[2025-05-07 18:34:43,286][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1003
[2025-05-07 18:34:43,561][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0796, Metrics: {'mse': 0.07959199696779251, 'rmse': 0.28212053623902056, 'r2': -0.2971843481063843}
[2025-05-07 18:34:43,562][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0938Epoch 5/15: [                              ] 2/75 batches, loss: 0.0783Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0870Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0973Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0936Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0896Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0892Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0859Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0859Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0813Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0837Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0847Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0900Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0883Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0856Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0844Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0837Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0847Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0851Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0852Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0842Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0860Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0853Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0863Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0876Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0858Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0859Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0853Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0863Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0851Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0855Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0855Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0849Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0841Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0839Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0828Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0819Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0821Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0816Epoch 5/15: [================              ] 40/75 batches, loss: 0.0808Epoch 5/15: [================              ] 41/75 batches, loss: 0.0800Epoch 5/15: [================              ] 42/75 batches, loss: 0.0818Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0811Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0805Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0800Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0803Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0799Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0809Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0805Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0803Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0793Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0796Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0796Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0805Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0807Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0808Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0809Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0810Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0813Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0814Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0820Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0834Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0833Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0826Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0830Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0831Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0834Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0830Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0829Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0835Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0832Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0828Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0825Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0825Epoch 5/15: [==============================] 75/75 batches, loss: 0.0832
[2025-05-07 18:34:45,886][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0832
[2025-05-07 18:34:46,227][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0906, Metrics: {'mse': 0.09041804820299149, 'rmse': 0.3006959397846793, 'r2': -0.47362661361694336}
[2025-05-07 18:34:46,228][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0941Epoch 6/15: [                              ] 2/75 batches, loss: 0.1147Epoch 6/15: [=                             ] 3/75 batches, loss: 0.1069Epoch 6/15: [=                             ] 4/75 batches, loss: 0.1068Epoch 6/15: [==                            ] 5/75 batches, loss: 0.1071Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0964Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0941Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0974Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0970Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0956Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0938Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0944Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0912Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0910Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0946Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0924Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0938Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0921Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0926Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0924Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0947Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0926Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0909Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0900Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0902Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0895Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0878Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0871Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0892Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0885Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0874Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0877Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0871Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0866Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0870Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0882Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0872Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0863Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0851Epoch 6/15: [================              ] 40/75 batches, loss: 0.0851Epoch 6/15: [================              ] 41/75 batches, loss: 0.0856Epoch 6/15: [================              ] 42/75 batches, loss: 0.0863Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0852Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0846Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0844Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0836Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0833Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0834Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0838Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0845Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0841Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0838Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0833Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0828Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0823Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0825Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0825Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0821Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0818Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0810Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0806Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0811Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0809Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0813Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0815Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0815Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0813Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0815Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0818Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0812Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0811Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0805Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0804Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0796Epoch 6/15: [==============================] 75/75 batches, loss: 0.0802
[2025-05-07 18:34:48,584][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0802
[2025-05-07 18:34:48,813][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0674, Metrics: {'mse': 0.06743551045656204, 'rmse': 0.2596834812932121, 'r2': -0.09905898571014404}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0548Epoch 7/15: [                              ] 2/75 batches, loss: 0.0597Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0543Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0596Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0730Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0676Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0721Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0782Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0789Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0841Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0824Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0805Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0794Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0776Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0778Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0779Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0764Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0760Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0783Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0775Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0773Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0767Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0769Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0771Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0761Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0753Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0747Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0755Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0756Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0758Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0765Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0764Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0749Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0778Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0771Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0760Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0752Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0761Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0761Epoch 7/15: [================              ] 40/75 batches, loss: 0.0765Epoch 7/15: [================              ] 41/75 batches, loss: 0.0761Epoch 7/15: [================              ] 42/75 batches, loss: 0.0754Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0749Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0751Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0748Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0744Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0747Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0754Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0752Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0747Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0745Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0737Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0742Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0737Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0739Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0739Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0740Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0741Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0746Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0756Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0749Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0748Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0745Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0743Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0744Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0740Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0735Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0734Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0730Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0734Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0730Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0723Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0719Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0720Epoch 7/15: [==============================] 75/75 batches, loss: 0.0719
[2025-05-07 18:34:51,567][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0719
[2025-05-07 18:34:51,829][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0709, Metrics: {'mse': 0.07090314477682114, 'rmse': 0.2662764442770354, 'r2': -0.15557420253753662}
[2025-05-07 18:34:51,830][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0809Epoch 8/15: [                              ] 2/75 batches, loss: 0.0808Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0758Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0701Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0645Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0668Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0670Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0691Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0699Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0732Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0707Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0680Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0679Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0704Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0737Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0726Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0735Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0748Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0762Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0754Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0746Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0750Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0750Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0743Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0736Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0738Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0744Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0745Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0748Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0746Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0742Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0744Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0731Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0743Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0739Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0730Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0737Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0740Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0738Epoch 8/15: [================              ] 40/75 batches, loss: 0.0737Epoch 8/15: [================              ] 41/75 batches, loss: 0.0729Epoch 8/15: [================              ] 42/75 batches, loss: 0.0728Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0723Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0733Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0729Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0722Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0714Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0717Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0718Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0728Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0729Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0725Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0720Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0720Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0715Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0708Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0707Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0701Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0695Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0691Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0690Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0684Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0682Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0678Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0676Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0672Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0676Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0672Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0674Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0675Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0670Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0668Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0664Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0664Epoch 8/15: [==============================] 75/75 batches, loss: 0.0659
[2025-05-07 18:34:54,178][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0659
[2025-05-07 18:34:54,486][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0933, Metrics: {'mse': 0.09302463382482529, 'rmse': 0.3049993997122376, 'r2': -0.5161086320877075}
[2025-05-07 18:34:54,487][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0441Epoch 9/15: [                              ] 2/75 batches, loss: 0.0656Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0670Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0646Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0677Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0649Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0635Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0698Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0714Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0723Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0710Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0680Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0700Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0696Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0697Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0696Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0675Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0656Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0658Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0650Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0643Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0649Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0663Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0668Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0667Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0660Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0649Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0644Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0650Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0647Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0641Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0645Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0651Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0641Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0634Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0632Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0628Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0637Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0629Epoch 9/15: [================              ] 40/75 batches, loss: 0.0637Epoch 9/15: [================              ] 41/75 batches, loss: 0.0631Epoch 9/15: [================              ] 42/75 batches, loss: 0.0635Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0633Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0635Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0630Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0628Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0632Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0627Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0624Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0621Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0620Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0614Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0612Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0614Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0608Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0615Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0620Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0617Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0617Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0617Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0623Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0620Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0622Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0627Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0627Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0627Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0625Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0627Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0630Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0627Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0631Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0626Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0624Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0620Epoch 9/15: [==============================] 75/75 batches, loss: 0.0627
[2025-05-07 18:34:56,819][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0627
[2025-05-07 18:34:57,077][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0760, Metrics: {'mse': 0.07594640552997589, 'rmse': 0.27558375411111574, 'r2': -0.2377687692642212}
[2025-05-07 18:34:57,078][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0685Epoch 10/15: [                              ] 2/75 batches, loss: 0.0568Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0517Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0682Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0601Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0557Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0525Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0555Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0575Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0588Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0580Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0592Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0586Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0601Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0597Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0612Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0611Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0626Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0609Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0604Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0604Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0591Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0622Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0628Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0632Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0637Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0645Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0644Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0650Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0641Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0631Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0630Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0626Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0632Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0628Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0626Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0641Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0633Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 10/15: [================              ] 40/75 batches, loss: 0.0629Epoch 10/15: [================              ] 41/75 batches, loss: 0.0626Epoch 10/15: [================              ] 42/75 batches, loss: 0.0624Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0623Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0618Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0611Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0609Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0607Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0615Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0616Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0612Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0612Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0606Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0601Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0602Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0600Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0600Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0608Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0606Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0601Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0601Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0603Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0604Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0607Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0610Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0616Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0618Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0614Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0615Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0624Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0624Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0625Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0621Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0622Epoch 10/15: [==============================] 75/75 batches, loss: 0.0619
[2025-05-07 18:34:59,429][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0619
[2025-05-07 18:34:59,670][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0800, Metrics: {'mse': 0.0798315778374672, 'rmse': 0.2825448244747498, 'r2': -0.30108916759490967}
[2025-05-07 18:34:59,670][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:34:59,671][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 18:34:59,671][src.training.lm_trainer][INFO] - Training completed in 28.61 seconds
[2025-05-07 18:34:59,671][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:35:02,629][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04070959612727165, 'rmse': 0.20176619173506657, 'r2': -0.016080141067504883}
[2025-05-07 18:35:02,630][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06743551045656204, 'rmse': 0.2596834812932121, 'r2': -0.09905898571014404}
[2025-05-07 18:35:02,630][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06967534124851227, 'rmse': 0.26396087067690976, 'r2': -0.3384033441543579}
[2025-05-07 18:35:04,275][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja/ja/model.pt
[2025-05-07 18:35:04,277][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▁
wandb:     best_val_mse █▇▁
wandb:      best_val_r2 ▁▂█
wandb:    best_val_rmse █▇▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▃▃▃▁▄▄▁▃
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄▅▄▄▇▁▂█▃▄
wandb:          val_mse ▄▅▄▄▇▁▂█▃▄
wandb:           val_r2 ▅▄▅▅▂█▇▁▆▅
wandb:         val_rmse ▄▅▄▄▇▁▂█▃▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06739
wandb:     best_val_mse 0.06744
wandb:      best_val_r2 -0.09906
wandb:    best_val_rmse 0.25968
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.06968
wandb:    final_test_r2 -0.3384
wandb:  final_test_rmse 0.26396
wandb:  final_train_mse 0.04071
wandb:   final_train_r2 -0.01608
wandb: final_train_rmse 0.20177
wandb:    final_val_mse 0.06744
wandb:     final_val_r2 -0.09906
wandb:   final_val_rmse 0.25968
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06191
wandb:       train_time 28.61241
wandb:         val_loss 0.07995
wandb:          val_mse 0.07983
wandb:           val_r2 -0.30109
wandb:         val_rmse 0.28254
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183409-a7yhfzwa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183409-a7yhfzwa/logs
Experiment probe_layer2_complexity_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/ja/ja/results.json for layer 2
Running experiment: probe_layer2_complexity_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_complexity_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:35:48,831][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja
experiment_name: probe_layer2_complexity_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:35:48,832][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:35:48,832][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:35:48,832][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:35:48,837][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:35:48,837][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:35:54,202][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:35:56,524][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:35:56,525][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:35:57,244][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:35:57,395][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:35:57,811][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:35:57,820][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:35:57,820][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:35:57,845][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:35:57,985][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:35:58,194][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:35:58,306][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:35:58,307][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:35:58,308][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:35:58,312][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:35:58,427][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:35:58,565][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:35:58,617][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:35:58,619][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:35:58,619][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:35:58,620][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:35:58,621][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:35:58,621][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:35:58,621][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:35:58,621][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:35:58,621][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:35:58,621][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Sample label: 0.2807745635509491
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:35:58,622][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:35:58,622][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:35:58,622][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:35:58,623][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:35:58,623][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:35:58,623][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:35:58,624][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:35:58,624][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:35:58,624][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:36:09,375][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:36:09,376][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:36:09,376][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:36:09,376][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:36:09,379][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:36:09,380][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:36:09,380][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:36:09,380][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:36:09,380][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:36:09,381][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:36:09,381][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4434Epoch 1/15: [                              ] 2/75 batches, loss: 0.4935Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4541Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4477Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4291Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4056Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3991Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4200Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4042Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3811Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3797Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3846Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3703Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3811Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3850Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3979Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3955Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4132Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4083Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4072Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4017Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4014Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3888Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3797Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3770Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3715Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3712Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3659Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3633Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3585Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3521Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3517Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3522Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3522Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3505Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3541Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3507Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3466Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3449Epoch 1/15: [================              ] 40/75 batches, loss: 0.3399Epoch 1/15: [================              ] 41/75 batches, loss: 0.3357Epoch 1/15: [================              ] 42/75 batches, loss: 0.3348Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3363Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3391Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3377Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3332Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3302Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3284Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3272Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3246Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3274Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3319Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3286Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3311Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3311Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3266Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3228Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3210Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3200Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3192Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3173Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3159Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3163Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3165Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3163Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3135Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3124Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3096Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3082Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3053Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3041Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3055Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3034Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3030Epoch 1/15: [==============================] 75/75 batches, loss: 0.2997
[2025-05-07 18:36:16,974][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2997
[2025-05-07 18:36:17,291][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0867, Metrics: {'mse': 0.08660884201526642, 'rmse': 0.2942938022032853, 'r2': -0.41154444217681885}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1891Epoch 2/15: [                              ] 2/75 batches, loss: 0.2414Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2205Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2239Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1999Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1831Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1864Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1842Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1911Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1850Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1778Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1773Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1759Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1712Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1768Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1777Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1750Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1730Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1739Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1704Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1679Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1712Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1683Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1681Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1640Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1708Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1716Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1727Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1720Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1717Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1705Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1748Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1739Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1729Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1725Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1707Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1732Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1713Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1698Epoch 2/15: [================              ] 40/75 batches, loss: 0.1701Epoch 2/15: [================              ] 41/75 batches, loss: 0.1692Epoch 2/15: [================              ] 42/75 batches, loss: 0.1707Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1687Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1727Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1710Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1687Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1677Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1670Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1663Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1654Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1646Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1634Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1636Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1625Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1613Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1621Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1609Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1595Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1582Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1595Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1582Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1595Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1589Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1589Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1579Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1578Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1587Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1585Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1578Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1578Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1573Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1570Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1557Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1552Epoch 2/15: [==============================] 75/75 batches, loss: 0.1553
[2025-05-07 18:36:20,072][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1553
[2025-05-07 18:36:20,377][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0811, Metrics: {'mse': 0.08099932223558426, 'rmse': 0.28460379870195734, 'r2': -0.3201209306716919}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2021Epoch 3/15: [                              ] 2/75 batches, loss: 0.1630Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1455Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1400Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1469Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1504Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1445Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1467Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1463Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1406Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1453Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1378Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1358Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1302Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1312Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1316Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1275Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1257Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1261Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1280Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1317Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1311Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1329Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1295Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1259Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1251Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1280Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1272Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1272Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1248Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1244Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1311Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1345Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1339Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1336Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1336Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1332Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1313Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1302Epoch 3/15: [================              ] 40/75 batches, loss: 0.1291Epoch 3/15: [================              ] 41/75 batches, loss: 0.1295Epoch 3/15: [================              ] 42/75 batches, loss: 0.1295Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1285Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1292Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1303Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1296Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1284Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1280Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1277Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1267Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1273Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1267Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1256Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1250Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1237Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1230Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1222Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1226Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1223Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1208Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1202Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1199Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1197Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1188Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1191Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1182Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1177Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1176Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1176Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1169Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1165Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1168Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1173Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1164Epoch 3/15: [==============================] 75/75 batches, loss: 0.1157
[2025-05-07 18:36:23,118][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1157
[2025-05-07 18:36:23,407][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0747, Metrics: {'mse': 0.07465153932571411, 'rmse': 0.27322433882382097, 'r2': -0.2166651487350464}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1667Epoch 4/15: [                              ] 2/75 batches, loss: 0.1359Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1326Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1404Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1257Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1172Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1101Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1041Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1066Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1057Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1100Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1062Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1067Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1037Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1013Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1027Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1055Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1067Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1052Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1038Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1039Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1027Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1028Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1021Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1005Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0996Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0996Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1006Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0995Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0977Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0982Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0974Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0973Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0959Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0960Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0958Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0954Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0963Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0952Epoch 4/15: [================              ] 40/75 batches, loss: 0.0957Epoch 4/15: [================              ] 41/75 batches, loss: 0.0954Epoch 4/15: [================              ] 42/75 batches, loss: 0.0943Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0942Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0935Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0930Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0938Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0938Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0939Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0938Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0921Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0917Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0920Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0915Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0915Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0912Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0904Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0910Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0911Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0906Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0905Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0910Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0911Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0909Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0913Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0916Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0915Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0914Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0913Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0907Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0905Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0909Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0906Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0915Epoch 4/15: [==============================] 75/75 batches, loss: 0.0916
[2025-05-07 18:36:26,095][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0916
[2025-05-07 18:36:26,349][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0806, Metrics: {'mse': 0.08049483597278595, 'rmse': 0.28371611863407753, 'r2': -0.3118988275527954}
[2025-05-07 18:36:26,350][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0580Epoch 5/15: [                              ] 2/75 batches, loss: 0.0770Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0815Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0841Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0861Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0995Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1011Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0921Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1003Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1005Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1023Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1015Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0978Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0986Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0961Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0972Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0963Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0979Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0958Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0941Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0921Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0946Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0951Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0942Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0942Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0948Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0938Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0925Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0917Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0925Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0932Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0917Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0908Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0916Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0920Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0921Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0916Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0917Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0911Epoch 5/15: [================              ] 40/75 batches, loss: 0.0909Epoch 5/15: [================              ] 41/75 batches, loss: 0.0914Epoch 5/15: [================              ] 42/75 batches, loss: 0.0939Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0930Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0917Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0909Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0908Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0908Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0915Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0912Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0901Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0897Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0898Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0894Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0888Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0888Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0894Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0889Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0888Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0895Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0891Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0902Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0912Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0907Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0902Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0906Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0906Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0905Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0898Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0891Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0896Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0899Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0896Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0894Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0898Epoch 5/15: [==============================] 75/75 batches, loss: 0.0891
[2025-05-07 18:36:28,654][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0891
[2025-05-07 18:36:28,935][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0738, Metrics: {'mse': 0.07357772439718246, 'rmse': 0.27125214173750306, 'r2': -0.1991642713546753}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0989Epoch 6/15: [                              ] 2/75 batches, loss: 0.0798Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0746Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0739Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0851Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0830Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0834Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0841Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0807Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0773Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0789Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0786Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0778Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0804Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0804Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0792Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0798Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0796Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0771Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0761Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0752Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0741Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0732Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0753Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0767Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0758Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0759Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0770Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0782Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0790Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0778Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0777Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0767Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0779Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0785Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0789Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0778Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0777Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0779Epoch 6/15: [================              ] 40/75 batches, loss: 0.0788Epoch 6/15: [================              ] 41/75 batches, loss: 0.0789Epoch 6/15: [================              ] 42/75 batches, loss: 0.0796Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0793Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0786Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0789Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0797Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0792Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0794Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0794Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0799Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0809Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0805Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0809Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0804Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0806Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0811Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0806Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0802Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0798Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0796Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0802Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0803Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0799Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0796Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0795Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0789Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0788Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0783Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0786Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0782Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0780Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0774Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0772Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0767Epoch 6/15: [==============================] 75/75 batches, loss: 0.0766
[2025-05-07 18:36:31,640][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0766
[2025-05-07 18:36:31,921][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0733, Metrics: {'mse': 0.07312262803316116, 'rmse': 0.2704119598559967, 'r2': -0.19174718856811523}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0321Epoch 7/15: [                              ] 2/75 batches, loss: 0.0604Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0678Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0620Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0592Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0666Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0645Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0744Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0706Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0724Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0717Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0716Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0724Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0745Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0728Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0741Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0724Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0745Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0742Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0734Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0727Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0712Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0697Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0689Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0678Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0679Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0679Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0677Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0682Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0693Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0703Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0696Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0698Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0714Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0716Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0724Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0721Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0721Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0720Epoch 7/15: [================              ] 40/75 batches, loss: 0.0717Epoch 7/15: [================              ] 41/75 batches, loss: 0.0709Epoch 7/15: [================              ] 42/75 batches, loss: 0.0704Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0694Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0694Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0691Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0689Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0701Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0697Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0696Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0706Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0703Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0696Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0715Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0711Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0712Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0711Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0706Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0708Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0710Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0706Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0704Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0703Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0703Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0701Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0699Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0702Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0707Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0706Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0701Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0707Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0707Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0705Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0703Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0706Epoch 7/15: [==============================] 75/75 batches, loss: 0.0707
[2025-05-07 18:36:34,715][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0707
[2025-05-07 18:36:35,028][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0691, Metrics: {'mse': 0.06896311044692993, 'rmse': 0.2626082832793549, 'r2': -0.1239556074142456}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0520Epoch 8/15: [                              ] 2/75 batches, loss: 0.0619Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0592Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0653Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0656Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0658Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0672Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0637Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0665Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0678Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0678Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0688Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0682Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0697Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0713Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0697Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0697Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0709Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0730Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0714Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0700Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0703Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0705Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0696Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0679Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0671Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0690Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0709Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0698Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0707Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0710Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0717Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0711Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0708Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0706Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0707Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0703Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0697Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0700Epoch 8/15: [================              ] 40/75 batches, loss: 0.0689Epoch 8/15: [================              ] 41/75 batches, loss: 0.0690Epoch 8/15: [================              ] 42/75 batches, loss: 0.0686Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0683Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0687Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0688Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0686Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0680Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0681Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0673Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0677Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0678Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0677Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0679Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0680Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0680Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0684Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0682Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0687Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0689Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0686Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0684Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0682Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0677Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0674Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0669Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0665Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0678Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0672Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0674Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0674Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0675Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0673Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0672Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0673Epoch 8/15: [==============================] 75/75 batches, loss: 0.0669
[2025-05-07 18:36:37,733][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0669
[2025-05-07 18:36:38,095][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0774, Metrics: {'mse': 0.07723559439182281, 'rmse': 0.27791292591713473, 'r2': -0.25878000259399414}
[2025-05-07 18:36:38,096][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0797Epoch 9/15: [                              ] 2/75 batches, loss: 0.0580Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0564Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0609Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0616Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0604Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0653Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0648Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0635Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0623Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0640Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0628Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0626Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0631Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0626Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0604Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0601Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0591Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0605Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0592Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0581Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0589Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0591Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0596Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0603Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0603Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0590Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0602Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0598Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0592Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0608Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0615Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0604Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0607Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0607Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0610Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0608Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0610Epoch 9/15: [================              ] 40/75 batches, loss: 0.0613Epoch 9/15: [================              ] 41/75 batches, loss: 0.0609Epoch 9/15: [================              ] 42/75 batches, loss: 0.0615Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0611Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0609Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0602Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0601Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0602Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0599Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0596Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0598Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0601Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0599Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0602Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0605Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0605Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0605Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0605Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0607Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0610Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0612Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0619Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0614Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0611Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0608Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0614Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0614Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0613Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0613Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0610Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0609Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0606Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0605Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0602Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0599Epoch 9/15: [==============================] 75/75 batches, loss: 0.0598
[2025-05-07 18:36:40,549][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0598
[2025-05-07 18:36:40,826][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0774, Metrics: {'mse': 0.07726334780454636, 'rmse': 0.2779628532817764, 'r2': -0.25923216342926025}
[2025-05-07 18:36:40,826][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0556Epoch 10/15: [                              ] 2/75 batches, loss: 0.0493Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0567Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0646Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0593Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0565Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0566Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0596Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0618Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0616Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0584Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0609Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0603Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0606Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0617Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0628Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0641Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0652Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0643Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0657Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0652Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0649Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0638Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0627Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0631Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0632Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0627Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0623Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0617Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0605Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0614Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0607Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0618Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0616Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0609Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0610Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0614Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0613Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0612Epoch 10/15: [================              ] 40/75 batches, loss: 0.0606Epoch 10/15: [================              ] 41/75 batches, loss: 0.0610Epoch 10/15: [================              ] 42/75 batches, loss: 0.0611Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0610Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0608Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0608Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0608Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0615Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0622Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0622Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0627Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0622Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0621Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0618Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0618Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0615Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0612Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0615Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0613Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0610Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0606Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0605Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0613Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0613Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0617Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0616Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0620Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0619Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0623Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0627Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0629Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0627Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0628Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0626Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0625Epoch 10/15: [==============================] 75/75 batches, loss: 0.0622
[2025-05-07 18:36:43,205][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0622
[2025-05-07 18:36:43,487][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0695, Metrics: {'mse': 0.06952007114887238, 'rmse': 0.26366659088491357, 'r2': -0.1330329179763794}
[2025-05-07 18:36:43,488][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0437Epoch 11/15: [                              ] 2/75 batches, loss: 0.0579Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0500Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0494Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0471Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0463Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0448Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0520Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0535Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0528Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0549Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0560Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0562Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0573Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0611Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0598Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0600Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0594Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0594Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0590Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0593Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0584Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0579Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0593Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0596Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0593Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0585Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0584Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0585Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0597Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0599Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0595Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0587Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0578Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0576Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0578Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0574Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0578Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0572Epoch 11/15: [================              ] 40/75 batches, loss: 0.0565Epoch 11/15: [================              ] 41/75 batches, loss: 0.0564Epoch 11/15: [================              ] 42/75 batches, loss: 0.0573Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0578Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0576Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0571Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0576Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0571Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0568Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0573Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0575Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0573Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0568Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0570Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0568Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0568Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0568Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0564Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0564Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0566Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0569Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0569Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0566Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0563Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0563Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0563Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0560Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0558Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0558Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0564Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0571Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0573Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0583Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0581Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0584Epoch 11/15: [==============================] 75/75 batches, loss: 0.0581
[2025-05-07 18:36:45,881][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0581
[2025-05-07 18:36:46,188][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0679, Metrics: {'mse': 0.06793241202831268, 'rmse': 0.2606384699700194, 'r2': -0.10715734958648682}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0722Epoch 12/15: [                              ] 2/75 batches, loss: 0.0605Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0631Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0598Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0610Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0578Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0565Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0537Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0584Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0576Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0577Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0594Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0605Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0614Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0589Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0591Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0588Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0599Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0595Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0603Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0596Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0585Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0582Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0569Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0558Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0556Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0564Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0572Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0574Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0581Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0586Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0588Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0591Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0595Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0593Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0588Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0585Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0586Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0583Epoch 12/15: [================              ] 40/75 batches, loss: 0.0588Epoch 12/15: [================              ] 41/75 batches, loss: 0.0589Epoch 12/15: [================              ] 42/75 batches, loss: 0.0590Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0585Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0583Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0596Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0595Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0593Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0588Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0585Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0582Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0580Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0586Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0591Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0594Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0590Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0589Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0587Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0583Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0579Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0580Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0581Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0582Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0583Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0583Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0585Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0592Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0595Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0591Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0587Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0592Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0589Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0587Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0590Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0586Epoch 12/15: [==============================] 75/75 batches, loss: 0.0588
[2025-05-07 18:36:48,878][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0588
[2025-05-07 18:36:49,154][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0978, Metrics: {'mse': 0.09749142825603485, 'rmse': 0.3122361738428699, 'r2': -0.5889080762863159}
[2025-05-07 18:36:49,154][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0675Epoch 13/15: [                              ] 2/75 batches, loss: 0.0699Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0739Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0662Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0592Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0640Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0591Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0621Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0615Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0596Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0605Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0586Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0575Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0556Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0572Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0572Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0590Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0588Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0590Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0587Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0588Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0591Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0586Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0571Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0573Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0587Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0585Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0581Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0587Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0585Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0586Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0581Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0572Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0568Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0577Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0574Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0579Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0584Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0581Epoch 13/15: [================              ] 40/75 batches, loss: 0.0582Epoch 13/15: [================              ] 41/75 batches, loss: 0.0583Epoch 13/15: [================              ] 42/75 batches, loss: 0.0574Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0572Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0570Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0566Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0563Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0564Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0557Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0562Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0559Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0553Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0551Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0551Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0556Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0556Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0554Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0557Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0553Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0552Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0552Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0554Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0553Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0548Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0548Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0551Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0548Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0548Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0548Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0549Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0549Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0549Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0546Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0545Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0547Epoch 13/15: [==============================] 75/75 batches, loss: 0.0552
[2025-05-07 18:36:51,457][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0552
[2025-05-07 18:36:51,758][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0702, Metrics: {'mse': 0.07021968066692352, 'rmse': 0.26498996333243174, 'r2': -0.14443504810333252}
[2025-05-07 18:36:51,759][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0376Epoch 14/15: [                              ] 2/75 batches, loss: 0.0391Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0481Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0494Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0520Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0472Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0487Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0471Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0473Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0474Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0477Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0504Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0496Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0514Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0498Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0481Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0499Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0507Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0512Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0504Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0509Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0509Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0510Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0517Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0506Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0509Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0516Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0523Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0533Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0543Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0541Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0534Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0533Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0528Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0534Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0533Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0539Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0541Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0541Epoch 14/15: [================              ] 40/75 batches, loss: 0.0544Epoch 14/15: [================              ] 41/75 batches, loss: 0.0543Epoch 14/15: [================              ] 42/75 batches, loss: 0.0547Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0544Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0550Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0555Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0555Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0552Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0554Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0554Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0558Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0558Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0557Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0553Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0555Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0554Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0556Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0555Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0552Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0553Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0548Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0550Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0547Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0548Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0549Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0547Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0548Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0545Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0545Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0548Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0548Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0548Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0549Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0550Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0547Epoch 14/15: [==============================] 75/75 batches, loss: 0.0546
[2025-05-07 18:36:54,106][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0546
[2025-05-07 18:36:54,373][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0663, Metrics: {'mse': 0.06636209785938263, 'rmse': 0.2576084196205214, 'r2': -0.08156442642211914}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0395Epoch 15/15: [                              ] 2/75 batches, loss: 0.0401Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0436Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0500Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0506Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0470Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0481Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0466Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0481Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0511Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0516Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0511Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0528Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0524Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0512Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0524Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0522Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0519Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0515Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0506Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0509Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0515Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0513Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0515Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0511Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0508Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0516Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0513Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0510Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0509Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0517Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0509Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0519Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0521Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0524Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0527Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0523Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0523Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0529Epoch 15/15: [================              ] 40/75 batches, loss: 0.0531Epoch 15/15: [================              ] 41/75 batches, loss: 0.0532Epoch 15/15: [================              ] 42/75 batches, loss: 0.0529Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0532Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0532Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0529Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0528Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0528Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0528Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0529Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0534Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0534Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0537Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0538Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0538Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0532Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0537Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0534Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0531Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0533Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0532Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0527Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0525Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0526Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0522Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0522Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0523Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0521Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0522Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0521Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0517Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0518Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0517Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0513Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0515Epoch 15/15: [==============================] 75/75 batches, loss: 0.0514
[2025-05-07 18:36:57,207][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0514
[2025-05-07 18:36:57,424][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0767, Metrics: {'mse': 0.07653631269931793, 'rmse': 0.27665197035141087, 'r2': -0.24738311767578125}
[2025-05-07 18:36:57,425][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-07 18:36:57,425][src.training.lm_trainer][INFO] - Training completed in 43.55 seconds
[2025-05-07 18:36:57,425][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:37:00,442][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0403858907520771, 'rmse': 0.20096241129145795, 'r2': -0.008000731468200684}
[2025-05-07 18:37:00,443][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06636209785938263, 'rmse': 0.2576084196205214, 'r2': -0.08156442642211914}
[2025-05-07 18:37:00,443][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06601814180612564, 'rmse': 0.25693995758956145, 'r2': -0.26815176010131836}
[2025-05-07 18:37:02,251][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja/ja/model.pt
[2025-05-07 18:37:02,253][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▄▃▂▂▁
wandb:     best_val_mse █▆▄▃▃▂▂▁
wandb:      best_val_r2 ▁▃▅▆▆▇▇█
wandb:    best_val_rmse █▆▄▄▃▂▂▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▃▄▃▄▄▅▄▄▄▅▁▄▅
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▄▃▄▃▃▂▃▃▂▁█▂▁▃
wandb:          val_mse ▆▄▃▄▃▃▂▃▃▂▁█▂▁▃
wandb:           val_r2 ▃▅▆▅▆▆▇▆▆▇█▁▇█▆
wandb:         val_rmse ▆▄▃▄▃▃▂▄▄▂▁█▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0663
wandb:     best_val_mse 0.06636
wandb:      best_val_r2 -0.08156
wandb:    best_val_rmse 0.25761
wandb:            epoch 15
wandb:   final_test_mse 0.06602
wandb:    final_test_r2 -0.26815
wandb:  final_test_rmse 0.25694
wandb:  final_train_mse 0.04039
wandb:   final_train_r2 -0.008
wandb: final_train_rmse 0.20096
wandb:    final_val_mse 0.06636
wandb:     final_val_r2 -0.08156
wandb:   final_val_rmse 0.25761
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0514
wandb:       train_time 43.5457
wandb:         val_loss 0.07666
wandb:          val_mse 0.07654
wandb:           val_r2 -0.24738
wandb:         val_rmse 0.27665
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183548-5kpw1d2f
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183548-5kpw1d2f/logs
Experiment probe_layer2_complexity_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer6_complexity_control1_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control1_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:37:43,836][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ru
experiment_name: probe_layer6_complexity_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:37:43,836][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:37:43,836][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:37:43,836][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:37:43,841][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:37:43,841][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:37:50,626][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:37:53,072][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:37:53,073][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:37:53,579][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:37:53,683][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:37:54,261][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:37:54,270][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:37:54,270][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:37:54,272][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:37:54,377][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:37:54,548][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:37:54,604][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:37:54,605][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:37:54,606][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:37:54,608][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:37:54,757][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:37:55,009][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:37:55,060][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:37:55,061][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:37:55,062][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:37:55,076][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:37:55,077][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:37:55,077][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:37:55,077][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:37:55,077][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:37:55,077][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:37:55,078][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Sample label: 0.639226496219635
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:37:55,078][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:37:55,078][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:37:55,079][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:37:55,079][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:37:55,079][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:37:55,079][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:37:55,080][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:37:55,080][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:37:55,080][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:37:55,080][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:37:55,080][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:38:08,305][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:38:08,306][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:38:08,306][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:38:08,306][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:38:08,309][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:38:08,310][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:38:08,310][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:38:08,310][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:38:08,310][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:38:08,311][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:38:08,311][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.1987Epoch 1/15: [                              ] 2/75 batches, loss: 0.5229Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4509Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4368Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4589Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4207Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4169Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4239Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4214Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4200Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4051Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4032Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3985Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3866Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3813Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3985Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3919Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3815Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3754Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3642Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3621Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3673Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3629Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3701Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3805Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3724Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3750Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3717Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3716Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3681Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3610Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3598Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3538Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3521Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3520Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3522Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3472Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3441Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3404Epoch 1/15: [================              ] 40/75 batches, loss: 0.3374Epoch 1/15: [================              ] 41/75 batches, loss: 0.3335Epoch 1/15: [================              ] 42/75 batches, loss: 0.3301Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3264Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3249Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3222Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3184Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3172Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3142Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3102Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3079Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3037Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3028Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3001Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3012Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3001Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2991Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2988Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2980Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2955Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2925Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2909Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2886Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2880Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2873Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2894Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2870Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2851Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2825Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2821Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2803Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2787Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2780Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2756Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2738Epoch 1/15: [==============================] 75/75 batches, loss: 0.2732
[2025-05-07 18:38:17,396][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2732
[2025-05-07 18:38:17,702][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0714, Metrics: {'mse': 0.07354896515607834, 'rmse': 0.2711991245488789, 'r2': -0.5810469388961792}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1660Epoch 2/15: [                              ] 2/75 batches, loss: 0.1382Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1190Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1710Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1567Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1507Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1466Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1572Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1527Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1471Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1446Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1496Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1486Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1438Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1390Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1386Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1374Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1384Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1381Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1410Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1402Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1468Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1486Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1483Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1493Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1514Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1528Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1503Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1544Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1536Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1538Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1527Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1541Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1547Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1536Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1509Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1489Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1493Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1511Epoch 2/15: [================              ] 40/75 batches, loss: 0.1501Epoch 2/15: [================              ] 41/75 batches, loss: 0.1511Epoch 2/15: [================              ] 42/75 batches, loss: 0.1499Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1487Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1467Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1470Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1478Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1469Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1469Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1459Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1454Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1448Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1455Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1465Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1461Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1453Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1453Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1435Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1433Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1434Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1422Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1414Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1415Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1416Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1415Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1407Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1404Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1404Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1401Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1390Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1385Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1378Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1371Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1365Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1358Epoch 2/15: [==============================] 75/75 batches, loss: 0.1351
[2025-05-07 18:38:20,473][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1351
[2025-05-07 18:38:20,786][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1098, Metrics: {'mse': 0.11586788296699524, 'rmse': 0.3403937175786228, 'r2': -1.4907565116882324}
[2025-05-07 18:38:20,787][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1922Epoch 3/15: [                              ] 2/75 batches, loss: 0.2083Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1824Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1533Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1346Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1348Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1333Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1230Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1222Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1175Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1149Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1084Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1085Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1072Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1061Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1030Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0999Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0980Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0950Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0951Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0927Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0948Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0954Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0986Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0984Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0984Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0984Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0968Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0975Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0961Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0960Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0968Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0964Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0969Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0959Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0957Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0952Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0950Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0942Epoch 3/15: [================              ] 40/75 batches, loss: 0.0933Epoch 3/15: [================              ] 41/75 batches, loss: 0.0941Epoch 3/15: [================              ] 42/75 batches, loss: 0.0936Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0927Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0932Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0922Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0919Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0927Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0934Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0936Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0927Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0926Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0923Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0913Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0911Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0903Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0903Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0895Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0893Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0892Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0885Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0875Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0877Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0876Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0880Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0872Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0870Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0865Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0867Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0867Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0867Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0864Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0862Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0861Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0858Epoch 3/15: [==============================] 75/75 batches, loss: 0.0868
[2025-05-07 18:38:23,109][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0868
[2025-05-07 18:38:23,471][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0735, Metrics: {'mse': 0.07578746229410172, 'rmse': 0.2752952275178444, 'r2': -0.629166841506958}
[2025-05-07 18:38:23,472][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1449Epoch 4/15: [                              ] 2/75 batches, loss: 0.1085Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0941Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0869Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0872Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0913Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0925Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0923Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1014Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1013Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1014Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1032Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0995Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0956Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0971Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0962Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0922Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0932Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0920Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0924Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0937Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0930Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0916Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0903Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0904Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0888Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0884Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0879Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0866Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0862Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0849Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0839Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0832Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0823Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0824Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0816Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0832Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0829Epoch 4/15: [================              ] 40/75 batches, loss: 0.0837Epoch 4/15: [================              ] 41/75 batches, loss: 0.0828Epoch 4/15: [================              ] 42/75 batches, loss: 0.0828Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0824Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0815Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0814Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0806Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0807Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0800Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0801Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0797Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0796Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0800Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0790Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0789Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0781Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0778Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0780Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0779Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0788Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0786Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0790Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0787Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0783Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0778Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0779Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0776Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0774Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0771Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0770Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0766Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0762Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0755Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0753Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0753Epoch 4/15: [==============================] 75/75 batches, loss: 0.0752
[2025-05-07 18:38:25,828][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0752
[2025-05-07 18:38:26,081][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0691, Metrics: {'mse': 0.06949061900377274, 'rmse': 0.2636107338553814, 'r2': -0.4938066005706787}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0870Epoch 5/15: [                              ] 2/75 batches, loss: 0.0743Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0717Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0765Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0734Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0715Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0858Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0823Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0786Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0752Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0739Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0715Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0691Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0729Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0721Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0698Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0723Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0732Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0731Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0730Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0720Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0706Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0707Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0694Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0685Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0678Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0669Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0695Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0691Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0679Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0676Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0671Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0660Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0665Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0662Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0659Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0654Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0652Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0664Epoch 5/15: [================              ] 40/75 batches, loss: 0.0662Epoch 5/15: [================              ] 41/75 batches, loss: 0.0667Epoch 5/15: [================              ] 42/75 batches, loss: 0.0672Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0674Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0674Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0672Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0682Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0688Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0683Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0688Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0682Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0682Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0677Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0669Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0670Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0663Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0660Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0657Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0656Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0652Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0650Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0643Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0640Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0639Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0638Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0635Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0635Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0631Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0626Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0623Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0619Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0620Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0619Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0616Epoch 5/15: [==============================] 75/75 batches, loss: 0.0614
[2025-05-07 18:38:28,840][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0614
[2025-05-07 18:38:29,138][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0742, Metrics: {'mse': 0.07580917328596115, 'rmse': 0.2753346568922284, 'r2': -0.6296335458755493}
[2025-05-07 18:38:29,139][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0553Epoch 6/15: [                              ] 2/75 batches, loss: 0.0506Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0429Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0374Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0426Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0508Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0532Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0522Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0502Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0493Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0508Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0530Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0518Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0501Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0521Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0503Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0529Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0527Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0527Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0527Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0538Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0546Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0541Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0553Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0558Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0552Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0542Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0541Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0554Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0562Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0553Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0544Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0545Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0547Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0541Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0541Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0537Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0537Epoch 6/15: [================              ] 40/75 batches, loss: 0.0534Epoch 6/15: [================              ] 41/75 batches, loss: 0.0526Epoch 6/15: [================              ] 42/75 batches, loss: 0.0527Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0534Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0538Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0545Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0540Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0541Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0538Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0538Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0539Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0535Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0532Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0530Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0525Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0524Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0532Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0533Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0536Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0536Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0531Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0535Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0534Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0531Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0537Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0533Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0532Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0532Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0532Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0531Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0532Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0529Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0525Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0527Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0527Epoch 6/15: [==============================] 75/75 batches, loss: 0.0527
[2025-05-07 18:38:31,500][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0527
[2025-05-07 18:38:31,861][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0716, Metrics: {'mse': 0.0723935067653656, 'rmse': 0.2690604147126916, 'r2': -0.556208610534668}
[2025-05-07 18:38:31,862][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0312Epoch 7/15: [                              ] 2/75 batches, loss: 0.0548Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0585Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0580Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0523Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0501Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0524Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0512Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0530Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0556Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0563Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0562Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0543Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0533Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0524Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0517Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0528Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0512Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0516Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0517Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0522Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0515Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0508Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0508Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0508Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0502Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0506Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0511Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0504Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0504Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0499Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0491Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0494Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0496Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0492Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0487Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0490Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0491Epoch 7/15: [================              ] 40/75 batches, loss: 0.0490Epoch 7/15: [================              ] 41/75 batches, loss: 0.0491Epoch 7/15: [================              ] 42/75 batches, loss: 0.0488Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0489Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0488Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0490Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0497Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0496Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0492Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0492Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0490Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0488Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0485Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0480Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0480Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0480Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0476Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0473Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0476Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0472Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0469Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0471Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0468Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0465Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0463Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0462Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0459Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0458Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0457Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0462Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0458Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0457Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0457Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0455Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0454Epoch 7/15: [==============================] 75/75 batches, loss: 0.0452
[2025-05-07 18:38:34,204][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0452
[2025-05-07 18:38:34,485][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0832, Metrics: {'mse': 0.08553813397884369, 'rmse': 0.29246903080299574, 'r2': -0.8387724161148071}
[2025-05-07 18:38:34,485][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0398Epoch 8/15: [                              ] 2/75 batches, loss: 0.0379Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0404Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0440Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0396Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0481Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0473Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0450Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0454Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0476Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0466Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0484Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0481Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0457Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0445Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0451Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0455Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0469Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0478Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0493Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0493Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0497Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0488Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0475Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0480Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0480Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0475Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0489Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0491Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0490Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0486Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0480Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0471Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0470Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0469Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0464Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0464Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0460Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0459Epoch 8/15: [================              ] 40/75 batches, loss: 0.0467Epoch 8/15: [================              ] 41/75 batches, loss: 0.0465Epoch 8/15: [================              ] 42/75 batches, loss: 0.0466Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0463Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0459Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0458Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0458Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0453Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0456Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0452Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0448Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0447Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0445Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0441Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0436Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0431Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0432Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0430Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0429Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0424Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0422Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0420Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0419Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0419Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0419Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0420Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0420Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0421Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0420Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0423Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0421Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 8/15: [==============================] 75/75 batches, loss: 0.0423
[2025-05-07 18:38:36,868][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0423
[2025-05-07 18:38:37,334][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0836, Metrics: {'mse': 0.08621305227279663, 'rmse': 0.29362059238547394, 'r2': -0.853280782699585}
[2025-05-07 18:38:37,334][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:38:37,334][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 18:38:37,335][src.training.lm_trainer][INFO] - Training completed in 22.94 seconds
[2025-05-07 18:38:37,335][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:38:40,479][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021190425381064415, 'rmse': 0.1455693146960046, 'r2': -0.0628589391708374}
[2025-05-07 18:38:40,479][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06949061900377274, 'rmse': 0.2636107338553814, 'r2': -0.4938066005706787}
[2025-05-07 18:38:40,479][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0688505619764328, 'rmse': 0.26239390613433233, 'r2': -0.7415101528167725}
[2025-05-07 18:38:42,110][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ru/ru/model.pt
[2025-05-07 18:38:42,111][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▅▅▅▅▄
wandb:       train_loss █▄▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▂▁▂▁▃▃
wandb:          val_mse ▂█▂▁▂▁▃▄
wandb:           val_r2 ▇▁▇█▇█▆▅
wandb:         val_rmse ▂█▂▁▂▁▄▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06908
wandb:     best_val_mse 0.06949
wandb:      best_val_r2 -0.49381
wandb:    best_val_rmse 0.26361
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.06885
wandb:    final_test_r2 -0.74151
wandb:  final_test_rmse 0.26239
wandb:  final_train_mse 0.02119
wandb:   final_train_r2 -0.06286
wandb: final_train_rmse 0.14557
wandb:    final_val_mse 0.06949
wandb:     final_val_r2 -0.49381
wandb:   final_val_rmse 0.26361
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04232
wandb:       train_time 22.94075
wandb:         val_loss 0.08359
wandb:          val_mse 0.08621
wandb:           val_r2 -0.85328
wandb:         val_rmse 0.29362
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183743-qu5m6x5z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183743-qu5m6x5z/logs
Experiment probe_layer6_complexity_control1_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ru/ru/results.json for layer 6
Running experiment: probe_layer6_complexity_control2_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control2_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:39:16,592][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ru
experiment_name: probe_layer6_complexity_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:39:16,592][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:39:16,592][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:39:16,592][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:39:16,597][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:39:16,597][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:39:22,778][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:39:25,174][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:39:25,175][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:39:25,714][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:39:25,927][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:39:26,587][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:39:26,596][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:39:26,597][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:39:26,600][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:39:26,863][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:39:27,137][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:39:27,235][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:39:27,236][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:39:27,236][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:39:27,247][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:39:27,506][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:39:27,948][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:39:28,117][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:39:28,119][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:39:28,119][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:39:28,124][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:39:28,125][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:39:28,125][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:39:28,125][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:39:28,125][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:39:28,125][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:39:28,125][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:39:28,125][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Sample label: 0.28674033284187317
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:39:28,126][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:39:28,126][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:39:28,126][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:39:28,127][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:39:28,127][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:39:28,127][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:39:28,128][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:39:28,128][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:39:28,128][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:39:47,443][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:39:47,444][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:39:47,444][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:39:47,445][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:39:47,447][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:39:47,448][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:39:47,448][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:39:47,448][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:39:47,448][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:39:47,449][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:39:47,449][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.1901Epoch 1/15: [                              ] 2/75 batches, loss: 0.4968Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4555Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4596Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4739Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4341Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4303Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4339Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4334Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4359Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4109Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4059Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3984Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3906Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3825Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3883Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3806Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3744Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3705Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3582Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3571Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3622Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3551Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3670Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3725Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3659Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3692Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3660Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3656Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3608Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3534Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3526Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3487Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3475Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3471Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3467Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3436Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3405Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3373Epoch 1/15: [================              ] 40/75 batches, loss: 0.3339Epoch 1/15: [================              ] 41/75 batches, loss: 0.3299Epoch 1/15: [================              ] 42/75 batches, loss: 0.3280Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3251Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3226Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3190Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3155Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3129Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3100Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3074Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3060Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3024Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2998Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2980Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2974Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2973Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2972Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2972Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2991Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2970Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2941Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2925Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2904Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2888Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2876Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2873Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2856Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2825Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2795Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2794Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2769Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2748Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2732Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2709Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2684Epoch 1/15: [==============================] 75/75 batches, loss: 0.2680
[2025-05-07 18:39:57,539][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2680
[2025-05-07 18:39:57,928][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0794, Metrics: {'mse': 0.08200432360172272, 'rmse': 0.2863639705020915, 'r2': -0.7628077268600464}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2007Epoch 2/15: [                              ] 2/75 batches, loss: 0.1678Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1365Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1626Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1479Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1410Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1424Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1394Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1366Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1367Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1407Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1433Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1450Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1424Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1410Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1380Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1354Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1345Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1352Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1373Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1366Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1428Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1466Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1469Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1486Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1506Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1505Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1482Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1530Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1513Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1495Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1492Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1502Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1494Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1489Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1459Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1446Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1454Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1459Epoch 2/15: [================              ] 40/75 batches, loss: 0.1443Epoch 2/15: [================              ] 41/75 batches, loss: 0.1440Epoch 2/15: [================              ] 42/75 batches, loss: 0.1426Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1425Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1422Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1420Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1423Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1418Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1420Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1410Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1403Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1402Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1404Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1414Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1408Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1395Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1419Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1401Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1403Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1410Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1400Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1400Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1399Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1402Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1405Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1394Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1398Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1399Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1393Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1381Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1375Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1367Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1358Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1355Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1352Epoch 2/15: [==============================] 75/75 batches, loss: 0.1341
[2025-05-07 18:40:01,162][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1341
[2025-05-07 18:40:01,546][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1116, Metrics: {'mse': 0.11807277798652649, 'rmse': 0.34361719687251757, 'r2': -1.538153886795044}
[2025-05-07 18:40:01,547][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2111Epoch 3/15: [                              ] 2/75 batches, loss: 0.1837Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1425Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1213Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1113Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1189Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1167Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1076Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1119Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1066Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1080Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1037Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1048Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1075Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1068Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1050Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1029Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0999Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0976Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0956Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0927Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0932Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0948Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0993Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1009Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0995Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0985Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0974Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0976Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0962Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0962Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0958Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0949Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0948Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0937Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0924Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0923Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0915Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0911Epoch 3/15: [================              ] 40/75 batches, loss: 0.0910Epoch 3/15: [================              ] 41/75 batches, loss: 0.0922Epoch 3/15: [================              ] 42/75 batches, loss: 0.0921Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0921Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0922Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0923Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0910Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0914Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0935Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0940Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0930Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0933Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0932Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0924Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0924Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0917Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0910Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0906Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0904Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0906Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0909Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0901Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0901Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0899Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0904Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0901Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0902Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0899Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0892Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0890Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0888Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0883Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0880Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0877Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0877Epoch 3/15: [==============================] 75/75 batches, loss: 0.0881
[2025-05-07 18:40:03,953][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0881
[2025-05-07 18:40:04,413][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0772, Metrics: {'mse': 0.0799725353717804, 'rmse': 0.28279415724477125, 'r2': -0.7191312313079834}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0792Epoch 4/15: [                              ] 2/75 batches, loss: 0.0814Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0832Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0778Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0759Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0745Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0766Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0760Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0831Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0810Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0809Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0827Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0844Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0818Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0875Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0877Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0869Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0849Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0867Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0870Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0879Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0893Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0901Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0897Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0898Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0889Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0864Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0870Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0870Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0851Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0849Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0840Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0827Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0820Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0815Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0822Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0815Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0815Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0806Epoch 4/15: [================              ] 40/75 batches, loss: 0.0797Epoch 4/15: [================              ] 41/75 batches, loss: 0.0787Epoch 4/15: [================              ] 42/75 batches, loss: 0.0785Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0788Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0780Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0776Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0774Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0781Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0774Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0771Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0765Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0770Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0773Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0772Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0770Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0766Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0766Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0757Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0753Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0750Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0743Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0767Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0766Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0761Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0759Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0755Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0747Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0743Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0740Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0738Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0736Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0733Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0733Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0729Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0729Epoch 4/15: [==============================] 75/75 batches, loss: 0.0737
[2025-05-07 18:40:07,186][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0737
[2025-05-07 18:40:07,498][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0714, Metrics: {'mse': 0.07189512252807617, 'rmse': 0.26813265845114087, 'r2': -0.5454950332641602}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0903Epoch 5/15: [                              ] 2/75 batches, loss: 0.0732Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0751Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0747Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0744Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0749Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0777Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0776Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0749Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0701Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0724Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0708Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0690Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0701Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0683Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0686Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0697Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0700Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0724Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0715Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0705Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0687Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0688Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0675Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0677Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0672Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0682Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0693Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0686Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0680Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0681Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0682Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0673Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0680Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0674Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0666Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0656Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0644Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0638Epoch 5/15: [================              ] 40/75 batches, loss: 0.0642Epoch 5/15: [================              ] 41/75 batches, loss: 0.0651Epoch 5/15: [================              ] 42/75 batches, loss: 0.0652Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0656Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0654Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0652Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0651Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0657Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0662Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0656Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0652Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0669Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0664Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0658Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0657Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0655Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0651Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0649Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0649Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0643Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0642Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0639Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0641Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0643Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0641Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0638Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0639Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0635Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0630Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0627Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0621Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0618Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0616Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0614Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0609Epoch 5/15: [==============================] 75/75 batches, loss: 0.0607
[2025-05-07 18:40:10,172][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0607
[2025-05-07 18:40:10,506][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0717, Metrics: {'mse': 0.07305984944105148, 'rmse': 0.27029585539007345, 'r2': -0.5705326795578003}
[2025-05-07 18:40:10,506][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0527Epoch 6/15: [                              ] 2/75 batches, loss: 0.0390Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0436Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0403Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0381Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0442Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0500Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0528Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0524Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0526Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0534Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0526Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0513Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0519Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0509Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0513Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0502Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0521Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0519Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0507Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0515Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0514Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0507Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0497Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0510Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0522Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0536Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0536Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0540Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0554Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0560Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0571Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0563Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0566Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0570Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0570Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0571Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0563Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0568Epoch 6/15: [================              ] 40/75 batches, loss: 0.0562Epoch 6/15: [================              ] 41/75 batches, loss: 0.0558Epoch 6/15: [================              ] 42/75 batches, loss: 0.0558Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0564Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0567Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0570Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0565Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0565Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0558Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0559Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0564Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0559Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0558Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0552Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0545Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0546Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0546Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0548Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0545Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0545Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0547Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0549Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0544Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0545Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0544Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0549Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0551Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0551Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0553Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0550Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0551Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0549Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0546Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0542Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0538Epoch 6/15: [==============================] 75/75 batches, loss: 0.0534
[2025-05-07 18:40:12,879][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0534
[2025-05-07 18:40:13,222][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0737, Metrics: {'mse': 0.07529516518115997, 'rmse': 0.27439964500917263, 'r2': -0.618584156036377}
[2025-05-07 18:40:13,222][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0613Epoch 7/15: [                              ] 2/75 batches, loss: 0.0663Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0619Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0637Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0553Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0517Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0510Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0515Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0501Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0477Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0482Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0528Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0514Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0536Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0520Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0519Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0525Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0527Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0516Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0502Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0506Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0503Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0508Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0503Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0517Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0511Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0515Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0509Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0518Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0514Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0506Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0503Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0499Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0498Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0498Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0497Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0496Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0493Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0487Epoch 7/15: [================              ] 40/75 batches, loss: 0.0485Epoch 7/15: [================              ] 41/75 batches, loss: 0.0491Epoch 7/15: [================              ] 42/75 batches, loss: 0.0490Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0486Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0483Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0482Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0483Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0484Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0482Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0481Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0477Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0481Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0480Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0487Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0488Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0493Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0492Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0492Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0493Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0491Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0491Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0487Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0490Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0487Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0487Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0486Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0482Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0483Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0480Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0478Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0478Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0476Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0473Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0474Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0472Epoch 7/15: [==============================] 75/75 batches, loss: 0.0472
[2025-05-07 18:40:15,577][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0472
[2025-05-07 18:40:15,897][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0818, Metrics: {'mse': 0.08378851413726807, 'rmse': 0.2894624572155568, 'r2': -0.8011616468429565}
[2025-05-07 18:40:15,897][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0465Epoch 8/15: [                              ] 2/75 batches, loss: 0.0400Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0400Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0414Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0398Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0495Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0477Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0458Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0487Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0493Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0478Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0466Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0458Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0448Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0450Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0455Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0451Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0467Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0457Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0464Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0456Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0459Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0452Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0451Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0457Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0458Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0453Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0452Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0458Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0462Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0457Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0455Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0453Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0446Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0443Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0437Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0433Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0434Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0438Epoch 8/15: [================              ] 40/75 batches, loss: 0.0443Epoch 8/15: [================              ] 41/75 batches, loss: 0.0441Epoch 8/15: [================              ] 42/75 batches, loss: 0.0441Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0443Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0445Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0440Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0446Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0446Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0445Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0451Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0449Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0445Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0446Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0442Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0439Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0436Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0434Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0431Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0427Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0429Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0426Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0425Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0424Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0424Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0422Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0425Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0424Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0423Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0423Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0421Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0421Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0419Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0423Epoch 8/15: [==============================] 75/75 batches, loss: 0.0421
[2025-05-07 18:40:18,242][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0421
[2025-05-07 18:40:18,552][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0791, Metrics: {'mse': 0.08095142245292664, 'rmse': 0.28451963456486906, 'r2': -0.7401740550994873}
[2025-05-07 18:40:18,553][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:40:18,553][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 18:40:18,553][src.training.lm_trainer][INFO] - Training completed in 24.18 seconds
[2025-05-07 18:40:18,553][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:40:21,674][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02163882926106453, 'rmse': 0.1471014250816916, 'r2': -0.085349440574646}
[2025-05-07 18:40:21,674][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07189512252807617, 'rmse': 0.26813265845114087, 'r2': -0.5454950332641602}
[2025-05-07 18:40:21,674][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07083676755428314, 'rmse': 0.26615177541072904, 'r2': -0.7917493581771851}
[2025-05-07 18:40:23,380][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ru/ru/model.pt
[2025-05-07 18:40:23,381][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▁
wandb:     best_val_mse █▇▁
wandb:      best_val_r2 ▁▂█
wandb:    best_val_rmse █▇▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▁▅▅▅▅▄
wandb:       train_loss █▄▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▂█▂▁▁▁▃▂
wandb:          val_mse ▃█▂▁▁▂▃▂
wandb:           val_r2 ▆▁▇██▇▆▇
wandb:         val_rmse ▃█▂▁▁▂▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0714
wandb:     best_val_mse 0.0719
wandb:      best_val_r2 -0.5455
wandb:    best_val_rmse 0.26813
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.07084
wandb:    final_test_r2 -0.79175
wandb:  final_test_rmse 0.26615
wandb:  final_train_mse 0.02164
wandb:   final_train_r2 -0.08535
wandb: final_train_rmse 0.1471
wandb:    final_val_mse 0.0719
wandb:     final_val_r2 -0.5455
wandb:   final_val_rmse 0.26813
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04209
wandb:       train_time 24.18123
wandb:         val_loss 0.07915
wandb:          val_mse 0.08095
wandb:           val_r2 -0.74017
wandb:         val_rmse 0.28452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183916-wiaj0lnt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_183916-wiaj0lnt/logs
Experiment probe_layer6_complexity_control2_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ru/ru/results.json for layer 6
Running experiment: probe_layer6_complexity_control3_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control3_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:40:58,652][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ru
experiment_name: probe_layer6_complexity_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:40:58,653][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:40:58,653][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:40:58,653][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:40:58,657][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-07 18:40:58,657][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:41:02,102][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:41:04,414][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:41:04,415][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:41:04,606][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:41:04,696][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:41:04,989][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:41:04,997][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:41:04,998][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:41:04,999][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:41:05,101][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:41:05,230][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:41:05,252][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:41:05,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:41:05,254][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:41:05,256][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:41:05,317][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:41:05,418][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:41:05,439][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:41:05,441][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:41:05,441][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:41:05,444][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:41:05,444][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:41:05,444][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:41:05,444][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:41:05,444][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:41:05,444][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:41:05,445][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Sample label: 0.4091160297393799
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:41:05,445][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:41:05,445][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:41:05,446][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:41:05,446][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:41:05,446][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:41:05,446][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-07 18:41:05,447][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:41:05,447][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:41:05,447][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:41:05,447][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:41:05,447][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:41:13,883][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:41:13,884][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:41:13,884][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:41:13,884][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:41:13,887][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:41:13,887][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:41:13,887][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:41:13,888][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:41:13,888][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:41:13,889][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:41:13,889][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.1614Epoch 1/15: [                              ] 2/75 batches, loss: 0.4872Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4179Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4263Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4718Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4384Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4342Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4342Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4329Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4228Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4147Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4050Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4058Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3938Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3873Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3958Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3920Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3818Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3778Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3645Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3604Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3663Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3614Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3692Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3766Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3678Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3701Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3686Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3685Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3671Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3617Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3609Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3575Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3549Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3547Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3551Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3500Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3465Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3431Epoch 1/15: [================              ] 40/75 batches, loss: 0.3398Epoch 1/15: [================              ] 41/75 batches, loss: 0.3378Epoch 1/15: [================              ] 42/75 batches, loss: 0.3342Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3306Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3294Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3257Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3232Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3213Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3194Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3150Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3130Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3106Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3082Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3058Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3064Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3059Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3044Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3046Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3042Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3017Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2991Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2986Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2963Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2955Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2946Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2954Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2940Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2905Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2873Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2876Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2863Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2846Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2826Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2802Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2789Epoch 1/15: [==============================] 75/75 batches, loss: 0.2771
[2025-05-07 18:41:21,054][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2771
[2025-05-07 18:41:21,480][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0766, Metrics: {'mse': 0.07855457067489624, 'rmse': 0.28027588314890073, 'r2': -0.6886500120162964}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1079Epoch 2/15: [                              ] 2/75 batches, loss: 0.1327Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1114Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1394Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1287Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1238Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1283Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1312Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1281Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1267Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1319Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1354Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1345Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1368Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1336Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1321Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1309Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1316Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1341Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1332Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1325Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1394Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1407Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1396Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1418Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1441Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1431Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1396Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1423Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1417Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1405Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1391Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1389Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1393Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1390Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1364Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1367Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1377Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1391Epoch 2/15: [================              ] 40/75 batches, loss: 0.1369Epoch 2/15: [================              ] 41/75 batches, loss: 0.1384Epoch 2/15: [================              ] 42/75 batches, loss: 0.1375Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1362Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1353Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1347Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1365Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1351Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1354Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1347Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1342Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1332Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1333Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1352Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1354Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1340Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1357Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1344Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1333Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1335Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1322Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1318Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1315Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1329Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1340Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1326Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1319Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1320Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1318Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1308Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1303Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1293Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1284Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1288Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1284Epoch 2/15: [==============================] 75/75 batches, loss: 0.1272
[2025-05-07 18:41:24,315][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1272
[2025-05-07 18:41:24,769][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0970, Metrics: {'mse': 0.10184109956026077, 'rmse': 0.3191255232040533, 'r2': -1.1892294883728027}
[2025-05-07 18:41:24,770][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2103Epoch 3/15: [                              ] 2/75 batches, loss: 0.1780Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1660Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1450Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1281Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1319Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1300Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1204Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1154Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1106Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1095Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1046Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1094Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1080Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1083Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1055Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1018Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1014Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1003Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1006Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0982Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0988Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1012Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1044Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1076Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1088Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1073Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1046Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1037Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1021Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1017Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1024Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1013Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1017Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1002Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0987Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0976Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0968Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0959Epoch 3/15: [================              ] 40/75 batches, loss: 0.0957Epoch 3/15: [================              ] 41/75 batches, loss: 0.0966Epoch 3/15: [================              ] 42/75 batches, loss: 0.0958Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0955Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0954Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0946Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0946Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0950Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0962Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0964Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0958Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0952Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0944Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0936Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0939Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0927Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0921Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0924Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0923Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0914Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0906Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0912Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0906Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0919Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0917Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0913Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0909Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0909Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0909Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0903Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0902Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0899Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0895Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0893Epoch 3/15: [==============================] 75/75 batches, loss: 0.0904
[2025-05-07 18:41:27,237][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0904
[2025-05-07 18:41:27,763][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0777, Metrics: {'mse': 0.08021216839551926, 'rmse': 0.2832175284044391, 'r2': -0.7242826223373413}
[2025-05-07 18:41:27,764][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0878Epoch 4/15: [                              ] 2/75 batches, loss: 0.0806Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0886Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0851Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0895Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0857Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0823Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0811Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0814Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0829Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0819Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0800Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0804Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0762Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0798Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0792Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0779Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0781Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0793Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0807Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0802Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0819Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0832Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0818Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0828Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0829Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0816Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0806Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0801Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0798Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0794Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0782Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0792Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0787Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0788Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0795Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0790Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0812Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0803Epoch 4/15: [================              ] 40/75 batches, loss: 0.0794Epoch 4/15: [================              ] 41/75 batches, loss: 0.0786Epoch 4/15: [================              ] 42/75 batches, loss: 0.0782Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0781Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0771Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0775Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0770Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0778Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0775Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0771Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0764Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0766Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0770Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0769Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0770Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0761Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0760Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0757Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0759Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0757Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0751Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0758Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0752Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0751Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0748Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0749Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0744Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0741Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0740Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0739Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0739Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0735Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0732Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0731Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0728Epoch 4/15: [==============================] 75/75 batches, loss: 0.0726
[2025-05-07 18:41:30,159][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0726
[2025-05-07 18:41:30,869][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0820, Metrics: {'mse': 0.0839519053697586, 'rmse': 0.2897445519242055, 'r2': -0.8046739101409912}
[2025-05-07 18:41:30,869][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0848Epoch 5/15: [                              ] 2/75 batches, loss: 0.0618Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0710Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0659Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0695Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0687Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0810Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0792Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0755Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0709Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0682Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0670Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0680Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0721Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0720Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0724Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0766Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0765Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0755Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0736Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0728Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0742Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0732Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0737Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0728Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0729Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0731Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0724Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0711Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0697Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0706Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0702Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0698Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0704Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0694Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0684Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0683Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0681Epoch 5/15: [================              ] 40/75 batches, loss: 0.0683Epoch 5/15: [================              ] 41/75 batches, loss: 0.0679Epoch 5/15: [================              ] 42/75 batches, loss: 0.0684Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0677Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0674Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0668Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0667Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0664Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0663Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0661Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0662Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0666Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0660Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0653Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0648Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0646Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0643Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0641Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0644Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0638Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0634Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0630Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0632Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0631Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0632Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0630Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0635Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0633Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0635Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0631Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0623Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0620Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0620Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0617Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0616Epoch 5/15: [==============================] 75/75 batches, loss: 0.0624
[2025-05-07 18:41:33,461][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0624
[2025-05-07 18:41:33,951][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0680, Metrics: {'mse': 0.06770381331443787, 'rmse': 0.26019956440093794, 'r2': -0.4553964138031006}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0320Epoch 6/15: [                              ] 2/75 batches, loss: 0.0415Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0403Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0457Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0486Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0504Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0532Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0528Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0511Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0508Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0528Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0524Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0513Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0498Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0501Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0512Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0497Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0523Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0534Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0544Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0544Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0542Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0542Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0533Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0536Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0532Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0528Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0524Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0526Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0536Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0555Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0548Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0538Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0541Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0539Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0539Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0535Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0534Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0534Epoch 6/15: [================              ] 40/75 batches, loss: 0.0532Epoch 6/15: [================              ] 41/75 batches, loss: 0.0537Epoch 6/15: [================              ] 42/75 batches, loss: 0.0546Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0550Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0555Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0552Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0548Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0550Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0548Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0543Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0547Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0543Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0542Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0539Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0534Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0529Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0532Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0534Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0529Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0528Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0527Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0526Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0526Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0524Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0522Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0520Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0523Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0521Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0527Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0522Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0526Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0527Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0528Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0525Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0528Epoch 6/15: [==============================] 75/75 batches, loss: 0.0528
[2025-05-07 18:41:36,914][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0528
[2025-05-07 18:41:37,350][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0747, Metrics: {'mse': 0.07552966475486755, 'rmse': 0.27482660852775437, 'r2': -0.6236251592636108}
[2025-05-07 18:41:37,351][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0401Epoch 7/15: [                              ] 2/75 batches, loss: 0.0623Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0582Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0714Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0707Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0655Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0607Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0584Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0592Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0581Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0589Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0586Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0561Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0570Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0555Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0564Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0549Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0541Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0530Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0543Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0544Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0548Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0554Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0555Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0554Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0556Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0558Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0557Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0555Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0545Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0538Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0535Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0529Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0527Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0528Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0522Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0520Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0520Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0513Epoch 7/15: [================              ] 40/75 batches, loss: 0.0509Epoch 7/15: [================              ] 41/75 batches, loss: 0.0514Epoch 7/15: [================              ] 42/75 batches, loss: 0.0514Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0520Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0521Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0522Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0530Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0535Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0534Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0533Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0531Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0525Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0519Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0514Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0512Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0512Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0510Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0509Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0509Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0506Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0504Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0499Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0498Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0495Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0490Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0488Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0488Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0485Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0483Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0481Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0481Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0479Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0476Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0480Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0478Epoch 7/15: [==============================] 75/75 batches, loss: 0.0475
[2025-05-07 18:41:39,809][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0475
[2025-05-07 18:41:40,196][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0861, Metrics: {'mse': 0.08828957378864288, 'rmse': 0.297135615146759, 'r2': -0.897918701171875}
[2025-05-07 18:41:40,197][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0496Epoch 8/15: [                              ] 2/75 batches, loss: 0.0370Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0424Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0448Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0397Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0484Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0481Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0472Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0442Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0463Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0439Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0452Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0448Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0434Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0440Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0435Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0439Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0448Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0439Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0436Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0436Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0437Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0431Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0433Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0445Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0446Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0443Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0439Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0443Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0442Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0435Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0435Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0437Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0434Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0428Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0422Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0423Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0419Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0424Epoch 8/15: [================              ] 40/75 batches, loss: 0.0421Epoch 8/15: [================              ] 41/75 batches, loss: 0.0424Epoch 8/15: [================              ] 42/75 batches, loss: 0.0428Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0430Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0431Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0430Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0436Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0436Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0436Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0438Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0442Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0444Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0447Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0446Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0445Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0443Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0442Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0442Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0439Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0439Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0436Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0434Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0435Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0433Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0435Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0434Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0432Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0432Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0432Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0436Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0437Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0434Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0432Epoch 8/15: [==============================] 75/75 batches, loss: 0.0431
[2025-05-07 18:41:42,744][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0431
[2025-05-07 18:41:43,100][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0850, Metrics: {'mse': 0.08712679147720337, 'rmse': 0.29517247750629355, 'r2': -0.8729230165481567}
[2025-05-07 18:41:43,101][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0196Epoch 9/15: [                              ] 2/75 batches, loss: 0.0218Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0328Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0325Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0324Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0316Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0330Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0367Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0378Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0360Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0350Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0341Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0339Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0340Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0346Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0347Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0351Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0344Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0360Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0356Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0356Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0357Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0351Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0358Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0368Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0365Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0365Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0369Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0378Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0375Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0380Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0376Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0376Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0383Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0388Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0388Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0393Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0389Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0400Epoch 9/15: [================              ] 40/75 batches, loss: 0.0406Epoch 9/15: [================              ] 41/75 batches, loss: 0.0406Epoch 9/15: [================              ] 42/75 batches, loss: 0.0404Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0399Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0396Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0396Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0394Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0390Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0390Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0391Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0392Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0393Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0395Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0404Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0402Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0403Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0400Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0399Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0398Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0404Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0405Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0408Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0407Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0410Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0410Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0407Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0410Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0408Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0407Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0408Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0409Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0406Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0409Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0411Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0409Epoch 9/15: [==============================] 75/75 batches, loss: 0.0407
[2025-05-07 18:41:45,592][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0407
[2025-05-07 18:41:45,983][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0740, Metrics: {'mse': 0.07437405735254288, 'rmse': 0.2727160746134024, 'r2': -0.5987836122512817}
[2025-05-07 18:41:45,984][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:41:45,984][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 18:41:45,984][src.training.lm_trainer][INFO] - Training completed in 27.98 seconds
[2025-05-07 18:41:45,984][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:41:49,410][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02030281163752079, 'rmse': 0.14248793505950175, 'r2': -0.01833820343017578}
[2025-05-07 18:41:49,410][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06770381331443787, 'rmse': 0.26019956440093794, 'r2': -0.4553964138031006}
[2025-05-07 18:41:49,411][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06665117293596268, 'rmse': 0.258168884523218, 'r2': -0.6858787536621094}
[2025-05-07 18:41:51,661][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ru/ru/model.pt
[2025-05-07 18:41:51,662][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▁▄▃▅▄▂▃
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▃█▃▄▁▃▅▅▂
wandb:          val_mse ▃█▄▄▁▃▅▅▂
wandb:           val_r2 ▆▁▅▅█▆▄▄▇
wandb:         val_rmse ▃█▄▅▁▃▅▅▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06796
wandb:     best_val_mse 0.0677
wandb:      best_val_r2 -0.4554
wandb:    best_val_rmse 0.2602
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.06665
wandb:    final_test_r2 -0.68588
wandb:  final_test_rmse 0.25817
wandb:  final_train_mse 0.0203
wandb:   final_train_r2 -0.01834
wandb: final_train_rmse 0.14249
wandb:    final_val_mse 0.0677
wandb:     final_val_r2 -0.4554
wandb:   final_val_rmse 0.2602
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04074
wandb:       train_time 27.97702
wandb:         val_loss 0.07399
wandb:          val_mse 0.07437
wandb:           val_r2 -0.59878
wandb:         val_rmse 0.27272
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184058-eiqz6m6r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184058-eiqz6m6r/logs
Experiment probe_layer6_complexity_control3_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ru/ru/results.json for layer 6
Running experiment: probe_layer6_complexity_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:42:28,876][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ja
experiment_name: probe_layer6_complexity_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:42:28,876][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:42:28,877][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:42:28,877][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:42:28,881][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:42:28,881][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:42:33,101][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:42:35,576][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:42:35,577][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:42:35,752][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:42:35,841][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 18:42:36,077][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:42:36,086][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:42:36,086][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:42:36,089][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:42:36,227][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:42:36,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:42:36,361][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:42:36,362][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:42:36,362][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:42:36,364][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:42:36,442][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:42:36,514][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:42:36,537][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:42:36,538][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:42:36,538][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:42:36,539][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:42:36,540][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:42:36,540][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:42:36,540][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:42:36,540][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:42:36,540][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:42:36,541][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Sample label: 0.5826417803764343
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:42:36,541][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:42:36,541][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:42:36,542][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:42:36,542][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:42:36,542][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:42:36,542][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:42:36,543][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:42:36,543][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:42:36,543][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:42:36,543][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:42:43,399][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:42:43,400][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:42:43,400][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:42:43,400][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:42:43,403][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:42:43,403][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:42:43,404][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:42:43,404][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:42:43,404][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:42:43,405][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:42:43,405][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2745Epoch 1/15: [                              ] 2/75 batches, loss: 0.4172Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3566Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3690Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3773Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3756Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3729Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3843Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4025Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4058Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3841Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3850Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3731Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3881Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3768Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3816Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3916Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3910Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3903Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3910Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3824Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3946Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3865Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3838Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3813Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3799Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3775Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3721Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3669Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3636Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3571Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3534Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3507Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3469Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3436Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3440Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3418Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3370Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3390Epoch 1/15: [================              ] 40/75 batches, loss: 0.3356Epoch 1/15: [================              ] 41/75 batches, loss: 0.3319Epoch 1/15: [================              ] 42/75 batches, loss: 0.3288Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3256Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3233Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3244Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3196Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3172Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3161Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3123Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3100Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3066Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3024Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3022Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3020Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3005Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2989Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2957Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2948Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2922Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2908Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2900Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2885Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2891Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2882Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2858Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2843Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2826Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2817Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2830Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2829Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2814Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2809Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2787Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2782Epoch 1/15: [==============================] 75/75 batches, loss: 0.2778
[2025-05-07 18:42:50,160][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2778
[2025-05-07 18:42:50,362][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0782, Metrics: {'mse': 0.07870540767908096, 'rmse': 0.28054484076361297, 'r2': -0.282734751701355}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2890Epoch 2/15: [                              ] 2/75 batches, loss: 0.2687Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2441Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2283Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2249Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2089Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2151Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2019Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2036Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2078Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2021Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1990Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1943Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1908Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1887Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1871Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1834Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1893Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1884Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1847Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1863Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1842Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1823Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1817Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1791Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1821Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1862Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1849Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1858Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1849Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1822Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1803Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1818Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1798Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1792Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1771Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1789Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1770Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1751Epoch 2/15: [================              ] 40/75 batches, loss: 0.1757Epoch 2/15: [================              ] 41/75 batches, loss: 0.1742Epoch 2/15: [================              ] 42/75 batches, loss: 0.1724Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1705Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1706Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1707Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1692Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1690Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1697Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1702Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1700Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1710Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1698Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1697Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1691Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1674Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1690Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1684Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1681Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1664Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1648Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1636Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1644Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1646Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1641Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1623Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1616Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1614Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1612Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1600Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1600Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1591Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1587Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1586Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1578Epoch 2/15: [==============================] 75/75 batches, loss: 0.1583
[2025-05-07 18:42:53,126][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1583
[2025-05-07 18:42:53,384][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0831, Metrics: {'mse': 0.08255162090063095, 'rmse': 0.2873179787285003, 'r2': -0.34542012214660645}
[2025-05-07 18:42:53,385][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2180Epoch 3/15: [                              ] 2/75 batches, loss: 0.1698Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1475Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1306Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1381Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1477Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1466Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1370Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1390Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1302Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1255Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1200Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1200Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1164Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1151Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1160Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1116Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1132Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1155Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1168Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1171Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1169Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1178Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1175Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1169Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1155Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1157Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1193Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1192Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1192Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1210Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1251Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1257Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1265Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1246Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1237Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1244Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1230Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1220Epoch 3/15: [================              ] 40/75 batches, loss: 0.1216Epoch 3/15: [================              ] 41/75 batches, loss: 0.1219Epoch 3/15: [================              ] 42/75 batches, loss: 0.1214Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1206Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1189Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1204Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1190Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1185Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1175Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1182Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1177Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1175Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1169Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1160Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1159Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1151Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1143Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1152Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1165Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1179Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1182Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1178Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1172Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1172Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1169Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1184Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1175Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1170Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1180Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1182Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1175Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1172Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1164Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1171Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1167Epoch 3/15: [==============================] 75/75 batches, loss: 0.1170
[2025-05-07 18:42:55,685][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1170
[2025-05-07 18:42:55,943][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0815, Metrics: {'mse': 0.08108482509851456, 'rmse': 0.2847539729284116, 'r2': -0.321514368057251}
[2025-05-07 18:42:55,944][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1188Epoch 4/15: [                              ] 2/75 batches, loss: 0.1073Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1209Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1068Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0976Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1005Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0992Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1019Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1031Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1003Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1068Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1056Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1040Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1010Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0975Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0975Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0990Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0996Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1005Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1020Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1010Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1013Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0995Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1014Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1014Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0996Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1004Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0998Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1000Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0993Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0993Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0992Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0986Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0971Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0973Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0969Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0956Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0946Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0936Epoch 4/15: [================              ] 40/75 batches, loss: 0.0944Epoch 4/15: [================              ] 41/75 batches, loss: 0.0944Epoch 4/15: [================              ] 42/75 batches, loss: 0.0950Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0954Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0959Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0956Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0956Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0954Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0961Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0960Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0950Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0948Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0944Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0947Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0941Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0934Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0934Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0927Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0927Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0928Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0926Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0928Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0927Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0928Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0923Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0922Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0925Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0930Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0930Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0930Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0926Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0923Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0926Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0922Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0928Epoch 4/15: [==============================] 75/75 batches, loss: 0.0938
[2025-05-07 18:42:58,268][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0938
[2025-05-07 18:42:58,515][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0824, Metrics: {'mse': 0.08206398785114288, 'rmse': 0.2864681271121499, 'r2': -0.33747267723083496}
[2025-05-07 18:42:58,516][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1415Epoch 5/15: [                              ] 2/75 batches, loss: 0.1242Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1070Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1038Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0949Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1094Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1063Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0985Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0979Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1021Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1053Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1088Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.1049Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.1063Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1078Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1050Epoch 5/15: [======                        ] 17/75 batches, loss: 0.1032Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1024Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.1003Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0978Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0974Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0969Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0984Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0965Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0968Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0956Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0960Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0941Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0934Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0937Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0921Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0922Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0919Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0906Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0913Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0903Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0902Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0904Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0889Epoch 5/15: [================              ] 40/75 batches, loss: 0.0884Epoch 5/15: [================              ] 41/75 batches, loss: 0.0889Epoch 5/15: [================              ] 42/75 batches, loss: 0.0894Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0886Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0880Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0869Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0875Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0872Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0866Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0860Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0862Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0857Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0856Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0857Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0854Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0852Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0850Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0850Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0847Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0848Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0854Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0854Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0854Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0850Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0848Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0856Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0857Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0854Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0851Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0847Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0845Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0844Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0843Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0840Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0841Epoch 5/15: [==============================] 75/75 batches, loss: 0.0845
[2025-05-07 18:43:00,839][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0845
[2025-05-07 18:43:01,072][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0719, Metrics: {'mse': 0.0717877745628357, 'rmse': 0.26793240670519064, 'r2': -0.16999173164367676}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0990Epoch 6/15: [                              ] 2/75 batches, loss: 0.0820Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0775Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0724Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0775Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0814Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0896Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0860Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0866Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0882Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0901Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0947Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0932Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0908Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0889Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0898Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0924Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0920Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0916Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0913Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0916Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0905Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0880Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0882Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0888Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0885Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0888Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0897Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0877Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0879Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0870Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0864Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0857Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0853Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0842Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0834Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0836Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0835Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0840Epoch 6/15: [================              ] 40/75 batches, loss: 0.0840Epoch 6/15: [================              ] 41/75 batches, loss: 0.0845Epoch 6/15: [================              ] 42/75 batches, loss: 0.0842Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0841Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0840Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0830Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0827Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0822Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0817Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0824Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0826Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0834Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0825Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0823Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0819Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0810Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0811Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0804Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0811Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0810Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0802Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0795Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0786Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0781Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0783Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0785Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0785Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0784Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0783Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0781Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0777Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0776Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0772Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0768Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0768Epoch 6/15: [==============================] 75/75 batches, loss: 0.0768
[2025-05-07 18:43:03,785][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0768
[2025-05-07 18:43:04,078][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0663, Metrics: {'mse': 0.06632266193628311, 'rmse': 0.25753186586572757, 'r2': -0.08092176914215088}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0481Epoch 7/15: [                              ] 2/75 batches, loss: 0.0549Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0488Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0518Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0579Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0608Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0595Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0607Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0624Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0615Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0604Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0580Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0566Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0577Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0564Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0546Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0537Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0525Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0547Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0559Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0561Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0555Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0556Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0561Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0592Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0613Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0617Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0610Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0624Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0645Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0640Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0640Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0638Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0639Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0651Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0642Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0634Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0645Epoch 7/15: [================              ] 40/75 batches, loss: 0.0668Epoch 7/15: [================              ] 41/75 batches, loss: 0.0661Epoch 7/15: [================              ] 42/75 batches, loss: 0.0660Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0666Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0677Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0675Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0671Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0663Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0660Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0661Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0668Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0660Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0653Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0659Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0662Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0659Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0661Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0660Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0655Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0659Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0663Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0666Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0663Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0664Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0662Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0677Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0674Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0670Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0672Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0670Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0668Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0671Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0670Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0671Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0670Epoch 7/15: [==============================] 75/75 batches, loss: 0.0668
[2025-05-07 18:43:06,845][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0668
[2025-05-07 18:43:07,081][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0707, Metrics: {'mse': 0.07060246169567108, 'rmse': 0.2657112374282862, 'r2': -0.15067362785339355}
[2025-05-07 18:43:07,082][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0682Epoch 8/15: [                              ] 2/75 batches, loss: 0.0850Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0738Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0703Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0673Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0643Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0629Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0601Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0586Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0596Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0607Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0596Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0595Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0606Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0617Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0600Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0613Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0600Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0597Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0605Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0601Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0609Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0627Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0631Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0645Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0632Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0642Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0646Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0639Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0641Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0645Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0656Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0656Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0666Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0667Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0664Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0667Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0674Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0670Epoch 8/15: [================              ] 40/75 batches, loss: 0.0670Epoch 8/15: [================              ] 41/75 batches, loss: 0.0668Epoch 8/15: [================              ] 42/75 batches, loss: 0.0664Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0663Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0666Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0670Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0672Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0668Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0671Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0663Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0665Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0662Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0657Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0661Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0664Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0663Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0661Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0667Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0668Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0662Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0664Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0665Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0666Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0664Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0663Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0663Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0662Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0659Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0657Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0657Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0658Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0659Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0656Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0658Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0658Epoch 8/15: [==============================] 75/75 batches, loss: 0.0660
[2025-05-07 18:43:09,454][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0660
[2025-05-07 18:43:09,703][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0734, Metrics: {'mse': 0.07334422320127487, 'rmse': 0.27082138615935575, 'r2': -0.1953587532043457}
[2025-05-07 18:43:09,704][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0352Epoch 9/15: [                              ] 2/75 batches, loss: 0.0723Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0657Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0605Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0616Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0564Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0578Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0567Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0553Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0537Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0588Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0597Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0625Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0656Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0642Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0633Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0638Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0633Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0627Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0639Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0640Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0628Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0616Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0613Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0604Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0594Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0597Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0601Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0598Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0607Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0598Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0602Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0604Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0602Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0606Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0602Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0607Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0619Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0618Epoch 9/15: [================              ] 40/75 batches, loss: 0.0612Epoch 9/15: [================              ] 41/75 batches, loss: 0.0617Epoch 9/15: [================              ] 42/75 batches, loss: 0.0618Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0629Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0628Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0624Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0628Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0631Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0637Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0637Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0632Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0631Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0626Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0633Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0636Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0641Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0635Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0636Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0636Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0636Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0636Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0636Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0632Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0636Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0637Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0632Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0632Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0633Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0637Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0641Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0639Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0634Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0633Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0634Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0635Epoch 9/15: [==============================] 75/75 batches, loss: 0.0630
[2025-05-07 18:43:12,033][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0630
[2025-05-07 18:43:12,380][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0700, Metrics: {'mse': 0.06994469463825226, 'rmse': 0.26447059314459187, 'r2': -0.1399533748626709}
[2025-05-07 18:43:12,381][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0586Epoch 10/15: [                              ] 2/75 batches, loss: 0.0641Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0719Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0672Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0617Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0617Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0572Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0592Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0581Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0556Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0527Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0547Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0551Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0563Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0568Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0560Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0591Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0615Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0595Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0607Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0617Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0617Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0621Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0616Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0610Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0608Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0604Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0607Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0612Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0614Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0608Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0612Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0618Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0616Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0617Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0622Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0612Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0607Epoch 10/15: [================              ] 40/75 batches, loss: 0.0609Epoch 10/15: [================              ] 41/75 batches, loss: 0.0607Epoch 10/15: [================              ] 42/75 batches, loss: 0.0610Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0617Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0622Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0627Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0624Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0625Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0626Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0625Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0630Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0630Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0629Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0626Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0621Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0619Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0622Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0626Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0627Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0623Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0623Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0620Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0624Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0622Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0620Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0618Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0619Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0623Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0621Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0628Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0625Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0629Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0632Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0631Epoch 10/15: [==============================] 75/75 batches, loss: 0.0627
[2025-05-07 18:43:14,725][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0627
[2025-05-07 18:43:14,988][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0720, Metrics: {'mse': 0.0719391256570816, 'rmse': 0.26821470067295267, 'r2': -0.17245852947235107}
[2025-05-07 18:43:14,989][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:43:14,989][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 18:43:14,989][src.training.lm_trainer][INFO] - Training completed in 28.04 seconds
[2025-05-07 18:43:14,990][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:43:18,131][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04041008651256561, 'rmse': 0.20102260199431707, 'r2': -0.008604645729064941}
[2025-05-07 18:43:18,131][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06632266193628311, 'rmse': 0.25753186586572757, 'r2': -0.08092176914215088}
[2025-05-07 18:43:18,131][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07144702225923538, 'rmse': 0.26729575802701283, 'r2': -0.3724358081817627}
[2025-05-07 18:43:21,258][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ja/ja/model.pt
[2025-05-07 18:43:21,259][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▁▁▃▄▃▃▃
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▇█▃▁▃▄▃▃
wandb:          val_mse ▆█▇█▃▁▃▄▃▃
wandb:           val_r2 ▃▁▂▁▆█▆▅▆▆
wandb:         val_rmse ▆█▇█▃▁▃▄▃▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06627
wandb:     best_val_mse 0.06632
wandb:      best_val_r2 -0.08092
wandb:    best_val_rmse 0.25753
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.07145
wandb:    final_test_r2 -0.37244
wandb:  final_test_rmse 0.2673
wandb:  final_train_mse 0.04041
wandb:   final_train_r2 -0.0086
wandb: final_train_rmse 0.20102
wandb:    final_val_mse 0.06632
wandb:     final_val_r2 -0.08092
wandb:   final_val_rmse 0.25753
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06271
wandb:       train_time 28.04234
wandb:         val_loss 0.07199
wandb:          val_mse 0.07194
wandb:           val_r2 -0.17246
wandb:         val_rmse 0.26821
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184228-xkn2lrgi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184228-xkn2lrgi/logs
Experiment probe_layer6_complexity_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer6/ja/ja/results.json for layer 6
Running experiment: probe_layer6_complexity_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:44:11,586][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ja
experiment_name: probe_layer6_complexity_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:44:11,586][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:44:11,586][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:44:11,586][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:44:11,590][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:44:11,591][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:44:16,200][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:44:18,586][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:44:18,586][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:44:18,889][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:44:19,084][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 18:44:19,411][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:44:19,420][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:44:19,421][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:44:19,423][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:44:19,529][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:44:19,676][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:44:19,720][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:44:19,721][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:44:19,721][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:44:19,724][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:44:19,818][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:44:19,937][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:44:19,989][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:44:19,991][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:44:19,991][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:44:19,994][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:44:19,995][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:44:19,995][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:44:19,995][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:44:19,995][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:44:19,995][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:44:19,995][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:44:19,995][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Sample label: 0.5349239110946655
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:44:19,996][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:44:19,996][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:44:19,996][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:44:19,997][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:44:19,997][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:44:19,997][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:44:19,998][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:44:19,998][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:44:28,058][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:44:28,059][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:44:28,059][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:44:28,059][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:44:28,062][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:44:28,063][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:44:28,063][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:44:28,063][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:44:28,063][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:44:28,064][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:44:28,064][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3741Epoch 1/15: [                              ] 2/75 batches, loss: 0.4011Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3433Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3862Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3922Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3777Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3723Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3844Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3955Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4004Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3906Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3975Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3835Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3906Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3847Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3886Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4046Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3962Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3944Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3887Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3824Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3887Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3794Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3760Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3700Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3668Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3644Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3592Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3560Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3530Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3483Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3461Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3440Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3417Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3410Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3416Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3383Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3339Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3341Epoch 1/15: [================              ] 40/75 batches, loss: 0.3319Epoch 1/15: [================              ] 41/75 batches, loss: 0.3277Epoch 1/15: [================              ] 42/75 batches, loss: 0.3285Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3262Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3249Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3259Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3229Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3198Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3197Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3167Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3149Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3125Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3095Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3068Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3050Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3022Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3001Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2978Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2978Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2954Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2931Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2922Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2906Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2899Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2897Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2880Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2861Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2852Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2840Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2843Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2839Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2832Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2830Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2816Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2811Epoch 1/15: [==============================] 75/75 batches, loss: 0.2791
[2025-05-07 18:44:34,626][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2791
[2025-05-07 18:44:34,891][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0758, Metrics: {'mse': 0.07636362314224243, 'rmse': 0.27633968796074593, 'r2': -0.2445685863494873}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.4373Epoch 2/15: [                              ] 2/75 batches, loss: 0.2987Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2521Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2313Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2165Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1932Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2015Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2016Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2040Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2172Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2113Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2005Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2029Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1985Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1911Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1899Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1855Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1910Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1866Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1825Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1835Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1798Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1800Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1786Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1801Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1817Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1899Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1866Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1862Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1873Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1857Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1847Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1843Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1837Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1817Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1796Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1814Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1801Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1786Epoch 2/15: [================              ] 40/75 batches, loss: 0.1777Epoch 2/15: [================              ] 41/75 batches, loss: 0.1766Epoch 2/15: [================              ] 42/75 batches, loss: 0.1744Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1736Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1744Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1716Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1705Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1689Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1712Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1711Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1703Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1708Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1697Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1692Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1684Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1681Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1683Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1677Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1658Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1649Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1636Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1626Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1640Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1636Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1637Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1621Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1617Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1613Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1611Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1599Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1609Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1597Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1586Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1581Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1586Epoch 2/15: [==============================] 75/75 batches, loss: 0.1588
[2025-05-07 18:44:37,668][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1588
[2025-05-07 18:44:37,904][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0798, Metrics: {'mse': 0.07932533323764801, 'rmse': 0.28164753369708034, 'r2': -0.29283833503723145}
[2025-05-07 18:44:37,905][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1548Epoch 3/15: [                              ] 2/75 batches, loss: 0.1492Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1272Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1125Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1243Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1359Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1420Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1327Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1329Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1292Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1284Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1289Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1240Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1218Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1176Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1171Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1130Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1158Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1132Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1157Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1177Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1176Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1175Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1150Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1138Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1131Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1121Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1165Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1169Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1145Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1162Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1232Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1249Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1260Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1254Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1242Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1234Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1238Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1228Epoch 3/15: [================              ] 40/75 batches, loss: 0.1229Epoch 3/15: [================              ] 41/75 batches, loss: 0.1229Epoch 3/15: [================              ] 42/75 batches, loss: 0.1210Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1201Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1188Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1204Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1190Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1197Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1191Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1194Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1184Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1179Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1179Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1170Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1169Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1161Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1156Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1179Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1181Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1184Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1180Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1187Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1179Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1171Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1171Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1171Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1163Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1157Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1161Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1162Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1163Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1160Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1161Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1158Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1149Epoch 3/15: [==============================] 75/75 batches, loss: 0.1143
[2025-05-07 18:44:40,186][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1143
[2025-05-07 18:44:40,497][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0851, Metrics: {'mse': 0.08461624383926392, 'rmse': 0.2908887138396124, 'r2': -0.3790692090988159}
[2025-05-07 18:44:40,497][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1237Epoch 4/15: [                              ] 2/75 batches, loss: 0.1157Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1153Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1120Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1022Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0967Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0988Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0907Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0970Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1008Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1047Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1045Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1021Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0988Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0976Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1027Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1038Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1054Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1045Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1058Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1053Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1049Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1057Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1062Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1061Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1040Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1054Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1066Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1070Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1067Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1051Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1047Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1045Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1038Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1041Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1034Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1026Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1009Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1016Epoch 4/15: [================              ] 40/75 batches, loss: 0.1023Epoch 4/15: [================              ] 41/75 batches, loss: 0.1016Epoch 4/15: [================              ] 42/75 batches, loss: 0.1019Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1020Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1013Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1013Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1011Epoch 4/15: [==================            ] 47/75 batches, loss: 0.1013Epoch 4/15: [===================           ] 48/75 batches, loss: 0.1016Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1024Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1017Epoch 4/15: [====================          ] 51/75 batches, loss: 0.1010Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0998Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.1016Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.1013Epoch 4/15: [======================        ] 55/75 batches, loss: 0.1005Epoch 4/15: [======================        ] 56/75 batches, loss: 0.1003Epoch 4/15: [======================        ] 57/75 batches, loss: 0.1000Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.1007Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.1008Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1011Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1019Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1016Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1025Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1022Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1019Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1015Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1018Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1018Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1015Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1015Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1006Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1009Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1009Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1016Epoch 4/15: [==============================] 75/75 batches, loss: 0.1019
[2025-05-07 18:44:42,853][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1019
[2025-05-07 18:44:43,169][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0854, Metrics: {'mse': 0.084911048412323, 'rmse': 0.29139500409636915, 'r2': -0.3838738203048706}
[2025-05-07 18:44:43,170][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0914Epoch 5/15: [                              ] 2/75 batches, loss: 0.0894Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0914Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1098Epoch 5/15: [==                            ] 5/75 batches, loss: 0.1011Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1020Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0998Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0928Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0922Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0904Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0903Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0943Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0980Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0968Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0976Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0952Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0948Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0956Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0955Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0939Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0922Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0915Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0924Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0911Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0889Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0881Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0875Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0871Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0866Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0862Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0871Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0855Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0856Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0849Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0838Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0826Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0836Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0835Epoch 5/15: [================              ] 40/75 batches, loss: 0.0832Epoch 5/15: [================              ] 41/75 batches, loss: 0.0832Epoch 5/15: [================              ] 42/75 batches, loss: 0.0839Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0829Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0829Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0823Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0817Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0810Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0812Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0805Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0801Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0795Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0795Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0793Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0793Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0792Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0800Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0803Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0803Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0803Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0817Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0825Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0829Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0830Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0827Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0832Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0838Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0839Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0841Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0837Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0837Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0835Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0832Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0829Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0827Epoch 5/15: [==============================] 75/75 batches, loss: 0.0833
[2025-05-07 18:44:45,537][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0833
[2025-05-07 18:44:45,816][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0810, Metrics: {'mse': 0.0806950107216835, 'rmse': 0.28406867254536095, 'r2': -0.31516122817993164}
[2025-05-07 18:44:45,816][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:44:45,816][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 18:44:45,817][src.training.lm_trainer][INFO] - Training completed in 14.39 seconds
[2025-05-07 18:44:45,817][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:44:48,933][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.053254615515470505, 'rmse': 0.23076961566781382, 'r2': -0.3291943073272705}
[2025-05-07 18:44:48,934][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07636362314224243, 'rmse': 0.27633968796074593, 'r2': -0.2445685863494873}
[2025-05-07 18:44:48,934][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07534508407115936, 'rmse': 0.2744905901322655, 'r2': -0.4473142623901367}
[2025-05-07 18:44:51,712][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ja/ja/model.pt
[2025-05-07 18:44:51,713][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▂▁▁
wandb:       train_loss █▄▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁▄██▅
wandb:          val_mse ▁▃██▅
wandb:           val_r2 █▆▁▁▄
wandb:         val_rmse ▁▃██▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07584
wandb:     best_val_mse 0.07636
wandb:      best_val_r2 -0.24457
wandb:    best_val_rmse 0.27634
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.07535
wandb:    final_test_r2 -0.44731
wandb:  final_test_rmse 0.27449
wandb:  final_train_mse 0.05325
wandb:   final_train_r2 -0.32919
wandb: final_train_rmse 0.23077
wandb:    final_val_mse 0.07636
wandb:     final_val_r2 -0.24457
wandb:   final_val_rmse 0.27634
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08326
wandb:       train_time 14.39407
wandb:         val_loss 0.08101
wandb:          val_mse 0.0807
wandb:           val_r2 -0.31516
wandb:         val_rmse 0.28407
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184411-eu1eu25k
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184411-eu1eu25k/logs
Experiment probe_layer6_complexity_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer6/ja/ja/results.json for layer 6
Running experiment: probe_layer6_complexity_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:45:32,270][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ja
experiment_name: probe_layer6_complexity_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 18:45:32,270][__main__][INFO] - Normalized task: complexity
[2025-05-07 18:45:32,270][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:45:32,270][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:45:32,274][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-07 18:45:32,274][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:45:41,454][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:45:43,823][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:45:43,823][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:45:44,069][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:45:44,222][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 18:45:44,681][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:45:44,690][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:45:44,690][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:45:44,708][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:45:44,824][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:45:44,937][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:45:45,052][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:45:45,053][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:45:45,053][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:45:45,060][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:45:45,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:45:45,331][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:45:45,380][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:45:45,381][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:45:45,381][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:45:45,384][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:45:45,385][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:45:45,385][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:45:45,385][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:45:45,385][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:45:45,385][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:45:45,385][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Sample label: 0.2807745635509491
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:45:45,386][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:45:45,386][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:45:45,386][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 18:45:45,387][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:45:45,387][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:45:45,387][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:45:45,388][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:45:45,388][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 18:45:45,388][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:45:55,861][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:45:55,863][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:45:55,863][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:45:55,863][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:45:55,866][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:45:55,866][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:45:55,866][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:45:55,867][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:45:55,867][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:45:55,867][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:45:55,868][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2722Epoch 1/15: [                              ] 2/75 batches, loss: 0.3985Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3322Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3595Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3729Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3636Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3610Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3750Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3906Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3888Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3813Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3818Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3734Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3824Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3800Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3847Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3985Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3992Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3982Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3939Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3907Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4017Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3944Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3897Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3835Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3782Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3796Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3745Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3708Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3712Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3637Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3649Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3608Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3574Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3536Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3544Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3531Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3485Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3487Epoch 1/15: [================              ] 40/75 batches, loss: 0.3445Epoch 1/15: [================              ] 41/75 batches, loss: 0.3409Epoch 1/15: [================              ] 42/75 batches, loss: 0.3389Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3372Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3363Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3364Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3317Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3276Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3266Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3236Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3213Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3209Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3175Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3162Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3155Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3151Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3127Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3097Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3096Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3073Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3061Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3037Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3023Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3015Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3006Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2986Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2960Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2944Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2922Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2924Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2924Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2912Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2913Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2893Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2894Epoch 1/15: [==============================] 75/75 batches, loss: 0.2875
[2025-05-07 18:46:04,598][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2875
[2025-05-07 18:46:04,923][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0811, Metrics: {'mse': 0.08113754540681839, 'rmse': 0.28484652956779793, 'r2': -0.32237374782562256}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.3598Epoch 2/15: [                              ] 2/75 batches, loss: 0.3083Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2448Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2164Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1925Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1726Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1763Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1701Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1701Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1709Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1679Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1654Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1631Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1625Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1613Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1606Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1568Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1608Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1615Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1610Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1593Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1609Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1598Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1598Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1587Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1638Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1672Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1688Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1676Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1687Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1686Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1693Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1683Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1661Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1670Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1648Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1699Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1692Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1682Epoch 2/15: [================              ] 40/75 batches, loss: 0.1692Epoch 2/15: [================              ] 41/75 batches, loss: 0.1675Epoch 2/15: [================              ] 42/75 batches, loss: 0.1665Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1645Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1651Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1635Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1630Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1629Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1643Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1644Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1649Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1649Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1639Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1641Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1628Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1623Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1634Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1625Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1613Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1600Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1584Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1571Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1594Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1585Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1591Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1581Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1591Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1585Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1581Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1573Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1574Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1567Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1561Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1555Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1557Epoch 2/15: [==============================] 75/75 batches, loss: 0.1562
[2025-05-07 18:46:07,715][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1562
[2025-05-07 18:46:07,920][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0759, Metrics: {'mse': 0.07559163868427277, 'rmse': 0.2749393363712671, 'r2': -0.2319868803024292}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1741Epoch 3/15: [                              ] 2/75 batches, loss: 0.1399Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1301Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1173Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1330Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1305Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1298Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1271Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1305Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1290Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1322Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1240Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1178Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1158Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1138Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1126Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1087Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1080Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1105Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1127Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1123Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1146Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1166Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1146Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1131Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1119Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1123Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1127Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1150Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1144Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1137Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1201Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1248Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1263Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1244Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1238Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1222Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1212Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1206Epoch 3/15: [================              ] 40/75 batches, loss: 0.1221Epoch 3/15: [================              ] 41/75 batches, loss: 0.1227Epoch 3/15: [================              ] 42/75 batches, loss: 0.1231Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1222Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1223Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1249Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1235Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1225Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1222Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1229Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1221Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1231Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1222Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1211Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1206Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1197Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1197Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1200Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1199Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1216Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1212Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1216Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1215Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1217Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1213Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1209Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1207Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1209Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1211Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1210Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1203Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1197Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1196Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1203Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1192Epoch 3/15: [==============================] 75/75 batches, loss: 0.1186
[2025-05-07 18:46:10,636][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1186
[2025-05-07 18:46:10,844][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0776, Metrics: {'mse': 0.07719074934720993, 'rmse': 0.2778322323763208, 'r2': -0.2580491304397583}
[2025-05-07 18:46:10,844][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0865Epoch 4/15: [                              ] 2/75 batches, loss: 0.0947Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0831Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0886Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0840Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0805Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0786Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0761Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0804Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0885Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0912Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0923Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0931Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0920Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0926Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0934Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0966Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1024Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1002Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1005Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1011Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1006Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0997Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0986Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0979Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0968Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0977Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1013Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1000Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0985Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0976Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0958Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0957Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0944Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0941Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0932Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0929Epoch 4/15: [================              ] 40/75 batches, loss: 0.0927Epoch 4/15: [================              ] 41/75 batches, loss: 0.0929Epoch 4/15: [================              ] 42/75 batches, loss: 0.0919Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0910Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0904Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0900Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0906Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0901Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0897Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0900Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0899Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0893Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0886Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0894Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0888Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0887Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0879Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0877Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0871Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0869Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0868Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0886Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0884Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0881Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0884Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0897Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0903Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0905Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0906Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0900Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0897Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0897Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0895Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0899Epoch 4/15: [==============================] 75/75 batches, loss: 0.0902
[2025-05-07 18:46:13,250][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0902
[2025-05-07 18:46:13,509][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0854, Metrics: {'mse': 0.08494720607995987, 'rmse': 0.29145703985314864, 'r2': -0.38446319103240967}
[2025-05-07 18:46:13,509][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0617Epoch 5/15: [                              ] 2/75 batches, loss: 0.0752Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0804Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0948Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0906Epoch 5/15: [==                            ] 6/75 batches, loss: 0.1102Epoch 5/15: [==                            ] 7/75 batches, loss: 0.1093Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0983Epoch 5/15: [===                           ] 9/75 batches, loss: 0.1039Epoch 5/15: [====                          ] 10/75 batches, loss: 0.1038Epoch 5/15: [====                          ] 11/75 batches, loss: 0.1040Epoch 5/15: [====                          ] 12/75 batches, loss: 0.1011Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0991Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0998Epoch 5/15: [======                        ] 15/75 batches, loss: 0.1002Epoch 5/15: [======                        ] 16/75 batches, loss: 0.1012Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0993Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.1008Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0988Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0972Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0960Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0948Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0941Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0923Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0916Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0921Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0925Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0912Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0902Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0907Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0912Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0910Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0899Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0904Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0908Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0905Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0900Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0909Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0912Epoch 5/15: [================              ] 40/75 batches, loss: 0.0909Epoch 5/15: [================              ] 41/75 batches, loss: 0.0921Epoch 5/15: [================              ] 42/75 batches, loss: 0.0936Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0925Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0912Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0903Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0899Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0896Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0898Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0898Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0890Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0885Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0889Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0889Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0882Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0878Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0886Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0888Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0885Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0887Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0885Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0897Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0901Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0899Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0896Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0905Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0908Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0907Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0901Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0896Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0898Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0902Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0904Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0898Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0898Epoch 5/15: [==============================] 75/75 batches, loss: 0.0898
[2025-05-07 18:46:15,832][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0898
[2025-05-07 18:46:16,153][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0699, Metrics: {'mse': 0.06977371871471405, 'rmse': 0.26414715352377743, 'r2': -0.13716673851013184}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.1004Epoch 6/15: [                              ] 2/75 batches, loss: 0.0702Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0673Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0663Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0688Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0682Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0701Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0713Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0711Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0706Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0698Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0719Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0720Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0736Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0724Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0727Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0749Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0740Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0717Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0715Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0726Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0713Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0709Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0720Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0728Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0733Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0732Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0743Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0742Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0732Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0722Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0723Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0727Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0737Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0734Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0736Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0722Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0723Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0728Epoch 6/15: [================              ] 40/75 batches, loss: 0.0733Epoch 6/15: [================              ] 41/75 batches, loss: 0.0736Epoch 6/15: [================              ] 42/75 batches, loss: 0.0742Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0737Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0729Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0738Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0747Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0746Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0741Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0739Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0738Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0750Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0745Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0744Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0739Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0744Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0743Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0738Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0737Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0735Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0733Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0736Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0731Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0727Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0728Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0730Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0728Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0728Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0724Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0723Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0722Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0719Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0714Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0711Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0710Epoch 6/15: [==============================] 75/75 batches, loss: 0.0712
[2025-05-07 18:46:18,939][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0712
[2025-05-07 18:46:19,220][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0753, Metrics: {'mse': 0.07508029788732529, 'rmse': 0.2740078427478405, 'r2': -0.22365307807922363}
[2025-05-07 18:46:19,221][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0560Epoch 7/15: [                              ] 2/75 batches, loss: 0.0773Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0745Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0694Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0686Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0822Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0806Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0850Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0823Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0770Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0759Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0738Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0717Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0714Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0702Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0700Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0684Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0688Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0700Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0700Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0704Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0690Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0674Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0659Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0656Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0662Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0662Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0657Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0661Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0675Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0689Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0682Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0677Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0684Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0690Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0704Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0704Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0704Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0708Epoch 7/15: [================              ] 40/75 batches, loss: 0.0711Epoch 7/15: [================              ] 41/75 batches, loss: 0.0710Epoch 7/15: [================              ] 42/75 batches, loss: 0.0711Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0707Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0708Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0708Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0709Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0714Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0709Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0707Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0713Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0708Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0701Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0722Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0722Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0720Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0721Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0715Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0715Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0709Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0706Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0704Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0704Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0704Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0700Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0701Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0697Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0695Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0698Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0693Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0694Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0696Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0699Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0698Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0701Epoch 7/15: [==============================] 75/75 batches, loss: 0.0699
[2025-05-07 18:46:21,580][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0699
[2025-05-07 18:46:21,892][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0678, Metrics: {'mse': 0.06771617382764816, 'rmse': 0.26022331530369863, 'r2': -0.10363316535949707}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0709Epoch 8/15: [                              ] 2/75 batches, loss: 0.0849Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0727Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0690Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0702Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0687Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0698Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0673Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0666Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0662Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0657Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0661Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0640Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0663Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0687Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0673Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0679Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0676Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0662Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0644Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0642Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0647Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0641Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0632Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0629Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0642Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0657Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0648Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0661Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0665Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0681Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0682Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0681Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0684Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0683Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0677Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0673Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0676Epoch 8/15: [================              ] 40/75 batches, loss: 0.0668Epoch 8/15: [================              ] 41/75 batches, loss: 0.0664Epoch 8/15: [================              ] 42/75 batches, loss: 0.0663Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0659Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0660Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0659Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0662Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0657Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0657Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0649Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0656Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0654Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0653Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0661Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0660Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0659Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0657Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0657Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0661Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0668Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0669Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0669Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0665Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0662Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0662Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0658Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0653Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0662Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0658Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0660Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0660Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0661Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0663Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0661Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0663Epoch 8/15: [==============================] 75/75 batches, loss: 0.0662
[2025-05-07 18:46:24,759][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0662
[2025-05-07 18:46:25,058][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0770, Metrics: {'mse': 0.07677493989467621, 'rmse': 0.27708291158906967, 'r2': -0.25127220153808594}
[2025-05-07 18:46:25,058][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0639Epoch 9/15: [                              ] 2/75 batches, loss: 0.0607Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0624Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0688Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0661Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0617Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0674Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0652Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0637Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0637Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0641Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0630Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0608Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0614Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0607Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0601Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0603Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0603Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0627Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0612Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0613Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0617Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0613Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0612Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0613Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0613Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0616Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0605Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0608Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0602Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0593Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0603Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0605Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0597Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0601Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0602Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0593Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0592Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0592Epoch 9/15: [================              ] 40/75 batches, loss: 0.0592Epoch 9/15: [================              ] 41/75 batches, loss: 0.0591Epoch 9/15: [================              ] 42/75 batches, loss: 0.0594Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0590Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0588Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0581Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0578Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0578Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0578Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0572Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0569Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0571Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0570Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0571Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0569Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0572Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0573Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0578Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0579Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0580Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0579Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0585Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0582Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0583Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0580Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0582Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0579Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0577Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0576Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0576Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0575Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0574Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0574Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0570Epoch 9/15: [==============================] 75/75 batches, loss: 0.0569
[2025-05-07 18:46:27,421][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0569
[2025-05-07 18:46:27,674][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0762, Metrics: {'mse': 0.0759827196598053, 'rmse': 0.27564963206905485, 'r2': -0.23836064338684082}
[2025-05-07 18:46:27,674][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0583Epoch 10/15: [                              ] 2/75 batches, loss: 0.0531Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0544Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0648Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0609Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0561Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0565Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0588Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0589Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0552Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0521Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0563Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0563Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0580Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0591Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0602Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0624Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0642Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0628Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0635Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0625Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0632Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0621Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0612Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0617Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0619Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0615Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0607Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0599Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0593Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0598Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0592Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0612Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0606Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0596Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0594Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0590Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0591Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0591Epoch 10/15: [================              ] 40/75 batches, loss: 0.0583Epoch 10/15: [================              ] 41/75 batches, loss: 0.0591Epoch 10/15: [================              ] 42/75 batches, loss: 0.0587Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0585Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0587Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0588Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0590Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0595Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0598Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0599Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0599Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0597Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0596Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0594Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0593Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0591Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0586Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0591Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0586Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0583Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0582Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0583Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0589Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0591Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0594Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0590Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0597Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0596Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0598Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0603Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0606Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0605Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0611Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0609Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0608Epoch 10/15: [==============================] 75/75 batches, loss: 0.0605
[2025-05-07 18:46:30,069][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0605
[2025-05-07 18:46:30,329][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0681, Metrics: {'mse': 0.06813185662031174, 'rmse': 0.26102079729460587, 'r2': -0.11040794849395752}
[2025-05-07 18:46:30,330][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0738Epoch 11/15: [                              ] 2/75 batches, loss: 0.0640Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0582Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0582Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0548Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0547Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0503Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0541Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0546Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0543Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0564Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0553Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0558Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0580Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0619Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0595Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0594Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0596Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0593Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0590Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0597Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0592Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0591Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0600Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0606Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0605Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0601Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0612Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0611Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0623Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0623Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0617Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0608Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0601Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0597Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0596Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0587Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0593Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0585Epoch 11/15: [================              ] 40/75 batches, loss: 0.0576Epoch 11/15: [================              ] 41/75 batches, loss: 0.0574Epoch 11/15: [================              ] 42/75 batches, loss: 0.0576Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0578Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0578Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0576Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0578Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0572Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0568Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0571Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0572Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0571Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0567Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0568Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0563Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0562Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0563Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0561Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0559Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0560Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0561Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0562Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0562Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0557Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0557Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0558Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0554Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0554Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0555Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0558Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0567Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0570Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0574Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0573Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0579Epoch 11/15: [==============================] 75/75 batches, loss: 0.0578
[2025-05-07 18:46:32,689][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0578
[2025-05-07 18:46:33,018][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0689, Metrics: {'mse': 0.06886442750692368, 'rmse': 0.262420326017105, 'r2': -0.12234735488891602}
[2025-05-07 18:46:33,019][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:46:33,019][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-07 18:46:33,019][src.training.lm_trainer][INFO] - Training completed in 31.55 seconds
[2025-05-07 18:46:33,019][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:46:36,002][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.041220273822546005, 'rmse': 0.2030277661369154, 'r2': -0.028826236724853516}
[2025-05-07 18:46:36,002][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06771617382764816, 'rmse': 0.26022331530369863, 'r2': -0.10363316535949707}
[2025-05-07 18:46:36,002][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06999038904905319, 'rmse': 0.2645569674929262, 'r2': -0.3444552421569824}
[2025-05-07 18:46:39,172][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ja/ja/model.pt
[2025-05-07 18:46:39,174][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▁
wandb:     best_val_mse █▅▂▁
wandb:      best_val_r2 ▁▄▇█
wandb:    best_val_rmse █▅▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▂▂▁▃▃▄▂▂▄
wandb:       train_loss █▄▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▄▅█▂▄▁▅▄▁▁
wandb:          val_mse ▆▄▅█▂▄▁▅▄▁▁
wandb:           val_r2 ▃▅▄▁▇▅█▄▅██
wandb:         val_rmse ▇▄▅█▂▄▁▅▄▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06778
wandb:     best_val_mse 0.06772
wandb:      best_val_r2 -0.10363
wandb:    best_val_rmse 0.26022
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.06999
wandb:    final_test_r2 -0.34446
wandb:  final_test_rmse 0.26456
wandb:  final_train_mse 0.04122
wandb:   final_train_r2 -0.02883
wandb: final_train_rmse 0.20303
wandb:    final_val_mse 0.06772
wandb:     final_val_r2 -0.10363
wandb:   final_val_rmse 0.26022
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05777
wandb:       train_time 31.55103
wandb:         val_loss 0.06885
wandb:          val_mse 0.06886
wandb:           val_r2 -0.12235
wandb:         val_rmse 0.26242
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184532-i3zcio2b
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184532-i3zcio2b/logs
Experiment probe_layer6_complexity_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer6/ja/ja/results.json for layer 6
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ru"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:47:11,331][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ru
experiment_name: probe_layer2_avg_links_len_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 18:47:11,331][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 18:47:11,331][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:47:11,331][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:47:11,332][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:47:11,336][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-05-07 18:47:11,336][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:47:11,336][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:47:15,107][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:47:17,581][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:47:17,582][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:47:18,096][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:18,290][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:18,911][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:47:18,920][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:47:18,920][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:47:18,944][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:47:19,170][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:19,368][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:19,439][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:47:19,441][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:47:19,441][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:47:19,447][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:47:19,724][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:19,989][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:47:20,033][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:47:20,034][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:47:20,034][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:47:20,039][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:47:20,039][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:47:20,040][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-05-07 18:47:20,040][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 18:47:20,040][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:47:20,041][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-05-07 18:47:20,041][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:47:20,041][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:47:20,042][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-05-07 18:47:20,042][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:47:20,042][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:47:20,042][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 18:47:20,043][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:47:34,582][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:47:34,583][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:47:34,583][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:47:34,584][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:47:34,586][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:47:34,587][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:47:34,587][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:47:34,587][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:47:34,587][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:47:34,588][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:47:34,588][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.2362Epoch 1/15: [                              ] 2/75 batches, loss: 0.4084Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3402Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3794Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3829Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3672Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4142Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4123Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4420Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4259Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4157Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4125Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3905Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3830Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3817Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3746Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3652Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3706Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3754Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3712Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3709Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3702Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3596Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3571Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3497Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3466Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3488Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3448Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3499Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3506Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3476Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3424Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3412Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3424Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3391Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3425Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3395Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3411Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3384Epoch 1/15: [================              ] 40/75 batches, loss: 0.3343Epoch 1/15: [================              ] 41/75 batches, loss: 0.3352Epoch 1/15: [================              ] 42/75 batches, loss: 0.3342Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3320Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3307Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3304Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3258Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3235Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3209Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3180Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3173Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3144Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3119Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3099Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3095Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3071Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3072Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3105Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3089Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3075Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3052Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3027Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3014Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2986Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2964Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2944Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2926Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2901Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2866Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2861Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2846Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2824Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2812Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2789Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2770Epoch 1/15: [==============================] 75/75 batches, loss: 0.2750
[2025-05-07 18:47:45,037][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2750
[2025-05-07 18:47:45,568][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0566, Metrics: {'mse': 0.060147710144519806, 'rmse': 0.2452503010080106, 'r2': -1.0153422355651855}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1208Epoch 2/15: [                              ] 2/75 batches, loss: 0.1597Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1437Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1668Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1575Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1471Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1576Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1541Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1472Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1619Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1645Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1635Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1655Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1626Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1573Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1545Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1542Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1611Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1588Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1563Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1558Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1526Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1588Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1582Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1564Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1564Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1558Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1552Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1552Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1512Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1494Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1479Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1500Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1498Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1518Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1514Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1511Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1506Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1503Epoch 2/15: [================              ] 40/75 batches, loss: 0.1496Epoch 2/15: [================              ] 41/75 batches, loss: 0.1491Epoch 2/15: [================              ] 42/75 batches, loss: 0.1501Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1507Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1485Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1476Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1471Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1466Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1457Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1454Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1437Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1439Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1436Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1438Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1438Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1429Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1436Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1431Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1426Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1429Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1422Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1426Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1423Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1424Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1421Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1417Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1409Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1404Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1399Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1390Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1386Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1378Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1373Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1367Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1359Epoch 2/15: [==============================] 75/75 batches, loss: 0.1350
[2025-05-07 18:47:48,582][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1350
[2025-05-07 18:47:49,082][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0596, Metrics: {'mse': 0.06400852650403976, 'rmse': 0.25299906423550217, 'r2': -1.144705057144165}
[2025-05-07 18:47:49,083][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.0846Epoch 3/15: [                              ] 2/75 batches, loss: 0.1230Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1233Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1154Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1144Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1191Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1190Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1101Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1114Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1114Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1082Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1079Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1112Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1105Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1073Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1059Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1052Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1045Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1035Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1002Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0994Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0983Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0988Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1006Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1003Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0990Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0975Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0967Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0989Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0986Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0985Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0990Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0982Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0970Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0972Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0960Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0955Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0960Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0958Epoch 3/15: [================              ] 40/75 batches, loss: 0.0952Epoch 3/15: [================              ] 41/75 batches, loss: 0.0985Epoch 3/15: [================              ] 42/75 batches, loss: 0.0988Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0990Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0981Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0979Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0982Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0975Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0973Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0978Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0967Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0984Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0973Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0973Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0984Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0976Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0972Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0963Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0965Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0955Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0949Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0944Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0949Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0952Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0952Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0944Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0939Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0939Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0940Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0936Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0932Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0930Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0926Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0920Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0921Epoch 3/15: [==============================] 75/75 batches, loss: 0.0927
[2025-05-07 18:47:51,544][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0927
[2025-05-07 18:47:52,036][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0305, Metrics: {'mse': 0.03248181194067001, 'rmse': 0.1802271121132168, 'r2': -0.08835351467132568}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0546Epoch 4/15: [                              ] 2/75 batches, loss: 0.0488Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0414Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0611Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0676Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0720Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0799Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0818Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0839Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0796Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0783Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0769Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0745Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0791Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0786Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0820Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0786Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0802Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0832Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0830Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0839Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0860Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0868Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0893Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0888Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0890Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0886Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0897Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0889Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0886Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0870Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0867Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0872Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0863Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0860Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0861Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0880Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0873Epoch 4/15: [================              ] 40/75 batches, loss: 0.0870Epoch 4/15: [================              ] 41/75 batches, loss: 0.0864Epoch 4/15: [================              ] 42/75 batches, loss: 0.0856Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0856Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0850Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0853Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0858Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0860Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0850Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0849Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0844Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0849Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0860Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0860Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0858Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0851Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0850Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0843Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0833Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0834Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0836Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0835Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0828Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0831Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0825Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0821Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0818Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0820Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0825Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0820Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0820Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0816Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0816Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0818Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0817Epoch 4/15: [==============================] 75/75 batches, loss: 0.0826
[2025-05-07 18:47:54,795][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0826
[2025-05-07 18:47:55,319][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0290, Metrics: {'mse': 0.030871417373418808, 'rmse': 0.1757026390621917, 'r2': -0.03439474105834961}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0399Epoch 5/15: [                              ] 2/75 batches, loss: 0.0312Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0593Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0733Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0698Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0724Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0776Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0753Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0752Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0738Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0731Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0776Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0810Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0782Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0796Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0814Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0792Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0783Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0798Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0790Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0800Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0790Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0784Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0772Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0768Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0762Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0760Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0772Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0757Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0752Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0744Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0745Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0738Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0737Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0735Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0729Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0725Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0719Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0731Epoch 5/15: [================              ] 40/75 batches, loss: 0.0732Epoch 5/15: [================              ] 41/75 batches, loss: 0.0735Epoch 5/15: [================              ] 42/75 batches, loss: 0.0732Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0736Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0739Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0739Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0746Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0753Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0745Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0747Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0746Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0739Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0731Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0729Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0723Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0728Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0723Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0724Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0719Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0722Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0723Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0722Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0716Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0709Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0707Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0716Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0721Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0718Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0718Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0718Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0716Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0715Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0721Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0720Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0717Epoch 5/15: [==============================] 75/75 batches, loss: 0.0718
[2025-05-07 18:47:58,232][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0718
[2025-05-07 18:47:58,645][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0256, Metrics: {'mse': 0.026874028146266937, 'rmse': 0.16393299895465507, 'r2': 0.09954404830932617}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0311Epoch 6/15: [                              ] 2/75 batches, loss: 0.0473Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0424Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0453Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0515Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0551Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0552Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0535Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0504Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0523Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0520Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0556Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0547Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0542Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0541Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0551Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0542Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0544Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0543Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0545Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0549Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0561Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0569Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0576Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0604Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0605Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0599Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0595Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0590Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0593Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0590Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0585Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0585Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0586Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0587Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0589Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0585Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0593Epoch 6/15: [================              ] 40/75 batches, loss: 0.0596Epoch 6/15: [================              ] 41/75 batches, loss: 0.0604Epoch 6/15: [================              ] 42/75 batches, loss: 0.0610Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0623Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0628Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0639Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0639Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0638Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0637Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0643Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0641Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0642Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0648Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0648Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0645Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0638Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0639Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0633Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0631Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0635Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0629Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0640Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0640Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0639Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0641Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0640Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0633Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0630Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0628Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0630Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0633Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0632Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0629Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0629Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0626Epoch 6/15: [==============================] 75/75 batches, loss: 0.0624
[2025-05-07 18:48:01,329][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0624
[2025-05-07 18:48:01,782][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0270, Metrics: {'mse': 0.028791405260562897, 'rmse': 0.16968030310134083, 'r2': 0.03529936075210571}
[2025-05-07 18:48:01,783][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0522Epoch 7/15: [                              ] 2/75 batches, loss: 0.0565Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0509Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0606Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0579Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0552Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0540Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0537Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0522Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0535Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0531Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0543Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0590Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0605Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0589Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0590Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0589Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0586Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0568Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0557Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0573Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0569Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0561Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0572Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0571Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0569Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0566Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0574Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0584Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0579Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0581Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0582Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0576Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0578Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0571Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0570Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0570Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0570Epoch 7/15: [================              ] 40/75 batches, loss: 0.0577Epoch 7/15: [================              ] 41/75 batches, loss: 0.0579Epoch 7/15: [================              ] 42/75 batches, loss: 0.0590Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0589Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0588Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0583Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0588Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0586Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0582Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0583Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0578Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0576Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0581Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0583Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0584Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0584Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0580Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0576Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0578Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0575Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0572Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0571Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0567Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0564Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0567Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0564Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0562Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0562Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0562Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0557Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0560Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0557Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0564Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0565Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0564Epoch 7/15: [==============================] 75/75 batches, loss: 0.0562
[2025-05-07 18:48:04,136][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0562
[2025-05-07 18:48:04,467][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0325, Metrics: {'mse': 0.03520302474498749, 'rmse': 0.18762469119225084, 'r2': -0.17953205108642578}
[2025-05-07 18:48:04,467][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0511Epoch 8/15: [                              ] 2/75 batches, loss: 0.0534Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0540Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0635Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0579Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0608Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0601Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0581Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0557Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0562Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0548Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0551Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0561Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0548Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0550Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0538Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0529Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0523Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0514Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0510Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0504Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0499Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0492Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0492Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0499Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0489Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0492Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0501Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0515Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0524Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0520Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0524Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0520Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0519Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0511Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0508Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0515Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0516Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0524Epoch 8/15: [================              ] 40/75 batches, loss: 0.0528Epoch 8/15: [================              ] 41/75 batches, loss: 0.0529Epoch 8/15: [================              ] 42/75 batches, loss: 0.0520Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0525Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0525Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0525Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0532Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0527Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0523Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0521Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0522Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0521Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0524Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0522Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0518Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0519Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0528Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0527Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0524Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0525Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0524Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0524Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0526Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0529Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0530Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0528Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0527Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0525Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0524Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0524Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0522Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0520Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0517Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0515Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0515Epoch 8/15: [==============================] 75/75 batches, loss: 0.0514
[2025-05-07 18:48:06,786][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0514
[2025-05-07 18:48:07,037][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0296, Metrics: {'mse': 0.032050732523202896, 'rmse': 0.1790271837548781, 'r2': -0.07390952110290527}
[2025-05-07 18:48:07,038][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0338Epoch 9/15: [                              ] 2/75 batches, loss: 0.0312Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0375Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0385Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0354Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0439Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0436Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0508Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0492Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0474Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0486Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0509Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0499Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0503Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0492Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0502Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0503Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0497Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0504Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0503Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0500Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0498Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0506Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0504Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0516Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0506Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0500Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0497Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0493Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0489Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0486Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0483Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0477Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0483Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0480Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0472Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0484Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0484Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0491Epoch 9/15: [================              ] 40/75 batches, loss: 0.0492Epoch 9/15: [================              ] 41/75 batches, loss: 0.0487Epoch 9/15: [================              ] 42/75 batches, loss: 0.0489Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0492Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0491Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0486Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0482Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0479Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0476Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0475Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0478Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0483Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0486Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0495Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0496Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0495Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0499Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0499Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0500Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0498Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0495Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0500Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0498Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0499Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0495Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0493Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0495Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0495Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0495Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0493Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0494Epoch 9/15: [==============================] 75/75 batches, loss: 0.0495
[2025-05-07 18:48:09,326][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0495
[2025-05-07 18:48:09,617][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0254, Metrics: {'mse': 0.0269770510494709, 'rmse': 0.1642469209740959, 'r2': 0.09609204530715942}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0620Epoch 10/15: [                              ] 2/75 batches, loss: 0.0637Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0552Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0539Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0580Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0658Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0603Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0600Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0576Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0580Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0570Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0549Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0557Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0548Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0544Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0538Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0535Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0533Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0525Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0522Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0509Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0505Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0521Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0526Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0523Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0518Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0522Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0515Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0523Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0514Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0516Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0506Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0500Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0497Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0500Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0513Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0509Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0505Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0509Epoch 10/15: [================              ] 40/75 batches, loss: 0.0507Epoch 10/15: [================              ] 41/75 batches, loss: 0.0505Epoch 10/15: [================              ] 42/75 batches, loss: 0.0503Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0499Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0491Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0487Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0481Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0477Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0476Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0473Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0469Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0466Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0464Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0466Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0464Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0464Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0464Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0464Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0465Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0470Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0468Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0466Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0467Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0467Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0465Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0468Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0468Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0467Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0471Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0472Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0470Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0472Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0473Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0470Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0473Epoch 10/15: [==============================] 75/75 batches, loss: 0.0478
[2025-05-07 18:48:12,435][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0478
[2025-05-07 18:48:12,795][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0338, Metrics: {'mse': 0.03675162047147751, 'rmse': 0.1917071215982273, 'r2': -0.23142004013061523}
[2025-05-07 18:48:12,796][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0455Epoch 11/15: [                              ] 2/75 batches, loss: 0.0327Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0381Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0499Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0496Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0464Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0546Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0521Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0479Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0466Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0462Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0464Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0442Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0441Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0441Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0434Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0434Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0428Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0429Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0428Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0429Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0436Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0436Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0436Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0430Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0436Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0436Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0434Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0430Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0428Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0427Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0420Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0418Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0421Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0428Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0425Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0421Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0422Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0420Epoch 11/15: [================              ] 40/75 batches, loss: 0.0424Epoch 11/15: [================              ] 41/75 batches, loss: 0.0427Epoch 11/15: [================              ] 42/75 batches, loss: 0.0423Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0426Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0429Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0432Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0433Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0434Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0429Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0434Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0431Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0427Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0428Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0428Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0430Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0428Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0426Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0428Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0425Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0420Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0421Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0422Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0421Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0421Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0424Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0423Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0425Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0429Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0425Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0426Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0427Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0426Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0430Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0431Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0429Epoch 11/15: [==============================] 75/75 batches, loss: 0.0429
[2025-05-07 18:48:15,156][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0429
[2025-05-07 18:48:15,451][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0254, Metrics: {'mse': 0.02717762067914009, 'rmse': 0.16485636378114157, 'r2': 0.0893716812133789}
[2025-05-07 18:48:15,452][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0487Epoch 12/15: [                              ] 2/75 batches, loss: 0.0333Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0468Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0468Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0503Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0500Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0474Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0480Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0538Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0519Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0509Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0514Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0492Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0469Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0464Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0469Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0476Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0464Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0456Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0446Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0441Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0441Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0442Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0432Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0434Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0432Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0429Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0432Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0440Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0438Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0436Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0432Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0441Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0440Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0441Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0441Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0435Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0438Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0437Epoch 12/15: [================              ] 40/75 batches, loss: 0.0432Epoch 12/15: [================              ] 41/75 batches, loss: 0.0429Epoch 12/15: [================              ] 42/75 batches, loss: 0.0438Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0434Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0431Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0430Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0430Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0433Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0433Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0431Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0427Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0431Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0440Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0439Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0440Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0442Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0443Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0441Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0443Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0443Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0443Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0439Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0440Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0441Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0438Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0434Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0433Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0433Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0434Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0430Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0429Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0429Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0426Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0425Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0426Epoch 12/15: [==============================] 75/75 batches, loss: 0.0424
[2025-05-07 18:48:17,811][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0424
[2025-05-07 18:48:18,184][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0255, Metrics: {'mse': 0.02732672542333603, 'rmse': 0.16530797144522713, 'r2': 0.08437561988830566}
[2025-05-07 18:48:18,185][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0332Epoch 13/15: [                              ] 2/75 batches, loss: 0.0406Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0400Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0504Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0459Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0473Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0451Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0464Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0478Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0456Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0458Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0448Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0444Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0434Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0441Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0432Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0418Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0417Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0415Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0413Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0406Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0405Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0404Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0413Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0419Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0416Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0405Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0410Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0408Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0403Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0400Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0399Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0397Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0407Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0401Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0412Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0418Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0413Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0411Epoch 13/15: [================              ] 40/75 batches, loss: 0.0413Epoch 13/15: [================              ] 41/75 batches, loss: 0.0413Epoch 13/15: [================              ] 42/75 batches, loss: 0.0416Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0414Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0413Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0422Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0420Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0420Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0417Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0417Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0422Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0420Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0422Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0422Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0427Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0425Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0425Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0428Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0424Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0425Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0424Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0424Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0422Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0421Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0424Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0419Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0420Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0420Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0424Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0422Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0419Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0423Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0423Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0420Epoch 13/15: [==============================] 75/75 batches, loss: 0.0417
[2025-05-07 18:48:20,529][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0417
[2025-05-07 18:48:20,927][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0272, Metrics: {'mse': 0.029291318729519844, 'rmse': 0.17114706754578018, 'r2': 0.018548965454101562}
[2025-05-07 18:48:20,928][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:48:20,928][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-07 18:48:20,928][src.training.lm_trainer][INFO] - Training completed in 38.82 seconds
[2025-05-07 18:48:20,928][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:48:24,167][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02808629721403122, 'rmse': 0.16758966917453838, 'r2': 0.1575465202331543}
[2025-05-07 18:48:24,167][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0269770510494709, 'rmse': 0.1642469209740959, 'r2': 0.09609204530715942}
[2025-05-07 18:48:24,167][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.018406137824058533, 'rmse': 0.1356692220957227, 'r2': -0.09267354011535645}
[2025-05-07 18:48:26,918][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ru/ru/model.pt
[2025-05-07 18:48:26,920][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁
wandb:     best_val_mse █▂▂▁▁
wandb:      best_val_r2 ▁▇▇██
wandb:    best_val_rmse █▂▂▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▆▆▇▇▆▆▇▆▇▇
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▂▁▁▂▂▁▃▁▁▁
wandb:          val_mse ▇█▂▂▁▁▃▂▁▃▁▁▁
wandb:           val_r2 ▂▁▇▇██▆▇█▆███
wandb:         val_rmse ▇█▂▂▁▁▃▂▁▃▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02537
wandb:     best_val_mse 0.02698
wandb:      best_val_r2 0.09609
wandb:    best_val_rmse 0.16425
wandb: early_stop_epoch 13
wandb:            epoch 13
wandb:   final_test_mse 0.01841
wandb:    final_test_r2 -0.09267
wandb:  final_test_rmse 0.13567
wandb:  final_train_mse 0.02809
wandb:   final_train_r2 0.15755
wandb: final_train_rmse 0.16759
wandb:    final_val_mse 0.02698
wandb:     final_val_r2 0.09609
wandb:   final_val_rmse 0.16425
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04171
wandb:       train_time 38.82318
wandb:         val_loss 0.02715
wandb:          val_mse 0.02929
wandb:           val_r2 0.01855
wandb:         val_rmse 0.17115
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184711-f1v0nzbc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184711-f1v0nzbc/logs
Experiment probe_layer2_avg_links_len_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ru/ru/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:49:00,706][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja
experiment_name: probe_layer2_avg_links_len_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 18:49:00,706][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 18:49:00,707][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:49:00,707][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:49:00,707][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:49:00,711][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 18:49:00,711][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:49:00,711][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:49:03,892][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:49:06,224][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:49:06,224][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:49:06,416][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:06,505][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:06,831][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:49:06,840][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:49:06,841][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:49:06,854][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:49:06,963][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:07,029][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:07,049][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:49:07,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:49:07,051][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:49:07,053][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:49:07,148][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:07,232][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:49:07,267][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:49:07,268][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:49:07,268][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:49:07,273][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:49:07,274][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:49:07,274][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:49:07,274][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:49:07,274][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:49:07,274][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:49:07,275][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:49:07,275][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:49:07,275][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 18:49:07,276][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:49:07,276][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:49:07,276][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 18:49:07,276][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:49:07,277][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 18:49:07,277][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:49:07,277][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:49:07,277][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:49:07,277][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 18:49:07,278][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:49:14,866][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:49:14,867][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:49:14,867][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 18:49:14,868][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:49:14,870][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:49:14,871][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:49:14,871][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:49:14,871][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:49:14,871][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:49:14,872][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:49:14,872][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4835Epoch 1/15: [                              ] 2/75 batches, loss: 0.4827Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4574Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4676Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4508Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4161Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4103Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4236Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4062Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3883Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3802Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3942Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3781Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3930Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3877Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3970Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3941Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4123Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4012Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4024Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3936Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3937Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3807Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3701Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3650Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3586Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3550Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3481Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3447Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3390Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3352Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3312Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3297Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3292Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3266Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3293Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3257Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3209Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3201Epoch 1/15: [================              ] 40/75 batches, loss: 0.3146Epoch 1/15: [================              ] 41/75 batches, loss: 0.3112Epoch 1/15: [================              ] 42/75 batches, loss: 0.3112Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3100Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3124Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3112Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3069Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3041Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3016Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2993Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2964Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2967Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3007Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2972Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2983Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2981Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2938Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2910Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2901Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2884Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2867Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2844Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2826Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2823Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2814Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2812Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2786Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2775Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2747Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2730Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2710Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2699Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2721Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2698Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2689Epoch 1/15: [==============================] 75/75 batches, loss: 0.2657
[2025-05-07 18:49:26,663][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2657
[2025-05-07 18:49:27,140][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0363, Metrics: {'mse': 0.03665649890899658, 'rmse': 0.19145887001911555, 'r2': -1.056147813796997}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1640Epoch 2/15: [                              ] 2/75 batches, loss: 0.1690Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1584Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1738Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1673Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1533Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1626Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1585Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1683Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1740Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1682Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1672Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1712Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1672Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1670Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1668Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1642Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1626Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1579Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1531Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1498Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1486Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1475Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1462Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1428Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1458Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1449Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1438Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1434Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1427Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1428Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1447Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1427Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1410Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1420Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1402Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1400Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1394Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1385Epoch 2/15: [================              ] 40/75 batches, loss: 0.1377Epoch 2/15: [================              ] 41/75 batches, loss: 0.1362Epoch 2/15: [================              ] 42/75 batches, loss: 0.1380Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1395Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1411Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1397Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1381Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1369Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1354Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1350Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1342Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1334Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1326Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1318Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1315Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1305Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1319Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1305Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1293Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1282Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1277Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1281Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1286Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1283Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1272Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1262Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1253Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1265Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1264Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1259Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1260Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1258Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1253Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1241Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1233Epoch 2/15: [==============================] 75/75 batches, loss: 0.1231
[2025-05-07 18:49:30,103][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1231
[2025-05-07 18:49:30,560][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0365, Metrics: {'mse': 0.0369163379073143, 'rmse': 0.19213624829093104, 'r2': -1.0707225799560547}
[2025-05-07 18:49:30,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1248Epoch 3/15: [                              ] 2/75 batches, loss: 0.1006Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0916Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0841Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0874Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0910Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0884Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0940Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0938Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0913Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0949Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0914Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0910Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0883Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0872Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0867Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0839Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0837Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0832Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0845Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0893Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0884Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0907Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0896Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0879Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0872Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0903Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0902Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0887Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0870Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0871Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0929Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0923Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0928Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0923Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0917Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0920Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0910Epoch 3/15: [================              ] 40/75 batches, loss: 0.0898Epoch 3/15: [================              ] 41/75 batches, loss: 0.0902Epoch 3/15: [================              ] 42/75 batches, loss: 0.0905Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0899Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0896Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0894Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0905Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0898Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0890Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0888Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0888Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0885Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0891Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0887Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0878Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0872Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0867Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0861Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0867Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0865Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0855Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0852Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0847Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0843Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0835Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0835Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0827Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0820Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0821Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0823Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0821Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0816Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0816Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0824Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0825Epoch 3/15: [==============================] 75/75 batches, loss: 0.0821
[2025-05-07 18:49:33,099][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0821
[2025-05-07 18:49:33,531][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0376, Metrics: {'mse': 0.03805581480264664, 'rmse': 0.19507899631340797, 'r2': -1.1346385478973389}
[2025-05-07 18:49:33,532][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0962Epoch 4/15: [                              ] 2/75 batches, loss: 0.0790Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0916Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0940Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0789Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0752Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0741Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0709Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0715Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0691Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0747Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0736Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0720Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0720Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0702Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0720Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0711Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0695Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0712Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0719Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0701Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0691Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0688Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0695Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0700Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0687Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0702Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0693Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0688Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0685Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0688Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0689Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0687Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0683Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0680Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0676Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0673Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0673Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0667Epoch 4/15: [================              ] 40/75 batches, loss: 0.0667Epoch 4/15: [================              ] 41/75 batches, loss: 0.0663Epoch 4/15: [================              ] 42/75 batches, loss: 0.0662Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0665Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0662Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0659Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0664Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0662Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0661Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0656Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0650Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0644Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0641Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0643Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0640Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0634Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0639Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0637Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0642Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0641Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0646Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0648Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0646Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0647Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0646Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0646Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0644Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0647Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0646Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0649Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0648Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0644Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0644Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0645Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0646Epoch 4/15: [==============================] 75/75 batches, loss: 0.0646
[2025-05-07 18:49:35,971][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0646
[2025-05-07 18:49:36,425][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0292, Metrics: {'mse': 0.029644686728715897, 'rmse': 0.17217632453016268, 'r2': -0.6628389358520508}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0603Epoch 5/15: [                              ] 2/75 batches, loss: 0.0654Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0595Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0570Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0560Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0593Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0596Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0583Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0580Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0560Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0611Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0629Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0635Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0615Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0603Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0600Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0605Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0591Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0584Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0570Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0553Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0580Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0583Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0583Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0573Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0570Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0558Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0571Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0579Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0584Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0575Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0569Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0565Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0566Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0557Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0554Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0548Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0550Epoch 5/15: [================              ] 40/75 batches, loss: 0.0549Epoch 5/15: [================              ] 41/75 batches, loss: 0.0546Epoch 5/15: [================              ] 42/75 batches, loss: 0.0553Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0547Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0550Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0547Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0548Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0544Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0542Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0541Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0538Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0533Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0533Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0534Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0532Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0532Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0529Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0523Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0521Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0524Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0524Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0531Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0535Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0535Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0531Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0535Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0534Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0533Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0532Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0530Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0532Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0529Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0526Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0522Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0519Epoch 5/15: [==============================] 75/75 batches, loss: 0.0527
[2025-05-07 18:49:39,350][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0527
[2025-05-07 18:49:39,754][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0263, Metrics: {'mse': 0.02671906165778637, 'rmse': 0.163459663702659, 'r2': -0.49873387813568115}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0476Epoch 6/15: [                              ] 2/75 batches, loss: 0.0533Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0484Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0476Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0633Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0599Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0623Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0610Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0577Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0556Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0523Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0558Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0540Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0526Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0528Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0526Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0508Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0494Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0486Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0490Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0483Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0471Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0477Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0485Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0488Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0480Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0477Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0480Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0482Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0474Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0481Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0477Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0474Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0472Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0476Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0481Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0484Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0481Epoch 6/15: [================              ] 40/75 batches, loss: 0.0479Epoch 6/15: [================              ] 41/75 batches, loss: 0.0480Epoch 6/15: [================              ] 42/75 batches, loss: 0.0476Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0468Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0470Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0467Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0465Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0460Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0461Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0464Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0468Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0465Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0461Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0463Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0459Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0456Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0456Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0458Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0457Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0458Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0454Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0454Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0453Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0451Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0449Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0446Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0444Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0442Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0441Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0439Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0437Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0438Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0437Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0436Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0432Epoch 6/15: [==============================] 75/75 batches, loss: 0.0438
[2025-05-07 18:49:42,519][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0438
[2025-05-07 18:49:42,793][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0212, Metrics: {'mse': 0.021574875339865685, 'rmse': 0.1468838838670386, 'r2': -0.21018457412719727}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0773Epoch 7/15: [                              ] 2/75 batches, loss: 0.0602Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0508Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0423Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0425Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0425Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0443Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0487Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0457Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0457Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0439Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0419Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0448Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0433Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0415Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0412Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0412Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0418Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0423Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0423Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0414Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0403Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0409Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0401Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0402Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0396Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0400Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0395Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0403Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0411Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0407Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0398Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0397Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0413Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0412Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0406Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0403Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0403Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0408Epoch 7/15: [================              ] 40/75 batches, loss: 0.0409Epoch 7/15: [================              ] 41/75 batches, loss: 0.0405Epoch 7/15: [================              ] 42/75 batches, loss: 0.0406Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0402Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0397Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0397Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0394Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0393Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0392Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0389Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0387Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0383Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0379Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0379Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0375Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0376Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0377Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0378Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0380Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0377Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0378Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0376Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0373Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0371Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0370Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0369Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0372Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0374Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0372Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0372Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0375Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0372Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0370Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0368Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0367Epoch 7/15: [==============================] 75/75 batches, loss: 0.0366
[2025-05-07 18:49:45,564][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0366
[2025-05-07 18:49:46,007][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0267, Metrics: {'mse': 0.02704608626663685, 'rmse': 0.16445694350387535, 'r2': -0.5170774459838867}
[2025-05-07 18:49:46,008][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0427Epoch 8/15: [                              ] 2/75 batches, loss: 0.0339Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0321Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0304Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0310Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0316Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0336Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0326Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0326Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0317Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0320Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0315Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0322Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0324Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0331Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0327Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0338Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0337Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0343Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0350Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0347Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0364Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0364Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0361Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0353Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0359Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0355Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0352Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0358Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0355Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0358Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0366Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0367Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0366Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0363Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0355Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0350Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0348Epoch 8/15: [================              ] 40/75 batches, loss: 0.0348Epoch 8/15: [================              ] 41/75 batches, loss: 0.0346Epoch 8/15: [================              ] 42/75 batches, loss: 0.0346Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0346Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0343Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0344Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0343Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0340Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0340Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0337Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0337Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0335Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0332Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0328Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0327Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0325Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0325Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0324Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0325Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0324Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0324Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0323Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0320Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0320Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0319Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0318Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0318Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0318Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0316Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0314Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0312Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0315Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0315Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0316Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0315Epoch 8/15: [==============================] 75/75 batches, loss: 0.0314
[2025-05-07 18:49:48,451][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0314
[2025-05-07 18:49:48,711][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0262, Metrics: {'mse': 0.026626193895936012, 'rmse': 0.16317534708385337, 'r2': -0.4935246706008911}
[2025-05-07 18:49:48,712][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0187Epoch 9/15: [                              ] 2/75 batches, loss: 0.0287Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0295Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0285Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0287Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0288Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0287Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0291Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0300Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0305Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0308Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0299Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0291Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0291Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0300Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0299Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0297Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0295Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0311Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0306Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0298Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0305Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0307Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0309Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0307Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0308Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0300Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0298Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0303Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0302Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0297Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0298Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0299Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0295Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0309Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0304Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0306Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0314Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0313Epoch 9/15: [================              ] 40/75 batches, loss: 0.0312Epoch 9/15: [================              ] 41/75 batches, loss: 0.0309Epoch 9/15: [================              ] 42/75 batches, loss: 0.0309Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0311Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0312Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0308Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0309Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0306Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0305Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0303Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0303Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0302Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0299Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0296Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0303Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0301Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0305Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0302Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0300Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0300Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0301Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0301Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0298Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0300Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0299Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0300Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0299Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0297Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0296Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0298Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0296Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0297Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0295Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0296Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0296Epoch 9/15: [==============================] 75/75 batches, loss: 0.0297
[2025-05-07 18:49:51,021][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0297
[2025-05-07 18:49:51,343][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0256, Metrics: {'mse': 0.025987988337874413, 'rmse': 0.1612079040800246, 'r2': -0.45772624015808105}
[2025-05-07 18:49:51,343][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0404Epoch 10/15: [                              ] 2/75 batches, loss: 0.0355Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0369Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0332Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0297Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0285Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0270Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0282Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0284Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0281Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0271Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0271Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0277Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0274Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0275Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0283Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0280Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0277Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0270Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0269Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0272Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0273Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0270Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0274Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0278Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0276Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0276Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0274Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0278Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0280Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0277Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0272Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0270Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0270Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0282Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0281Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0283Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0279Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0278Epoch 10/15: [================              ] 40/75 batches, loss: 0.0279Epoch 10/15: [================              ] 41/75 batches, loss: 0.0280Epoch 10/15: [================              ] 42/75 batches, loss: 0.0280Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0278Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0278Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0278Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0276Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0276Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0280Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0278Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0280Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0278Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0281Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0280Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0280Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0280Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0281Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0281Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0282Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0281Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0278Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0278Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0281Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0283Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0280Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0281Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0278Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0278Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0278Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0280Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0280Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0279Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0278Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0277Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0276Epoch 10/15: [==============================] 75/75 batches, loss: 0.0276
[2025-05-07 18:49:53,677][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0276
[2025-05-07 18:49:54,124][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0224, Metrics: {'mse': 0.022825991734862328, 'rmse': 0.15108273142507825, 'r2': -0.280362606048584}
[2025-05-07 18:49:54,125][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 18:49:54,125][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 18:49:54,125][src.training.lm_trainer][INFO] - Training completed in 30.57 seconds
[2025-05-07 18:49:54,125][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:49:57,261][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.008233362808823586, 'rmse': 0.09073787968000788, 'r2': 0.11485564708709717}
[2025-05-07 18:49:57,262][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.021574875339865685, 'rmse': 0.1468838838670386, 'r2': -0.21018457412719727}
[2025-05-07 18:49:57,262][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12042810767889023, 'rmse': 0.3470275315863141, 'r2': -1.832331895828247}
[2025-05-07 18:49:59,516][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja/ja/model.pt
[2025-05-07 18:49:59,517][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▁
wandb:     best_val_mse █▅▃▁
wandb:      best_val_r2 ▁▄▆█
wandb:    best_val_rmse █▅▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▁▄▅▆▅▅▅
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇██▄▃▁▃▃▃▂
wandb:          val_mse ▇██▄▃▁▃▃▃▂
wandb:           val_r2 ▂▁▁▅▆█▆▆▆▇
wandb:         val_rmse ▇██▅▃▁▄▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0212
wandb:     best_val_mse 0.02157
wandb:      best_val_r2 -0.21018
wandb:    best_val_rmse 0.14688
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.12043
wandb:    final_test_r2 -1.83233
wandb:  final_test_rmse 0.34703
wandb:  final_train_mse 0.00823
wandb:   final_train_r2 0.11486
wandb: final_train_rmse 0.09074
wandb:    final_val_mse 0.02157
wandb:     final_val_r2 -0.21018
wandb:   final_val_rmse 0.14688
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02764
wandb:       train_time 30.57431
wandb:         val_loss 0.02241
wandb:          val_mse 0.02283
wandb:           val_r2 -0.28036
wandb:         val_rmse 0.15108
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184900-tvecfar0
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_184900-tvecfar0/logs
Experiment probe_layer2_avg_links_len_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/ja/ja/results.json for layer 2
=======================
PROBING LAYER 6 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer6_avg_links_len_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_avg_links_len_ru"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ru"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:50:35,722][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ru
experiment_name: probe_layer6_avg_links_len_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 18:50:35,723][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 18:50:35,723][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:50:35,723][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:50:35,723][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:50:35,727][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-05-07 18:50:35,727][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:50:35,727][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:50:39,510][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:50:41,897][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:50:41,898][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:50:42,207][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:42,339][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:42,786][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-07 18:50:42,794][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:50:42,794][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-07 18:50:42,799][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:50:42,927][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:43,051][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:43,084][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-07 18:50:43,086][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:50:43,086][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-07 18:50:43,089][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:50:43,209][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:43,361][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:50:43,392][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-07 18:50:43,394][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:50:43,394][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-07 18:50:43,397][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-07 18:50:43,397][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:50:43,397][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:50:43,397][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:50:43,397][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:50:43,398][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-05-07 18:50:43,398][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:50:43,398][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:50:43,398][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-05-07 18:50:43,399][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:50:43,399][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-05-07 18:50:43,399][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-05-07 18:50:43,399][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-07 18:50:43,400][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-05-07 18:50:43,400][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-07 18:50:43,400][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:50:43,400][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:50:43,400][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 18:50:43,400][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:50:52,653][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:50:52,654][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:50:52,654][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:50:52,654][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:50:52,657][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:50:52,658][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:50:52,658][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:50:52,658][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:50:52,658][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-07 18:50:52,659][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:50:52,659][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.1987Epoch 1/15: [                              ] 2/75 batches, loss: 0.5106Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4167Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4145Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4267Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3984Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3881Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3983Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3950Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3904Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3861Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3873Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3791Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3697Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3726Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3765Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3707Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3604Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3620Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3484Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3477Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3478Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3431Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3498Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3538Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3484Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3537Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3523Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3544Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3512Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3436Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3425Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3409Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3394Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3373Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3380Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3367Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3354Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3322Epoch 1/15: [================              ] 40/75 batches, loss: 0.3312Epoch 1/15: [================              ] 41/75 batches, loss: 0.3307Epoch 1/15: [================              ] 42/75 batches, loss: 0.3279Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3262Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3264Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3253Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3227Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3199Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3174Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3145Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3119Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3085Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3065Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3037Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3043Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3028Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3021Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3033Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3031Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3004Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2976Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2962Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2944Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2931Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2916Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2923Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2913Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2886Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2855Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2852Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2832Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2815Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2793Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2766Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2749Epoch 1/15: [==============================] 75/75 batches, loss: 0.2734
[2025-05-07 18:50:59,743][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2734
[2025-05-07 18:50:59,985][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0423, Metrics: {'mse': 0.046022217720746994, 'rmse': 0.21452789497113656, 'r2': -0.5420457124710083}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1211Epoch 2/15: [                              ] 2/75 batches, loss: 0.1172Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1083Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1513Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1419Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1347Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1461Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1469Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1429Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1432Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1490Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1493Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1475Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1467Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1445Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1425Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1429Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1447Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1419Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1408Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1400Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1423Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1493Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1489Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1494Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1519Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1528Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1500Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1502Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1488Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1478Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1462Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1494Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1491Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1496Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1474Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1463Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1465Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1466Epoch 2/15: [================              ] 40/75 batches, loss: 0.1465Epoch 2/15: [================              ] 41/75 batches, loss: 0.1481Epoch 2/15: [================              ] 42/75 batches, loss: 0.1478Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1482Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1467Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1466Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1477Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1472Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1467Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1458Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1440Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1443Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1449Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1458Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1460Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1445Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1486Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1472Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1463Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1464Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1453Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1456Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1462Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1473Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1482Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1473Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1467Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1479Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1478Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1468Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1464Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1458Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1450Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1444Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1436Epoch 2/15: [==============================] 75/75 batches, loss: 0.1421
[2025-05-07 18:51:02,658][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1421
[2025-05-07 18:51:02,928][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0497, Metrics: {'mse': 0.053871989250183105, 'rmse': 0.23210340206507768, 'r2': -0.8050645589828491}
[2025-05-07 18:51:02,928][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1996Epoch 3/15: [                              ] 2/75 batches, loss: 0.1674Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1484Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1315Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1231Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1334Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1293Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1171Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1161Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1118Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1132Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1092Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1084Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1089Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1061Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1048Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1033Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1013Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1006Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0994Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0974Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0985Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0988Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1020Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1044Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1030Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1010Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0999Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1013Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0997Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0984Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0996Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1004Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0993Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0977Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0972Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0963Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0961Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0967Epoch 3/15: [================              ] 40/75 batches, loss: 0.0955Epoch 3/15: [================              ] 41/75 batches, loss: 0.0982Epoch 3/15: [================              ] 42/75 batches, loss: 0.0978Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0980Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0970Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0966Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0955Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0954Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0954Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0964Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0950Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0961Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0956Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0954Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0962Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0957Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0949Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0945Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0946Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0941Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0934Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0928Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0925Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0922Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0927Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0921Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0916Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0917Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0921Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0916Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0913Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0911Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0906Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0901Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0904Epoch 3/15: [==============================] 75/75 batches, loss: 0.0910
[2025-05-07 18:51:05,242][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0910
[2025-05-07 18:51:05,487][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0283, Metrics: {'mse': 0.030560771003365517, 'rmse': 0.1748163922616112, 'r2': -0.023986101150512695}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1405Epoch 4/15: [                              ] 2/75 batches, loss: 0.0991Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0894Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0983Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1025Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1032Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1025Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1044Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1067Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1012Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0967Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0941Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0904Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0914Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0948Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0973Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0943Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0925Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0939Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0973Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0962Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0979Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0990Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0989Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0989Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0977Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0965Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0956Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0954Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0943Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0938Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0929Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0918Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0914Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0901Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0908Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0913Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0929Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0920Epoch 4/15: [================              ] 40/75 batches, loss: 0.0911Epoch 4/15: [================              ] 41/75 batches, loss: 0.0897Epoch 4/15: [================              ] 42/75 batches, loss: 0.0892Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0886Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0874Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0875Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0874Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0882Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0871Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0876Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0875Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0872Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0880Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0877Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0875Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0868Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0869Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0866Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0854Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0848Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0847Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0854Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0848Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0847Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0846Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0843Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0840Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0839Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0844Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0841Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0845Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0844Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0842Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0845Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0841Epoch 4/15: [==============================] 75/75 batches, loss: 0.0850
[2025-05-07 18:51:08,204][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0850
[2025-05-07 18:51:08,472][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0268, Metrics: {'mse': 0.028671396896243095, 'rmse': 0.16932630302538085, 'r2': 0.03932034969329834}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0647Epoch 5/15: [                              ] 2/75 batches, loss: 0.0477Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0671Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0692Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0709Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0730Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0822Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0784Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0775Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0737Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0718Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0773Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0795Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0808Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0809Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0805Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0787Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0787Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0800Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0786Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0785Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0784Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0779Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0763Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0752Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0744Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0741Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0762Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0749Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0739Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0728Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0732Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0728Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0728Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0725Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0718Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0716Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0710Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0729Epoch 5/15: [================              ] 40/75 batches, loss: 0.0735Epoch 5/15: [================              ] 41/75 batches, loss: 0.0735Epoch 5/15: [================              ] 42/75 batches, loss: 0.0735Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0738Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0733Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0732Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0736Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0741Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0736Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0735Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0732Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0734Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0725Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0720Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0715Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0720Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0722Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0719Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0714Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0714Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0712Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0708Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0703Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0700Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0699Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0709Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0712Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0708Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0706Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0703Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0702Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0699Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0706Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0703Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0698Epoch 5/15: [==============================] 75/75 batches, loss: 0.0700
[2025-05-07 18:51:11,128][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0700
[2025-05-07 18:51:11,468][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0244, Metrics: {'mse': 0.02502993308007717, 'rmse': 0.15820851140212772, 'r2': 0.16133326292037964}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0310Epoch 6/15: [                              ] 2/75 batches, loss: 0.0481Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0437Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0457Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0457Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0488Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0549Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0532Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0524Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0536Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0521Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0554Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0547Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0539Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0544Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0546Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0531Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0547Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0544Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0543Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0556Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0561Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0562Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0561Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0570Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0602Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0606Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0604Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0600Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0599Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0602Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0598Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0587Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0589Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0590Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0583Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0586Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0583Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0592Epoch 6/15: [================              ] 40/75 batches, loss: 0.0596Epoch 6/15: [================              ] 41/75 batches, loss: 0.0599Epoch 6/15: [================              ] 42/75 batches, loss: 0.0608Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0625Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0633Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0635Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0632Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0629Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0628Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0630Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0636Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0634Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0636Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0637Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0636Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0631Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0630Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0624Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0619Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0619Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0616Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0623Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0623Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0621Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0622Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0621Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0614Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0610Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0612Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0613Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0620Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0619Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0617Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0619Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0620Epoch 6/15: [==============================] 75/75 batches, loss: 0.0620
[2025-05-07 18:51:14,165][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0620
[2025-05-07 18:51:14,532][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0245, Metrics: {'mse': 0.025399351492524147, 'rmse': 0.1593717399432037, 'r2': 0.14895528554916382}
[2025-05-07 18:51:14,533][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0315Epoch 7/15: [                              ] 2/75 batches, loss: 0.0507Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0511Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0606Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0587Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0596Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0569Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0556Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0546Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0564Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0562Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0556Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0570Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0579Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0570Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0566Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0558Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0555Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0536Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0531Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0544Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0537Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0527Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0528Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0536Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0536Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0533Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0535Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0545Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0545Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0538Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0540Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0537Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0533Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0533Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0529Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0530Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0528Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0527Epoch 7/15: [================              ] 40/75 batches, loss: 0.0536Epoch 7/15: [================              ] 41/75 batches, loss: 0.0544Epoch 7/15: [================              ] 42/75 batches, loss: 0.0552Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0552Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0557Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0554Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0563Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0561Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0557Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0555Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0551Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0549Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0557Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0555Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0556Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0558Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0557Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0555Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0559Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0555Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0553Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0554Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0552Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0551Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0551Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0548Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0545Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0542Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0542Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0538Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0539Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0536Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0540Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0537Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0536Epoch 7/15: [==============================] 75/75 batches, loss: 0.0534
[2025-05-07 18:51:16,840][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0534
[2025-05-07 18:51:17,388][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0295, Metrics: {'mse': 0.03164688125252724, 'rmse': 0.1778957032997909, 'r2': -0.06037783622741699}
[2025-05-07 18:51:17,388][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0356Epoch 8/15: [                              ] 2/75 batches, loss: 0.0436Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0457Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0590Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0515Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0596Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0567Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0550Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0538Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0559Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0553Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0552Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0546Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0543Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0533Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0530Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0521Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0538Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0533Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0532Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0529Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0531Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0521Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0522Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0529Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0525Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0521Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0528Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0544Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0556Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0551Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0555Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0550Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0544Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0534Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0528Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0537Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0540Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0545Epoch 8/15: [================              ] 40/75 batches, loss: 0.0549Epoch 8/15: [================              ] 41/75 batches, loss: 0.0552Epoch 8/15: [================              ] 42/75 batches, loss: 0.0545Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0550Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0549Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0550Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0559Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0554Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0549Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0545Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0544Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0546Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0546Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0544Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0539Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0541Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0545Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0542Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0538Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0537Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0540Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0538Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0539Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0538Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0539Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0538Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0538Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0536Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0536Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0536Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0537Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0535Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0533Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0531Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0530Epoch 8/15: [==============================] 75/75 batches, loss: 0.0530
[2025-05-07 18:51:19,867][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0530
[2025-05-07 18:51:20,371][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0281, Metrics: {'mse': 0.030155159533023834, 'rmse': 0.17365241009851787, 'r2': -0.010395407676696777}
[2025-05-07 18:51:20,371][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0251Epoch 9/15: [                              ] 2/75 batches, loss: 0.0242Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0327Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0350Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0318Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0438Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0426Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0489Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0475Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0459Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0451Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0459Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0450Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0462Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0457Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0460Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0456Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0458Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0458Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0469Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0464Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0470Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0476Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0479Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0494Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0489Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0492Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0491Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0494Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0493Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0494Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0488Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0486Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0489Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0490Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0485Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0489Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0488Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0495Epoch 9/15: [================              ] 40/75 batches, loss: 0.0503Epoch 9/15: [================              ] 41/75 batches, loss: 0.0498Epoch 9/15: [================              ] 42/75 batches, loss: 0.0499Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0496Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0494Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0489Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0485Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0481Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0477Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0479Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0477Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0478Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0478Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0483Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0482Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0490Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0490Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0491Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0492Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0492Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0491Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0489Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0484Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0486Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0487Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0486Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0485Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0483Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0484Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0482Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0481Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0482Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0484Epoch 9/15: [==============================] 75/75 batches, loss: 0.0482
[2025-05-07 18:51:22,870][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0482
[2025-05-07 18:51:23,373][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0236, Metrics: {'mse': 0.02462451532483101, 'rmse': 0.1569220039536553, 'r2': 0.17491745948791504}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0416Epoch 10/15: [                              ] 2/75 batches, loss: 0.0474Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0438Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0466Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0476Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0568Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0543Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0543Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0525Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0537Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0529Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0517Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0539Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0533Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0538Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0524Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0521Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0518Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0507Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0504Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0492Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0489Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0499Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0503Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0506Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0499Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0501Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0496Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0508Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0503Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0502Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0494Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0496Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0493Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0496Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0503Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0497Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0493Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0493Epoch 10/15: [================              ] 40/75 batches, loss: 0.0489Epoch 10/15: [================              ] 41/75 batches, loss: 0.0487Epoch 10/15: [================              ] 42/75 batches, loss: 0.0488Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0485Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0477Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0472Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0467Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0462Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0462Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0458Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0452Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0450Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0448Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0450Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0448Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0450Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0448Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0445Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0449Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0453Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0454Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0455Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0455Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0456Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0451Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0451Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0449Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0451Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0450Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0451Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0450Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0451Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0451Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0447Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0448Epoch 10/15: [==============================] 75/75 batches, loss: 0.0453
[2025-05-07 18:51:26,183][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0453
[2025-05-07 18:51:26,754][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0250, Metrics: {'mse': 0.026659738272428513, 'rmse': 0.16327810101917684, 'r2': 0.1067240834236145}
[2025-05-07 18:51:26,755][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0411Epoch 11/15: [                              ] 2/75 batches, loss: 0.0298Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0315Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0481Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0472Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0431Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0518Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0515Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0477Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0479Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0473Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0476Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0462Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0447Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0450Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0439Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0432Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0427Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0424Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0427Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0431Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0442Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0448Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0450Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0446Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0450Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0451Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0444Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0441Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0436Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0435Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0428Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0425Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0424Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0434Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0431Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0426Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0423Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0421Epoch 11/15: [================              ] 40/75 batches, loss: 0.0420Epoch 11/15: [================              ] 41/75 batches, loss: 0.0421Epoch 11/15: [================              ] 42/75 batches, loss: 0.0416Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0417Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0418Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0416Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0418Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0418Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0414Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0416Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0414Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0410Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0411Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0411Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0410Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0409Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0406Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0405Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0403Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0401Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0401Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0400Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0406Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0405Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0405Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0407Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0410Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0413Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0409Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0409Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0410Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0410Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0415Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0416Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0415Epoch 11/15: [==============================] 75/75 batches, loss: 0.0414
[2025-05-07 18:51:29,278][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0414
[2025-05-07 18:51:29,669][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0218, Metrics: {'mse': 0.022698281332850456, 'rmse': 0.15065948802797138, 'r2': 0.2394588589668274}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0378Epoch 12/15: [                              ] 2/75 batches, loss: 0.0355Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0490Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0484Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0517Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0488Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0463Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0462Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0516Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0496Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0503Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0510Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0484Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0467Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0451Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0455Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0456Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0442Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0440Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0434Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0434Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0436Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0440Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0432Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0427Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0433Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0433Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0428Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0437Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0437Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0438Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0434Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0441Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0436Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0434Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0436Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0431Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0435Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0430Epoch 12/15: [================              ] 40/75 batches, loss: 0.0428Epoch 12/15: [================              ] 41/75 batches, loss: 0.0429Epoch 12/15: [================              ] 42/75 batches, loss: 0.0434Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0429Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0426Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0426Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0425Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0430Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0429Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0425Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0421Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0423Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0432Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0430Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0432Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0436Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0435Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0434Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0437Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0437Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0437Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0433Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0431Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0428Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0425Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0422Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0422Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0423Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0421Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0419Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0418Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0415Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0413Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0413Epoch 12/15: [==============================] 75/75 batches, loss: 0.0413
[2025-05-07 18:51:32,517][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0413
[2025-05-07 18:51:32,884][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0222, Metrics: {'mse': 0.023067038506269455, 'rmse': 0.1518783674730192, 'r2': 0.22710305452346802}
[2025-05-07 18:51:32,884][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0577Epoch 13/15: [                              ] 2/75 batches, loss: 0.0472Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0473Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0526Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0465Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0505Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0466Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0522Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0490Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0513Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0495Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0482Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0468Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0488Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0473Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0466Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0471Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0464Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0466Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0455Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0455Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0458Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0457Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0459Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0453Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0442Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0442Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0441Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0437Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0432Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0430Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0427Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0440Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0434Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0446Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0446Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0442Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0437Epoch 13/15: [================              ] 40/75 batches, loss: 0.0436Epoch 13/15: [================              ] 41/75 batches, loss: 0.0433Epoch 13/15: [================              ] 42/75 batches, loss: 0.0437Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0435Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0434Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0441Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0438Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0436Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0434Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0434Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0434Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0432Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0434Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0432Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0434Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0432Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0432Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0434Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0430Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0432Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0432Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0432Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0433Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0431Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0430Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0434Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0430Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0431Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0432Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0436Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0433Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0433Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0433Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0430Epoch 13/15: [==============================] 75/75 batches, loss: 0.0426
[2025-05-07 18:51:35,319][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0426
[2025-05-07 18:51:35,721][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0221, Metrics: {'mse': 0.022909261286258698, 'rmse': 0.15135805656210938, 'r2': 0.2323896884918213}
[2025-05-07 18:51:35,722][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0286Epoch 14/15: [                              ] 2/75 batches, loss: 0.0375Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0450Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0394Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0344Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0327Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0335Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0369Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0380Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0379Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0398Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0412Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0399Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0383Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0381Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0382Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0373Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0378Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0369Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0377Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0380Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0384Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0391Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0392Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0392Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0385Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0394Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0396Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0404Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0399Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0398Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0394Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0400Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0393Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0391Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0390Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0391Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0389Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0390Epoch 14/15: [================              ] 40/75 batches, loss: 0.0386Epoch 14/15: [================              ] 41/75 batches, loss: 0.0388Epoch 14/15: [================              ] 42/75 batches, loss: 0.0386Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0384Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0388Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0390Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0392Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0390Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0387Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0387Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0385Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0381Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0380Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0379Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0381Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0380Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0376Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0382Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0381Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0383Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0380Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0382Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0384Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0382Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0386Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0384Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0385Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0385Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0384Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0384Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0383Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0385Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0386Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0386Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0384Epoch 14/15: [==============================] 75/75 batches, loss: 0.0381
[2025-05-07 18:51:38,193][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0381
[2025-05-07 18:51:38,455][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0211, Metrics: {'mse': 0.020746275782585144, 'rmse': 0.1440356753814316, 'r2': 0.3048638701438904}
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0479Epoch 15/15: [                              ] 2/75 batches, loss: 0.0387Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0353Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0347Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0362Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0369Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0353Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0352Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0348Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0380Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0361Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0361Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0354Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0364Epoch 15/15: [======                        ] 17/75 batches, loss: 0.0364Epoch 15/15: [=======                       ] 18/75 batches, loss: 0.0367Epoch 15/15: [=======                       ] 19/75 batches, loss: 0.0373Epoch 15/15: [========                      ] 20/75 batches, loss: 0.0373Epoch 15/15: [========                      ] 21/75 batches, loss: 0.0370Epoch 15/15: [========                      ] 22/75 batches, loss: 0.0365Epoch 15/15: [=========                     ] 23/75 batches, loss: 0.0367Epoch 15/15: [=========                     ] 24/75 batches, loss: 0.0368Epoch 15/15: [==========                    ] 25/75 batches, loss: 0.0373Epoch 15/15: [==========                    ] 26/75 batches, loss: 0.0377Epoch 15/15: [==========                    ] 27/75 batches, loss: 0.0374Epoch 15/15: [===========                   ] 28/75 batches, loss: 0.0374Epoch 15/15: [===========                   ] 29/75 batches, loss: 0.0372Epoch 15/15: [============                  ] 30/75 batches, loss: 0.0378Epoch 15/15: [============                  ] 31/75 batches, loss: 0.0375Epoch 15/15: [============                  ] 32/75 batches, loss: 0.0377Epoch 15/15: [=============                 ] 33/75 batches, loss: 0.0374Epoch 15/15: [=============                 ] 34/75 batches, loss: 0.0372Epoch 15/15: [==============                ] 35/75 batches, loss: 0.0376Epoch 15/15: [==============                ] 36/75 batches, loss: 0.0376Epoch 15/15: [==============                ] 37/75 batches, loss: 0.0376Epoch 15/15: [===============               ] 38/75 batches, loss: 0.0377Epoch 15/15: [===============               ] 39/75 batches, loss: 0.0376Epoch 15/15: [================              ] 40/75 batches, loss: 0.0380Epoch 15/15: [================              ] 41/75 batches, loss: 0.0381Epoch 15/15: [================              ] 42/75 batches, loss: 0.0381Epoch 15/15: [=================             ] 43/75 batches, loss: 0.0375Epoch 15/15: [=================             ] 44/75 batches, loss: 0.0378Epoch 15/15: [==================            ] 45/75 batches, loss: 0.0376Epoch 15/15: [==================            ] 46/75 batches, loss: 0.0377Epoch 15/15: [==================            ] 47/75 batches, loss: 0.0381Epoch 15/15: [===================           ] 48/75 batches, loss: 0.0385Epoch 15/15: [===================           ] 49/75 batches, loss: 0.0382Epoch 15/15: [====================          ] 50/75 batches, loss: 0.0380Epoch 15/15: [====================          ] 51/75 batches, loss: 0.0381Epoch 15/15: [====================          ] 52/75 batches, loss: 0.0382Epoch 15/15: [=====================         ] 53/75 batches, loss: 0.0384Epoch 15/15: [=====================         ] 54/75 batches, loss: 0.0383Epoch 15/15: [======================        ] 55/75 batches, loss: 0.0380Epoch 15/15: [======================        ] 56/75 batches, loss: 0.0382Epoch 15/15: [======================        ] 57/75 batches, loss: 0.0382Epoch 15/15: [=======================       ] 58/75 batches, loss: 0.0381Epoch 15/15: [=======================       ] 59/75 batches, loss: 0.0379Epoch 15/15: [========================      ] 60/75 batches, loss: 0.0377Epoch 15/15: [========================      ] 61/75 batches, loss: 0.0379Epoch 15/15: [========================      ] 62/75 batches, loss: 0.0380Epoch 15/15: [=========================     ] 63/75 batches, loss: 0.0381Epoch 15/15: [=========================     ] 64/75 batches, loss: 0.0381Epoch 15/15: [==========================    ] 65/75 batches, loss: 0.0381Epoch 15/15: [==========================    ] 66/75 batches, loss: 0.0381Epoch 15/15: [==========================    ] 67/75 batches, loss: 0.0383Epoch 15/15: [===========================   ] 68/75 batches, loss: 0.0380Epoch 15/15: [===========================   ] 69/75 batches, loss: 0.0386Epoch 15/15: [============================  ] 70/75 batches, loss: 0.0382Epoch 15/15: [============================  ] 71/75 batches, loss: 0.0386Epoch 15/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 15/15: [============================= ] 73/75 batches, loss: 0.0383Epoch 15/15: [============================= ] 74/75 batches, loss: 0.0381Epoch 15/15: [==============================] 75/75 batches, loss: 0.0379
[2025-05-07 18:51:41,216][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0379
[2025-05-07 18:51:41,578][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0201, Metrics: {'mse': 0.020433731377124786, 'rmse': 0.14294660323744943, 'r2': 0.3153361678123474}
[2025-05-07 18:51:41,961][src.training.lm_trainer][INFO] - Training completed in 45.46 seconds
[2025-05-07 18:51:41,961][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 18:51:45,193][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02211105078458786, 'rmse': 0.14869785063876298, 'r2': 0.336775004863739}
[2025-05-07 18:51:45,193][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.020433731377124786, 'rmse': 0.14294660323744943, 'r2': 0.3153361678123474}
[2025-05-07 18:51:45,193][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.014325984753668308, 'rmse': 0.1196912058326271, 'r2': 0.14954322576522827}
[2025-05-07 18:51:47,239][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ru/ru/model.pt
[2025-05-07 18:51:47,240][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▂▂▂▁▁
wandb:     best_val_mse █▄▃▂▂▂▁▁
wandb:      best_val_r2 ▁▅▆▇▇▇██
wandb:    best_val_rmse █▄▄▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▆▆▆▆▆▆▆▆▇▇▇▇
wandb:       train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▃▃▂▂▃▃▂▂▁▁▁▁▁
wandb:          val_mse ▆█▃▃▂▂▃▃▂▂▁▂▂▁▁
wandb:           val_r2 ▃▁▆▆▇▇▆▆▇▇█▇▇██
wandb:         val_rmse ▇█▄▃▂▂▄▃▂▃▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02012
wandb:     best_val_mse 0.02043
wandb:      best_val_r2 0.31534
wandb:    best_val_rmse 0.14295
wandb:            epoch 15
wandb:   final_test_mse 0.01433
wandb:    final_test_r2 0.14954
wandb:  final_test_rmse 0.11969
wandb:  final_train_mse 0.02211
wandb:   final_train_r2 0.33678
wandb: final_train_rmse 0.1487
wandb:    final_val_mse 0.02043
wandb:     final_val_r2 0.31534
wandb:   final_val_rmse 0.14295
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03787
wandb:       train_time 45.46086
wandb:         val_loss 0.02012
wandb:          val_mse 0.02043
wandb:           val_r2 0.31534
wandb:         val_rmse 0.14295
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_185035-zfj31xgt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_185035-zfj31xgt/logs
Experiment probe_layer6_avg_links_len_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ru/ru/results.json for layer 6
Running experiment: probe_layer6_avg_links_len_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_avg_links_len_ja"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 18:52:23,678][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer6/ja
experiment_name: probe_layer6_avg_links_len_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 18:52:23,678][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 18:52:23,678][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:52:23,678][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 18:52:23,678][__main__][INFO] - Determined Task Type: regression
[2025-05-07 18:52:23,683][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ja']
[2025-05-07 18:52:23,683][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 18:52:23,683][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 18:52:27,643][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 18:52:30,204][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 18:52:30,205][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:52:30,459][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:30,597][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:31,004][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-07 18:52:31,012][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:52:31,012][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-07 18:52:31,016][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:52:31,126][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:31,279][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:31,329][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-07 18:52:31,330][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:52:31,331][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-07 18:52:31,334][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 18:52:31,443][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:31,546][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 18:52:31,565][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-07 18:52:31,567][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 18:52:31,567][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-07 18:52:31,569][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-07 18:52:31,569][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:52:31,569][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:52:31,569][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:52:31,569][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:52:31,570][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:52:31,570][src.data.datasets][INFO] -   Mean: 0.1654, Std: 0.0964
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:52:31,570][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:52:31,570][src.data.datasets][INFO] -   Min: 0.0580, Max: 0.6190
[2025-05-07 18:52:31,571][src.data.datasets][INFO] -   Mean: 0.2214, Std: 0.1335
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Sample label: 0.3889999985694885
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 18:52:31,571][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 18:52:31,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 18:52:31,571][src.data.datasets][INFO] -   Mean: 0.4217, Std: 0.2062
[2025-05-07 18:52:31,572][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-07 18:52:31,572][src.data.datasets][INFO] - Sample label: 0.3799999952316284
[2025-05-07 18:52:31,572][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-07 18:52:31,572][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 18:52:31,572][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 18:52:31,572][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 18:52:31,572][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 18:52:40,007][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 18:52:40,008][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 18:52:40,008][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-07 18:52:40,008][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 18:52:40,011][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 18:52:40,011][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 18:52:40,011][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 18:52:40,012][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 18:52:40,012][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-07 18:52:40,013][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 18:52:40,013][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3080Epoch 1/15: [                              ] 2/75 batches, loss: 0.3692Epoch 1/15: [=                             ] 3/75 batches, loss: 0.2986Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3316Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3546Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3543Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3534Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3564Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3742Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3704Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3579Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3663Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3556Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3709Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3608Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3616Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3709Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3668Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3602Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3595Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3547Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3630Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3537Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3491Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3421Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3373Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3375Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3337Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3296Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3277Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3231Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3199Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3154Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3124Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3090Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3108Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3077Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3024Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3037Epoch 1/15: [================              ] 40/75 batches, loss: 0.3001Epoch 1/15: [================              ] 41/75 batches, loss: 0.2966Epoch 1/15: [================              ] 42/75 batches, loss: 0.2934Epoch 1/15: [=================             ] 43/75 batches, loss: 0.2901Epoch 1/15: [=================             ] 44/75 batches, loss: 0.2883Epoch 1/15: [==================            ] 45/75 batches, loss: 0.2883Epoch 1/15: [==================            ] 46/75 batches, loss: 0.2839Epoch 1/15: [==================            ] 47/75 batches, loss: 0.2815Epoch 1/15: [===================           ] 48/75 batches, loss: 0.2806Epoch 1/15: [===================           ] 49/75 batches, loss: 0.2768Epoch 1/15: [====================          ] 50/75 batches, loss: 0.2746Epoch 1/15: [====================          ] 51/75 batches, loss: 0.2730Epoch 1/15: [====================          ] 52/75 batches, loss: 0.2701Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.2698Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.2693Epoch 1/15: [======================        ] 55/75 batches, loss: 0.2677Epoch 1/15: [======================        ] 56/75 batches, loss: 0.2655Epoch 1/15: [======================        ] 57/75 batches, loss: 0.2624Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.2637Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.2615Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2593Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2571Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2553Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2545Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2531Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2514Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2489Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2467Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2451Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2450Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2446Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2434Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2450Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2427Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2422Epoch 1/15: [==============================] 75/75 batches, loss: 0.2403
[2025-05-07 18:52:47,023][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2403
[2025-05-07 18:52:47,313][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0319, Metrics: {'mse': 0.032094549387693405, 'rmse': 0.17914951685029298, 'r2': -0.8002573251724243}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.3107Epoch 2/15: [                              ] 2/75 batches, loss: 0.2262Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1774Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1579Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1572Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1414Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1498Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1468Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1523Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1619Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1632Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1587Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1556Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1557Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1495Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1489Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1444Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1475Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1450Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1416Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1396Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1367Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1362Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1344Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1333Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1354Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1370Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1360Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1351Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1366Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1364Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1358Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1346Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1326Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1338Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1321Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1337Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1334Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1328Epoch 2/15: [================              ] 40/75 batches, loss: 0.1326Epoch 2/15: [================              ] 41/75 batches, loss: 0.1313Epoch 2/15: [================              ] 42/75 batches, loss: 0.1300Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1311Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1309Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1298Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1293Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1284Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1300Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1297Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1291Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1291Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1279Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1274Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1259Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1251Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1268Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1258Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1248Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1237Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1224Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1218Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1222Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1217Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1208Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1195Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1194Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1189Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1188Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1182Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1182Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1173Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1168Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1161Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1157Epoch 2/15: [==============================] 75/75 batches, loss: 0.1159
[2025-05-07 18:52:50,001][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1159
[2025-05-07 18:52:50,230][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0386, Metrics: {'mse': 0.03847459703683853, 'rmse': 0.19614942527786955, 'r2': -1.1581292152404785}
[2025-05-07 18:52:50,231][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1265Epoch 3/15: [                              ] 2/75 batches, loss: 0.1070Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0964Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0818Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0835Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0922Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0937Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0897Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0902Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0874Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0887Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0847Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0806Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0800Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0775Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0758Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0735Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0746Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0757Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0762Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0767Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0774Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0781Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0773Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0768Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0767Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0770Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0785Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0788Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0770Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0772Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0824Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0827Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0848Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0835Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0820Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0818Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0812Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0803Epoch 3/15: [================              ] 40/75 batches, loss: 0.0810Epoch 3/15: [================              ] 41/75 batches, loss: 0.0815Epoch 3/15: [================              ] 42/75 batches, loss: 0.0816Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0806Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0803Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0813Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0818Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0813Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0809Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0818Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0813Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0815Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0814Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0809Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0803Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0795Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0794Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0801Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0804Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0817Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0810Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0811Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0807Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0807Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0797Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0796Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0790Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0784Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0792Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0795Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0794Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0791Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0788Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0793Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0789Epoch 3/15: [==============================] 75/75 batches, loss: 0.0785
[2025-05-07 18:52:52,540][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0785
[2025-05-07 18:52:52,758][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0450, Metrics: {'mse': 0.04497002437710762, 'rmse': 0.2120613693653505, 'r2': -1.522472620010376}
[2025-05-07 18:52:52,759][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0640Epoch 4/15: [                              ] 2/75 batches, loss: 0.0747Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0726Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0640Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0570Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0549Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0516Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0497Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0582Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0611Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0644Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0626Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0619Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0607Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0622Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0622Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0629Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0633Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0637Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0635Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0629Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0623Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0623Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0637Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0621Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0650Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0648Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0643Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0638Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0639Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0639Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0641Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0636Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0638Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0642Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0634Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0633Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 4/15: [================              ] 40/75 batches, loss: 0.0628Epoch 4/15: [================              ] 41/75 batches, loss: 0.0632Epoch 4/15: [================              ] 42/75 batches, loss: 0.0630Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0631Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0631Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0634Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0638Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0634Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0628Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0631Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0629Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0623Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0619Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0623Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0622Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0615Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0614Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0613Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0610Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0615Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0620Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0627Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0629Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0626Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0624Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0622Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0625Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0629Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0630Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0627Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0626Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0622Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0620Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0618Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0619Epoch 4/15: [==============================] 75/75 batches, loss: 0.0625
[2025-05-07 18:52:55,012][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0625
[2025-05-07 18:52:55,248][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0362, Metrics: {'mse': 0.03637167811393738, 'rmse': 0.19071360233066068, 'r2': -1.0401716232299805}
[2025-05-07 18:52:55,248][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0860Epoch 5/15: [                              ] 2/75 batches, loss: 0.0795Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0662Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0679Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0596Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0723Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0705Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0676Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0646Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0623Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0625Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0640Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0632Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0623Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0648Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0637Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0625Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0616Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0608Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0610Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0591Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0587Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0582Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0575Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0572Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0558Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0555Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0543Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0539Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0546Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0544Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0546Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0541Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0541Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0543Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0539Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0535Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0531Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0531Epoch 5/15: [================              ] 40/75 batches, loss: 0.0532Epoch 5/15: [================              ] 41/75 batches, loss: 0.0537Epoch 5/15: [================              ] 42/75 batches, loss: 0.0535Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0530Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0538Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0531Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0530Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0525Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0520Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0517Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0514Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0510Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0509Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0511Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0509Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0505Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0504Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0504Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0504Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0503Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0503Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0506Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0506Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0506Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0503Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0511Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0512Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0512Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0509Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0507Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0505Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0504Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0503Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0498Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0496Epoch 5/15: [==============================] 75/75 batches, loss: 0.0497
[2025-05-07 18:52:57,510][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0497
[2025-05-07 18:52:57,746][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0254, Metrics: {'mse': 0.025619709864258766, 'rmse': 0.1600615814749397, 'r2': -0.4370687007904053}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0577Epoch 6/15: [                              ] 2/75 batches, loss: 0.0421Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0373Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0358Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0399Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0423Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0506Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0498Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0480Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0478Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0476Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0498Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0487Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0481Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0467Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0480Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0472Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0459Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0449Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0442Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0443Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0445Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0434Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0436Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0443Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0444Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0442Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0439Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0429Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0428Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0422Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0418Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0420Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0416Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0409Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0408Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0413Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0411Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0412Epoch 6/15: [================              ] 40/75 batches, loss: 0.0413Epoch 6/15: [================              ] 41/75 batches, loss: 0.0416Epoch 6/15: [================              ] 42/75 batches, loss: 0.0415Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0409Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0409Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0405Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0406Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0405Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0403Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0407Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0406Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0408Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0407Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0405Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0402Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0401Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0401Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0397Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0399Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0399Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0396Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0393Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0390Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0388Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0388Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0386Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0384Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0382Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0381Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0378Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0378Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0377Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0374Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0372Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0373Epoch 6/15: [==============================] 75/75 batches, loss: 0.0380
[2025-05-07 18:53:00,583][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0380
[2025-05-07 18:53:00,784][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0229, Metrics: {'mse': 0.023198043927550316, 'rmse': 0.1523090408595311, 'r2': -0.30123186111450195}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0888Epoch 7/15: [                              ] 2/75 batches, loss: 0.0696Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0598Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0501Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0451Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0420Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0422Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0375Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0367Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0350Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0359Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0352Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0343Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0339Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0342Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0335Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0357Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0353Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0351Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0344Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0344Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0338Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0336Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0340Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0340Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0339Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0350Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0364Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0363Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0355Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0352Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0358Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0356Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0352Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0350Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0349Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0359Epoch 7/15: [================              ] 40/75 batches, loss: 0.0359Epoch 7/15: [================              ] 41/75 batches, loss: 0.0358Epoch 7/15: [================              ] 42/75 batches, loss: 0.0361Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0360Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0357Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0359Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0357Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0360Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0358Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0358Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0354Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0353Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0349Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0352Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0350Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0348Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0345Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0345Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0345Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0344Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0342Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0341Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0341Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0340Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0338Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0341Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0340Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0340Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0338Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0337Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0337Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0338Epoch 7/15: [==============================] 75/75 batches, loss: 0.0340
[2025-05-07 18:53:03,492][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0340
[2025-05-07 18:53:03,842][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0283, Metrics: {'mse': 0.02849888801574707, 'rmse': 0.16881613671609438, 'r2': -0.5985684394836426}
[2025-05-07 18:53:03,843][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0327Epoch 8/15: [                              ] 2/75 batches, loss: 0.0406Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0323Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0274Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0321Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0300Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0341Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0337Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0329Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0316Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0310Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0303Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0293Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0296Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0315Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0303Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0323Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0321Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0323Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0331Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0325Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0324Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0332Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0329Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0333Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0326Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0336Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0334Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0329Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0328Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0333Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0341Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0360Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0358Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0357Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0351Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0346Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0344Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0342Epoch 8/15: [================              ] 40/75 batches, loss: 0.0343Epoch 8/15: [================              ] 41/75 batches, loss: 0.0340Epoch 8/15: [================              ] 42/75 batches, loss: 0.0343Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0342Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0341Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0340Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0340Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0336Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0335Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0332Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0336Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0334Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0331Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0333Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0332Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0330Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0328Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0330Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0327Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0326Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0325Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0327Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0324Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0325Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0324Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0323Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0322Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0323Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0321Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0320Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0321Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0319Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0320Epoch 8/15: [==============================] 75/75 batches, loss: 0.0319
[2025-05-07 18:53:06,161][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0319
[2025-05-07 18:53:06,492][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0275, Metrics: {'mse': 0.027729550376534462, 'rmse': 0.16652192160954205, 'r2': -0.5554145574569702}
[2025-05-07 18:53:06,493][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0184Epoch 9/15: [                              ] 2/75 batches, loss: 0.0305Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0301Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0298Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0286Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0286Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0290Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0289Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0289Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0281Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0282Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0276Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0267Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0275Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0273Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0279Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0287Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0294Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0305Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0305Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0306Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0311Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0317Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0316Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0311Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0306Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0305Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0301Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0301Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0306Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0304Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0302Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0309Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0303Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0302Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0305Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0305Epoch 9/15: [================              ] 40/75 batches, loss: 0.0302Epoch 9/15: [================              ] 41/75 batches, loss: 0.0302Epoch 9/15: [================              ] 42/75 batches, loss: 0.0300Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0302Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0303Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0298Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0297Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0294Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0294Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0293Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0290Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0292Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0289Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0285Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0286Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0284Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0289Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0287Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0285Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0284Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0283Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0282Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0280Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0280Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0279Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0277Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0277Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0278Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0278Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0279Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0277Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0278Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0279Epoch 9/15: [==============================] 75/75 batches, loss: 0.0277
[2025-05-07 18:53:08,827][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0277
[2025-05-07 18:53:09,119][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0244, Metrics: {'mse': 0.02468043565750122, 'rmse': 0.1571000816597535, 'r2': -0.38438260555267334}
[2025-05-07 18:53:09,120][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0456Epoch 10/15: [                              ] 2/75 batches, loss: 0.0435Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0396Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0394Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0383Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0358Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0337Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0323Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0310Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0304Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0289Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0295Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0290Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0292Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0287Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0284Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0289Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0291Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0291Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0283Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0282Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0282Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0279Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0279Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0281Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0278Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0274Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0270Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0278Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0277Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0275Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0271Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0268Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0266Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0278Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0278Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0276Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0272Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0269Epoch 10/15: [================              ] 40/75 batches, loss: 0.0270Epoch 10/15: [================              ] 41/75 batches, loss: 0.0268Epoch 10/15: [================              ] 42/75 batches, loss: 0.0265Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0263Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0263Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0263Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0261Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0262Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0263Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0260Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0259Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0259Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0263Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0261Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0260Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0262Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0259Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0261Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0260Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0259Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0257Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0257Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0259Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0260Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0258Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0255Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0254Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0252Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0252Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0256Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0256Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0255Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0258Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0257Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0259Epoch 10/15: [==============================] 75/75 batches, loss: 0.0260
[2025-05-07 18:53:11,538][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0260
[2025-05-07 18:53:11,885][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0208, Metrics: {'mse': 0.021112624555826187, 'rmse': 0.14530183947846698, 'r2': -0.18425583839416504}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.0133Epoch 11/15: [                              ] 2/75 batches, loss: 0.0168Epoch 11/15: [=                             ] 3/75 batches, loss: 0.0224Epoch 11/15: [=                             ] 4/75 batches, loss: 0.0193Epoch 11/15: [==                            ] 5/75 batches, loss: 0.0182Epoch 11/15: [==                            ] 6/75 batches, loss: 0.0188Epoch 11/15: [==                            ] 7/75 batches, loss: 0.0291Epoch 11/15: [===                           ] 8/75 batches, loss: 0.0271Epoch 11/15: [===                           ] 9/75 batches, loss: 0.0273Epoch 11/15: [====                          ] 10/75 batches, loss: 0.0277Epoch 11/15: [====                          ] 11/75 batches, loss: 0.0264Epoch 11/15: [====                          ] 12/75 batches, loss: 0.0261Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.0269Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.0268Epoch 11/15: [======                        ] 15/75 batches, loss: 0.0261Epoch 11/15: [======                        ] 16/75 batches, loss: 0.0251Epoch 11/15: [======                        ] 17/75 batches, loss: 0.0251Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.0261Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.0260Epoch 11/15: [========                      ] 20/75 batches, loss: 0.0255Epoch 11/15: [========                      ] 21/75 batches, loss: 0.0261Epoch 11/15: [========                      ] 22/75 batches, loss: 0.0256Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.0252Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.0250Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.0246Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.0249Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.0246Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.0254Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.0254Epoch 11/15: [============                  ] 30/75 batches, loss: 0.0252Epoch 11/15: [============                  ] 31/75 batches, loss: 0.0248Epoch 11/15: [============                  ] 32/75 batches, loss: 0.0247Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.0245Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.0244Epoch 11/15: [==============                ] 35/75 batches, loss: 0.0242Epoch 11/15: [==============                ] 36/75 batches, loss: 0.0242Epoch 11/15: [==============                ] 37/75 batches, loss: 0.0238Epoch 11/15: [===============               ] 38/75 batches, loss: 0.0235Epoch 11/15: [===============               ] 39/75 batches, loss: 0.0235Epoch 11/15: [================              ] 40/75 batches, loss: 0.0231Epoch 11/15: [================              ] 41/75 batches, loss: 0.0232Epoch 11/15: [================              ] 42/75 batches, loss: 0.0233Epoch 11/15: [=================             ] 43/75 batches, loss: 0.0231Epoch 11/15: [=================             ] 44/75 batches, loss: 0.0229Epoch 11/15: [==================            ] 45/75 batches, loss: 0.0229Epoch 11/15: [==================            ] 46/75 batches, loss: 0.0227Epoch 11/15: [==================            ] 47/75 batches, loss: 0.0227Epoch 11/15: [===================           ] 48/75 batches, loss: 0.0226Epoch 11/15: [===================           ] 49/75 batches, loss: 0.0227Epoch 11/15: [====================          ] 50/75 batches, loss: 0.0225Epoch 11/15: [====================          ] 51/75 batches, loss: 0.0224Epoch 11/15: [====================          ] 52/75 batches, loss: 0.0224Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.0221Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.0224Epoch 11/15: [======================        ] 55/75 batches, loss: 0.0226Epoch 11/15: [======================        ] 56/75 batches, loss: 0.0224Epoch 11/15: [======================        ] 57/75 batches, loss: 0.0222Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.0221Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.0219Epoch 11/15: [========================      ] 60/75 batches, loss: 0.0218Epoch 11/15: [========================      ] 61/75 batches, loss: 0.0218Epoch 11/15: [========================      ] 62/75 batches, loss: 0.0218Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.0218Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.0217Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.0216Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.0220Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.0221Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.0221Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.0222Epoch 11/15: [============================  ] 70/75 batches, loss: 0.0224Epoch 11/15: [============================  ] 71/75 batches, loss: 0.0222Epoch 11/15: [============================  ] 72/75 batches, loss: 0.0223Epoch 11/15: [============================= ] 73/75 batches, loss: 0.0224Epoch 11/15: [============================= ] 74/75 batches, loss: 0.0228Epoch 11/15: [==============================] 75/75 batches, loss: 0.0227
[2025-05-07 18:53:14,559][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0227
[2025-05-07 18:53:14,807][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0205, Metrics: {'mse': 0.02083647809922695, 'rmse': 0.1443484606749478, 'r2': -0.16876614093780518}
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.0255Epoch 12/15: [                              ] 2/75 batches, loss: 0.0288Epoch 12/15: [=                             ] 3/75 batches, loss: 0.0293Epoch 12/15: [=                             ] 4/75 batches, loss: 0.0265Epoch 12/15: [==                            ] 5/75 batches, loss: 0.0266Epoch 12/15: [==                            ] 6/75 batches, loss: 0.0255Epoch 12/15: [==                            ] 7/75 batches, loss: 0.0241Epoch 12/15: [===                           ] 8/75 batches, loss: 0.0225Epoch 12/15: [===                           ] 9/75 batches, loss: 0.0219Epoch 12/15: [====                          ] 10/75 batches, loss: 0.0209Epoch 12/15: [====                          ] 11/75 batches, loss: 0.0202Epoch 12/15: [====                          ] 12/75 batches, loss: 0.0195Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.0199Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.0196Epoch 12/15: [======                        ] 15/75 batches, loss: 0.0187Epoch 12/15: [======                        ] 16/75 batches, loss: 0.0193Epoch 12/15: [======                        ] 17/75 batches, loss: 0.0193Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.0194Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.0192Epoch 12/15: [========                      ] 20/75 batches, loss: 0.0193Epoch 12/15: [========                      ] 21/75 batches, loss: 0.0195Epoch 12/15: [========                      ] 22/75 batches, loss: 0.0195Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.0193Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.0193Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.0194Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.0196Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.0194Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.0202Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.0210Epoch 12/15: [============                  ] 30/75 batches, loss: 0.0212Epoch 12/15: [============                  ] 31/75 batches, loss: 0.0212Epoch 12/15: [============                  ] 32/75 batches, loss: 0.0216Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.0223Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.0222Epoch 12/15: [==============                ] 35/75 batches, loss: 0.0218Epoch 12/15: [==============                ] 36/75 batches, loss: 0.0218Epoch 12/15: [==============                ] 37/75 batches, loss: 0.0217Epoch 12/15: [===============               ] 38/75 batches, loss: 0.0218Epoch 12/15: [===============               ] 39/75 batches, loss: 0.0215Epoch 12/15: [================              ] 40/75 batches, loss: 0.0215Epoch 12/15: [================              ] 41/75 batches, loss: 0.0215Epoch 12/15: [================              ] 42/75 batches, loss: 0.0215Epoch 12/15: [=================             ] 43/75 batches, loss: 0.0215Epoch 12/15: [=================             ] 44/75 batches, loss: 0.0213Epoch 12/15: [==================            ] 45/75 batches, loss: 0.0213Epoch 12/15: [==================            ] 46/75 batches, loss: 0.0212Epoch 12/15: [==================            ] 47/75 batches, loss: 0.0210Epoch 12/15: [===================           ] 48/75 batches, loss: 0.0209Epoch 12/15: [===================           ] 49/75 batches, loss: 0.0208Epoch 12/15: [====================          ] 50/75 batches, loss: 0.0206Epoch 12/15: [====================          ] 51/75 batches, loss: 0.0213Epoch 12/15: [====================          ] 52/75 batches, loss: 0.0212Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.0211Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.0211Epoch 12/15: [======================        ] 55/75 batches, loss: 0.0213Epoch 12/15: [======================        ] 56/75 batches, loss: 0.0212Epoch 12/15: [======================        ] 57/75 batches, loss: 0.0212Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.0210Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.0212Epoch 12/15: [========================      ] 60/75 batches, loss: 0.0213Epoch 12/15: [========================      ] 61/75 batches, loss: 0.0213Epoch 12/15: [========================      ] 62/75 batches, loss: 0.0212Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.0212Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.0211Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.0210Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.0213Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.0213Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.0212Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.0211Epoch 12/15: [============================  ] 70/75 batches, loss: 0.0211Epoch 12/15: [============================  ] 71/75 batches, loss: 0.0209Epoch 12/15: [============================  ] 72/75 batches, loss: 0.0209Epoch 12/15: [============================= ] 73/75 batches, loss: 0.0208Epoch 12/15: [============================= ] 74/75 batches, loss: 0.0206Epoch 12/15: [==============================] 75/75 batches, loss: 0.0205
[2025-05-07 18:53:17,612][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0205
[2025-05-07 18:53:17,979][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0285, Metrics: {'mse': 0.028779173269867897, 'rmse': 0.16964425504527966, 'r2': -0.6142902374267578}
[2025-05-07 18:53:17,980][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.0684Epoch 13/15: [                              ] 2/75 batches, loss: 0.0422Epoch 13/15: [=                             ] 3/75 batches, loss: 0.0339Epoch 13/15: [=                             ] 4/75 batches, loss: 0.0292Epoch 13/15: [==                            ] 5/75 batches, loss: 0.0285Epoch 13/15: [==                            ] 6/75 batches, loss: 0.0266Epoch 13/15: [==                            ] 7/75 batches, loss: 0.0249Epoch 13/15: [===                           ] 8/75 batches, loss: 0.0246Epoch 13/15: [===                           ] 9/75 batches, loss: 0.0242Epoch 13/15: [====                          ] 10/75 batches, loss: 0.0230Epoch 13/15: [====                          ] 11/75 batches, loss: 0.0227Epoch 13/15: [====                          ] 12/75 batches, loss: 0.0220Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.0221Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.0221Epoch 13/15: [======                        ] 15/75 batches, loss: 0.0215Epoch 13/15: [======                        ] 16/75 batches, loss: 0.0209Epoch 13/15: [======                        ] 17/75 batches, loss: 0.0205Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.0207Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.0210Epoch 13/15: [========                      ] 20/75 batches, loss: 0.0207Epoch 13/15: [========                      ] 21/75 batches, loss: 0.0208Epoch 13/15: [========                      ] 22/75 batches, loss: 0.0212Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.0214Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.0215Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.0214Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.0219Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.0213Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.0211Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.0209Epoch 13/15: [============                  ] 30/75 batches, loss: 0.0211Epoch 13/15: [============                  ] 31/75 batches, loss: 0.0210Epoch 13/15: [============                  ] 32/75 batches, loss: 0.0210Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.0208Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.0209Epoch 13/15: [==============                ] 35/75 batches, loss: 0.0213Epoch 13/15: [==============                ] 36/75 batches, loss: 0.0214Epoch 13/15: [==============                ] 37/75 batches, loss: 0.0214Epoch 13/15: [===============               ] 38/75 batches, loss: 0.0214Epoch 13/15: [===============               ] 39/75 batches, loss: 0.0213Epoch 13/15: [================              ] 40/75 batches, loss: 0.0213Epoch 13/15: [================              ] 41/75 batches, loss: 0.0211Epoch 13/15: [================              ] 42/75 batches, loss: 0.0210Epoch 13/15: [=================             ] 43/75 batches, loss: 0.0209Epoch 13/15: [=================             ] 44/75 batches, loss: 0.0208Epoch 13/15: [==================            ] 45/75 batches, loss: 0.0209Epoch 13/15: [==================            ] 46/75 batches, loss: 0.0208Epoch 13/15: [==================            ] 47/75 batches, loss: 0.0205Epoch 13/15: [===================           ] 48/75 batches, loss: 0.0205Epoch 13/15: [===================           ] 49/75 batches, loss: 0.0204Epoch 13/15: [====================          ] 50/75 batches, loss: 0.0204Epoch 13/15: [====================          ] 51/75 batches, loss: 0.0203Epoch 13/15: [====================          ] 52/75 batches, loss: 0.0203Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.0201Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.0201Epoch 13/15: [======================        ] 55/75 batches, loss: 0.0202Epoch 13/15: [======================        ] 56/75 batches, loss: 0.0200Epoch 13/15: [======================        ] 57/75 batches, loss: 0.0200Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.0198Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.0198Epoch 13/15: [========================      ] 60/75 batches, loss: 0.0198Epoch 13/15: [========================      ] 61/75 batches, loss: 0.0196Epoch 13/15: [========================      ] 62/75 batches, loss: 0.0198Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.0198Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.0201Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.0200Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.0199Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.0199Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.0200Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.0200Epoch 13/15: [============================  ] 70/75 batches, loss: 0.0201Epoch 13/15: [============================  ] 71/75 batches, loss: 0.0199Epoch 13/15: [============================  ] 72/75 batches, loss: 0.0200Epoch 13/15: [============================= ] 73/75 batches, loss: 0.0200Epoch 13/15: [============================= ] 74/75 batches, loss: 0.0201Epoch 13/15: [==============================] 75/75 batches, loss: 0.0204
[2025-05-07 18:53:20,378][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0204
[2025-05-07 18:53:20,679][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0266, Metrics: {'mse': 0.026936125010252, 'rmse': 0.16412228675671076, 'r2': -0.5109094381332397}
[2025-05-07 18:53:20,679][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/75 batches, loss: 0.0266Epoch 14/15: [                              ] 2/75 batches, loss: 0.0230Epoch 14/15: [=                             ] 3/75 batches, loss: 0.0188Epoch 14/15: [=                             ] 4/75 batches, loss: 0.0183Epoch 14/15: [==                            ] 5/75 batches, loss: 0.0226Epoch 14/15: [==                            ] 6/75 batches, loss: 0.0221Epoch 14/15: [==                            ] 7/75 batches, loss: 0.0219Epoch 14/15: [===                           ] 8/75 batches, loss: 0.0231Epoch 14/15: [===                           ] 9/75 batches, loss: 0.0225Epoch 14/15: [====                          ] 10/75 batches, loss: 0.0222Epoch 14/15: [====                          ] 11/75 batches, loss: 0.0216Epoch 14/15: [====                          ] 12/75 batches, loss: 0.0213Epoch 14/15: [=====                         ] 13/75 batches, loss: 0.0227Epoch 14/15: [=====                         ] 14/75 batches, loss: 0.0237Epoch 14/15: [======                        ] 15/75 batches, loss: 0.0230Epoch 14/15: [======                        ] 16/75 batches, loss: 0.0231Epoch 14/15: [======                        ] 17/75 batches, loss: 0.0227Epoch 14/15: [=======                       ] 18/75 batches, loss: 0.0222Epoch 14/15: [=======                       ] 19/75 batches, loss: 0.0214Epoch 14/15: [========                      ] 20/75 batches, loss: 0.0215Epoch 14/15: [========                      ] 21/75 batches, loss: 0.0216Epoch 14/15: [========                      ] 22/75 batches, loss: 0.0212Epoch 14/15: [=========                     ] 23/75 batches, loss: 0.0210Epoch 14/15: [=========                     ] 24/75 batches, loss: 0.0208Epoch 14/15: [==========                    ] 25/75 batches, loss: 0.0207Epoch 14/15: [==========                    ] 26/75 batches, loss: 0.0205Epoch 14/15: [==========                    ] 27/75 batches, loss: 0.0205Epoch 14/15: [===========                   ] 28/75 batches, loss: 0.0211Epoch 14/15: [===========                   ] 29/75 batches, loss: 0.0208Epoch 14/15: [============                  ] 30/75 batches, loss: 0.0206Epoch 14/15: [============                  ] 31/75 batches, loss: 0.0205Epoch 14/15: [============                  ] 32/75 batches, loss: 0.0203Epoch 14/15: [=============                 ] 33/75 batches, loss: 0.0201Epoch 14/15: [=============                 ] 34/75 batches, loss: 0.0201Epoch 14/15: [==============                ] 35/75 batches, loss: 0.0205Epoch 14/15: [==============                ] 36/75 batches, loss: 0.0202Epoch 14/15: [==============                ] 37/75 batches, loss: 0.0202Epoch 14/15: [===============               ] 38/75 batches, loss: 0.0203Epoch 14/15: [===============               ] 39/75 batches, loss: 0.0202Epoch 14/15: [================              ] 40/75 batches, loss: 0.0202Epoch 14/15: [================              ] 41/75 batches, loss: 0.0199Epoch 14/15: [================              ] 42/75 batches, loss: 0.0202Epoch 14/15: [=================             ] 43/75 batches, loss: 0.0205Epoch 14/15: [=================             ] 44/75 batches, loss: 0.0204Epoch 14/15: [==================            ] 45/75 batches, loss: 0.0203Epoch 14/15: [==================            ] 46/75 batches, loss: 0.0202Epoch 14/15: [==================            ] 47/75 batches, loss: 0.0213Epoch 14/15: [===================           ] 48/75 batches, loss: 0.0212Epoch 14/15: [===================           ] 49/75 batches, loss: 0.0211Epoch 14/15: [====================          ] 50/75 batches, loss: 0.0214Epoch 14/15: [====================          ] 51/75 batches, loss: 0.0213Epoch 14/15: [====================          ] 52/75 batches, loss: 0.0211Epoch 14/15: [=====================         ] 53/75 batches, loss: 0.0209Epoch 14/15: [=====================         ] 54/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 55/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 56/75 batches, loss: 0.0208Epoch 14/15: [======================        ] 57/75 batches, loss: 0.0208Epoch 14/15: [=======================       ] 58/75 batches, loss: 0.0209Epoch 14/15: [=======================       ] 59/75 batches, loss: 0.0210Epoch 14/15: [========================      ] 60/75 batches, loss: 0.0209Epoch 14/15: [========================      ] 61/75 batches, loss: 0.0207Epoch 14/15: [========================      ] 62/75 batches, loss: 0.0208Epoch 14/15: [=========================     ] 63/75 batches, loss: 0.0210Epoch 14/15: [=========================     ] 64/75 batches, loss: 0.0211Epoch 14/15: [==========================    ] 65/75 batches, loss: 0.0208Epoch 14/15: [==========================    ] 66/75 batches, loss: 0.0206Epoch 14/15: [==========================    ] 67/75 batches, loss: 0.0205Epoch 14/15: [===========================   ] 68/75 batches, loss: 0.0205Epoch 14/15: [===========================   ] 69/75 batches, loss: 0.0204Epoch 14/15: [============================  ] 70/75 batches, loss: 0.0204Epoch 14/15: [============================  ] 71/75 batches, loss: 0.0202Epoch 14/15: [============================  ] 72/75 batches, loss: 0.0201Epoch 14/15: [============================= ] 73/75 batches, loss: 0.0200Epoch 14/15: [============================= ] 74/75 batches, loss: 0.0200Epoch 14/15: [==============================] 75/75 batches, loss: 0.0201
[2025-05-07 18:53:22,994][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0201
[2025-05-07 18:53:23,312][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0241, Metrics: {'mse': 0.024420030415058136, 'rmse': 0.15626909616126325, 'r2': -0.3697758913040161}
[2025-05-07 18:53:23,312][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/75 batches, loss: 0.0054Epoch 15/15: [                              ] 2/75 batches, loss: 0.0098Epoch 15/15: [=                             ] 3/75 batches, loss: 0.0136Epoch 15/15: [=                             ] 4/75 batches, loss: 0.0132Epoch 15/15: [==                            ] 5/75 batches, loss: 0.0160Epoch 15/15: [==                            ] 6/75 batches, loss: 0.0151Epoch 15/15: [==                            ] 7/75 batches, loss: 0.0167Epoch 15/15: [===                           ] 8/75 batches, loss: 0.0160Epoch 15/15: [===                           ] 9/75 batches, loss: 0.0154Epoch 15/15: [====                          ] 10/75 batches, loss: 0.0150Epoch 15/15: [====                          ] 11/75 batches, loss: 0.0159Epoch 15/15: [====                          ] 12/75 batches, loss: 0.0155Epoch 15/15: [=====                         ] 13/75 batches, loss: 0.0161Epoch 15/15: [=====                         ] 14/75 batches, loss: 0.0155Epoch 15/15: [======                        ] 15/75 batches, loss: 0.0152Epoch 15/15: [======                        ] 16/75 batches, loss: 0.0156slurmstepd: error: *** JOB 64466379 ON k28i22 CANCELLED AT 2025-05-07T18:53:24 DUE TO TIME LIMIT ***
