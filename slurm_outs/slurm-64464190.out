SLURM_JOB_ID: 64464190
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Wed May  7 13:21:06 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10
=======================
Experiment probe_layer10_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/layer10/fi/fi/results.json for layer 10
Running control probing experiments...
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer10_question_type_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:21:55,947][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer10/fi
experiment_name: probe_layer10_question_type_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 13:21:55,947][__main__][INFO] - Normalized task: question_type
[2025-05-07 13:21:55,947][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 13:21:55,947][__main__][INFO] - Determined Task Type: classification
[2025-05-07 13:21:55,951][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 13:21:55,952][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:22:00,391][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:22:02,788][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:22:02,789][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:22:03,179][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 13:22:03,364][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-07 13:22:03,666][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:22:03,675][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:22:03,676][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:22:03,677][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:22:03,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:22:03,955][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:22:04,023][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:22:04,024][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:22:04,024][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:22:04,025][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:22:04,136][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:22:04,331][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:22:04,346][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:22:04,347][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:22:04,347][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:22:04,348][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:22:04,349][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 13:22:04,349][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:22:04,349][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:22:04,350][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 13:22:04,350][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:22:04,350][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:22:04,351][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 13:22:04,351][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:22:04,351][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:22:04,351][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 13:22:04,352][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:22:14,523][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:22:14,524][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:22:14,524][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:22:14,524][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 13:22:14,530][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 13:22:14,530][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 13:22:14,530][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 13:22:14,530][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 13:22:14,530][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:22:14,531][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 13:22:14,531][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 13:22:14,532][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6615Epoch 1/15: [                              ] 2/75 batches, loss: 0.6585Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6806Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6782Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6896Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6640Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6707Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6753Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6835Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6846Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6772Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6784Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6787Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6781Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6867Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6866Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6907Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6933Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6945Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6963Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6955Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6955Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6963Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6966Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6961Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6964Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6967Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6965Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6965Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6960Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6964Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6963Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6961Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6959Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6962Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6959Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6958Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6956Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6956Epoch 1/15: [================              ] 40/75 batches, loss: 0.6955Epoch 1/15: [================              ] 41/75 batches, loss: 0.6956Epoch 1/15: [================              ] 42/75 batches, loss: 0.6956Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6955Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6955Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6955Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6956Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6956Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6955Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6955Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6954Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6953Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6953Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6952Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6951Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6951Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6950Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6950Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6950Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6950Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6950Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6948Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6948Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6948Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6948Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6947Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6947Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6947Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6947Epoch 1/15: [==============================] 75/75 batches, loss: 0.6947
[2025-05-07 13:22:22,103][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6947
[2025-05-07 13:22:22,389][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6921Epoch 2/15: [                              ] 2/75 batches, loss: 0.6929Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6932Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6934Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6934Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6933Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6933Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6933Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6933Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6933Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 2/15: [================              ] 40/75 batches, loss: 0.6931Epoch 2/15: [================              ] 41/75 batches, loss: 0.6931Epoch 2/15: [================              ] 42/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 13:22:25,130][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-07 13:22:25,519][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:25,520][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6931Epoch 3/15: [                              ] 2/75 batches, loss: 0.6927Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6930Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6934Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6927Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6927Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6928Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6928Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6928Epoch 3/15: [================              ] 40/75 batches, loss: 0.6927Epoch 3/15: [================              ] 41/75 batches, loss: 0.6927Epoch 3/15: [================              ] 42/75 batches, loss: 0.6928Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6928Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6928Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6929Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6929Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:22:27,772][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 13:22:28,038][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:28,038][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6935Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6934Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6934Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6934Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6934Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6936Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6936Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6934Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6936Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6936Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6934Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6933Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6934Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6934Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6935Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6935Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6934Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6934Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6934Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6934Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6934Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6933Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6934Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6934Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6934Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6934Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6934Epoch 4/15: [================              ] 40/75 batches, loss: 0.6934Epoch 4/15: [================              ] 41/75 batches, loss: 0.6934Epoch 4/15: [================              ] 42/75 batches, loss: 0.6934Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6934Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6934Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6933Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 4/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:22:30,288][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6931
[2025-05-07 13:22:30,589][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:30,589][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 13:22:30,589][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-07 13:22:30,590][src.training.lm_trainer][INFO] - Training completed in 11.65 seconds
[2025-05-07 13:22:30,590][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:22:33,630][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:33,631][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:33,631][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:22:35,296][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer10/fi/fi/model.pt
[2025-05-07 13:22:35,298][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▁██
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69316
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69313
wandb:            train_time 11.6463
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.6932
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132156-35yhxcal
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132156-35yhxcal/logs
Experiment probe_layer10_question_type_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control1/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_question_type_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:23:07,129][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer10/fi
experiment_name: probe_layer10_question_type_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 13:23:07,129][__main__][INFO] - Normalized task: question_type
[2025-05-07 13:23:07,130][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 13:23:07,130][__main__][INFO] - Determined Task Type: classification
[2025-05-07 13:23:07,134][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 13:23:07,134][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:23:11,046][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:23:13,312][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:23:13,312][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:23:13,512][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 13:23:13,586][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-07 13:23:13,887][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:23:13,895][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:23:13,896][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:23:13,898][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:23:13,970][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:23:14,070][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:23:14,141][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:23:14,142][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:23:14,142][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:23:14,143][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:23:14,297][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:23:14,426][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:23:14,448][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:23:14,449][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:23:14,449][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:23:14,453][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:23:14,454][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 13:23:14,454][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:23:14,454][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:23:14,455][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 13:23:14,455][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:23:14,455][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:23:14,456][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 13:23:14,456][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 13:23:14,456][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:23:14,456][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 13:23:14,456][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:23:14,456][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:23:14,456][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:23:14,456][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 13:23:14,457][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:23:22,720][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:23:22,721][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:23:22,721][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:23:22,721][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 13:23:22,726][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 13:23:22,727][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 13:23:22,727][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 13:23:22,727][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 13:23:22,727][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:23:22,728][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 13:23:22,728][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 13:23:22,729][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7299Epoch 1/15: [                              ] 2/75 batches, loss: 0.7109Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7011Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7027Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7072Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7034Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6991Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6977Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6989Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6984Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6965Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6955Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6944Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6935Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6935Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6934Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6934Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6933Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6917Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6919Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6903Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6902Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6895Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6901Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6911Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6916Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6916Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6903Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6898Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6900Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6896Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6898Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6902Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6898Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6900Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6908Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6904Epoch 1/15: [================              ] 40/75 batches, loss: 0.6906Epoch 1/15: [================              ] 41/75 batches, loss: 0.6902Epoch 1/15: [================              ] 42/75 batches, loss: 0.6903Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6903Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6904Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6899Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6911Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6909Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6908Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6909Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6905Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6905Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6910Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6911Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6912Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6909Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6909Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6911Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6912Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6915Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6917Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6917Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6917Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6912Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6910Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6913Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6916Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6911Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6911Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6909Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6913Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6913Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6913Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6913Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6913Epoch 1/15: [==============================] 75/75 batches, loss: 0.6914
[2025-05-07 13:23:29,515][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6914
[2025-05-07 13:23:29,827][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6936, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.7099Epoch 2/15: [                              ] 2/75 batches, loss: 0.7014Epoch 2/15: [=                             ] 3/75 batches, loss: 0.7090Epoch 2/15: [=                             ] 4/75 batches, loss: 0.7074Epoch 2/15: [==                            ] 5/75 batches, loss: 0.7045Epoch 2/15: [==                            ] 6/75 batches, loss: 0.7024Epoch 2/15: [==                            ] 7/75 batches, loss: 0.7011Epoch 2/15: [===                           ] 8/75 batches, loss: 0.7021Epoch 2/15: [===                           ] 9/75 batches, loss: 0.7014Epoch 2/15: [====                          ] 10/75 batches, loss: 0.7011Epoch 2/15: [====                          ] 11/75 batches, loss: 0.7006Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6999Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6994Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6987Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6984Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6980Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6977Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6976Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6974Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6972Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6970Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6968Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6967Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6965Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6963Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6962Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6960Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6958Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6957Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6956Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6956Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6955Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6955Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6954Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6954Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6953Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6953Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6952Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6952Epoch 2/15: [================              ] 40/75 batches, loss: 0.6951Epoch 2/15: [================              ] 41/75 batches, loss: 0.6951Epoch 2/15: [================              ] 42/75 batches, loss: 0.6950Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6950Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6949Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6948Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6948Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6948Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6947Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6947Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6946Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6946Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6945Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6945Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6945Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6945Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6945Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6945Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6945Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6945Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6944Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6944Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6944Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6944Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6944Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6944Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6944Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6944Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6943Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6943Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6944Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6943Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6944Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6944Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6944Epoch 2/15: [==============================] 75/75 batches, loss: 0.6943
[2025-05-07 13:23:32,655][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6943
[2025-05-07 13:23:32,875][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6927Epoch 3/15: [                              ] 2/75 batches, loss: 0.6927Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6927Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 3/15: [================              ] 40/75 batches, loss: 0.6932Epoch 3/15: [================              ] 41/75 batches, loss: 0.6932Epoch 3/15: [================              ] 42/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:23:35,600][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-07 13:23:35,844][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6933Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 4/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:23:38,509][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6931
[2025-05-07 13:23:38,811][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:38,812][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6930Epoch 5/15: [                              ] 2/75 batches, loss: 0.6925Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6926Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6926Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6927Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 5/15: [================              ] 40/75 batches, loss: 0.6931Epoch 5/15: [================              ] 41/75 batches, loss: 0.6931Epoch 5/15: [================              ] 42/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 5/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:23:41,172][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6931
[2025-05-07 13:23:41,459][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:41,460][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6921Epoch 6/15: [                              ] 2/75 batches, loss: 0.6926Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 6/15: [================              ] 40/75 batches, loss: 0.6930Epoch 6/15: [================              ] 41/75 batches, loss: 0.6930Epoch 6/15: [================              ] 42/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 6/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:23:43,830][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6931
[2025-05-07 13:23:44,198][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:44,199][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 13:23:44,199][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-07 13:23:44,199][src.training.lm_trainer][INFO] - Training completed in 17.91 seconds
[2025-05-07 13:23:44,199][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:23:47,230][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:47,230][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:47,230][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:23:48,868][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer10/fi/fi/model.pt
[2025-05-07 13:23:48,870][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁
wandb:           best_val_f1 ▁▁▁
wandb:         best_val_loss █▁▁
wandb:    best_val_precision ▁▁▁
wandb:       best_val_recall ▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁
wandb:            train_loss ▁█▅▅▅▅
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁
wandb:              val_loss █▁▁▁▂▃
wandb:         val_precision ▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.6932
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 6
wandb:                 epoch 6
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69305
wandb:            train_time 17.90796
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.6933
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132307-r2q2h584
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132307-r2q2h584/logs
Experiment probe_layer10_question_type_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control2/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_question_type_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:24:21,466][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer10/fi
experiment_name: probe_layer10_question_type_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-07 13:24:21,467][__main__][INFO] - Normalized task: question_type
[2025-05-07 13:24:21,467][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-07 13:24:21,467][__main__][INFO] - Determined Task Type: classification
[2025-05-07 13:24:21,471][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['fi']
[2025-05-07 13:24:21,471][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:24:24,866][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:24:27,349][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:24:27,350][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:24:27,658][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 13:24:27,773][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-07 13:24:27,999][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:24:28,009][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:24:28,009][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:24:28,012][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:24:28,073][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:24:28,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:24:28,175][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:24:28,176][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:24:28,177][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:24:28,177][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:24:28,271][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:24:28,351][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:24:28,417][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:24:28,419][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:24:28,419][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:24:28,430][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:24:28,430][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:24:28,430][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:24:28,430][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:24:28,430][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:24:28,431][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-07 13:24:28,431][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:24:28,431][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:24:28,431][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-05-07 13:24:28,432][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Sample label: 1
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-07 13:24:28,432][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-07 13:24:28,432][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Sample label: 0
[2025-05-07 13:24:28,432][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:24:28,433][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:24:28,433][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:24:28,433][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-07 13:24:28,433][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:24:35,805][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:24:35,806][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:24:35,807][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:24:35,807][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-07 13:24:35,813][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-07 13:24:35,813][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-07 13:24:35,813][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-07 13:24:35,813][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-07 13:24:35,813][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:24:35,814][__main__][INFO] - Total parameters: 394,568,839
[2025-05-07 13:24:35,814][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-07 13:24:35,815][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7607Epoch 1/15: [                              ] 2/75 batches, loss: 0.7602Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7471Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7384Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7271Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7211Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7170Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7152Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7135Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7112Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7092Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7076Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7065Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7055Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7047Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7040Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7034Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7028Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7023Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7019Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7014Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7011Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7007Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7004Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.7001Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6998Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6995Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6992Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6990Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6989Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6988Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6986Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6985Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6984Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6982Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6980Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6979Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6977Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6976Epoch 1/15: [================              ] 40/75 batches, loss: 0.6975Epoch 1/15: [================              ] 41/75 batches, loss: 0.6974Epoch 1/15: [================              ] 42/75 batches, loss: 0.6973Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6972Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6971Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6970Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6970Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6970Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6969Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6968Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6968Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6967Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6966Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6966Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6965Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6964Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6963Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6963Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6962Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6962Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6962Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6961Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6961Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6960Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6960Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6959Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6959Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6958Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6957Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6957Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6957Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6956Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6956Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6956Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6955Epoch 1/15: [==============================] 75/75 batches, loss: 0.6955
[2025-05-07 13:24:42,589][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6955
[2025-05-07 13:24:42,863][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6921Epoch 2/15: [                              ] 2/75 batches, loss: 0.6924Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 2/15: [================              ] 40/75 batches, loss: 0.6931Epoch 2/15: [================              ] 41/75 batches, loss: 0.6931Epoch 2/15: [================              ] 42/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-07 13:24:45,532][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-07 13:24:45,804][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:45,805][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6925Epoch 3/15: [                              ] 2/75 batches, loss: 0.6930Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6932Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6930Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6929Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 3/15: [================              ] 40/75 batches, loss: 0.6930Epoch 3/15: [================              ] 41/75 batches, loss: 0.6930Epoch 3/15: [================              ] 42/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 3/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-07 13:24:48,186][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6930
[2025-05-07 13:24:48,545][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:48,546][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6935Epoch 4/15: [                              ] 2/75 batches, loss: 0.6933Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6928Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6927Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6933Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 4/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-07 13:24:50,810][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6931
[2025-05-07 13:24:51,069][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:51,070][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-07 13:24:51,070][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-07 13:24:51,070][src.training.lm_trainer][INFO] - Training completed in 11.52 seconds
[2025-05-07 13:24:51,070][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:24:54,247][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.499581589958159, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:54,247][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:54,247][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-07 13:24:55,870][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer10/fi/fi/model.pt
[2025-05-07 13:24:55,872][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▂█▇
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.52381
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69326
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.52381
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69312
wandb:            train_time 11.52423
wandb:          val_accuracy 0.52381
wandb:                val_f1 0
wandb:              val_loss 0.69338
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132421-i832erfu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132421-i832erfu/logs
Experiment probe_layer10_question_type_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/question_type/control3/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:25:28,114][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer10/fi
experiment_name: probe_layer10_complexity_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 13:25:28,114][__main__][INFO] - Normalized task: complexity
[2025-05-07 13:25:28,114][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:25:28,114][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:25:28,119][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 13:25:28,119][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:25:32,856][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:25:35,078][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:25:35,078][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:25:35,322][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 13:25:35,372][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-07 13:25:35,725][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:25:35,733][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:25:35,734][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:25:35,735][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:25:35,858][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:25:35,985][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:25:36,020][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:25:36,022][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:25:36,022][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:25:36,033][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:25:36,127][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:25:36,227][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:25:36,264][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:25:36,266][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:25:36,266][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:25:36,267][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:25:36,268][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:25:36,268][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:25:36,268][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:25:36,268][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:25:36,268][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:25:36,268][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Sample label: 0.21079079806804657
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:25:36,269][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:25:36,269][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:25:36,269][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:25:36,270][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:25:36,270][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:25:36,270][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:25:36,271][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:25:36,271][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 13:25:36,271][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:25:44,444][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:25:44,445][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:25:44,445][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:25:44,445][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:25:44,448][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:25:44,449][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:25:44,449][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:25:44,449][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:25:44,449][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:25:44,450][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:25:44,450][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7532Epoch 1/15: [                              ] 2/75 batches, loss: 0.9368Epoch 1/15: [=                             ] 3/75 batches, loss: 0.9148Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8637Epoch 1/15: [==                            ] 5/75 batches, loss: 0.8019Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7166Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6773Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6751Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6457Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6544Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6202Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5944Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5777Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5636Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5456Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5294Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5296Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.5218Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.5107Epoch 1/15: [========                      ] 20/75 batches, loss: 0.5073Epoch 1/15: [========                      ] 21/75 batches, loss: 0.5139Epoch 1/15: [========                      ] 22/75 batches, loss: 0.5139Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.5038Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4932Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4858Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4827Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4888Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4853Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4786Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4786Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4735Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4724Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4649Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4631Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4581Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4606Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4540Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4513Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4450Epoch 1/15: [================              ] 40/75 batches, loss: 0.4436Epoch 1/15: [================              ] 41/75 batches, loss: 0.4422Epoch 1/15: [================              ] 42/75 batches, loss: 0.4388Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4351Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4317Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4296Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4227Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4200Epoch 1/15: [===================           ] 48/75 batches, loss: 0.4143Epoch 1/15: [===================           ] 49/75 batches, loss: 0.4104Epoch 1/15: [====================          ] 50/75 batches, loss: 0.4077Epoch 1/15: [====================          ] 51/75 batches, loss: 0.4045Epoch 1/15: [====================          ] 52/75 batches, loss: 0.4015Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3991Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3989Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3967Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3940Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3911Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3896Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3862Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3820Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3784Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3759Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3725Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3698Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3677Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3650Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3618Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3588Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3589Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3563Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3523Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3501Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3474Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3440Epoch 1/15: [==============================] 75/75 batches, loss: 0.3426
[2025-05-07 13:25:50,993][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3426
[2025-05-07 13:25:51,277][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0720, Metrics: {'mse': 0.07199946790933609, 'rmse': 0.2683271658057307, 'r2': -0.0982133150100708}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1803Epoch 2/15: [                              ] 2/75 batches, loss: 0.1758Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1747Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2107Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2027Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1811Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1812Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1863Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1729Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1800Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1771Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1774Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1717Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1702Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1694Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1655Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1644Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1646Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1690Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1654Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1655Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1641Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1652Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1616Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1628Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1638Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1731Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1709Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1740Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1743Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1713Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1722Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1692Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1728Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1716Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1711Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1688Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1665Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1672Epoch 2/15: [================              ] 40/75 batches, loss: 0.1651Epoch 2/15: [================              ] 41/75 batches, loss: 0.1641Epoch 2/15: [================              ] 42/75 batches, loss: 0.1623Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1623Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1604Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1613Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1617Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1611Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1608Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1621Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1606Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1589Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1585Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1589Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1580Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1586Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1589Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1573Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1561Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1552Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1547Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1543Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1534Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1526Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1515Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1521Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1518Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1522Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1511Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1496Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1487Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1483Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1473Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1476Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1464Epoch 2/15: [==============================] 75/75 batches, loss: 0.1448
[2025-05-07 13:25:54,024][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1448
[2025-05-07 13:25:54,285][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1188, Metrics: {'mse': 0.11898122727870941, 'rmse': 0.344936555439851, 'r2': -0.8148297071456909}
[2025-05-07 13:25:54,286][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1764Epoch 3/15: [                              ] 2/75 batches, loss: 0.1348Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1567Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1374Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1287Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1241Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1199Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1200Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1215Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1253Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1241Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1229Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1253Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1271Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1286Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1258Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1283Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1309Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1258Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1245Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1228Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1197Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1189Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1186Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1176Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1173Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1156Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1152Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1163Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1153Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1137Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1134Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1127Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1123Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1110Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1123Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1101Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1105Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1106Epoch 3/15: [================              ] 40/75 batches, loss: 0.1098Epoch 3/15: [================              ] 41/75 batches, loss: 0.1112Epoch 3/15: [================              ] 42/75 batches, loss: 0.1117Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1119Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1112Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1101Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1099Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1092Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1095Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1089Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1081Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1067Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1068Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1065Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1056Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1053Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1051Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1057Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1058Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1057Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1053Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1049Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1046Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1044Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1039Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1033Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1032Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1029Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1034Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1041Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1044Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1043Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1046Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1044Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1042Epoch 3/15: [==============================] 75/75 batches, loss: 0.1039
[2025-05-07 13:25:56,625][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1039
[2025-05-07 13:25:56,852][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1077, Metrics: {'mse': 0.1077963337302208, 'rmse': 0.32832351991628744, 'r2': -0.6442257165908813}
[2025-05-07 13:25:56,852][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1013Epoch 4/15: [                              ] 2/75 batches, loss: 0.1288Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1058Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1116Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1014Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1048Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0981Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0999Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1021Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0950Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0953Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0944Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0929Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0891Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0920Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0941Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0948Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0973Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0981Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0977Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0974Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0998Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0989Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0990Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1008Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0979Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0987Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0991Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0996Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0988Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0972Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0962Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0965Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0950Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0937Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0937Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0926Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0922Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0908Epoch 4/15: [================              ] 40/75 batches, loss: 0.0920Epoch 4/15: [================              ] 41/75 batches, loss: 0.0932Epoch 4/15: [================              ] 42/75 batches, loss: 0.0926Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0927Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0916Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0911Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0913Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0912Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0907Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0911Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0904Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0899Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0894Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0888Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0888Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0883Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0885Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0876Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0875Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0881Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0888Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0886Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0886Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0888Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0889Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0881Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0872Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0869Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0876Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0884Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0883Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0885Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0882Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0877Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0872Epoch 4/15: [==============================] 75/75 batches, loss: 0.0872
[2025-05-07 13:25:59,140][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0872
[2025-05-07 13:25:59,407][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1041, Metrics: {'mse': 0.10413358360528946, 'rmse': 0.3226973560556229, 'r2': -0.5883574485778809}
[2025-05-07 13:25:59,408][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0578Epoch 5/15: [                              ] 2/75 batches, loss: 0.0626Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0633Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0655Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0688Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0659Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0768Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0816Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0789Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0750Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0728Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0690Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0659Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0659Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0666Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0682Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0691Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0705Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0709Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0699Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0674Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0683Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0695Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0712Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0706Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0708Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0722Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0714Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0704Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0710Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0732Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0725Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0720Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0722Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0725Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0727Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0726Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0722Epoch 5/15: [================              ] 40/75 batches, loss: 0.0722Epoch 5/15: [================              ] 41/75 batches, loss: 0.0728Epoch 5/15: [================              ] 42/75 batches, loss: 0.0720Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0715Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0715Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0709Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0705Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0698Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0701Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0715Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0709Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0707Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0698Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0699Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0695Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0690Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0691Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0689Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0687Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0682Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0679Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0675Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0672Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0673Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0669Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0672Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0671Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0668Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0673Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0670Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0667Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0667Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0675Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0673Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0668Epoch 5/15: [==============================] 75/75 batches, loss: 0.0665
[2025-05-07 13:26:01,692][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0665
[2025-05-07 13:26:01,968][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0904, Metrics: {'mse': 0.09036466479301453, 'rmse': 0.30060716024907747, 'r2': -0.37833917140960693}
[2025-05-07 13:26:01,969][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:26:01,969][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:26:01,969][src.training.lm_trainer][INFO] - Training completed in 14.11 seconds
[2025-05-07 13:26:01,970][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:26:05,045][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03537963330745697, 'rmse': 0.1880947455604674, 'r2': -0.7507696151733398}
[2025-05-07 13:26:05,046][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07199946790933609, 'rmse': 0.2683271658057307, 'r2': -0.0982133150100708}
[2025-05-07 13:26:05,046][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04771328344941139, 'rmse': 0.2184337049299201, 'r2': -0.2083287239074707}
[2025-05-07 13:26:06,733][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer10/fi/fi/model.pt
[2025-05-07 13:26:06,734][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▃
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▆▆▄
wandb:          val_mse ▁█▆▆▄
wandb:           val_r2 █▁▃▃▅
wandb:         val_rmse ▁█▆▆▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07205
wandb:     best_val_mse 0.072
wandb:      best_val_r2 -0.09821
wandb:    best_val_rmse 0.26833
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.04771
wandb:    final_test_r2 -0.20833
wandb:  final_test_rmse 0.21843
wandb:  final_train_mse 0.03538
wandb:   final_train_r2 -0.75077
wandb: final_train_rmse 0.18809
wandb:    final_val_mse 0.072
wandb:     final_val_r2 -0.09821
wandb:   final_val_rmse 0.26833
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0665
wandb:       train_time 14.10599
wandb:         val_loss 0.0904
wandb:          val_mse 0.09036
wandb:           val_r2 -0.37834
wandb:         val_rmse 0.30061
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132528-zj3yd5c6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132528-zj3yd5c6/logs
Experiment probe_layer10_complexity_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control1/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:26:34,885][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer10/fi
experiment_name: probe_layer10_complexity_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 13:26:34,885][__main__][INFO] - Normalized task: complexity
[2025-05-07 13:26:34,885][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:26:34,885][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:26:34,890][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 13:26:34,890][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:26:38,503][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:26:40,800][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:26:40,801][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:26:40,985][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 13:26:41,035][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-07 13:26:41,246][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:26:41,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:26:41,264][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:26:41,268][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:26:41,329][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:26:41,405][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:26:41,452][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:26:41,454][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:26:41,454][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:26:41,455][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:26:41,543][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:26:41,638][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:26:41,663][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:26:41,665][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:26:41,665][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:26:41,666][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:26:41,667][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:26:41,667][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:26:41,667][src.data.datasets][INFO] - Sample label: 0.6458609104156494
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:26:41,668][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:26:41,668][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:26:41,668][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:26:41,669][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:26:41,669][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:26:41,669][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:26:41,670][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 13:26:41,670][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:26:49,513][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:26:49,514][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:26:49,514][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:26:49,515][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:26:49,517][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:26:49,518][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:26:49,518][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:26:49,518][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:26:49,518][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:26:49,519][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:26:49,519][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6625Epoch 1/15: [                              ] 2/75 batches, loss: 0.8841Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8861Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8130Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7840Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6863Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6460Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6530Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6205Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6272Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5969Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5681Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5583Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5426Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5237Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5110Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5173Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.5111Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4994Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4968Epoch 1/15: [========                      ] 21/75 batches, loss: 0.5015Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4988Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4864Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4776Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4702Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4672Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4739Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4731Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4658Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4625Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4544Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4539Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4471Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4444Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4414Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4432Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4378Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4347Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4312Epoch 1/15: [================              ] 40/75 batches, loss: 0.4315Epoch 1/15: [================              ] 41/75 batches, loss: 0.4300Epoch 1/15: [================              ] 42/75 batches, loss: 0.4277Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4242Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4212Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4204Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4147Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4117Epoch 1/15: [===================           ] 48/75 batches, loss: 0.4075Epoch 1/15: [===================           ] 49/75 batches, loss: 0.4034Epoch 1/15: [====================          ] 50/75 batches, loss: 0.4015Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3979Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3951Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3941Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3956Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3934Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3893Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3862Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3854Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3825Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3781Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3742Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3715Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3687Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3674Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3650Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3627Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3596Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3568Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3575Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3544Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3508Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3476Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3465Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3433Epoch 1/15: [==============================] 75/75 batches, loss: 0.3405
[2025-05-07 13:26:55,717][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3405
[2025-05-07 13:26:55,942][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0752, Metrics: {'mse': 0.07509806007146835, 'rmse': 0.2740402526481618, 'r2': -0.14547646045684814}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2214Epoch 2/15: [                              ] 2/75 batches, loss: 0.1954Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2033Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2072Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1943Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1775Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1858Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1920Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1798Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1894Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1888Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1821Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1806Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1786Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1787Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1747Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1714Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1751Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1751Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1725Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1730Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1708Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1726Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1702Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1716Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1699Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1789Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1772Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1777Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1793Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1764Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1776Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1749Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1757Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1776Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1747Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1735Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1725Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1736Epoch 2/15: [================              ] 40/75 batches, loss: 0.1714Epoch 2/15: [================              ] 41/75 batches, loss: 0.1706Epoch 2/15: [================              ] 42/75 batches, loss: 0.1681Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1684Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1672Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1663Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1694Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1693Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1694Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1703Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1690Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1677Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1665Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1673Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1660Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1673Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1680Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1660Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1643Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1640Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1632Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1623Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1612Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1605Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1597Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1598Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1592Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1591Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1584Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1572Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1559Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1563Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1554Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1546Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1543Epoch 2/15: [==============================] 75/75 batches, loss: 0.1531
[2025-05-07 13:26:58,703][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1531
[2025-05-07 13:26:58,994][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1168, Metrics: {'mse': 0.11701660603284836, 'rmse': 0.34207690075894975, 'r2': -0.7848632335662842}
[2025-05-07 13:26:58,994][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1475Epoch 3/15: [                              ] 2/75 batches, loss: 0.1250Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1308Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1305Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1259Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1155Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1106Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1154Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1146Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1231Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1186Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1149Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1178Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1180Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1189Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1171Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1188Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1199Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1183Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1175Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1154Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1143Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1126Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1154Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1138Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1132Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1110Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1112Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1117Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1096Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1079Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1096Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1096Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1084Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1071Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1063Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1051Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1052Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1056Epoch 3/15: [================              ] 40/75 batches, loss: 0.1050Epoch 3/15: [================              ] 41/75 batches, loss: 0.1057Epoch 3/15: [================              ] 42/75 batches, loss: 0.1054Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1053Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1049Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1045Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1059Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1056Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1060Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1056Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1052Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1044Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1041Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1038Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1033Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1029Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1026Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1026Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1025Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1026Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1019Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1014Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1013Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1001Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1000Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0993Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0993Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0992Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0996Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0998Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0995Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0993Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0988Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0988Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0983Epoch 3/15: [==============================] 75/75 batches, loss: 0.0990
[2025-05-07 13:27:01,277][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0990
[2025-05-07 13:27:01,511][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1055, Metrics: {'mse': 0.10559704899787903, 'rmse': 0.3249569956130796, 'r2': -0.6106798648834229}
[2025-05-07 13:27:01,512][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0698Epoch 4/15: [                              ] 2/75 batches, loss: 0.0962Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0883Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0884Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0852Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0880Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0922Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0913Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1002Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0941Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0999Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1005Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0972Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1002Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0983Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0993Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1014Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1013Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0990Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0998Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1023Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1004Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1001Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1024Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1014Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1005Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0995Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0986Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0962Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0947Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0939Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0917Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0926Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0934Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0923Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0920Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0905Epoch 4/15: [================              ] 40/75 batches, loss: 0.0906Epoch 4/15: [================              ] 41/75 batches, loss: 0.0903Epoch 4/15: [================              ] 42/75 batches, loss: 0.0891Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0892Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0878Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0877Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0886Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0887Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0889Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0891Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0893Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0888Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0889Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0877Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0870Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0871Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0881Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0870Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0873Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0877Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0882Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0879Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0875Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0871Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0872Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0865Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0861Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0862Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0864Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0863Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0857Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0850Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0848Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0842Epoch 4/15: [==============================] 75/75 batches, loss: 0.0842
[2025-05-07 13:27:03,827][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0842
[2025-05-07 13:27:04,073][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1163, Metrics: {'mse': 0.11648093163967133, 'rmse': 0.341293028993666, 'r2': -0.7766925096511841}
[2025-05-07 13:27:04,074][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0604Epoch 5/15: [                              ] 2/75 batches, loss: 0.0714Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0747Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0765Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0764Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0781Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0855Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0865Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0846Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0810Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0787Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0748Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0725Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0729Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0718Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0710Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0692Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0727Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0711Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0708Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0694Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0701Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0707Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0719Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0712Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0708Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0715Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0709Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0707Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0704Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0710Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0707Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0708Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0715Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0719Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0724Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0717Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0717Epoch 5/15: [================              ] 40/75 batches, loss: 0.0741Epoch 5/15: [================              ] 41/75 batches, loss: 0.0740Epoch 5/15: [================              ] 42/75 batches, loss: 0.0735Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0741Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0744Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0737Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0747Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0737Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0732Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0734Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0725Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0722Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0717Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0715Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0714Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0707Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0705Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0700Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0700Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0695Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0690Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0683Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0681Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0682Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0679Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0681Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0679Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0681Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0679Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0675Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0672Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0674Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0680Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0676Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0672Epoch 5/15: [==============================] 75/75 batches, loss: 0.0665
[2025-05-07 13:27:06,367][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0665
[2025-05-07 13:27:06,608][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0990, Metrics: {'mse': 0.09899085015058517, 'rmse': 0.3146281140498814, 'r2': -0.5099149942398071}
[2025-05-07 13:27:06,609][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:27:06,609][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:27:06,609][src.training.lm_trainer][INFO] - Training completed in 13.99 seconds
[2025-05-07 13:27:06,609][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:27:09,546][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0343533456325531, 'rmse': 0.18534655549147144, 'r2': -0.6999833583831787}
[2025-05-07 13:27:09,547][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07509806007146835, 'rmse': 0.2740402526481618, 'r2': -0.14547646045684814}
[2025-05-07 13:27:09,547][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050482336431741714, 'rmse': 0.22468274618168105, 'r2': -0.27845442295074463}
[2025-05-07 13:27:11,225][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer10/fi/fi/model.pt
[2025-05-07 13:27:11,226][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▆█▅
wandb:          val_mse ▁█▆█▅
wandb:           val_r2 █▁▃▁▄
wandb:         val_rmse ▁█▆█▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07515
wandb:     best_val_mse 0.0751
wandb:      best_val_r2 -0.14548
wandb:    best_val_rmse 0.27404
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05048
wandb:    final_test_r2 -0.27845
wandb:  final_test_rmse 0.22468
wandb:  final_train_mse 0.03435
wandb:   final_train_r2 -0.69998
wandb: final_train_rmse 0.18535
wandb:    final_val_mse 0.0751
wandb:     final_val_r2 -0.14548
wandb:   final_val_rmse 0.27404
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06652
wandb:       train_time 13.9926
wandb:         val_loss 0.09897
wandb:          val_mse 0.09899
wandb:           val_r2 -0.50991
wandb:         val_rmse 0.31463
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132634-qvwlwj6c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132634-qvwlwj6c/logs
Experiment probe_layer10_complexity_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control2/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:27:40,725][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer10/fi
experiment_name: probe_layer10_complexity_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-07 13:27:40,725][__main__][INFO] - Normalized task: complexity
[2025-05-07 13:27:40,725][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:27:40,725][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:27:40,729][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-07 13:27:40,730][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:27:44,127][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:27:46,557][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:27:46,558][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:27:46,804][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 13:27:46,932][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-07 13:27:47,147][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:27:47,157][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:27:47,158][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:27:47,159][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:27:47,223][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:27:47,308][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:27:47,333][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:27:47,334][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:27:47,334][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:27:47,336][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:27:47,406][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:27:47,479][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:27:47,505][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:27:47,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:27:47,507][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:27:47,509][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:27:47,510][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:27:47,510][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:27:47,510][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:27:47,510][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:27:47,510][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:27:47,511][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Sample label: 0.3422375023365021
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:27:47,511][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:27:47,511][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:27:47,511][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-07 13:27:47,512][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:27:47,512][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:27:47,512][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:27:47,513][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:27:47,513][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-07 13:27:47,513][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:27:56,068][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:27:56,069][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:27:56,069][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:27:56,069][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:27:56,072][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:27:56,072][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:27:56,072][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:27:56,072][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:27:56,073][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:27:56,073][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:27:56,074][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6673Epoch 1/15: [                              ] 2/75 batches, loss: 0.9061Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8736Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8396Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7970Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7175Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6765Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6675Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6313Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6295Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5986Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5699Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5544Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5353Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5116Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4964Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4970Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4868Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4758Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4706Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4777Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4809Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4699Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4605Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4583Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4524Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4576Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4551Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4487Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4452Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4413Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4407Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4346Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4320Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4280Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4320Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4257Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4230Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4189Epoch 1/15: [================              ] 40/75 batches, loss: 0.4183Epoch 1/15: [================              ] 41/75 batches, loss: 0.4159Epoch 1/15: [================              ] 42/75 batches, loss: 0.4131Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4082Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4064Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4062Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3997Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3970Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3918Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3875Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3850Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3806Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3793Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3783Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3786Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3766Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3729Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3710Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3692Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3659Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3634Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3601Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3566Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3536Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3513Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3481Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3474Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3437Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3411Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3404Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3377Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3344Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3322Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3297Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3267Epoch 1/15: [==============================] 75/75 batches, loss: 0.3243
[2025-05-07 13:28:02,365][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3243
[2025-05-07 13:28:02,642][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0777, Metrics: {'mse': 0.07765040546655655, 'rmse': 0.2786582233966128, 'r2': -0.18440747261047363}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2069Epoch 2/15: [                              ] 2/75 batches, loss: 0.1890Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1990Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2138Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1995Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1783Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1887Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1989Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1910Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2000Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1979Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1937Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1908Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1879Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1836Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1803Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1760Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1824Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1834Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1773Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1768Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1765Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1767Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1741Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1762Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1760Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1855Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1838Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1845Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1863Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1832Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1829Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1812Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1827Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1836Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1808Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1778Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1753Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1759Epoch 2/15: [================              ] 40/75 batches, loss: 0.1746Epoch 2/15: [================              ] 41/75 batches, loss: 0.1738Epoch 2/15: [================              ] 42/75 batches, loss: 0.1718Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1721Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1706Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1708Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1718Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1705Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1689Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1700Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1684Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1667Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1659Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1661Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1653Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1662Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1659Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1642Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1627Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1610Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1602Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1594Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1581Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1566Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1561Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1574Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1573Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1572Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1563Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1546Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1534Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1528Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1514Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1511Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1503Epoch 2/15: [==============================] 75/75 batches, loss: 0.1490
[2025-05-07 13:28:05,332][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1490
[2025-05-07 13:28:05,575][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1284, Metrics: {'mse': 0.12857870757579803, 'rmse': 0.35857873274330987, 'r2': -0.9612207412719727}
[2025-05-07 13:28:05,575][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1394Epoch 3/15: [                              ] 2/75 batches, loss: 0.1190Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1464Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1400Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1312Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1218Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1165Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1163Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1166Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1232Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1216Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1228Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1232Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1253Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1252Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1217Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1221Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1213Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1185Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1170Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1149Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1120Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1106Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1116Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1108Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1098Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1080Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1076Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1093Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1092Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1078Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1090Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1084Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1085Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1066Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1071Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1061Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1064Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1057Epoch 3/15: [================              ] 40/75 batches, loss: 0.1047Epoch 3/15: [================              ] 41/75 batches, loss: 0.1048Epoch 3/15: [================              ] 42/75 batches, loss: 0.1045Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1040Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1044Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1038Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1027Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1025Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1022Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1016Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1007Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0998Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0992Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0990Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0980Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0977Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0969Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0975Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0980Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0984Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0975Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0970Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0967Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0958Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0959Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0962Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0957Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0958Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0958Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0965Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0963Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0960Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0965Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0963Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0963Epoch 3/15: [==============================] 75/75 batches, loss: 0.0966
[2025-05-07 13:28:07,848][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0966
[2025-05-07 13:28:08,093][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1055, Metrics: {'mse': 0.10557249933481216, 'rmse': 0.3249192197067021, 'r2': -0.6103053092956543}
[2025-05-07 13:28:08,094][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1305Epoch 4/15: [                              ] 2/75 batches, loss: 0.1238Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0982Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1064Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1049Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1009Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1060Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1021Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1036Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0991Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1083Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1120Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1091Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1035Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1075Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1072Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1057Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1038Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1051Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1028Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1034Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1072Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1053Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1048Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1058Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1042Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1058Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1046Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1044Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1037Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1014Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1002Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0991Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0985Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0974Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0974Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0963Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0964Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0952Epoch 4/15: [================              ] 40/75 batches, loss: 0.0951Epoch 4/15: [================              ] 41/75 batches, loss: 0.0947Epoch 4/15: [================              ] 42/75 batches, loss: 0.0938Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0940Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0937Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0938Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0941Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0936Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0944Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0940Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0940Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0936Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0937Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0928Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0921Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0915Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0903Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0894Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0895Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0890Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0895Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0889Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0885Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0888Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0882Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0881Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0876Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0880Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0879Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0883Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0878Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0878Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0874Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0868Epoch 4/15: [==============================] 75/75 batches, loss: 0.0861
[2025-05-07 13:28:10,423][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0861
[2025-05-07 13:28:10,733][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1169, Metrics: {'mse': 0.11707070469856262, 'rmse': 0.34215596545809723, 'r2': -0.7856884002685547}
[2025-05-07 13:28:10,734][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0819Epoch 5/15: [                              ] 2/75 batches, loss: 0.0851Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0824Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0790Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0749Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0727Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0779Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0828Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0802Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0754Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0781Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0764Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0735Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0732Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0730Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0721Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0729Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0717Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0724Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0711Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0715Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0703Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0697Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0710Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0721Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0708Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0711Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0711Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0699Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0694Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0701Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0702Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0697Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0700Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0694Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0698Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0714Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0705Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0703Epoch 5/15: [================              ] 40/75 batches, loss: 0.0705Epoch 5/15: [================              ] 41/75 batches, loss: 0.0707Epoch 5/15: [================              ] 42/75 batches, loss: 0.0705Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0695Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0694Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0691Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0697Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0690Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0687Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0688Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0683Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0680Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0674Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0676Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0674Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0668Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0667Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0667Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0664Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0663Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0664Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0656Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0657Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0663Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0662Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0659Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0655Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0653Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0650Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0644Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0651Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0650Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0656Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0656Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0652Epoch 5/15: [==============================] 75/75 batches, loss: 0.0645
[2025-05-07 13:28:13,026][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0645
[2025-05-07 13:28:13,368][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0995, Metrics: {'mse': 0.0995156466960907, 'rmse': 0.3154610066174435, 'r2': -0.5179197788238525}
[2025-05-07 13:28:13,369][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:28:13,369][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:28:13,369][src.training.lm_trainer][INFO] - Training completed in 14.06 seconds
[2025-05-07 13:28:13,369][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:28:16,362][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03202337026596069, 'rmse': 0.17895074815702977, 'r2': -0.5846841335296631}
[2025-05-07 13:28:16,363][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07765040546655655, 'rmse': 0.2786582233966128, 'r2': -0.18440747261047363}
[2025-05-07 13:28:16,363][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050216205418109894, 'rmse': 0.22408972626631032, 'r2': -0.2717146873474121}
[2025-05-07 13:28:17,987][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer10/fi/fi/model.pt
[2025-05-07 13:28:17,988][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▃▂
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▅▆▄
wandb:          val_mse ▁█▅▆▄
wandb:           val_r2 █▁▄▃▅
wandb:         val_rmse ▁█▅▇▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07769
wandb:     best_val_mse 0.07765
wandb:      best_val_r2 -0.18441
wandb:    best_val_rmse 0.27866
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05022
wandb:    final_test_r2 -0.27171
wandb:  final_test_rmse 0.22409
wandb:  final_train_mse 0.03202
wandb:   final_train_r2 -0.58468
wandb: final_train_rmse 0.17895
wandb:    final_val_mse 0.07765
wandb:     final_val_r2 -0.18441
wandb:   final_val_rmse 0.27866
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06453
wandb:       train_time 14.05944
wandb:         val_loss 0.09949
wandb:          val_mse 0.09952
wandb:           val_r2 -0.51792
wandb:         val_rmse 0.31546
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132740-nsxqv3lq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132740-nsxqv3lq/logs
Experiment probe_layer10_complexity_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/complexity/control3/layer10/fi/fi/results.json for layer 10
Running submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/fi"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:28:46,546][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/fi
experiment_name: probe_layer2_avg_links_len_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:28:46,546][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:28:46,546][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:28:46,546][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:28:46,546][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:28:46,551][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:28:46,551][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:28:46,551][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:28:49,862][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:28:52,199][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:28:52,199][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:28:52,425][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:52,514][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:52,748][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:28:52,757][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:28:52,757][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:28:52,758][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:28:52,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:52,940][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:52,954][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:28:52,955][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:28:52,955][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:28:52,956][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:28:53,076][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:53,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:28:53,189][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:28:53,191][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:28:53,191][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:28:53,192][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:28:53,193][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:28:53,193][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:28:53,193][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:28:53,193][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:28:53,194][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:28:53,194][src.data.datasets][INFO] -   Mean: 0.1395, Std: 0.0869
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Sample label: 0.2280000001192093
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:28:53,194][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:28:53,194][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5520
[2025-05-07 13:28:53,195][src.data.datasets][INFO] -   Mean: 0.2101, Std: 0.1311
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Sample label: 0.36800000071525574
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:28:53,195][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:28:53,195][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6740
[2025-05-07 13:28:53,195][src.data.datasets][INFO] -   Mean: 0.2318, Std: 0.1347
[2025-05-07 13:28:53,196][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:28:53,196][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 13:28:53,196][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:28:53,196][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:28:53,196][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:28:53,196][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 13:28:53,196][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:29:00,858][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:29:00,859][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:29:00,859][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:29:00,859][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:29:00,862][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:29:00,862][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:29:00,862][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:29:00,862][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:29:00,863][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:29:00,863][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:29:00,863][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6008Epoch 1/15: [                              ] 2/75 batches, loss: 0.6061Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5145Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4623Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4454Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4143Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4018Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4394Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4452Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4276Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4203Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4127Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3973Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4055Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3996Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4162Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4066Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4042Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4036Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4010Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4090Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4131Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4079Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3957Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3895Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3819Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3743Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3701Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3693Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3693Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3628Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3582Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3539Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3520Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3468Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3472Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3422Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3383Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3365Epoch 1/15: [================              ] 40/75 batches, loss: 0.3318Epoch 1/15: [================              ] 41/75 batches, loss: 0.3280Epoch 1/15: [================              ] 42/75 batches, loss: 0.3244Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3220Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3200Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3226Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3185Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3200Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3153Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3124Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3110Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3113Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3093Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3055Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3051Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3030Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3016Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3046Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3042Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3019Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2999Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2962Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2951Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2929Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2912Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2894Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2887Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2859Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2826Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2833Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2825Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2801Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2783Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2757Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2734Epoch 1/15: [==============================] 75/75 batches, loss: 0.2715
[2025-05-07 13:29:07,445][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2715
[2025-05-07 13:29:07,673][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0663, Metrics: {'mse': 0.06627380847930908, 'rmse': 0.25743699904891115, 'r2': -2.8533482551574707}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1502Epoch 2/15: [                              ] 2/75 batches, loss: 0.1304Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1014Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1222Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1182Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1098Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1182Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1171Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1184Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1246Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1209Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1213Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1191Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1146Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1172Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1153Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1143Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1138Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1113Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1116Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1117Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1129Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1129Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1119Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1131Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1173Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1178Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1178Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1198Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1214Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1208Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1193Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1186Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1205Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1217Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1205Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1199Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1190Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1187Epoch 2/15: [================              ] 40/75 batches, loss: 0.1181Epoch 2/15: [================              ] 41/75 batches, loss: 0.1175Epoch 2/15: [================              ] 42/75 batches, loss: 0.1166Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1160Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1145Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1139Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1159Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1155Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1151Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1178Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1163Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1159Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1165Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1163Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1160Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1157Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1164Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1157Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1152Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1143Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1131Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1130Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1131Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1123Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1119Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1113Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1110Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1106Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1107Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1102Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1093Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1087Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1095Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1097Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1091Epoch 2/15: [==============================] 75/75 batches, loss: 0.1088
[2025-05-07 13:29:10,434][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1088
[2025-05-07 13:29:10,683][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0692, Metrics: {'mse': 0.06925296038389206, 'rmse': 0.26315957209247026, 'r2': -3.026564598083496}
[2025-05-07 13:29:10,684][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1208Epoch 3/15: [                              ] 2/75 batches, loss: 0.0904Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0894Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0810Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0726Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0775Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0752Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0706Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0675Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0716Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0690Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0677Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0712Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0750Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0737Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0736Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0748Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0748Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0753Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0755Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0767Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0774Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0760Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0809Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0816Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0808Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0801Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0801Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0805Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0796Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0791Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0791Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0786Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0790Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0777Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0767Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0761Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0757Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0753Epoch 3/15: [================              ] 40/75 batches, loss: 0.0754Epoch 3/15: [================              ] 41/75 batches, loss: 0.0751Epoch 3/15: [================              ] 42/75 batches, loss: 0.0749Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0746Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0746Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0748Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0748Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0762Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0768Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0775Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0770Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0767Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0768Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0760Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0768Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0762Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0756Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0749Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0748Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0742Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0749Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0745Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0745Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0751Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0757Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0755Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0748Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0748Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0743Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0739Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0743Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0739Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0734Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0731Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0729Epoch 3/15: [==============================] 75/75 batches, loss: 0.0737
[2025-05-07 13:29:13,011][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0737
[2025-05-07 13:29:13,236][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0387, Metrics: {'mse': 0.0387154147028923, 'rmse': 0.19676233049771572, 'r2': -1.2510247230529785}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0652Epoch 4/15: [                              ] 2/75 batches, loss: 0.0640Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0543Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0620Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0607Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0646Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0663Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0646Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0653Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0617Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0636Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0644Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0607Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0604Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0651Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0658Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0667Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0652Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0635Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0632Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0647Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0648Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0646Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0664Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0666Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0663Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0656Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0656Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0645Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0646Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0644Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0636Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0634Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0628Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0625Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0618Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0623Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0613Epoch 4/15: [================              ] 40/75 batches, loss: 0.0608Epoch 4/15: [================              ] 41/75 batches, loss: 0.0610Epoch 4/15: [================              ] 42/75 batches, loss: 0.0605Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0612Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0606Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0601Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0595Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0591Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0591Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0596Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0590Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0590Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0592Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0597Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0605Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0599Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0604Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0600Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0596Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0593Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0591Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0595Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0589Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0586Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0583Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0587Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0584Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0578Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0580Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0579Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0578Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0572Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0571Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0574Epoch 4/15: [==============================] 75/75 batches, loss: 0.0574
[2025-05-07 13:29:15,962][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0574
[2025-05-07 13:29:16,252][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0295, Metrics: {'mse': 0.029506061226129532, 'rmse': 0.17177328437836173, 'r2': -0.7155665159225464}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1052Epoch 5/15: [                              ] 2/75 batches, loss: 0.0733Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0776Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0737Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0636Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0607Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0619Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0601Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0568Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0550Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0566Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0553Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0560Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0567Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0576Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0556Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0551Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0556Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0548Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0537Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0537Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0527Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0531Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0520Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0532Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0531Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0538Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0537Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0532Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0522Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0517Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0526Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0527Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0529Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0538Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0535Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0530Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0530Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0528Epoch 5/15: [================              ] 40/75 batches, loss: 0.0524Epoch 5/15: [================              ] 41/75 batches, loss: 0.0522Epoch 5/15: [================              ] 42/75 batches, loss: 0.0517Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0518Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0514Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0514Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0514Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0509Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0507Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0515Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0515Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0510Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0506Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0503Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0505Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0501Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0498Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0499Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0498Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0493Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0496Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0494Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0494Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0494Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0494Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0491Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0489Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0487Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0484Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0484Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0485Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0487Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0484Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0484Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0483Epoch 5/15: [==============================] 75/75 batches, loss: 0.0484
[2025-05-07 13:29:18,900][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0484
[2025-05-07 13:29:19,165][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0309, Metrics: {'mse': 0.03097641095519066, 'rmse': 0.1760011674824649, 'r2': -0.8010568618774414}
[2025-05-07 13:29:19,165][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0285Epoch 6/15: [                              ] 2/75 batches, loss: 0.0531Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0440Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0404Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0442Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0427Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0404Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0412Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0387Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0415Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0412Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0398Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0412Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0418Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0438Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0446Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0430Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0440Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0425Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0421Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0421Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0415Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0413Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0414Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0419Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0415Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0421Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0411Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0401Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0398Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0394Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0388Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0382Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0377Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0383Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0380Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0376Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0372Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0389Epoch 6/15: [================              ] 40/75 batches, loss: 0.0387Epoch 6/15: [================              ] 41/75 batches, loss: 0.0386Epoch 6/15: [================              ] 42/75 batches, loss: 0.0388Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0386Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0387Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0394Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0390Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0390Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0386Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0383Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0382Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0385Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0384Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0385Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0382Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0382Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0380Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0382Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0384Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0381Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0382Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0387Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0387Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0387Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0387Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0389Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0388Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0387Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0391Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0390Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0389Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0392Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0390Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0388Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0389Epoch 6/15: [==============================] 75/75 batches, loss: 0.0390
[2025-05-07 13:29:21,562][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0390
[2025-05-07 13:29:21,854][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0292, Metrics: {'mse': 0.0292560625821352, 'rmse': 0.1710440369674874, 'r2': -0.7010308504104614}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0413Epoch 7/15: [                              ] 2/75 batches, loss: 0.0400Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0354Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0386Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0366Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0335Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0354Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0349Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0339Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0354Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0352Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0351Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0342Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0389Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0385Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0390Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0396Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0398Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0381Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0374Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0372Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0370Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0364Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0367Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0364Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0361Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0361Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0359Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0357Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0350Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0348Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0348Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0343Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0348Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0341Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0335Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0341Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0340Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0337Epoch 7/15: [================              ] 40/75 batches, loss: 0.0336Epoch 7/15: [================              ] 41/75 batches, loss: 0.0344Epoch 7/15: [================              ] 42/75 batches, loss: 0.0346Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0344Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0343Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0339Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0342Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0343Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0341Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0341Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0338Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0335Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0337Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0338Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0338Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0341Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0341Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0341Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0346Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0344Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0346Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0341Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0342Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0343Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0342Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0336Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0332Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0332Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0333Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0333Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0332Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0330Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0330Epoch 7/15: [==============================] 75/75 batches, loss: 0.0328
[2025-05-07 13:29:24,555][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0328
[2025-05-07 13:29:24,820][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0343, Metrics: {'mse': 0.03432445228099823, 'rmse': 0.1852685949668703, 'r2': -0.9957215785980225}
[2025-05-07 13:29:24,821][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0303Epoch 8/15: [                              ] 2/75 batches, loss: 0.0253Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0261Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0291Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0376Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0393Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0365Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0351Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0348Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0334Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0323Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0312Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0318Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0314Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0320Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0314Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0314Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0313Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0310Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0313Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0307Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0314Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0308Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0311Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0312Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0303Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0306Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0305Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0305Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0304Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0302Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0305Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0302Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0300Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0298Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0297Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0295Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0296Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0295Epoch 8/15: [================              ] 40/75 batches, loss: 0.0299Epoch 8/15: [================              ] 41/75 batches, loss: 0.0297Epoch 8/15: [================              ] 42/75 batches, loss: 0.0295Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0295Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0293Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0292Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0293Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0295Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0294Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0293Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0292Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0292Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0293Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0290Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0288Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0288Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0285Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0286Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0284Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0285Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0286Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0284Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0284Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0283Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0285Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0284Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0283Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0282Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0283Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0282Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0280Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0281Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0279Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0278Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0277Epoch 8/15: [==============================] 75/75 batches, loss: 0.0281
[2025-05-07 13:29:27,120][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0281
[2025-05-07 13:29:27,426][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0313, Metrics: {'mse': 0.0313810333609581, 'rmse': 0.17714692591450212, 'r2': -0.8245828151702881}
[2025-05-07 13:29:27,426][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0489Epoch 9/15: [                              ] 2/75 batches, loss: 0.0391Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0298Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0272Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0264Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0253Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0272Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0279Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0269Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0261Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0253Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0244Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0236Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0237Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0236Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0243Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0240Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0243Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0251Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0250Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0255Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0259Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0263Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0262Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0260Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0253Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0251Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0252Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0252Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0248Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0247Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0245Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0249Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0245Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0242Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0242Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0242Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0243Epoch 9/15: [================              ] 40/75 batches, loss: 0.0242Epoch 9/15: [================              ] 41/75 batches, loss: 0.0245Epoch 9/15: [================              ] 42/75 batches, loss: 0.0246Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0248Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0248Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0247Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0245Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0247Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0248Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0247Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0246Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0246Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0247Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0246Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0243Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0243Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0243Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0241Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0243Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0242Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0241Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0244Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0246Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0247Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0248Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0246Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0245Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0245Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0243Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0242Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0242Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0242Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0240Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0239Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0237Epoch 9/15: [==============================] 75/75 batches, loss: 0.0235
[2025-05-07 13:29:29,742][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0235
[2025-05-07 13:29:30,035][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0355, Metrics: {'mse': 0.03559422865509987, 'rmse': 0.188664327987831, 'r2': -1.0695500373840332}
[2025-05-07 13:29:30,036][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0336Epoch 10/15: [                              ] 2/75 batches, loss: 0.0236Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0194Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0198Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0194Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0217Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0225Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0236Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0235Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0244Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0242Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0228Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0236Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0248Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0248Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0249Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0246Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0244Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0245Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0244Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0242Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0242Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0240Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0240Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0236Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0233Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0231Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0236Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0237Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0239Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0240Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0236Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0240Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0247Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0247Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0252Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0250Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0249Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0249Epoch 10/15: [================              ] 40/75 batches, loss: 0.0251Epoch 10/15: [================              ] 41/75 batches, loss: 0.0251Epoch 10/15: [================              ] 42/75 batches, loss: 0.0248Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0246Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0247Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0249Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0247Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0247Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0247Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0245Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0247Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0249Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0247Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0248Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0246Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0247Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0247Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0248Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0247Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0245Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0245Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0246Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0245Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0244Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0243Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0244Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0243Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0241Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0242Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0241Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0240Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0240Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0245Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0245Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0250Epoch 10/15: [==============================] 75/75 batches, loss: 0.0250
[2025-05-07 13:29:32,326][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0250
[2025-05-07 13:29:32,677][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0361, Metrics: {'mse': 0.03611920773983002, 'rmse': 0.1900505399619533, 'r2': -1.1000738143920898}
[2025-05-07 13:29:32,678][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:29:32,678][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 13:29:32,678][src.training.lm_trainer][INFO] - Training completed in 28.37 seconds
[2025-05-07 13:29:32,678][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:29:35,797][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.007692293729633093, 'rmse': 0.08770572233117456, 'r2': -0.018056392669677734}
[2025-05-07 13:29:35,798][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0292560625821352, 'rmse': 0.1710440369674874, 'r2': -0.7010308504104614}
[2025-05-07 13:29:35,798][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.037595659494400024, 'rmse': 0.19389600174939148, 'r2': -1.0729491710662842}
[2025-05-07 13:29:37,426][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/fi/fi/model.pt
[2025-05-07 13:29:37,427][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁
wandb:     best_val_mse █▃▁▁
wandb:      best_val_r2 ▁▆██
wandb:    best_val_rmse █▃▁▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▆▇▇▇▆▇▆
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▃▁▁▁▂▁▂▂
wandb:          val_mse ▇█▃▁▁▁▂▁▂▂
wandb:           val_r2 ▂▁▆███▇█▇▇
wandb:         val_rmse ██▃▁▁▁▂▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02921
wandb:     best_val_mse 0.02926
wandb:      best_val_r2 -0.70103
wandb:    best_val_rmse 0.17104
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.0376
wandb:    final_test_r2 -1.07295
wandb:  final_test_rmse 0.1939
wandb:  final_train_mse 0.00769
wandb:   final_train_r2 -0.01806
wandb: final_train_rmse 0.08771
wandb:    final_val_mse 0.02926
wandb:     final_val_r2 -0.70103
wandb:   final_val_rmse 0.17104
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02495
wandb:       train_time 28.3736
wandb:         val_loss 0.03605
wandb:          val_mse 0.03612
wandb:           val_r2 -1.10007
wandb:         val_rmse 0.19005
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132846-uijyheh8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_132846-uijyheh8/logs
Experiment probe_layer2_avg_links_len_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_max_depth_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_max_depth_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/fi"         "wandb.mode=offline" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:30:07,664][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/fi
experiment_name: probe_layer2_avg_max_depth_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:30:07,664][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:30:07,664][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:30:07,664][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:30:07,664][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:30:07,669][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:30:07,669][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:30:07,669][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:30:11,458][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:30:13,759][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:30:13,759][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:30:13,955][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,068][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,358][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:30:14,367][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:30:14,367][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:30:14,368][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:30:14,452][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,560][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,597][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:30:14,598][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:30:14,598][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:30:14,600][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:30:14,704][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,828][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:30:14,874][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:30:14,876][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:30:14,876][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:30:14,877][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:30:14,878][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:30:14,878][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:30:14,878][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:30:14,878][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:30:14,878][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:30:14,879][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:30:14,879][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:30:14,879][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:30:14,880][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:30:14,880][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:30:14,880][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:30:14,880][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:30:14,881][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:30:14,881][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:30:14,881][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:30:14,881][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:30:14,881][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:30:21,991][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:30:21,992][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:30:21,992][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:30:21,992][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:30:21,995][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:30:21,995][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:30:21,995][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:30:21,996][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:30:21,996][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:30:21,997][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:30:21,997][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5220Epoch 1/15: [                              ] 2/75 batches, loss: 0.5933Epoch 1/15: [=                             ] 3/75 batches, loss: 0.4904Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4743Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4442Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4110Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4028Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4588Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4566Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4400Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4309Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4197Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4072Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4196Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4127Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4274Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4213Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4194Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4177Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4142Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4249Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4260Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4232Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4127Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4083Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4003Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3950Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3899Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3881Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3886Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3825Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3774Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3730Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3713Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3656Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3662Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3618Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3607Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3603Epoch 1/15: [================              ] 40/75 batches, loss: 0.3547Epoch 1/15: [================              ] 41/75 batches, loss: 0.3513Epoch 1/15: [================              ] 42/75 batches, loss: 0.3472Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3446Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3417Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3441Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3397Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3393Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3344Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3312Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3293Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3290Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3270Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3234Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3251Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3228Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3213Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3231Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3218Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3228Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3206Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3166Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3154Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3129Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3111Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3090Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3084Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3057Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3028Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3034Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3031Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3019Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3002Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2971Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2939Epoch 1/15: [==============================] 75/75 batches, loss: 0.2924
[2025-05-07 13:30:28,204][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2924
[2025-05-07 13:30:28,519][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0589, Metrics: {'mse': 0.05875507742166519, 'rmse': 0.24239446656568955, 'r2': -0.2862660884857178}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1347Epoch 2/15: [                              ] 2/75 batches, loss: 0.1322Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1099Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1340Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1341Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1221Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1311Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1277Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1247Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1331Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1317Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1291Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1292Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1254Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1266Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1255Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1257Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1233Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1239Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1222Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1218Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1220Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1202Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1194Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1194Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1245Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1262Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1264Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1276Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1299Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1313Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1308Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1307Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1329Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1340Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1330Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1325Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1315Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1319Epoch 2/15: [================              ] 40/75 batches, loss: 0.1317Epoch 2/15: [================              ] 41/75 batches, loss: 0.1317Epoch 2/15: [================              ] 42/75 batches, loss: 0.1302Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1291Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1275Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1280Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1310Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1305Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1299Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1346Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1333Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1332Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1335Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1334Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1330Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1330Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1352Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1345Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1343Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1330Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1319Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1321Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1320Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1311Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1303Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1298Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1294Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1288Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1298Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1294Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1282Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1275Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1285Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1287Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1284Epoch 2/15: [==============================] 75/75 batches, loss: 0.1285
[2025-05-07 13:30:31,200][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1285
[2025-05-07 13:30:31,474][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0699, Metrics: {'mse': 0.06986386328935623, 'rmse': 0.26431773169682776, 'r2': -0.5294595956802368}
[2025-05-07 13:30:31,475][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1576Epoch 3/15: [                              ] 2/75 batches, loss: 0.1367Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1137Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1038Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0924Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0954Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0967Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0918Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0882Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0896Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0870Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0883Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0950Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0990Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0992Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0976Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0971Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0976Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0971Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0952Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0956Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0959Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0948Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0988Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1003Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1008Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0998Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0989Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1005Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0991Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0992Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0991Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0988Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0998Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0989Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0974Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0966Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0962Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0953Epoch 3/15: [================              ] 40/75 batches, loss: 0.0962Epoch 3/15: [================              ] 41/75 batches, loss: 0.0952Epoch 3/15: [================              ] 42/75 batches, loss: 0.0940Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0935Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0940Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0940Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0935Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0949Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0962Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0976Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0968Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0971Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0964Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0954Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0962Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0962Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0960Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0950Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0947Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0940Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0929Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0923Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0925Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0932Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0938Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0932Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0927Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0927Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0925Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0923Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0924Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0922Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0918Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0918Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0915Epoch 3/15: [==============================] 75/75 batches, loss: 0.0916
[2025-05-07 13:30:33,822][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0916
[2025-05-07 13:30:34,094][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0547, Metrics: {'mse': 0.05468333512544632, 'rmse': 0.23384468162745614, 'r2': -0.19712746143341064}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0955Epoch 4/15: [                              ] 2/75 batches, loss: 0.0866Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0661Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0782Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0732Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0837Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0775Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0782Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0794Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0767Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0788Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0793Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0750Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0722Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0794Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0813Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0815Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0830Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0823Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0817Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0811Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0833Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0826Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0824Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0837Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0850Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0852Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0854Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0860Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0851Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0849Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0850Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0834Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0843Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0846Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0834Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0829Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0839Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0829Epoch 4/15: [================              ] 40/75 batches, loss: 0.0818Epoch 4/15: [================              ] 41/75 batches, loss: 0.0816Epoch 4/15: [================              ] 42/75 batches, loss: 0.0812Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0817Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0806Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0803Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0793Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0788Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0785Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0783Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0777Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0772Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0776Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0783Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0785Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0775Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0779Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0774Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0767Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0764Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0769Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0769Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0760Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0755Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0757Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0756Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0756Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0751Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0751Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0752Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0749Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0744Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0742Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0745Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0746Epoch 4/15: [==============================] 75/75 batches, loss: 0.0752
[2025-05-07 13:30:36,774][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0752
[2025-05-07 13:30:37,031][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0418, Metrics: {'mse': 0.04173724725842476, 'rmse': 0.2042969585148657, 'r2': 0.08628827333450317}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1717Epoch 5/15: [                              ] 2/75 batches, loss: 0.1134Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1274Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1163Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0985Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0922Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0929Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0882Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0830Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0783Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0798Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0767Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0784Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0782Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0804Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0772Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0751Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0743Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0743Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0727Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0743Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0739Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0739Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0723Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0739Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0746Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0744Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0737Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0732Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0725Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0715Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0715Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0717Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0728Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0740Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0731Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0732Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0733Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0728Epoch 5/15: [================              ] 40/75 batches, loss: 0.0727Epoch 5/15: [================              ] 41/75 batches, loss: 0.0721Epoch 5/15: [================              ] 42/75 batches, loss: 0.0715Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0716Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0707Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0708Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0708Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0705Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0699Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0699Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0701Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0697Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0698Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0695Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0693Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0690Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0687Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0687Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0685Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0685Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0681Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0685Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0686Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0687Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0686Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0687Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0685Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0684Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0682Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0681Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0683Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0681Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0680Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0677Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0677Epoch 5/15: [==============================] 75/75 batches, loss: 0.0675
[2025-05-07 13:30:39,689][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0675
[2025-05-07 13:30:40,000][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0372, Metrics: {'mse': 0.03719232976436615, 'rmse': 0.19285313003518026, 'r2': 0.18578559160232544}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0454Epoch 6/15: [                              ] 2/75 batches, loss: 0.0496Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0505Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0455Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0443Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0423Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0426Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0465Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0438Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0464Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0463Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0476Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0493Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0512Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0519Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0533Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0531Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0573Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0555Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0570Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0570Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0555Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0579Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0576Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0580Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0578Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0585Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0572Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0563Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0555Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0554Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0548Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0542Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0539Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0546Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0541Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0536Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0529Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0555Epoch 6/15: [================              ] 40/75 batches, loss: 0.0548Epoch 6/15: [================              ] 41/75 batches, loss: 0.0552Epoch 6/15: [================              ] 42/75 batches, loss: 0.0558Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0552Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0566Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0589Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0590Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0587Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0581Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0584Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0591Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0588Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0596Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0597Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0592Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0593Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0593Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0593Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0601Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0598Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0597Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0599Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0598Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0599Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0606Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0609Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0601Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0599Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0603Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0600Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0596Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0597Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0598Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0593Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0591Epoch 6/15: [==============================] 75/75 batches, loss: 0.0586
[2025-05-07 13:30:42,627][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0586
[2025-05-07 13:30:42,912][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0463, Metrics: {'mse': 0.04628916084766388, 'rmse': 0.21514915953278524, 'r2': -0.013362288475036621}
[2025-05-07 13:30:42,912][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0541Epoch 7/15: [                              ] 2/75 batches, loss: 0.0586Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0481Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0500Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0459Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0475Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0504Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0503Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0484Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0520Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0513Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0539Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0526Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0541Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0530Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0538Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0558Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0544Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0532Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0533Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0535Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0540Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0540Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0549Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0547Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0558Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0557Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0560Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0557Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0555Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0547Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0542Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0536Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0538Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0527Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0518Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0525Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0525Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0521Epoch 7/15: [================              ] 40/75 batches, loss: 0.0519Epoch 7/15: [================              ] 41/75 batches, loss: 0.0526Epoch 7/15: [================              ] 42/75 batches, loss: 0.0526Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0529Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0534Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0532Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0534Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0540Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0535Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0533Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0529Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0526Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0525Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0526Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0525Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0532Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0529Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0528Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0525Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0522Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0527Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0538Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0536Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0535Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0536Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0536Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0535Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0532Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0527Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0530Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0533Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0532Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0534Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0530Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0527Epoch 7/15: [==============================] 75/75 batches, loss: 0.0527
[2025-05-07 13:30:45,236][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0527
[2025-05-07 13:30:45,548][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0434, Metrics: {'mse': 0.04343056678771973, 'rmse': 0.2084000162853154, 'r2': 0.04921811819076538}
[2025-05-07 13:30:45,549][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0491Epoch 8/15: [                              ] 2/75 batches, loss: 0.0494Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0465Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0461Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0423Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0421Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0393Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0397Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0407Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0421Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0455Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0432Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0433Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0455Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0458Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0456Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0452Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0449Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0457Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0450Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0449Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0458Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0454Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0456Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0460Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0456Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0456Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0468Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0467Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0467Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0468Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0468Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0466Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0458Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0452Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0450Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0453Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0451Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0450Epoch 8/15: [================              ] 40/75 batches, loss: 0.0453Epoch 8/15: [================              ] 41/75 batches, loss: 0.0457Epoch 8/15: [================              ] 42/75 batches, loss: 0.0455Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0459Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0459Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0463Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0462Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0466Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0468Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0469Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0466Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0462Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0463Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0461Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0460Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0461Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0457Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0457Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0452Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0451Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0452Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0453Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0457Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0456Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0459Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0457Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0456Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0459Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0459Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0456Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0453Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0459Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0457Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0457Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0456Epoch 8/15: [==============================] 75/75 batches, loss: 0.0461
[2025-05-07 13:30:47,866][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0461
[2025-05-07 13:30:48,123][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0382, Metrics: {'mse': 0.03823588788509369, 'rmse': 0.19553999050090415, 'r2': 0.16294002532958984}
[2025-05-07 13:30:48,123][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0408Epoch 9/15: [                              ] 2/75 batches, loss: 0.0436Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0403Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0389Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0381Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0386Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0390Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0398Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0385Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0434Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0425Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0403Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0402Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0385Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0381Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0378Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0373Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0379Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0370Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0385Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0391Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0393Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0407Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0424Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0421Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0415Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0413Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0409Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0410Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0407Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0402Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0414Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0420Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0428Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0421Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0415Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0418Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0415Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0416Epoch 9/15: [================              ] 40/75 batches, loss: 0.0422Epoch 9/15: [================              ] 41/75 batches, loss: 0.0425Epoch 9/15: [================              ] 42/75 batches, loss: 0.0422Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0425Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0419Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0417Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0416Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0416Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0417Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0418Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0417Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0420Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0421Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0418Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0414Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0413Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0415Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0413Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0419Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0417Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0418Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0420Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0424Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0424Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0427Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0425Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0424Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0428Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0426Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0426Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0429Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0427Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0424Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0421Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0419Epoch 9/15: [==============================] 75/75 batches, loss: 0.0417
[2025-05-07 13:30:50,416][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0417
[2025-05-07 13:30:50,670][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0400, Metrics: {'mse': 0.0400267019867897, 'rmse': 0.2000667438301271, 'r2': 0.12373548746109009}
[2025-05-07 13:30:50,670][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:30:50,670][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 13:30:50,671][src.training.lm_trainer][INFO] - Training completed in 25.58 seconds
[2025-05-07 13:30:50,671][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:30:53,620][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02402043342590332, 'rmse': 0.15498526841575402, 'r2': 0.22533392906188965}
[2025-05-07 13:30:53,621][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03719232976436615, 'rmse': 0.19285313003518026, 'r2': 0.18578559160232544}
[2025-05-07 13:30:53,621][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02190539985895157, 'rmse': 0.14800472917765692, 'r2': 0.28162771463394165}
[2025-05-07 13:30:55,324][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/fi/fi/model.pt
[2025-05-07 13:30:55,327][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▂▁
wandb:     best_val_mse █▇▂▁
wandb:      best_val_r2 ▁▂▇█
wandb:    best_val_rmse █▇▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▄▆▆▅▅▆
wandb:       train_loss █▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▅▂▁▃▂▁▂
wandb:          val_mse ▆█▅▂▁▃▂▁▂
wandb:           val_r2 ▃▁▄▇█▆▇█▇
wandb:         val_rmse ▆█▅▂▁▃▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03724
wandb:     best_val_mse 0.03719
wandb:      best_val_r2 0.18579
wandb:    best_val_rmse 0.19285
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.02191
wandb:    final_test_r2 0.28163
wandb:  final_test_rmse 0.148
wandb:  final_train_mse 0.02402
wandb:   final_train_r2 0.22533
wandb: final_train_rmse 0.15499
wandb:    final_val_mse 0.03719
wandb:     final_val_r2 0.18579
wandb:   final_val_rmse 0.19285
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04172
wandb:       train_time 25.57854
wandb:         val_loss 0.04
wandb:          val_mse 0.04003
wandb:           val_r2 0.12374
wandb:         val_rmse 0.20007
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133007-zzqyjmrm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133007-zzqyjmrm/logs
Experiment probe_layer2_avg_max_depth_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_subordinate_chain_len_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_subordinate_chain_len_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/fi"         "wandb.mode=offline" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:31:26,556][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/fi
experiment_name: probe_layer2_avg_subordinate_chain_len_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:31:26,557][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:31:26,557][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:31:26,557][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:31:26,557][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:31:26,561][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:31:26,561][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:31:26,561][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:31:30,212][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:31:32,598][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:31:32,599][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:31:32,880][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:32,954][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:33,195][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:31:33,204][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:31:33,205][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:31:33,207][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:31:33,259][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:33,337][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:33,383][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:31:33,384][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:31:33,384][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:31:33,385][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:31:33,450][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:33,523][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:31:33,548][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:31:33,550][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:31:33,550][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:31:33,551][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:31:33,552][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:31:33,552][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:31:33,552][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:31:33,552][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:31:33,552][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:31:33,553][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:31:33,553][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:31:33,553][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:31:33,553][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:31:33,554][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:31:33,554][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:31:33,554][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:31:33,555][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:31:33,555][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:31:33,555][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:31:33,555][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:31:33,555][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:31:41,077][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:31:41,079][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:31:41,079][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:31:41,079][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:31:41,082][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:31:41,082][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:31:41,082][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:31:41,082][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:31:41,083][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:31:41,083][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:31:41,084][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6851Epoch 1/15: [                              ] 2/75 batches, loss: 0.6751Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5548Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5196Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4938Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4569Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4381Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4734Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4782Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4669Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4623Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4531Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4336Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4350Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4286Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4460Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4332Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4313Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4280Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4240Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4322Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4321Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4255Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4124Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4050Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3966Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3890Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3851Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3854Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3839Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3781Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3733Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3684Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3651Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3611Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3612Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3556Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3540Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3530Epoch 1/15: [================              ] 40/75 batches, loss: 0.3480Epoch 1/15: [================              ] 41/75 batches, loss: 0.3433Epoch 1/15: [================              ] 42/75 batches, loss: 0.3400Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3374Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3351Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3368Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3320Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3325Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3279Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3251Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3232Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3239Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3217Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3185Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3206Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3184Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3169Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3180Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3184Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3174Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3151Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3113Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3101Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3079Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3069Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3048Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3041Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3015Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2979Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2988Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2972Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2958Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2943Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2912Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2883Epoch 1/15: [==============================] 75/75 batches, loss: 0.2860
[2025-05-07 13:31:47,732][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2860
[2025-05-07 13:31:48,026][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1042, Metrics: {'mse': 0.10404395312070847, 'rmse': 0.322558449154116, 'r2': -1.1376550197601318}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1683Epoch 2/15: [                              ] 2/75 batches, loss: 0.1407Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1172Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1560Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1547Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1395Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1430Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1388Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1364Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1488Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1445Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1416Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1385Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1317Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1346Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1321Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1302Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1277Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1266Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1275Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1259Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1277Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1273Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1252Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1255Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1294Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1299Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1291Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1313Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1378Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1364Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1343Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1329Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1352Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1375Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1359Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1344Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1328Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1326Epoch 2/15: [================              ] 40/75 batches, loss: 0.1331Epoch 2/15: [================              ] 41/75 batches, loss: 0.1316Epoch 2/15: [================              ] 42/75 batches, loss: 0.1305Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1293Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1283Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1273Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1296Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1289Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1281Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1299Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1286Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1282Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1280Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1283Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1280Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1276Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1290Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1277Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1265Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1254Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1239Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1232Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1229Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1222Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1216Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1215Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1207Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1201Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1201Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1198Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1192Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1188Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1199Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1199Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1197Epoch 2/15: [==============================] 75/75 batches, loss: 0.1205
[2025-05-07 13:31:50,698][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1205
[2025-05-07 13:31:50,936][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0950, Metrics: {'mse': 0.09494469314813614, 'rmse': 0.3081309675253952, 'r2': -0.9507045745849609}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1807Epoch 3/15: [                              ] 2/75 batches, loss: 0.1412Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1316Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1149Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0999Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0994Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1012Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0947Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0904Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0895Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0865Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0841Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0901Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0920Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0895Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0905Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0915Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0894Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0895Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0882Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0879Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0895Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0877Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0934Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0927Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0911Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0904Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0895Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0901Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0897Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0895Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0891Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0882Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0901Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0885Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0870Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0858Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0855Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0847Epoch 3/15: [================              ] 40/75 batches, loss: 0.0867Epoch 3/15: [================              ] 41/75 batches, loss: 0.0875Epoch 3/15: [================              ] 42/75 batches, loss: 0.0865Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0857Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0857Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0869Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0865Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0874Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0881Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0889Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0879Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0874Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0873Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0864Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0869Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0859Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0850Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0841Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0845Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0838Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0831Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0823Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0823Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0827Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0833Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0828Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0826Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0828Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0821Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0819Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0820Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0813Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0808Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0805Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0808Epoch 3/15: [==============================] 75/75 batches, loss: 0.0814
[2025-05-07 13:31:53,661][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0814
[2025-05-07 13:31:53,903][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0769, Metrics: {'mse': 0.07686012983322144, 'rmse': 0.27723659540764356, 'r2': -0.5791447162628174}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0663Epoch 4/15: [                              ] 2/75 batches, loss: 0.0819Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0663Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0785Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0719Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0719Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0701Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0685Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0735Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0721Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0724Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0723Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0694Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0680Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0697Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0714Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0727Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0734Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0725Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0725Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0718Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0728Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0732Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0732Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0744Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0745Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0742Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0736Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0734Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0724Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0727Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0719Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0711Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0731Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0727Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0720Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0715Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0718Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0707Epoch 4/15: [================              ] 40/75 batches, loss: 0.0700Epoch 4/15: [================              ] 41/75 batches, loss: 0.0705Epoch 4/15: [================              ] 42/75 batches, loss: 0.0704Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0708Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0699Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0696Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0692Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0688Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0689Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0689Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0690Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0685Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0684Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0683Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0682Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0677Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0680Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0673Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0667Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0666Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0667Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0669Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0663Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0666Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0663Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0661Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0655Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0650Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0657Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0658Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0653Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0648Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0644Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0640Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0646Epoch 4/15: [==============================] 75/75 batches, loss: 0.0647
[2025-05-07 13:31:56,557][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0647
[2025-05-07 13:31:56,785][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0673, Metrics: {'mse': 0.06731240451335907, 'rmse': 0.25944634226243984, 'r2': -0.38297998905181885}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1642Epoch 5/15: [                              ] 2/75 batches, loss: 0.1045Epoch 5/15: [=                             ] 3/75 batches, loss: 0.1050Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0951Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0815Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0765Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0774Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0733Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0692Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0671Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0666Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0652Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0664Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0659Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0659Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0644Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0634Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0638Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0633Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0618Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0666Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0656Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0658Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0648Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0694Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0710Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0706Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0702Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0692Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0676Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0665Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0660Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0657Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0663Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0666Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0656Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0652Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0649Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0643Epoch 5/15: [================              ] 40/75 batches, loss: 0.0640Epoch 5/15: [================              ] 41/75 batches, loss: 0.0640Epoch 5/15: [================              ] 42/75 batches, loss: 0.0633Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0638Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0630Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0624Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0622Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0614Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0612Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0618Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0618Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0610Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0606Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0603Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0598Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0592Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0588Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0585Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0579Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0577Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0574Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0572Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0575Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0574Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0583Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0581Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0583Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0578Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0575Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0572Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0572Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0574Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0577Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0573Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0570Epoch 5/15: [==============================] 75/75 batches, loss: 0.0565
[2025-05-07 13:31:59,433][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0565
[2025-05-07 13:31:59,672][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0620, Metrics: {'mse': 0.061933405697345734, 'rmse': 0.24886423145431272, 'r2': -0.2724646329879761}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0286Epoch 6/15: [                              ] 2/75 batches, loss: 0.0494Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0472Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0415Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0450Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0454Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0420Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0450Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0422Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0460Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0455Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0457Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0472Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0465Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0479Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0474Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0469Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0519Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0500Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0489Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0479Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0470Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0469Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0478Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0480Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0473Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0472Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0464Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0464Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0458Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0460Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0454Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0453Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0447Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0448Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0442Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0437Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0434Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0457Epoch 6/15: [================              ] 40/75 batches, loss: 0.0459Epoch 6/15: [================              ] 41/75 batches, loss: 0.0456Epoch 6/15: [================              ] 42/75 batches, loss: 0.0451Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0446Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0450Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0454Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0449Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0456Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0456Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0457Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0456Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0466Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0474Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0474Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0468Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0469Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0466Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0464Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0474Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0468Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0468Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0469Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0468Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0478Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0483Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0485Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0480Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0476Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0478Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0475Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0471Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0473Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0469Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0465Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0464Epoch 6/15: [==============================] 75/75 batches, loss: 0.0462
[2025-05-07 13:32:02,357][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0462
[2025-05-07 13:32:02,623][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0674, Metrics: {'mse': 0.06741121411323547, 'rmse': 0.2596366963917764, 'r2': -0.3850100040435791}
[2025-05-07 13:32:02,623][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0344Epoch 7/15: [                              ] 2/75 batches, loss: 0.0344Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0300Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0348Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0360Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0364Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0400Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0383Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0399Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0445Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0435Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0455Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0440Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0454Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0441Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0432Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0433Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0427Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0413Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0403Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0411Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0412Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0420Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0427Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0422Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0435Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0434Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0433Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0427Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0418Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0419Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0412Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0413Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0414Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0407Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0403Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0412Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0420Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0414Epoch 7/15: [================              ] 40/75 batches, loss: 0.0411Epoch 7/15: [================              ] 41/75 batches, loss: 0.0408Epoch 7/15: [================              ] 42/75 batches, loss: 0.0408Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0426Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0423Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0419Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0421Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0416Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0413Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0413Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0412Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0414Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0414Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0412Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0411Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0411Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0408Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0406Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0409Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0407Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0407Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0407Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0409Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0408Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0408Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0407Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0403Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0402Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0400Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0400Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0404Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0401Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0401Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0398Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0395Epoch 7/15: [==============================] 75/75 batches, loss: 0.0395
[2025-05-07 13:32:04,898][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0395
[2025-05-07 13:32:05,184][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0662, Metrics: {'mse': 0.06615360081195831, 'rmse': 0.2572034230175763, 'r2': -0.35917162895202637}
[2025-05-07 13:32:05,185][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0270Epoch 8/15: [                              ] 2/75 batches, loss: 0.0237Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0222Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0239Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0215Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0254Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0245Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0248Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0257Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0327Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0313Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0303Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0313Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0302Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0302Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0297Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0294Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0289Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0304Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0300Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0296Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0308Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0300Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0298Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0299Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0298Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0300Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0302Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0298Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0305Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0308Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0312Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0312Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0319Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0314Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0314Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0311Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0310Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0313Epoch 8/15: [================              ] 40/75 batches, loss: 0.0318Epoch 8/15: [================              ] 41/75 batches, loss: 0.0320Epoch 8/15: [================              ] 42/75 batches, loss: 0.0318Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0318Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0319Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0318Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0316Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0315Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0316Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0317Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0320Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0318Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0324Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0321Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0321Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0328Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0326Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0325Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0321Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0322Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0324Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0322Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0326Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0333Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0336Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0334Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0332Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0337Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0335Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0335Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0332Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0333Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0330Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0332Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0334Epoch 8/15: [==============================] 75/75 batches, loss: 0.0352
[2025-05-07 13:32:07,474][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0352
[2025-05-07 13:32:07,752][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0688, Metrics: {'mse': 0.0687461569905281, 'rmse': 0.26219488360860155, 'r2': -0.41243743896484375}
[2025-05-07 13:32:07,753][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0200Epoch 9/15: [                              ] 2/75 batches, loss: 0.0199Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0231Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0234Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0222Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0217Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0220Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0271Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0249Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0290Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0282Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0271Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0258Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0257Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0248Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0248Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0255Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0257Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0261Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0271Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0277Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0276Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0282Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0288Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0283Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0291Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0289Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0285Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0284Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0285Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0282Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0287Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0283Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0289Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0285Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0281Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0280Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0288Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0288Epoch 9/15: [================              ] 40/75 batches, loss: 0.0289Epoch 9/15: [================              ] 41/75 batches, loss: 0.0285Epoch 9/15: [================              ] 42/75 batches, loss: 0.0283Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0283Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0280Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0277Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0279Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0277Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0278Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0287Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0293Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0300Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0301Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0299Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0296Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0296Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0293Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0291Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0293Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0308Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0305Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0305Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0309Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0307Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0308Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0307Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0308Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0312Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0309Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0311Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0312Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0311Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0308Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0306Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0304Epoch 9/15: [==============================] 75/75 batches, loss: 0.0303
[2025-05-07 13:32:10,099][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0303
[2025-05-07 13:32:10,344][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0669, Metrics: {'mse': 0.06693453341722488, 'rmse': 0.2587170914671562, 'r2': -0.37521636486053467}
[2025-05-07 13:32:10,344][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:32:10,344][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 13:32:10,345][src.training.lm_trainer][INFO] - Training completed in 25.83 seconds
[2025-05-07 13:32:10,345][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:32:13,444][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.014125035144388676, 'rmse': 0.11884879109350956, 'r2': -0.014199376106262207}
[2025-05-07 13:32:13,445][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.061933405697345734, 'rmse': 0.24886423145431272, 'r2': -0.2724646329879761}
[2025-05-07 13:32:13,445][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06172771379351616, 'rmse': 0.24845062647036364, 'r2': -0.39046335220336914}
[2025-05-07 13:32:15,114][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/fi/fi/model.pt
[2025-05-07 13:32:15,115][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▃▂▁
wandb:     best_val_mse █▆▃▂▁
wandb:      best_val_r2 ▁▃▆▇█
wandb:    best_val_rmse █▇▄▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▄▅▆▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▃▂▁▂▂▂▂
wandb:          val_mse █▆▃▂▁▂▂▂▂
wandb:           val_r2 ▁▃▆▇█▇▇▇▇
wandb:         val_rmse █▇▄▂▁▂▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06197
wandb:     best_val_mse 0.06193
wandb:      best_val_r2 -0.27246
wandb:    best_val_rmse 0.24886
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.06173
wandb:    final_test_r2 -0.39046
wandb:  final_test_rmse 0.24845
wandb:  final_train_mse 0.01413
wandb:   final_train_r2 -0.0142
wandb: final_train_rmse 0.11885
wandb:    final_val_mse 0.06193
wandb:     final_val_r2 -0.27246
wandb:   final_val_rmse 0.24886
wandb:    learning_rate 0.0001
wandb:       train_loss 0.03028
wandb:       train_time 25.83489
wandb:         val_loss 0.06694
wandb:          val_mse 0.06693
wandb:           val_r2 -0.37522
wandb:         val_rmse 0.25872
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133126-yizacqdu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133126-yizacqdu/logs
Experiment probe_layer2_avg_subordinate_chain_len_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer2/fi/fi/results.json for layer 2
=======================
PROBING LAYER 10 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer10_avg_links_len_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_links_len_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer10/fi"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:32:44,058][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer10/fi
experiment_name: probe_layer10_avg_links_len_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:32:44,059][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:32:44,059][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:32:44,059][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:32:44,059][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:32:44,063][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:32:44,063][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:32:44,063][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:32:47,331][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:32:49,763][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:32:49,763][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:32:49,978][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,050][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,251][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:32:50,262][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:32:50,263][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:32:50,264][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:32:50,334][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,483][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,496][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:32:50,498][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:32:50,499][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:32:50,499][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:32:50,547][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,631][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:32:50,655][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:32:50,657][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:32:50,657][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:32:50,658][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:32:50,659][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:32:50,659][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:32:50,659][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:32:50,660][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:32:50,660][src.data.datasets][INFO] -   Mean: 0.1395, Std: 0.0869
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Sample label: 0.2280000001192093
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:32:50,660][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:32:50,661][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5520
[2025-05-07 13:32:50,661][src.data.datasets][INFO] -   Mean: 0.2101, Std: 0.1311
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Sample label: 0.36800000071525574
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:32:50,661][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:32:50,661][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6740
[2025-05-07 13:32:50,662][src.data.datasets][INFO] -   Mean: 0.2318, Std: 0.1347
[2025-05-07 13:32:50,662][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:32:50,662][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 13:32:50,662][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:32:50,662][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:32:50,662][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:32:50,662][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 13:32:50,663][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:32:57,955][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:32:57,956][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:32:57,956][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:32:57,956][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:32:57,959][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:32:57,959][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:32:57,959][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:32:57,960][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:32:57,960][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:32:57,960][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:32:57,961][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6558Epoch 1/15: [                              ] 2/75 batches, loss: 0.8136Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7844Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7524Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6977Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6162Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5941Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5918Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5601Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5653Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5405Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5160Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5030Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4897Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4737Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4629Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4648Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4587Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4486Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4470Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4517Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4535Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4439Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4361Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4309Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4249Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4287Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4293Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4222Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4201Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4140Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4139Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4066Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4044Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4002Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4023Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3978Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3943Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3898Epoch 1/15: [================              ] 40/75 batches, loss: 0.3900Epoch 1/15: [================              ] 41/75 batches, loss: 0.3886Epoch 1/15: [================              ] 42/75 batches, loss: 0.3860Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3823Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3802Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3796Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3737Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3713Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3669Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3627Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3606Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3573Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3545Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3522Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3517Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3501Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3476Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3449Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3442Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3411Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3375Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3340Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3308Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3276Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3263Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3241Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3227Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3195Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3166Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3162Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3134Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3102Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3079Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3059Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3029Epoch 1/15: [==============================] 75/75 batches, loss: 0.3006
[2025-05-07 13:33:04,375][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3006
[2025-05-07 13:33:04,592][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0360, Metrics: {'mse': 0.03608979657292366, 'rmse': 0.1899731469785234, 'r2': -1.0983636379241943}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1729Epoch 2/15: [                              ] 2/75 batches, loss: 0.1640Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1646Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1805Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1746Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1566Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1596Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1655Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1563Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1600Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1613Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1631Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1582Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1562Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1543Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1509Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1483Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1512Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1526Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1490Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1501Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1488Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1497Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1474Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1498Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1496Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1564Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1551Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1563Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1569Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1558Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1565Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1542Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1573Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1574Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1555Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1524Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1513Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1505Epoch 2/15: [================              ] 40/75 batches, loss: 0.1483Epoch 2/15: [================              ] 41/75 batches, loss: 0.1479Epoch 2/15: [================              ] 42/75 batches, loss: 0.1458Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1452Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1443Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1441Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1456Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1452Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1448Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1460Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1445Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1434Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1430Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1429Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1422Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1435Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1441Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1425Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1414Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1403Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1391Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1381Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1367Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1360Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1352Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1357Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1351Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1353Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1344Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1329Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1316Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1315Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1305Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1304Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1298Epoch 2/15: [==============================] 75/75 batches, loss: 0.1285
[2025-05-07 13:33:07,303][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1285
[2025-05-07 13:33:07,554][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0593, Metrics: {'mse': 0.05938461795449257, 'rmse': 0.2436895934472635, 'r2': -2.452791213989258}
[2025-05-07 13:33:07,555][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1276Epoch 3/15: [                              ] 2/75 batches, loss: 0.0954Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1141Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0994Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0917Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0897Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0850Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0849Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0860Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0899Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0868Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0845Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0856Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0886Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0889Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0872Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0884Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0890Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0864Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0854Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0847Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0837Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0840Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0846Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0846Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0848Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0843Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0853Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0858Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0846Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0836Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0850Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0842Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0836Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0826Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0828Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0821Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0828Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0824Epoch 3/15: [================              ] 40/75 batches, loss: 0.0817Epoch 3/15: [================              ] 41/75 batches, loss: 0.0833Epoch 3/15: [================              ] 42/75 batches, loss: 0.0832Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0833Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0825Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0814Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0811Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0815Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0821Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0817Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0812Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0809Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0807Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0804Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0801Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0805Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0802Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0802Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0804Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0809Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0817Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0814Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0812Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0806Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0804Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0802Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0799Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0797Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0802Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0804Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0802Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0797Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0796Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0796Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0795Epoch 3/15: [==============================] 75/75 batches, loss: 0.0800
[2025-05-07 13:33:09,887][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0800
[2025-05-07 13:33:10,146][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0330, Metrics: {'mse': 0.03304008021950722, 'rmse': 0.18176930494312624, 'r2': -0.9210445880889893}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0773Epoch 4/15: [                              ] 2/75 batches, loss: 0.0895Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0739Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0802Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0800Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0842Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0865Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0853Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0879Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0812Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0818Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0816Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0782Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0780Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0786Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0783Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0790Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0801Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0781Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0785Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0816Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0799Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0800Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0813Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0798Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0805Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0799Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0798Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0783Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0772Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0764Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0762Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0759Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0757Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0762Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0752Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0748Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0737Epoch 4/15: [================              ] 40/75 batches, loss: 0.0739Epoch 4/15: [================              ] 41/75 batches, loss: 0.0738Epoch 4/15: [================              ] 42/75 batches, loss: 0.0730Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0729Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0722Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0720Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0722Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0720Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0720Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0731Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0731Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0732Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0732Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0725Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0726Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0723Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0726Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0716Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0710Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0707Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0702Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0707Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0701Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0700Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0698Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0698Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0692Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0686Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0692Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0694Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0695Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0690Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0687Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0683Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0679Epoch 4/15: [==============================] 75/75 batches, loss: 0.0673
[2025-05-07 13:33:12,876][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0673
[2025-05-07 13:33:13,134][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0322, Metrics: {'mse': 0.03227080777287483, 'rmse': 0.17964077424926345, 'r2': -0.8763166666030884}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0646Epoch 5/15: [                              ] 2/75 batches, loss: 0.0596Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0624Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0602Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0529Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0564Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0591Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0634Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0618Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0595Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0578Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0555Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0522Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0535Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0527Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0515Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0514Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0510Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0528Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0525Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0520Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0505Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0511Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0518Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0520Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0513Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0529Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0537Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0534Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0535Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0536Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0542Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0545Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0543Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0552Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0556Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0554Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0548Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0549Epoch 5/15: [================              ] 40/75 batches, loss: 0.0552Epoch 5/15: [================              ] 41/75 batches, loss: 0.0552Epoch 5/15: [================              ] 42/75 batches, loss: 0.0551Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0557Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0554Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0550Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0552Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0548Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0546Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0546Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0540Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0534Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0528Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0525Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0523Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0517Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0519Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0517Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0515Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0513Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0509Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0505Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0504Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0507Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0503Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0501Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0497Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0497Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0495Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0491Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0492Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0491Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0499Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0495Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0491Epoch 5/15: [==============================] 75/75 batches, loss: 0.0487
[2025-05-07 13:33:15,813][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0487
[2025-05-07 13:33:16,091][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0226, Metrics: {'mse': 0.022669879719614983, 'rmse': 0.1505652008918893, 'r2': -0.31809139251708984}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0668Epoch 6/15: [                              ] 2/75 batches, loss: 0.0658Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0712Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0624Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0559Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0556Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0537Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0501Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0479Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0481Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0473Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0455Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0439Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0445Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0464Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0454Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0452Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0467Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0455Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0468Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0460Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0466Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0467Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0463Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0480Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0488Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0490Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0487Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0475Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0471Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0462Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0457Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0455Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0449Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0451Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0445Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0438Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0434Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0427Epoch 6/15: [================              ] 40/75 batches, loss: 0.0436Epoch 6/15: [================              ] 41/75 batches, loss: 0.0436Epoch 6/15: [================              ] 42/75 batches, loss: 0.0443Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0438Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0442Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0441Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0438Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0439Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0438Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0437Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0445Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0444Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0442Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0437Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0436Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0433Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0442Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0444Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0441Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0442Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0439Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0445Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0443Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0440Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0443Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0450Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0447Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0445Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0444Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0445Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0441Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0443Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0444Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0440Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0439Epoch 6/15: [==============================] 75/75 batches, loss: 0.0437
[2025-05-07 13:33:18,764][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0437
[2025-05-07 13:33:19,015][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0276, Metrics: {'mse': 0.02769957110285759, 'rmse': 0.16643188126935773, 'r2': -0.6105320453643799}
[2025-05-07 13:33:19,016][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0252Epoch 7/15: [                              ] 2/75 batches, loss: 0.0328Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0342Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0367Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0350Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0346Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0349Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0359Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0361Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0376Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0394Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0410Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0417Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0439Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0437Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0444Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0445Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0442Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0443Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0434Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0442Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0440Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0430Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0430Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0427Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0426Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0423Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0423Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0422Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0419Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0423Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0416Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0416Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0413Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0406Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0402Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0403Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0400Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0396Epoch 7/15: [================              ] 40/75 batches, loss: 0.0391Epoch 7/15: [================              ] 41/75 batches, loss: 0.0393Epoch 7/15: [================              ] 42/75 batches, loss: 0.0388Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0385Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0386Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0384Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0386Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0385Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0383Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0386Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0385Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0388Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0392Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0391Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0390Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0391Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0393Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0393Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0395Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0393Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0393Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0392Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0392Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0392Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0391Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0390Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0388Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0387Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0383Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0383Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0380Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0381Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0378Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0377Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0376Epoch 7/15: [==============================] 75/75 batches, loss: 0.0374
[2025-05-07 13:33:21,407][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0374
[2025-05-07 13:33:21,644][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0323, Metrics: {'mse': 0.03242481127381325, 'rmse': 0.18006890701565678, 'r2': -0.8852709531784058}
[2025-05-07 13:33:21,645][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0300Epoch 8/15: [                              ] 2/75 batches, loss: 0.0287Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0334Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0414Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0478Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0496Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0457Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0422Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0404Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0392Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0379Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0366Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0372Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0366Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0351Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0346Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0360Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0358Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0353Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0347Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0337Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0340Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0338Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0340Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0342Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0344Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0340Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0340Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0348Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0340Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0336Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0330Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0326Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0324Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0323Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0324Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0322Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0323Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0319Epoch 8/15: [================              ] 40/75 batches, loss: 0.0318Epoch 8/15: [================              ] 41/75 batches, loss: 0.0315Epoch 8/15: [================              ] 42/75 batches, loss: 0.0313Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0314Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0312Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0311Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0313Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0313Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0315Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0313Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0312Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0310Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0308Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0306Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0303Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0301Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0299Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0300Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0298Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0300Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0299Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0300Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0297Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0299Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0300Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0299Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0298Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0297Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0298Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0299Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0297Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0297Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0296Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0298Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0298Epoch 8/15: [==============================] 75/75 batches, loss: 0.0302
[2025-05-07 13:33:23,931][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0302
[2025-05-07 13:33:24,167][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0294, Metrics: {'mse': 0.02943871170282364, 'rmse': 0.1715771304772977, 'r2': -0.7116506099700928}
[2025-05-07 13:33:24,167][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0796Epoch 9/15: [                              ] 2/75 batches, loss: 0.0519Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0433Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0428Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0427Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0397Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0435Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0402Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0403Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0387Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0378Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0375Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0372Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0355Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0352Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0352Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0351Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0354Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0350Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0349Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0341Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0340Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0341Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0333Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0332Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0329Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0321Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0318Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0315Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0315Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0312Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0306Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0307Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0307Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0308Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0305Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0307Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0304Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0309Epoch 9/15: [================              ] 40/75 batches, loss: 0.0309Epoch 9/15: [================              ] 41/75 batches, loss: 0.0309Epoch 9/15: [================              ] 42/75 batches, loss: 0.0314Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0315Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0317Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0317Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0314Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0313Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0313Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0312Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0311Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0307Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0304Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0304Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0304Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0305Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0302Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0300Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0298Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0296Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0294Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0293Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0294Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0297Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0298Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0297Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0299Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0300Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0299Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0296Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0297Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0296Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0295Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0294Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0291Epoch 9/15: [==============================] 75/75 batches, loss: 0.0289
[2025-05-07 13:33:26,455][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0289
[2025-05-07 13:33:26,692][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0323, Metrics: {'mse': 0.03239027410745621, 'rmse': 0.17997298160406247, 'r2': -0.8832628726959229}
[2025-05-07 13:33:26,694][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:33:26,694][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-07 13:33:26,695][src.training.lm_trainer][INFO] - Training completed in 25.46 seconds
[2025-05-07 13:33:26,695][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:33:29,658][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.006764571648091078, 'rmse': 0.08224701604369047, 'r2': 0.10472553968429565}
[2025-05-07 13:33:29,660][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.022669879719614983, 'rmse': 0.1505652008918893, 'r2': -0.31809139251708984}
[2025-05-07 13:33:29,660][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.02877366915345192, 'rmse': 0.16962803174431967, 'r2': -0.5865224599838257}
[2025-05-07 13:33:31,314][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer10/fi/fi/model.pt
[2025-05-07 13:33:31,315][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▆▁
wandb:     best_val_mse █▆▆▁
wandb:      best_val_r2 ▁▃▃█
wandb:    best_val_rmse █▇▆▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▆▆▇▆▆▆
wandb:       train_loss █▄▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▄█▃▃▁▂▃▂▃
wandb:          val_mse ▄█▃▃▁▂▃▂▃
wandb:           val_r2 ▅▁▆▆█▇▆▇▆
wandb:         val_rmse ▄█▃▃▁▂▃▃▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02264
wandb:     best_val_mse 0.02267
wandb:      best_val_r2 -0.31809
wandb:    best_val_rmse 0.15057
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.02877
wandb:    final_test_r2 -0.58652
wandb:  final_test_rmse 0.16963
wandb:  final_train_mse 0.00676
wandb:   final_train_r2 0.10473
wandb: final_train_rmse 0.08225
wandb:    final_val_mse 0.02267
wandb:     final_val_r2 -0.31809
wandb:   final_val_rmse 0.15057
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02888
wandb:       train_time 25.46072
wandb:         val_loss 0.03231
wandb:          val_mse 0.03239
wandb:           val_r2 -0.88326
wandb:         val_rmse 0.17997
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133244-ux0otbo5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133244-ux0otbo5/logs
Experiment probe_layer10_avg_links_len_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_max_depth_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_max_depth_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer10/fi"         "wandb.mode=offline" "experiment.submetric=avg_max_depth"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:33:58,181][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer10/fi
experiment_name: probe_layer10_avg_max_depth_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:33:58,181][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:33:58,181][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:33:58,181][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:33:58,181][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:33:58,186][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:33:58,186][__main__][INFO] - Using submetric: avg_max_depth
[2025-05-07 13:33:58,186][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:34:01,495][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:34:03,809][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:34:03,809][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:34:04,031][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,091][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,444][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:34:04,452][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:34:04,453][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:34:04,465][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:34:04,522][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,633][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,648][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:34:04,649][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:34:04,649][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:34:04,661][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:34:04,747][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,868][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:34:04,892][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:34:04,893][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:34:04,893][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:34:04,894][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:34:04,894][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:34:04,894][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:34:04,894][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:34:04,895][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:34:04,895][src.data.datasets][INFO] -   Mean: 0.1934, Std: 0.1761
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Sample label: 0.25
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:34:04,895][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:34:04,896][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-05-07 13:34:04,896][src.data.datasets][INFO] -   Mean: 0.2487, Std: 0.2137
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Sample label: 0.5
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-05-07 13:34:04,896][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-05-07 13:34:04,896][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-05-07 13:34:04,897][src.data.datasets][INFO] -   Mean: 0.1785, Std: 0.1746
[2025-05-07 13:34:04,897][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:34:04,897][src.data.datasets][INFO] - Sample label: 0.125
[2025-05-07 13:34:04,897][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:34:04,897][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:34:04,897][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:34:04,897][__main__][INFO] - Using model type: lm_probe for submetric avg_max_depth
[2025-05-07 13:34:04,898][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:34:12,464][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:34:12,465][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:34:12,465][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:34:12,465][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:34:12,468][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:34:12,468][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:34:12,468][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:34:12,469][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:34:12,469][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:34:12,469][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:34:12,470][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5773Epoch 1/15: [                              ] 2/75 batches, loss: 0.8456Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8187Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8239Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7393Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6512Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6279Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6345Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5973Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5971Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5658Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5372Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5228Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5145Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4979Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4856Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4867Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4801Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4695Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4665Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4764Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4754Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4673Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4613Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4572Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4491Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4545Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4537Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4428Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4412Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4366Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4367Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4291Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4283Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4254Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4303Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4244Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4227Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4198Epoch 1/15: [================              ] 40/75 batches, loss: 0.4192Epoch 1/15: [================              ] 41/75 batches, loss: 0.4187Epoch 1/15: [================              ] 42/75 batches, loss: 0.4159Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4112Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4088Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4084Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4016Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3975Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3941Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3897Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3876Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3847Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3822Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3803Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3819Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3798Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3761Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3733Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3717Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3715Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3680Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3638Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3605Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3576Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3555Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3529Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3506Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3475Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3447Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3431Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3406Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3377Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3362Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3337Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3300Epoch 1/15: [==============================] 75/75 batches, loss: 0.3282
[2025-05-07 13:34:18,911][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3282
[2025-05-07 13:34:19,195][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0316, Metrics: {'mse': 0.031558744609355927, 'rmse': 0.17764781059544732, 'r2': 0.3091159462928772}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1812Epoch 2/15: [                              ] 2/75 batches, loss: 0.1986Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1981Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2092Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2056Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1858Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1853Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1894Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1753Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1799Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1807Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1759Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1753Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1756Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1733Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1696Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1661Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1662Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1679Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1636Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1632Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1616Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1611Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1582Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1603Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1603Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1683Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1669Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1680Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1704Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1714Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1719Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1700Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1729Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1727Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1706Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1678Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1673Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1685Epoch 2/15: [================              ] 40/75 batches, loss: 0.1667Epoch 2/15: [================              ] 41/75 batches, loss: 0.1654Epoch 2/15: [================              ] 42/75 batches, loss: 0.1631Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1615Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1615Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1631Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1650Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1645Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1634Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1655Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1646Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1638Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1634Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1625Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1614Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1627Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1640Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1623Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1613Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1597Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1586Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1578Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1564Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1559Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1545Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1554Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1548Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1548Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1544Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1529Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1517Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1516Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1507Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1512Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1506Epoch 2/15: [==============================] 75/75 batches, loss: 0.1498
[2025-05-07 13:34:21,936][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1498
[2025-05-07 13:34:22,173][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0469, Metrics: {'mse': 0.04708630591630936, 'rmse': 0.21699379234510224, 'r2': -0.03081333637237549}
[2025-05-07 13:34:22,173][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1620Epoch 3/15: [                              ] 2/75 batches, loss: 0.1344Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1408Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1184Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1065Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1065Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1056Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1034Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1027Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1086Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1036Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1039Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1092Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1118Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1141Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1113Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1106Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1127Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1096Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1090Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1086Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1069Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1060Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1066Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1067Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1067Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1057Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1058Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1081Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1070Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1067Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1090Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1070Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1070Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1057Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1062Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1053Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1057Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1037Epoch 3/15: [================              ] 40/75 batches, loss: 0.1034Epoch 3/15: [================              ] 41/75 batches, loss: 0.1049Epoch 3/15: [================              ] 42/75 batches, loss: 0.1038Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1040Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1035Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1022Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1015Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1024Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1038Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1036Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1029Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1023Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1016Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1010Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1006Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1009Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1004Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1003Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1007Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1005Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0995Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0990Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0987Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0986Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0982Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0977Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0976Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0973Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0981Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0985Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0985Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0983Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0981Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0983Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0989Epoch 3/15: [==============================] 75/75 batches, loss: 0.0990
[2025-05-07 13:34:24,521][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0990
[2025-05-07 13:34:24,793][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0417, Metrics: {'mse': 0.041801948100328445, 'rmse': 0.2044552471821852, 'r2': 0.08487182855606079}
[2025-05-07 13:34:24,794][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0918Epoch 4/15: [                              ] 2/75 batches, loss: 0.1147Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0900Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1019Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0969Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1072Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1029Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1028Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1052Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0993Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0974Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0975Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0937Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0902Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0935Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0947Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0935Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0962Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0982Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0963Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0963Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0994Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0979Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0984Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0989Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0977Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0987Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0980Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0975Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0965Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0958Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0957Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0945Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0945Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0952Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0957Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0943Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0944Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0931Epoch 4/15: [================              ] 40/75 batches, loss: 0.0929Epoch 4/15: [================              ] 41/75 batches, loss: 0.0926Epoch 4/15: [================              ] 42/75 batches, loss: 0.0918Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0916Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0904Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0894Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0896Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0897Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0904Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0902Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0899Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0896Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0895Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0892Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0890Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0884Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0885Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0875Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0868Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0867Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0866Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0869Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0863Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0858Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0858Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0856Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0854Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0851Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0866Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0871Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0867Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0859Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0855Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0856Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0850Epoch 4/15: [==============================] 75/75 batches, loss: 0.0850
[2025-05-07 13:34:27,059][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0850
[2025-05-07 13:34:27,296][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0371, Metrics: {'mse': 0.03715765103697777, 'rmse': 0.19276319938457592, 'r2': 0.18654471635818481}
[2025-05-07 13:34:27,296][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0979Epoch 5/15: [                              ] 2/75 batches, loss: 0.0793Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0841Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0807Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0695Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0726Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0748Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0761Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0748Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0714Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0692Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0681Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0647Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0664Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0660Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0647Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0645Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0640Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0652Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0652Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0682Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0675Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0691Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0694Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0698Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0703Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0710Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0719Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0719Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0717Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0713Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0723Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0723Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0729Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0748Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0746Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0747Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0743Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0741Epoch 5/15: [================              ] 40/75 batches, loss: 0.0741Epoch 5/15: [================              ] 41/75 batches, loss: 0.0733Epoch 5/15: [================              ] 42/75 batches, loss: 0.0729Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0739Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0733Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0729Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0730Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0732Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0726Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0730Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0726Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0721Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0717Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0714Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0710Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0705Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0708Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0703Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0701Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0702Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0695Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0697Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0698Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0692Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0691Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0694Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0688Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0688Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0689Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0684Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0683Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0685Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0695Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0694Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0690Epoch 5/15: [==============================] 75/75 batches, loss: 0.0685
[2025-05-07 13:34:29,580][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0685
[2025-05-07 13:34:29,811][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0324, Metrics: {'mse': 0.032381776720285416, 'rmse': 0.17994937265877148, 'r2': 0.2910981774330139}
[2025-05-07 13:34:29,811][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:34:29,812][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:34:29,812][src.training.lm_trainer][INFO] - Training completed in 14.00 seconds
[2025-05-07 13:34:29,812][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:34:32,842][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03257208317518234, 'rmse': 0.18047737579869214, 'r2': -0.05045938491821289}
[2025-05-07 13:34:32,843][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.031558744609355927, 'rmse': 0.17764781059544732, 'r2': 0.3091159462928772}
[2025-05-07 13:34:32,843][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03178423270583153, 'rmse': 0.1782813302222965, 'r2': -0.04234158992767334}
[2025-05-07 13:34:34,469][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer10/fi/fi/model.pt
[2025-05-07 13:34:34,470][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▄
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▆▄▁
wandb:          val_mse ▁█▆▄▁
wandb:           val_r2 █▁▃▅█
wandb:         val_rmse ▁█▆▄▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03158
wandb:     best_val_mse 0.03156
wandb:      best_val_r2 0.30912
wandb:    best_val_rmse 0.17765
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.03178
wandb:    final_test_r2 -0.04234
wandb:  final_test_rmse 0.17828
wandb:  final_train_mse 0.03257
wandb:   final_train_r2 -0.05046
wandb: final_train_rmse 0.18048
wandb:    final_val_mse 0.03156
wandb:     final_val_r2 0.30912
wandb:   final_val_rmse 0.17765
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06845
wandb:       train_time 13.99859
wandb:         val_loss 0.0324
wandb:          val_mse 0.03238
wandb:           val_r2 0.2911
wandb:         val_rmse 0.17995
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133358-ytf3spuf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133358-ytf3spuf/logs
Experiment probe_layer10_avg_max_depth_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_max_depth/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_avg_subordinate_chain_len_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_avg_subordinate_chain_len_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer10/fi"         "wandb.mode=offline" "experiment.submetric=avg_subordinate_chain_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:34:59,991][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer10/fi
experiment_name: probe_layer10_avg_subordinate_chain_len_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:34:59,991][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:34:59,991][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:34:59,992][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:34:59,992][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:34:59,997][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:34:59,997][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-05-07 13:34:59,997][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:35:03,371][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:35:05,886][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:35:05,886][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:35:06,170][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,236][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,425][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:35:06,433][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:35:06,434][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:35:06,435][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:35:06,571][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,707][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,719][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:35:06,720][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:35:06,721][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:35:06,722][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:35:06,772][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,901][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:35:06,927][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:35:06,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:35:06,929][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:35:06,930][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:35:06,930][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:35:06,930][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:35:06,930][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:35:06,931][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:35:06,931][src.data.datasets][INFO] -   Mean: 0.0268, Std: 0.1180
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:35:06,931][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:35:06,932][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:35:06,932][src.data.datasets][INFO] -   Mean: 0.1217, Std: 0.2206
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-05-07 13:35:06,932][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-05-07 13:35:06,932][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:35:06,933][src.data.datasets][INFO] -   Mean: 0.1272, Std: 0.2107
[2025-05-07 13:35:06,933][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:35:06,933][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-07 13:35:06,933][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:35:06,933][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:35:06,933][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:35:06,933][__main__][INFO] - Using model type: lm_probe for submetric avg_subordinate_chain_len
[2025-05-07 13:35:06,934][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:35:14,244][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:35:14,245][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:35:14,245][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-07 13:35:14,245][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:35:14,248][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:35:14,249][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:35:14,249][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:35:14,249][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:35:14,249][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:35:14,250][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:35:14,250][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5735Epoch 1/15: [                              ] 2/75 batches, loss: 0.7757Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7466Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7529Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7031Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6304Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6021Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5991Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5679Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5786Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5536Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5268Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5109Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4982Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4821Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4709Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4714Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4647Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4539Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4506Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4551Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4548Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4441Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4346Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4282Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4222Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4262Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4260Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4202Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4171Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4137Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4138Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4059Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4026Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3998Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4018Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3965Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3943Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3898Epoch 1/15: [================              ] 40/75 batches, loss: 0.3878Epoch 1/15: [================              ] 41/75 batches, loss: 0.3857Epoch 1/15: [================              ] 42/75 batches, loss: 0.3835Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3794Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3772Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3765Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3708Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3679Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3638Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3600Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3575Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3547Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3516Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3495Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3510Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3493Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3468Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3436Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3435Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3425Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3393Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3357Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3332Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3306Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3288Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3272Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3249Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3217Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3189Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3187Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3161Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3135Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3115Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3092Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3061Epoch 1/15: [==============================] 75/75 batches, loss: 0.3030
[2025-05-07 13:35:20,801][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3030
[2025-05-07 13:35:21,083][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0535, Metrics: {'mse': 0.053501781076192856, 'rmse': 0.23130452022429837, 'r2': -0.09923112392425537}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1740Epoch 2/15: [                              ] 2/75 batches, loss: 0.1626Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1840Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2010Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2029Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1813Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1762Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1778Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1662Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1673Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1699Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1668Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1627Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1588Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1571Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1536Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1522Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1532Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1557Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1514Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1512Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1514Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1523Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1487Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1501Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1502Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1590Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1574Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1595Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1641Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1623Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1618Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1588Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1626Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1631Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1604Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1575Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1557Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1556Epoch 2/15: [================              ] 40/75 batches, loss: 0.1554Epoch 2/15: [================              ] 41/75 batches, loss: 0.1545Epoch 2/15: [================              ] 42/75 batches, loss: 0.1524Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1514Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1506Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1499Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1514Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1504Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1498Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1506Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1494Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1483Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1472Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1471Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1463Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1465Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1472Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1456Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1443Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1432Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1420Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1409Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1395Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1390Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1384Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1390Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1381Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1380Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1372Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1357Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1351Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1352Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1349Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1346Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1342Epoch 2/15: [==============================] 75/75 batches, loss: 0.1337
[2025-05-07 13:35:23,834][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1337
[2025-05-07 13:35:24,067][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0801, Metrics: {'mse': 0.08014573156833649, 'rmse': 0.2831002147090964, 'r2': -0.6466494798660278}
[2025-05-07 13:35:24,068][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1718Epoch 3/15: [                              ] 2/75 batches, loss: 0.1361Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1541Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1281Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1181Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1102Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1063Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1063Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1022Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1041Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1000Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0969Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1006Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1019Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1012Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0997Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1013Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1011Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0976Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0959Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0939Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0929Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0919Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0933Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0923Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0912Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0906Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0905Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0917Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0905Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0892Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0913Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0899Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0897Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0885Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0881Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0868Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0877Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0864Epoch 3/15: [================              ] 40/75 batches, loss: 0.0874Epoch 3/15: [================              ] 41/75 batches, loss: 0.0899Epoch 3/15: [================              ] 42/75 batches, loss: 0.0894Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0895Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0886Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0880Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0882Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0887Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0889Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0883Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0874Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0870Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0870Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0867Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0861Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0858Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0854Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0855Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0864Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0867Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0867Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0862Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0860Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0855Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0853Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0849Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0851Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0849Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0852Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0860Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0854Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0848Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0846Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0845Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0848Epoch 3/15: [==============================] 75/75 batches, loss: 0.0849
[2025-05-07 13:35:26,349][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0849
[2025-05-07 13:35:26,604][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0688, Metrics: {'mse': 0.06883895397186279, 'rmse': 0.26237178577709686, 'r2': -0.4143439531326294}
[2025-05-07 13:35:26,605][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0622Epoch 4/15: [                              ] 2/75 batches, loss: 0.0908Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0747Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0885Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0838Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0840Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0856Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0856Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0917Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0889Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0892Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0884Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0851Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0813Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0893Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0938Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0943Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0935Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0942Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0928Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0922Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0937Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0918Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0918Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0919Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0900Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0900Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0898Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0891Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0875Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0857Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0850Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0845Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0867Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0854Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0859Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0842Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0838Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0826Epoch 4/15: [================              ] 40/75 batches, loss: 0.0818Epoch 4/15: [================              ] 41/75 batches, loss: 0.0826Epoch 4/15: [================              ] 42/75 batches, loss: 0.0821Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0815Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0803Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0801Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0806Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0804Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0804Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0808Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0821Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0819Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0815Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0806Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0804Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0799Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0799Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0789Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0782Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0786Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0784Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0785Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0779Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0780Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0776Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0773Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0765Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0757Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0767Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0770Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0767Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0760Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0755Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0748Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0745Epoch 4/15: [==============================] 75/75 batches, loss: 0.0746
[2025-05-07 13:35:28,895][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0746
[2025-05-07 13:35:29,160][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0734, Metrics: {'mse': 0.07346292585134506, 'rmse': 0.27104045058135706, 'r2': -0.5093467235565186}
[2025-05-07 13:35:29,161][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0715Epoch 5/15: [                              ] 2/75 batches, loss: 0.0621Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0706Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0665Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0581Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0607Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0622Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0647Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0626Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0603Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0592Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0568Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0558Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0554Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0541Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0536Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0531Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0529Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0548Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0541Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0581Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0566Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0572Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0579Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0605Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0599Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0603Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0611Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0602Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0599Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0594Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0595Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0590Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0593Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0596Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0596Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0597Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0590Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0587Epoch 5/15: [================              ] 40/75 batches, loss: 0.0595Epoch 5/15: [================              ] 41/75 batches, loss: 0.0599Epoch 5/15: [================              ] 42/75 batches, loss: 0.0597Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0604Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0597Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0589Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0589Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0585Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0586Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0587Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0584Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0577Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0572Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0570Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0565Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0558Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0559Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0557Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0552Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0556Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0552Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0551Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0555Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0554Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0563Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0563Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0564Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0563Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0563Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0558Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0557Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0554Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0569Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0567Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0563Epoch 5/15: [==============================] 75/75 batches, loss: 0.0558
[2025-05-07 13:35:31,474][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0558
[2025-05-07 13:35:31,719][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0552, Metrics: {'mse': 0.05516975745558739, 'rmse': 0.23488243326308458, 'r2': -0.1335008144378662}
[2025-05-07 13:35:31,720][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:35:31,720][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-07 13:35:31,720][src.training.lm_trainer][INFO] - Training completed in 14.07 seconds
[2025-05-07 13:35:31,720][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:35:34,664][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02331506833434105, 'rmse': 0.15269272521748065, 'r2': -0.6740578413009644}
[2025-05-07 13:35:34,665][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.053501781076192856, 'rmse': 0.23130452022429837, 'r2': -0.09923112392425537}
[2025-05-07 13:35:34,665][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07287804037332535, 'rmse': 0.26995933096176794, 'r2': -0.6416327953338623}
[2025-05-07 13:35:36,569][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer10/fi/fi/model.pt
[2025-05-07 13:35:36,570][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▃▂
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▅▆▁
wandb:          val_mse ▁█▅▆▁
wandb:           val_r2 █▁▄▃█
wandb:         val_rmse ▁█▅▆▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05345
wandb:     best_val_mse 0.0535
wandb:      best_val_r2 -0.09923
wandb:    best_val_rmse 0.2313
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.07288
wandb:    final_test_r2 -0.64163
wandb:  final_test_rmse 0.26996
wandb:  final_train_mse 0.02332
wandb:   final_train_r2 -0.67406
wandb: final_train_rmse 0.15269
wandb:    final_val_mse 0.0535
wandb:     final_val_r2 -0.09923
wandb:   final_val_rmse 0.2313
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05575
wandb:       train_time 14.06794
wandb:         val_loss 0.05517
wandb:          val_mse 0.05517
wandb:           val_r2 -0.1335
wandb:         val_rmse 0.23488
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133500-vascvo6r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133500-vascvo6r/logs
Experiment probe_layer10_avg_subordinate_chain_len_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_subordinate_chain_len/layer10/fi/fi/results.json for layer 10
Running control submetric probing experiments...
=======================
PROBING LAYER 2 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer2_avg_links_len_control1_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control1_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:36:03,340][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/fi
experiment_name: probe_layer2_avg_links_len_control1_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:36:03,340][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:36:03,340][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:36:03,341][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:36:03,341][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:36:03,391][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:36:03,391][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:36:03,391][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:36:06,372][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:36:08,820][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:36:08,820][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:36:08,999][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 13:36:09,108][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-05-07 13:36:09,369][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:36:09,377][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:36:09,378][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:36:09,379][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:36:09,419][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:36:09,492][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:36:09,506][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:36:09,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:36:09,508][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:36:09,508][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:36:09,572][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:36:09,710][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:36:09,748][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:36:09,750][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:36:09,750][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:36:09,751][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:36:09,751][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:36:09,751][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:36:09,751][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:36:09,752][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:36:09,752][src.data.datasets][INFO] -   Mean: 0.1395, Std: 0.0869
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Sample label: 0.1899999976158142
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:36:09,752][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:36:09,753][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5520
[2025-05-07 13:36:09,753][src.data.datasets][INFO] -   Mean: 0.2101, Std: 0.1311
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Sample label: 0.36800000071525574
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:36:09,753][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:36:09,753][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6740
[2025-05-07 13:36:09,754][src.data.datasets][INFO] -   Mean: 0.2318, Std: 0.1347
[2025-05-07 13:36:09,754][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:36:09,754][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 13:36:09,754][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:36:09,754][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:36:09,754][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:36:09,754][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 13:36:09,755][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:36:16,928][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:36:16,929][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:36:16,929][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:36:16,929][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:36:16,932][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:36:16,933][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:36:16,933][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:36:16,933][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:36:16,933][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:36:16,934][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:36:16,934][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6570Epoch 1/15: [                              ] 2/75 batches, loss: 0.6468Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5527Epoch 1/15: [=                             ] 4/75 batches, loss: 0.5052Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4787Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4456Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4245Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4693Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4838Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4626Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4495Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4454Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4274Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4284Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4216Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4370Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4247Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4221Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4188Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4138Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4215Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4222Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4173Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4038Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3972Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3901Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3830Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3783Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3778Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3789Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3721Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3674Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3636Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3600Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3561Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3560Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3515Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3476Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3447Epoch 1/15: [================              ] 40/75 batches, loss: 0.3401Epoch 1/15: [================              ] 41/75 batches, loss: 0.3369Epoch 1/15: [================              ] 42/75 batches, loss: 0.3334Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3299Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3284Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3295Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3250Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3255Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3211Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3183Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3160Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3168Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3147Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3111Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3111Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3092Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3072Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3086Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3080Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3060Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3041Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3004Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2992Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2975Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2956Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2942Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2927Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2903Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2872Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2876Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2864Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2838Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2821Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2792Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2766Epoch 1/15: [==============================] 75/75 batches, loss: 0.2747
[2025-05-07 13:36:23,312][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2747
[2025-05-07 13:36:23,587][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0685, Metrics: {'mse': 0.06844429671764374, 'rmse': 0.2616186092724364, 'r2': -2.9795467853546143}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1642Epoch 2/15: [                              ] 2/75 batches, loss: 0.1319Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1063Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1216Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1147Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1172Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1239Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1195Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1229Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1317Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1278Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1240Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1231Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1177Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1198Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1191Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1193Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1179Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1162Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1169Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1161Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1182Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1190Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1167Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1175Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1214Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1211Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1199Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1217Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1229Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1215Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1200Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1191Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1209Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1220Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1206Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1207Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1196Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1202Epoch 2/15: [================              ] 40/75 batches, loss: 0.1200Epoch 2/15: [================              ] 41/75 batches, loss: 0.1195Epoch 2/15: [================              ] 42/75 batches, loss: 0.1190Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1181Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1163Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1157Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1174Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1170Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1166Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1190Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1177Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1170Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1170Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1172Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1172Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1164Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1174Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1167Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1158Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1149Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1139Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1135Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1134Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1128Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1125Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1119Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1119Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1115Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1114Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1112Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1103Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1100Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1105Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1117Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1109Epoch 2/15: [==============================] 75/75 batches, loss: 0.1103
[2025-05-07 13:36:26,355][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1103
[2025-05-07 13:36:26,614][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0705, Metrics: {'mse': 0.07050350308418274, 'rmse': 0.2655249575542434, 'r2': -3.0992746353149414}
[2025-05-07 13:36:26,615][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1202Epoch 3/15: [                              ] 2/75 batches, loss: 0.0904Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0856Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0791Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0760Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0803Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0800Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0743Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0741Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0761Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0748Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0753Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0782Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0794Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0772Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0784Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0800Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0786Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0778Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0779Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0782Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0773Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0809Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0796Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0785Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0780Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0780Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0788Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0779Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0782Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0779Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0779Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0781Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0774Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0765Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0757Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0754Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0751Epoch 3/15: [================              ] 40/75 batches, loss: 0.0753Epoch 3/15: [================              ] 41/75 batches, loss: 0.0754Epoch 3/15: [================              ] 42/75 batches, loss: 0.0753Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0755Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0755Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0751Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0748Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0760Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0765Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0771Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0762Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0753Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0752Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0745Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0755Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0748Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0751Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0744Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0745Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0740Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0733Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0730Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0731Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0738Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0743Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0740Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0734Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0735Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0731Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0730Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0736Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0735Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0732Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0727Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0729Epoch 3/15: [==============================] 75/75 batches, loss: 0.0738
[2025-05-07 13:36:28,926][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0738
[2025-05-07 13:36:29,176][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0472, Metrics: {'mse': 0.047198738902807236, 'rmse': 0.21725270746945188, 'r2': -1.7442693710327148}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0764Epoch 4/15: [                              ] 2/75 batches, loss: 0.0726Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0631Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0782Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0696Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0703Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0634Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0619Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0632Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0597Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0626Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0638Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0611Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0605Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0640Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0645Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0679Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0666Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0657Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0643Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0647Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0663Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0670Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0668Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0695Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0691Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0693Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0687Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0692Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0684Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0687Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0680Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0672Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0664Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0660Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0655Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0646Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0653Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0646Epoch 4/15: [================              ] 40/75 batches, loss: 0.0642Epoch 4/15: [================              ] 41/75 batches, loss: 0.0649Epoch 4/15: [================              ] 42/75 batches, loss: 0.0644Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0650Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0643Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0640Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0633Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0628Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0627Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0629Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0624Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0622Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0623Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0621Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0623Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0618Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0624Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0618Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0616Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0615Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0614Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0616Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0611Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0607Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0605Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0605Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0601Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0595Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0597Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0595Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0593Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0589Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0587Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0584Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0590Epoch 4/15: [==============================] 75/75 batches, loss: 0.0590
[2025-05-07 13:36:31,876][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0590
[2025-05-07 13:36:32,126][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0357, Metrics: {'mse': 0.03572910279035568, 'rmse': 0.18902143473785105, 'r2': -1.0773918628692627}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0956Epoch 5/15: [                              ] 2/75 batches, loss: 0.0782Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0750Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0697Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0604Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0570Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0588Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0558Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0525Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0507Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0511Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0525Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0521Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0525Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0547Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0544Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0540Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0548Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0536Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0528Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0539Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0527Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0535Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0531Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0550Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0560Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0579Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0579Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0574Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0557Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0548Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0551Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0546Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0542Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0545Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0540Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0535Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0537Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0534Epoch 5/15: [================              ] 40/75 batches, loss: 0.0532Epoch 5/15: [================              ] 41/75 batches, loss: 0.0532Epoch 5/15: [================              ] 42/75 batches, loss: 0.0527Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0523Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0518Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0516Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0517Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0514Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0512Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0525Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0522Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0519Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0514Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0513Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0511Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0508Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0507Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0504Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0499Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0494Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0493Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0493Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0495Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0496Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0498Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0496Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0494Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0492Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0490Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0490Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0492Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0490Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0488Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0486Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0485Epoch 5/15: [==============================] 75/75 batches, loss: 0.0481
[2025-05-07 13:36:34,775][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0481
[2025-05-07 13:36:35,092][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0363, Metrics: {'mse': 0.03629286587238312, 'rmse': 0.19050686568305908, 'r2': -1.110170841217041}
[2025-05-07 13:36:35,093][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0444Epoch 6/15: [                              ] 2/75 batches, loss: 0.0429Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0397Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0381Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0431Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0445Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0405Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0405Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0384Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0390Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0397Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0381Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0393Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0389Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0423Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0428Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0411Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0426Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0417Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0416Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0413Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0406Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0414Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0420Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0422Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0431Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0436Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0426Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0419Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0411Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0409Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0407Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0403Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0402Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0405Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0398Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0392Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0390Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0412Epoch 6/15: [================              ] 40/75 batches, loss: 0.0407Epoch 6/15: [================              ] 41/75 batches, loss: 0.0406Epoch 6/15: [================              ] 42/75 batches, loss: 0.0405Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0401Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0404Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0413Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0410Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0412Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0408Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0405Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0402Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0406Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0410Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0409Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0407Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0404Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0400Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0398Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0398Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0397Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0400Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0402Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0401Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0403Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0403Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0405Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0403Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0401Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0402Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0403Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0407Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0407Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0404Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0401Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0401Epoch 6/15: [==============================] 75/75 batches, loss: 0.0401
[2025-05-07 13:36:37,396][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0401
[2025-05-07 13:36:37,639][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0312, Metrics: {'mse': 0.031265296041965485, 'rmse': 0.17681995374381673, 'r2': -0.817853569984436}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0344Epoch 7/15: [                              ] 2/75 batches, loss: 0.0341Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0326Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0375Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0379Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0378Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0399Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0395Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0408Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0423Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0425Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0426Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0417Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0411Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0404Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0398Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0411Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0404Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0390Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0381Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0375Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0367Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0360Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0364Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0362Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0363Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0368Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0368Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0366Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0359Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0358Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0354Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0352Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0348Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0343Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0338Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0347Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0345Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0344Epoch 7/15: [================              ] 40/75 batches, loss: 0.0343Epoch 7/15: [================              ] 41/75 batches, loss: 0.0342Epoch 7/15: [================              ] 42/75 batches, loss: 0.0346Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0345Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0344Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0342Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0340Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0350Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0349Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0349Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0347Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0347Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0348Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0347Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0347Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0351Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0348Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0345Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0347Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0345Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0340Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0340Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0340Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0339Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0337Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0335Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0332Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0331Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0331Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0329Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0327Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0326Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0324Epoch 7/15: [==============================] 75/75 batches, loss: 0.0323
[2025-05-07 13:36:40,374][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0323
[2025-05-07 13:36:40,639][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0327, Metrics: {'mse': 0.032732147723436356, 'rmse': 0.18092028002254573, 'r2': -0.9031403064727783}
[2025-05-07 13:36:40,640][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0243Epoch 8/15: [                              ] 2/75 batches, loss: 0.0256Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0252Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0247Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0239Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0293Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0281Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0281Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0307Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0299Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0298Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0291Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0288Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0285Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0278Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0274Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0270Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0267Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0273Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0270Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0275Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0291Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0288Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0289Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0292Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0283Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0285Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0289Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0286Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0283Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0281Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0283Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0285Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0285Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0283Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0279Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0279Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0281Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0284Epoch 8/15: [================              ] 40/75 batches, loss: 0.0287Epoch 8/15: [================              ] 41/75 batches, loss: 0.0288Epoch 8/15: [================              ] 42/75 batches, loss: 0.0294Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0294Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0293Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0292Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0290Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0290Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0289Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0286Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0283Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0285Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0286Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0286Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0283Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0288Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0288Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0289Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0288Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0288Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0291Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0290Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0291Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0291Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0292Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0290Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0288Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0294Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0295Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0296Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0294Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0294Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0292Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0291Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0290Epoch 8/15: [==============================] 75/75 batches, loss: 0.0293
[2025-05-07 13:36:42,936][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0293
[2025-05-07 13:36:43,182][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0341, Metrics: {'mse': 0.03413090109825134, 'rmse': 0.18474550359413716, 'r2': -0.9844679832458496}
[2025-05-07 13:36:43,183][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0348Epoch 9/15: [                              ] 2/75 batches, loss: 0.0351Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0290Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0263Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0258Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0241Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0239Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0235Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0230Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0218Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0224Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0232Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0221Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0219Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0220Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0226Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0237Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0242Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0256Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0258Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0260Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0255Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0256Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0271Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0268Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0266Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0259Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0258Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0256Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0261Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0260Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0258Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0256Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0264Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0260Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0257Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0254Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0254Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0253Epoch 9/15: [================              ] 40/75 batches, loss: 0.0249Epoch 9/15: [================              ] 41/75 batches, loss: 0.0247Epoch 9/15: [================              ] 42/75 batches, loss: 0.0245Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0246Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0244Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0244Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0242Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0244Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0244Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0245Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0243Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0244Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0244Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0243Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0242Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0241Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0242Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0241Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0241Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0240Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0240Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0238Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0242Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0242Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0241Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0240Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0241Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0240Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0238Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0239Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0237Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0238Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0236Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0236Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0235Epoch 9/15: [==============================] 75/75 batches, loss: 0.0233
[2025-05-07 13:36:45,476][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0233
[2025-05-07 13:36:45,737][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0374, Metrics: {'mse': 0.03742919862270355, 'rmse': 0.19346627257148352, 'r2': -1.1762404441833496}
[2025-05-07 13:36:45,738][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.0308Epoch 10/15: [                              ] 2/75 batches, loss: 0.0213Epoch 10/15: [=                             ] 3/75 batches, loss: 0.0200Epoch 10/15: [=                             ] 4/75 batches, loss: 0.0219Epoch 10/15: [==                            ] 5/75 batches, loss: 0.0229Epoch 10/15: [==                            ] 6/75 batches, loss: 0.0291Epoch 10/15: [==                            ] 7/75 batches, loss: 0.0277Epoch 10/15: [===                           ] 8/75 batches, loss: 0.0269Epoch 10/15: [===                           ] 9/75 batches, loss: 0.0263Epoch 10/15: [====                          ] 10/75 batches, loss: 0.0258Epoch 10/15: [====                          ] 11/75 batches, loss: 0.0253Epoch 10/15: [====                          ] 12/75 batches, loss: 0.0239Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.0244Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.0252Epoch 10/15: [======                        ] 15/75 batches, loss: 0.0259Epoch 10/15: [======                        ] 16/75 batches, loss: 0.0260Epoch 10/15: [======                        ] 17/75 batches, loss: 0.0253Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.0255Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.0250Epoch 10/15: [========                      ] 20/75 batches, loss: 0.0250Epoch 10/15: [========                      ] 21/75 batches, loss: 0.0249Epoch 10/15: [========                      ] 22/75 batches, loss: 0.0245Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.0246Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.0241Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.0240Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.0239Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.0244Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.0243Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.0248Epoch 10/15: [============                  ] 30/75 batches, loss: 0.0252Epoch 10/15: [============                  ] 31/75 batches, loss: 0.0268Epoch 10/15: [============                  ] 32/75 batches, loss: 0.0264Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.0266Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.0269Epoch 10/15: [==============                ] 35/75 batches, loss: 0.0270Epoch 10/15: [==============                ] 36/75 batches, loss: 0.0270Epoch 10/15: [==============                ] 37/75 batches, loss: 0.0265Epoch 10/15: [===============               ] 38/75 batches, loss: 0.0261Epoch 10/15: [===============               ] 39/75 batches, loss: 0.0263Epoch 10/15: [================              ] 40/75 batches, loss: 0.0265Epoch 10/15: [================              ] 41/75 batches, loss: 0.0269Epoch 10/15: [================              ] 42/75 batches, loss: 0.0266Epoch 10/15: [=================             ] 43/75 batches, loss: 0.0265Epoch 10/15: [=================             ] 44/75 batches, loss: 0.0268Epoch 10/15: [==================            ] 45/75 batches, loss: 0.0267Epoch 10/15: [==================            ] 46/75 batches, loss: 0.0264Epoch 10/15: [==================            ] 47/75 batches, loss: 0.0260Epoch 10/15: [===================           ] 48/75 batches, loss: 0.0257Epoch 10/15: [===================           ] 49/75 batches, loss: 0.0255Epoch 10/15: [====================          ] 50/75 batches, loss: 0.0256Epoch 10/15: [====================          ] 51/75 batches, loss: 0.0256Epoch 10/15: [====================          ] 52/75 batches, loss: 0.0255Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.0257Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.0259Epoch 10/15: [======================        ] 55/75 batches, loss: 0.0258Epoch 10/15: [======================        ] 56/75 batches, loss: 0.0257Epoch 10/15: [======================        ] 57/75 batches, loss: 0.0256Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.0255Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.0253Epoch 10/15: [========================      ] 60/75 batches, loss: 0.0252Epoch 10/15: [========================      ] 61/75 batches, loss: 0.0251Epoch 10/15: [========================      ] 62/75 batches, loss: 0.0252Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.0251Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.0251Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.0251Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.0251Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.0249Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.0250Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.0250Epoch 10/15: [============================  ] 70/75 batches, loss: 0.0251Epoch 10/15: [============================  ] 71/75 batches, loss: 0.0250Epoch 10/15: [============================  ] 72/75 batches, loss: 0.0253Epoch 10/15: [============================= ] 73/75 batches, loss: 0.0252Epoch 10/15: [============================= ] 74/75 batches, loss: 0.0253Epoch 10/15: [==============================] 75/75 batches, loss: 0.0251
[2025-05-07 13:36:48,042][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0251
[2025-05-07 13:36:48,333][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0401, Metrics: {'mse': 0.0401158332824707, 'rmse': 0.20028937386309514, 'r2': -1.332448959350586}
[2025-05-07 13:36:48,334][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:36:48,334][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-07 13:36:48,334][src.training.lm_trainer][INFO] - Training completed in 28.10 seconds
[2025-05-07 13:36:48,334][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:36:51,342][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.008984330110251904, 'rmse': 0.0947857062549618, 'r2': -0.18905413150787354}
[2025-05-07 13:36:51,343][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.031265296041965485, 'rmse': 0.17681995374381673, 'r2': -0.817853569984436}
[2025-05-07 13:36:51,343][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.038926199078559875, 'rmse': 0.19729723535457833, 'r2': -1.1463124752044678}
[2025-05-07 13:36:53,014][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/fi/fi/model.pt
[2025-05-07 13:36:53,015][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁
wandb:     best_val_mse █▄▂▁
wandb:      best_val_r2 ▁▅▇█
wandb:    best_val_rmse █▄▂▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▅▆▆▇▆▆▆
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▄▂▂▁▁▂▂▃
wandb:          val_mse ██▄▂▂▁▁▂▂▃
wandb:           val_r2 ▁▁▅▇▇██▇▇▆
wandb:         val_rmse ██▄▂▂▁▁▂▂▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03122
wandb:     best_val_mse 0.03127
wandb:      best_val_r2 -0.81785
wandb:    best_val_rmse 0.17682
wandb: early_stop_epoch 10
wandb:            epoch 10
wandb:   final_test_mse 0.03893
wandb:    final_test_r2 -1.14631
wandb:  final_test_rmse 0.1973
wandb:  final_train_mse 0.00898
wandb:   final_train_r2 -0.18905
wandb: final_train_rmse 0.09479
wandb:    final_val_mse 0.03127
wandb:     final_val_r2 -0.81785
wandb:   final_val_rmse 0.17682
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02509
wandb:       train_time 28.09784
wandb:         val_loss 0.04006
wandb:          val_mse 0.04012
wandb:           val_r2 -1.33245
wandb:         val_rmse 0.20029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133603-dsw4x7g4
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133603-dsw4x7g4/logs
Experiment probe_layer2_avg_links_len_control1_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control1/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:37:21,013][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/fi
experiment_name: probe_layer2_avg_links_len_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:37:21,013][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:37:21,014][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:37:21,014][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:37:21,014][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:37:21,018][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:37:21,018][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:37:21,018][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:37:24,325][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:37:26,684][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:37:26,685][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:37:26,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 13:37:26,893][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-05-07 13:37:27,178][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:37:27,186][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:37:27,187][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:37:27,198][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:37:27,298][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:37:27,419][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:37:27,488][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:37:27,489][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:37:27,489][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:37:27,491][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:37:27,628][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:37:27,776][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:37:27,842][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:37:27,843][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:37:27,844][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:37:27,844][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:37:27,845][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:37:27,845][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:37:27,846][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:37:27,846][src.data.datasets][INFO] -   Mean: 0.1395, Std: 0.0869
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Sample label: 0.5490000247955322
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:37:27,846][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:37:27,847][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5520
[2025-05-07 13:37:27,847][src.data.datasets][INFO] -   Mean: 0.2101, Std: 0.1311
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Sample label: 0.36800000071525574
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:37:27,847][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:37:27,847][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6740
[2025-05-07 13:37:27,848][src.data.datasets][INFO] -   Mean: 0.2318, Std: 0.1347
[2025-05-07 13:37:27,848][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:37:27,848][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 13:37:27,848][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:37:27,848][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:37:27,848][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:37:27,848][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 13:37:27,849][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-07 13:37:35,243][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-07 13:37:35,245][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-07 13:37:35,245][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=2, freeze_model=True
[2025-05-07 13:37:35,245][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-07 13:37:35,248][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-07 13:37:35,248][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-07 13:37:35,248][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-07 13:37:35,248][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-07 13:37:35,249][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-07 13:37:35,249][__main__][INFO] - Total parameters: 394,255,105
[2025-05-07 13:37:35,250][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5912Epoch 1/15: [                              ] 2/75 batches, loss: 0.6129Epoch 1/15: [=                             ] 3/75 batches, loss: 0.5253Epoch 1/15: [=                             ] 4/75 batches, loss: 0.4721Epoch 1/15: [==                            ] 5/75 batches, loss: 0.4641Epoch 1/15: [==                            ] 6/75 batches, loss: 0.4270Epoch 1/15: [==                            ] 7/75 batches, loss: 0.4032Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4406Epoch 1/15: [===                           ] 9/75 batches, loss: 0.4499Epoch 1/15: [====                          ] 10/75 batches, loss: 0.4329Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4265Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4209Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4044Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4099Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4050Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4199Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4085Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4075Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4059Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4020Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4110Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4129Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4066Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3944Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3878Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3811Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3733Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3695Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3679Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3678Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3607Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3555Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3511Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3482Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3454Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3455Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3406Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3360Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3352Epoch 1/15: [================              ] 40/75 batches, loss: 0.3303Epoch 1/15: [================              ] 41/75 batches, loss: 0.3262Epoch 1/15: [================              ] 42/75 batches, loss: 0.3226Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3191Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3175Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3199Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3160Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3189Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3148Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3125Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3107Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3103Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3083Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3049Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3062Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3050Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3033Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3047Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3037Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3016Epoch 1/15: [========================      ] 60/75 batches, loss: 0.2996Epoch 1/15: [========================      ] 61/75 batches, loss: 0.2960Epoch 1/15: [========================      ] 62/75 batches, loss: 0.2955Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.2941Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2931Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2913Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2899Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2876Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2842Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2850Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2841Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2817Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2798Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2771Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2747Epoch 1/15: [==============================] 75/75 batches, loss: 0.2731
[2025-05-07 13:37:41,547][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2731
[2025-05-07 13:37:41,747][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0751, Metrics: {'mse': 0.07502835988998413, 'rmse': 0.27391305169703783, 'r2': -3.362362861633301}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1962Epoch 2/15: [                              ] 2/75 batches, loss: 0.1564Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1202Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1352Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1286Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1237Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1331Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1300Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1275Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1326Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1295Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1246Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1240Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1200Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1225Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1243Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1225Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1199Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1166Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1173Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1159Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1191Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1195Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1174Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1169Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1210Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1231Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1228Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1254Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1269Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1259Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1246Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1230Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1247Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1261Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1244Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1244Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1233Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1226Epoch 2/15: [================              ] 40/75 batches, loss: 0.1214Epoch 2/15: [================              ] 41/75 batches, loss: 0.1201Epoch 2/15: [================              ] 42/75 batches, loss: 0.1195Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1187Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1167Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1152Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1181Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1177Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1173Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1196Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1183Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1179Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1180Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1180Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1180Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1174Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1187Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1179Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1174Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1167Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1165Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1163Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1162Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1156Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1150Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1143Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1141Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1139Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1138Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1135Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1128Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1125Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1130Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1131Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1126Epoch 2/15: [==============================] 75/75 batches, loss: 0.1122
[2025-05-07 13:37:44,470][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1122
[2025-05-07 13:37:44,733][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0668, Metrics: {'mse': 0.06682777404785156, 'rmse': 0.2585106845912787, 'r2': -2.8855574131011963}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1007Epoch 3/15: [                              ] 2/75 batches, loss: 0.0747Epoch 3/15: [=                             ] 3/75 batches, loss: 0.0789Epoch 3/15: [=                             ] 4/75 batches, loss: 0.0760Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0720Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0761Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0734Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0687Epoch 3/15: [===                           ] 9/75 batches, loss: 0.0671Epoch 3/15: [====                          ] 10/75 batches, loss: 0.0694Epoch 3/15: [====                          ] 11/75 batches, loss: 0.0653Epoch 3/15: [====                          ] 12/75 batches, loss: 0.0671Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.0715Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.0740Epoch 3/15: [======                        ] 15/75 batches, loss: 0.0742Epoch 3/15: [======                        ] 16/75 batches, loss: 0.0741Epoch 3/15: [======                        ] 17/75 batches, loss: 0.0796Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.0781Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.0781Epoch 3/15: [========                      ] 20/75 batches, loss: 0.0775Epoch 3/15: [========                      ] 21/75 batches, loss: 0.0781Epoch 3/15: [========                      ] 22/75 batches, loss: 0.0799Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.0782Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.0824Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.0820Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.0803Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.0794Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.0793Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.0804Epoch 3/15: [============                  ] 30/75 batches, loss: 0.0797Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0795Epoch 3/15: [============                  ] 32/75 batches, loss: 0.0796Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.0794Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.0799Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0786Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0777Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0768Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0763Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0760Epoch 3/15: [================              ] 40/75 batches, loss: 0.0763Epoch 3/15: [================              ] 41/75 batches, loss: 0.0765Epoch 3/15: [================              ] 42/75 batches, loss: 0.0757Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0753Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0759Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0761Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0759Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0769Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0777Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0780Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0773Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0770Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0768Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0760Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0772Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0765Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0761Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0754Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0748Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0744Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0737Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0731Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0732Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0738Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0747Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0744Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0738Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0739Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0734Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0736Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0739Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0735Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0730Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0727Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0726Epoch 3/15: [==============================] 75/75 batches, loss: 0.0732
[2025-05-07 13:37:47,480][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0732
[2025-05-07 13:37:47,730][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0421, Metrics: {'mse': 0.0421181246638298, 'rmse': 0.2052270076374691, 'r2': -1.4488680362701416}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0615Epoch 4/15: [                              ] 2/75 batches, loss: 0.0502Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0461Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0594Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0567Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0610Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0594Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0605Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0654Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0619Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0631Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0647Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0613Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0603Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0632Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0618Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0604Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0609Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0606Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0607Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0602Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0617Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0623Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0628Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0647Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0649Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0648Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0646Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0648Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0638Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0646Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0638Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0628Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0623Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0623Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0636Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0633Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0636Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0630Epoch 4/15: [================              ] 40/75 batches, loss: 0.0627Epoch 4/15: [================              ] 41/75 batches, loss: 0.0632Epoch 4/15: [================              ] 42/75 batches, loss: 0.0628Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0635Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0628Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0627Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0621Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0616Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0614Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0615Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0609Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0610Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0613Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0611Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0613Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0608Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0613Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0608Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0607Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0605Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0605Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0608Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0602Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0598Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0596Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0600Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0595Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0593Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0594Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0595Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0594Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0592Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0588Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0587Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0589Epoch 4/15: [==============================] 75/75 batches, loss: 0.0590
[2025-05-07 13:37:50,400][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0590
[2025-05-07 13:37:50,663][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0312, Metrics: {'mse': 0.03127161040902138, 'rmse': 0.17683780820011702, 'r2': -0.8182204961776733}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0834Epoch 5/15: [                              ] 2/75 batches, loss: 0.0722Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0752Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0760Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0703Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0632Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0634Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0619Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0596Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0579Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0579Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0574Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0562Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0566Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0575Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0555Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0558Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0563Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0553Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0541Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0544Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0538Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0533Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0527Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0545Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0545Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0543Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0548Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0545Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0532Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0524Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0531Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0529Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0531Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0540Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0534Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0533Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0532Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0534Epoch 5/15: [================              ] 40/75 batches, loss: 0.0537Epoch 5/15: [================              ] 41/75 batches, loss: 0.0541Epoch 5/15: [================              ] 42/75 batches, loss: 0.0533Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0543Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0538Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0534Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0533Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0526Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0524Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0535Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0531Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0527Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0524Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0519Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0517Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0516Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0516Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0515Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0511Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0507Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0505Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0501Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0502Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0502Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0502Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0499Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0496Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0496Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0494Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0491Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0491Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0492Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0490Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0487Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0486Epoch 5/15: [==============================] 75/75 batches, loss: 0.0483
[2025-05-07 13:37:53,333][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0483
[2025-05-07 13:37:53,603][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0365, Metrics: {'mse': 0.036574505269527435, 'rmse': 0.19124462154405136, 'r2': -1.1265459060668945}
[2025-05-07 13:37:53,604][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0154Epoch 6/15: [                              ] 2/75 batches, loss: 0.0282Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0291Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0272Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0335Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0319Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0311Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0351Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0327Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0336Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0352Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0351Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0376Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0370Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0387Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0396Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0383Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0404Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0395Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0390Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0390Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0381Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0391Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0402Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0414Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0408Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0407Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0396Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0390Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0389Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0387Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0386Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0388Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0385Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0392Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0387Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0381Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0376Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0402Epoch 6/15: [================              ] 40/75 batches, loss: 0.0401Epoch 6/15: [================              ] 41/75 batches, loss: 0.0400Epoch 6/15: [================              ] 42/75 batches, loss: 0.0397Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0394Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0394Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0402Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0398Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0398Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0395Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0391Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0390Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0395Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0394Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0392Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0388Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0386Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0384Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0381Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0381Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0385Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0389Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0392Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0392Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0391Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0390Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0392Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0390Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0389Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0391Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0390Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0387Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0388Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0383Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0382Epoch 6/15: [==============================] 75/75 batches, loss: 0.0382
[2025-05-07 13:37:55,904][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0382
[2025-05-07 13:37:56,169][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0361, Metrics: {'mse': 0.03610594570636749, 'rmse': 0.19001564595150447, 'r2': -1.0993027687072754}
[2025-05-07 13:37:56,170][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0262Epoch 7/15: [                              ] 2/75 batches, loss: 0.0269Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0259Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0307Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0395Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0393Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0398Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0376Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0376Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0381Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0377Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0374Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0367Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0358Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0365Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0363Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0361Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0367Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0358Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0349Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0355Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0352Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0349Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0353Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0351Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0353Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0361Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0357Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0357Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0351Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0348Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0344Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0340Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0345Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0341Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0338Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0341Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0345Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0342Epoch 7/15: [================              ] 40/75 batches, loss: 0.0342Epoch 7/15: [================              ] 41/75 batches, loss: 0.0342Epoch 7/15: [================              ] 42/75 batches, loss: 0.0343Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0343Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0341Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0339Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0340Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0338Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0339Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0336Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0334Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0331Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0332Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0335Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0336Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0339Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0338Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0339Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0343Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0343Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0344Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0341Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0338Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0342Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0344Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0342Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0340Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0341Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0338Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0339Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0340Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0338Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0338Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0336Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0334Epoch 7/15: [==============================] 75/75 batches, loss: 0.0335
[2025-05-07 13:37:58,501][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0335
[2025-05-07 13:37:58,761][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0330, Metrics: {'mse': 0.033092860132455826, 'rmse': 0.18191443079771277, 'r2': -0.9241131544113159}
[2025-05-07 13:37:58,762][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0501Epoch 8/15: [                              ] 2/75 batches, loss: 0.0352Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0322Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0305Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0277Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0292Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0299Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0295Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0315Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0319Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0316Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0303Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0305Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0302Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0301Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0298Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0293Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0296Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0292Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0291Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0284Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0289Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0284Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0290Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0295Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0292Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0294Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0294Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0290Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0291Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0291Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0292Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0293Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0293Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0292Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0289Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0286Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0288Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0287Epoch 8/15: [================              ] 40/75 batches, loss: 0.0290Epoch 8/15: [================              ] 41/75 batches, loss: 0.0288Epoch 8/15: [================              ] 42/75 batches, loss: 0.0289Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0288Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0286Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0289Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0291Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0293Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0290Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0297Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0296Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0296Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0299Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0295Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0293Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0293Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0294Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0294Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0291Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0292Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0292Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0290Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0293Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0293Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0297Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0296Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0295Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0294Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0294Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0293Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0290Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0290Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0287Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0287Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0289Epoch 8/15: [==============================] 75/75 batches, loss: 0.0289
[2025-05-07 13:38:01,077][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0289
[2025-05-07 13:38:01,325][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0392, Metrics: {'mse': 0.039306651800870895, 'rmse': 0.19825905225454624, 'r2': -1.2854008674621582}
[2025-05-07 13:38:01,325][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-07 13:38:01,325][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-07 13:38:01,326][src.training.lm_trainer][INFO] - Training completed in 22.84 seconds
[2025-05-07 13:38:01,326][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-07 13:38:04,302][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.008919218555092812, 'rmse': 0.09444161453031609, 'r2': -0.18043673038482666}
[2025-05-07 13:38:04,303][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.03127161040902138, 'rmse': 0.17683780820011702, 'r2': -0.8182204961776733}
[2025-05-07 13:38:04,303][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03961867839097977, 'rmse': 0.19904441311169668, 'r2': -1.1844944953918457}
[2025-05-07 13:38:05,979][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/fi/fi/model.pt
[2025-05-07 13:38:05,980][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▃▁
wandb:     best_val_mse █▇▃▁
wandb:      best_val_r2 ▁▂▆█
wandb:    best_val_rmse █▇▃▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▂▆▇▆▆▇
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▃▁▂▂▁▂
wandb:          val_mse █▇▃▁▂▂▁▂
wandb:           val_r2 ▁▂▆█▇▇█▇
wandb:         val_rmse █▇▃▁▂▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03124
wandb:     best_val_mse 0.03127
wandb:      best_val_r2 -0.81822
wandb:    best_val_rmse 0.17684
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.03962
wandb:    final_test_r2 -1.18449
wandb:  final_test_rmse 0.19904
wandb:  final_train_mse 0.00892
wandb:   final_train_r2 -0.18044
wandb: final_train_rmse 0.09444
wandb:    final_val_mse 0.03127
wandb:     final_val_r2 -0.81822
wandb:   final_val_rmse 0.17684
wandb:    learning_rate 0.0001
wandb:       train_loss 0.02887
wandb:       train_time 22.84256
wandb:         val_loss 0.03924
wandb:          val_mse 0.03931
wandb:           val_r2 -1.2854
wandb:         val_rmse 0.19826
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133721-h884mc0e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250507_133721-h884mc0e/logs
Experiment probe_layer2_avg_links_len_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control2/layer2/fi/fi/results.json for layer 2
Running experiment: probe_layer2_avg_links_len_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=2"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer2_avg_links_len_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-07 13:38:33,923][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/makeup_probes_output/submetrics/avg_links_len/control3/layer2/fi
experiment_name: probe_layer2_avg_links_len_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-07 13:38:33,923][__main__][INFO] - Normalized task: single_submetric
[2025-05-07 13:38:33,923][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:38:33,924][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-07 13:38:33,924][__main__][INFO] - Determined Task Type: regression
[2025-05-07 13:38:33,928][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['fi']
[2025-05-07 13:38:33,928][__main__][INFO] - Using submetric: avg_links_len
[2025-05-07 13:38:33,928][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-07 13:38:36,992][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-07 13:38:39,480][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-07 13:38:39,481][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:38:39,630][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 13:38:39,756][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 10:17:58 2025).
[2025-05-07 13:38:40,140][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-07 13:38:40,148][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:38:40,149][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-07 13:38:40,150][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:38:40,225][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:38:40,307][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:38:40,321][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-07 13:38:40,322][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:38:40,323][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-07 13:38:40,324][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-07 13:38:40,402][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:38:40,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-07 13:38:40,476][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-07 13:38:40,477][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-07 13:38:40,477][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-07 13:38:40,478][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-07 13:38:40,479][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:38:40,479][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:38:40,479][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:38:40,479][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:38:40,479][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-07 13:38:40,479][src.data.datasets][INFO] -   Mean: 0.1395, Std: 0.0869
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Sample label: 0.2849999964237213
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:38:40,480][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5520
[2025-05-07 13:38:40,480][src.data.datasets][INFO] -   Mean: 0.2101, Std: 0.1311
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-07 13:38:40,480][src.data.datasets][INFO] - Sample label: 0.36800000071525574
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-07 13:38:40,481][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6740
[2025-05-07 13:38:40,481][src.data.datasets][INFO] -   Mean: 0.2318, Std: 0.1347
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Sample label: 0.2070000022649765
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-07 13:38:40,481][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-07 13:38:40,482][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-07 13:38:40,482][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-07 13:38:40,482][src.models.model_factory][INFO] - Creating lm_probe model for regression task
slurmstepd: error: *** JOB 64464190 ON k28i22 CANCELLED AT 2025-05-07T13:38:41 ***
