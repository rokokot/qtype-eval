SLURM_JOB_ID: 64439803
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: sweep_probes
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_h100
SLURM_NNODES: 1
SLURM_NODELIST: s16g09
SLURM_JOB_CPUS_PER_NODE: 16
SLURM_JOB_GPUS: 2
Date: Fri May  2 17:47:29 CEST 2025
Walltime: 00-03:00:00
========================================================================
==============================================
Running experiment ID: 1
Language: ar, Task: question_type, Layer: 1
Hidden Size: 10, Dropout: 0.01, LR: 1e-4
==============================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 17:47:52,558][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr1e-4
experiment_name: probe_ar_question_type_layer1_h10_d0.01_lr1e-4
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.01
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 10
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 17:47:52,559][__main__][INFO] - Normalized task: question_type
[2025-05-02 17:47:52,559][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 17:47:52,559][__main__][INFO] - Determined Task Type: classification
[2025-05-02 17:47:52,562][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 17:47:52,563][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 17:47:55,301][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 17:47:57,372][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 17:47:57,373][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:47:57,551][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:57,669][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:57,807][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 17:47:57,814][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:47:57,815][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 17:47:57,816][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:47:57,871][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:57,924][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:57,938][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 17:47:57,939][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:47:57,939][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 17:47:57,940][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:47:58,007][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:58,068][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:47:58,080][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 17:47:58,081][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:47:58,081][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 17:47:58,082][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 17:47:58,082][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:47:58,082][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:47:58,082][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:47:58,083][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 17:47:58,083][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:47:58,083][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 17:47:58,083][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 17:47:58,083][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:47:58,084][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 17:47:58,084][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 17:47:58,084][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 17:47:58,084][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 17:47:58,085][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 17:48:02,615][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 17:48:02,616][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 17:48:02,616][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 17:48:02,616][src.models.model_factory][INFO] - Using provided probe_hidden_size: 10
[2025-05-02 17:48:02,617][src.models.model_factory][INFO] - Model has 9,367 trainable parameters out of 394,130,839 total parameters
[2025-05-02 17:48:02,617][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 9,367 trainable parameters
[2025-05-02 17:48:02,618][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=10, depth=2, activation=gelu, normalization=layer
[2025-05-02 17:48:02,618][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 10 hidden size
[2025-05-02 17:48:02,618][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 17:48:02,618][__main__][INFO] - Total parameters: 394,130,839
[2025-05-02 17:48:02,618][__main__][INFO] - Trainable parameters: 9,367 (0.00%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6690Epoch 1/15: [                              ] 2/63 batches, loss: 0.6957Epoch 1/15: [=                             ] 3/63 batches, loss: 0.6943Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7035Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7156Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7128Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7107Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7056Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7149Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7168Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7152Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7122Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7119Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7105Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7106Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7124Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7142Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7143Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7141Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7119Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7114Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7099Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7091Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7092Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7083Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7079Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7066Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7067Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7056Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7054Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7044Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7041Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7035Epoch 1/15: [================              ] 34/63 batches, loss: 0.7033Epoch 1/15: [================              ] 35/63 batches, loss: 0.7030Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7022Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7009Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7018Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7012Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7016Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7014Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7015Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7005Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7000Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6994Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7000Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6999Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7004Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7006Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7001Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6992Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6989Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6989Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6993Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6997Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6992Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6986Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6986Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6981Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6978Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6976Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6980Epoch 1/15: [==============================] 63/63 batches, loss: 0.6966
[2025-05-02 17:48:07,581][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6966
[2025-05-02 17:48:07,980][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6646Epoch 2/15: [                              ] 2/63 batches, loss: 0.6883Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6726Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6801Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6836Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6834Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6837Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6822Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6846Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6835Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6841Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6836Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6842Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6832Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6844Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6849Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6837Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6843Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6847Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6837Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6835Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6835Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6849Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6851Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6847Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6836Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6848Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6852Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6847Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6839Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6848Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6834Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6849Epoch 2/15: [================              ] 34/63 batches, loss: 0.6850Epoch 2/15: [================              ] 35/63 batches, loss: 0.6849Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6847Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6831Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6835Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6850Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6858Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6848Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6844Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6845Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6835Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6830Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6830Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6824Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6828Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6825Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6817Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6819Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6817Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6814Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6806Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6806Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6810Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6795Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6793Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6789Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6791Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6780Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6777Epoch 2/15: [==============================] 63/63 batches, loss: 0.6769
[2025-05-02 17:48:09,321][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6769
[2025-05-02 17:48:09,665][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6869, Metrics: {'accuracy': 0.5227272727272727, 'f1': 0.08695652173913043, 'precision': 0.3333333333333333, 'recall': 0.05}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6644Epoch 3/15: [                              ] 2/63 batches, loss: 0.6639Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6623Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6618Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6690Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6708Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6612Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6670Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6731Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6705Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6720Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6706Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6686Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6663Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6629Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6608Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6605Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6564Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6553Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6576Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6596Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6582Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6591Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6593Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6606Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6613Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6620Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6615Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6597Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6593Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6595Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6585Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6569Epoch 3/15: [================              ] 34/63 batches, loss: 0.6557Epoch 3/15: [================              ] 35/63 batches, loss: 0.6554Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6549Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6535Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6530Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6515Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6506Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6482Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6490Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6492Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6478Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6485Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6470Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6467Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6467Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6464Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6469Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6489Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6499Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6497Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6496Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6496Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6491Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6489Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6484Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6485Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6478Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6467Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6460Epoch 3/15: [==============================] 63/63 batches, loss: 0.6458
[2025-05-02 17:48:11,027][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6458
[2025-05-02 17:48:11,422][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6707, Metrics: {'accuracy': 0.6363636363636364, 'f1': 0.5, 'precision': 0.6666666666666666, 'recall': 0.4}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6369Epoch 4/15: [                              ] 2/63 batches, loss: 0.6282Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5986Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6168Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6230Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6277Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6214Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6164Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6100Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6157Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6195Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6165Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6206Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6215Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6239Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6244Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6234Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6229Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6245Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6215Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6198Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6232Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6201Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6243Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6236Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6243Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6230Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6229Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6223Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6229Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6227Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6207Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6186Epoch 4/15: [================              ] 34/63 batches, loss: 0.6193Epoch 4/15: [================              ] 35/63 batches, loss: 0.6202Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6195Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6199Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6195Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6195Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6202Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6225Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6235Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6248Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6241Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6261Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6256Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6256Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6250Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6248Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6237Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6237Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6228Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6231Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6223Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6215Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6201Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6191Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6194Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6196Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6188Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6196Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6197Epoch 4/15: [==============================] 63/63 batches, loss: 0.6188
[2025-05-02 17:48:12,712][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6188
[2025-05-02 17:48:13,140][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6523, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8095238095238095, 'precision': 0.7727272727272727, 'recall': 0.85}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6139Epoch 5/15: [                              ] 2/63 batches, loss: 0.5797Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6088Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5912Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6003Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5928Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5950Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5786Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5807Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5890Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5829Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5865Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5943Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5986Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5929Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5903Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5866Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5891Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5902Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5934Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5994Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5978Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5967Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5939Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5939Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5944Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5947Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5918Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5910Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5920Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5908Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5921Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5937Epoch 5/15: [================              ] 34/63 batches, loss: 0.5926Epoch 5/15: [================              ] 35/63 batches, loss: 0.5940Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5934Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5946Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5962Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5956Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5959Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5969Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5967Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5962Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5953Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5949Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5944Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5950Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5934Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5944Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5944Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5950Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5945Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5955Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5970Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5971Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5973Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5976Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5976Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5984Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5995Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5999Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5989Epoch 5/15: [==============================] 63/63 batches, loss: 0.5964
[2025-05-02 17:48:14,504][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5964
[2025-05-02 17:48:14,911][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6372, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8205128205128205, 'precision': 0.8421052631578947, 'recall': 0.8}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5995Epoch 6/15: [                              ] 2/63 batches, loss: 0.6373Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6163Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6055Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5995Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6073Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6148Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6197Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6182Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6151Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6084Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5978Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5931Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5975Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5933Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5918Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5894Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5846Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5842Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5830Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5860Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5846Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5875Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5882Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5870Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5884Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5858Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5840Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5854Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5841Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5870Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5868Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5853Epoch 6/15: [================              ] 34/63 batches, loss: 0.5864Epoch 6/15: [================              ] 35/63 batches, loss: 0.5860Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5855Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5829Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5853Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5866Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5861Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5863Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5865Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5856Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5846Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5841Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5843Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5833Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5825Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5829Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5819Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5834Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5831Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5832Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5842Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5853Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5857Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5852Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5833Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5832Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5836Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5833Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5847Epoch 6/15: [==============================] 63/63 batches, loss: 0.5862
[2025-05-02 17:48:16,273][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5862
[2025-05-02 17:48:16,666][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6288, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8372093023255814, 'precision': 0.782608695652174, 'recall': 0.9}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5238Epoch 7/15: [                              ] 2/63 batches, loss: 0.5533Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5812Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5675Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5673Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5720Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5724Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5684Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5769Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5741Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5782Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5791Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5673Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5695Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5715Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5757Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5731Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5719Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5730Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5730Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5734Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5738Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5697Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5701Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5696Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5685Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5695Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5708Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5717Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5702Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5707Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5723Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5742Epoch 7/15: [================              ] 34/63 batches, loss: 0.5732Epoch 7/15: [================              ] 35/63 batches, loss: 0.5716Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5711Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5713Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5718Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5694Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5698Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5695Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5691Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5697Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5715Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5724Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5723Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5725Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5716Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5704Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5714Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5707Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5712Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5715Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5710Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5709Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5709Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5720Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5733Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5731Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5711Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5705Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5711Epoch 7/15: [==============================] 63/63 batches, loss: 0.5700
[2025-05-02 17:48:17,990][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5700
[2025-05-02 17:48:18,406][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6172, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.85, 'precision': 0.85, 'recall': 0.85}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5818Epoch 8/15: [                              ] 2/63 batches, loss: 0.5248Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5281Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5457Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5577Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5703Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5685Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5671Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5658Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5666Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5767Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5763Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5793Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5763Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5705Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5669Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5682Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5676Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5703Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5732Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5731Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5741Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5763Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5725Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5722Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5697Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5697Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5681Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5669Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5661Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5649Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5632Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5632Epoch 8/15: [================              ] 34/63 batches, loss: 0.5628Epoch 8/15: [================              ] 35/63 batches, loss: 0.5624Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5618Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5601Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5621Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5613Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5622Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5639Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5658Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5681Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5660Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5650Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5634Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5631Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5638Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5647Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5639Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5654Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5652Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5644Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5653Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5639Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5623Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5627Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5633Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5628Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5632Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5631Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5626Epoch 8/15: [==============================] 63/63 batches, loss: 0.5599
[2025-05-02 17:48:19,808][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5599
[2025-05-02 17:48:20,335][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6119, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.4940Epoch 9/15: [                              ] 2/63 batches, loss: 0.5086Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5152Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5151Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5387Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5457Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5539Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5601Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5561Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5577Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5641Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5660Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5658Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5677Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5661Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5651Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5648Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5653Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5648Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5656Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5702Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5689Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5649Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5662Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5652Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5651Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5645Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5628Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5636Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5633Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5588Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5567Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5546Epoch 9/15: [================              ] 34/63 batches, loss: 0.5545Epoch 9/15: [================              ] 35/63 batches, loss: 0.5553Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5557Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5579Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5543Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5533Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5517Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5530Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5530Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5538Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5548Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5537Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5529Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5528Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5521Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5520Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5526Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5526Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5530Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5525Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5524Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5512Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5522Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5535Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5545Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5553Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5563Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5556Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5562Epoch 9/15: [==============================] 63/63 batches, loss: 0.5568
[2025-05-02 17:48:21,664][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5568
[2025-05-02 17:48:22,037][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6054, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5727Epoch 10/15: [                              ] 2/63 batches, loss: 0.5842Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5813Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5899Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5796Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5800Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5723Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5752Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5739Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5604Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5545Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5578Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5642Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5629Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5625Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5661Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5684Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5699Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5653Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5660Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5638Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5659Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5658Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5653Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5600Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5581Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5582Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5580Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5583Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5599Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5607Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5602Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5587Epoch 10/15: [================              ] 34/63 batches, loss: 0.5586Epoch 10/15: [================              ] 35/63 batches, loss: 0.5596Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5582Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5578Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5574Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5574Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5577Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5563Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5555Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5562Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5560Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5550Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5530Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5540Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5527Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5533Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5514Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5509Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5510Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5499Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5494Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5484Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5483Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5486Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5483Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5473Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5481Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5486Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5501Epoch 10/15: [==============================] 63/63 batches, loss: 0.5504
[2025-05-02 17:48:23,405][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5504
[2025-05-02 17:48:23,810][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6009, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5154Epoch 11/15: [                              ] 2/63 batches, loss: 0.5677Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5476Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5446Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5377Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5242Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5288Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5268Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5262Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5264Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5262Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5263Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5290Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5328Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5362Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5368Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5346Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5334Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5311Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5325Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5386Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5404Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5392Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5369Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5360Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5348Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5336Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5341Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5338Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5332Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5318Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5323Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5340Epoch 11/15: [================              ] 34/63 batches, loss: 0.5343Epoch 11/15: [================              ] 35/63 batches, loss: 0.5333Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5336Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5327Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5349Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5354Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5390Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5402Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5393Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5398Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5398Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5415Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5420Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5430Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5430Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5428Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5420Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5423Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5429Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5437Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5422Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5426Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5432Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5438Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5424Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5430Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5439Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5433Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5440Epoch 11/15: [==============================] 63/63 batches, loss: 0.5449
[2025-05-02 17:48:25,158][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5449
[2025-05-02 17:48:25,598][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5959, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.5009Epoch 12/15: [                              ] 2/63 batches, loss: 0.5360Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5551Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5600Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5577Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5570Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5549Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5523Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5443Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5470Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5430Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5472Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5459Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5406Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5411Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5479Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5467Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5485Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5475Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5468Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5507Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5501Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5496Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5506Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5506Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5519Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5502Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5477Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5455Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5455Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5419Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5434Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5439Epoch 12/15: [================              ] 34/63 batches, loss: 0.5442Epoch 12/15: [================              ] 35/63 batches, loss: 0.5447Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5461Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5450Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5455Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5446Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5441Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5421Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5438Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5456Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5443Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5440Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5443Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5437Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5433Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5408Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5410Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5410Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5406Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5405Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5405Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5394Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5400Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5386Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5390Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5390Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5402Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5402Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5387Epoch 12/15: [==============================] 63/63 batches, loss: 0.5399
[2025-05-02 17:48:26,961][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5399
[2025-05-02 17:48:27,370][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5927, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5004Epoch 13/15: [                              ] 2/63 batches, loss: 0.5147Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5087Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5199Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5213Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5228Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5271Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5236Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5250Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5243Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5232Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5182Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5228Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5245Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5280Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5247Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5283Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5253Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5279Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5291Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5288Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5311Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5317Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5299Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5277Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5252Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5258Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5255Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5274Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5273Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5251Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5239Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5260Epoch 13/15: [================              ] 34/63 batches, loss: 0.5268Epoch 13/15: [================              ] 35/63 batches, loss: 0.5292Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5311Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5316Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5322Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5306Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5316Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5303Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5295Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5302Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5295Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5289Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5294Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5293Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5291Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5295Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5290Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5298Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5302Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5308Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5324Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5331Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5339Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5335Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5345Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5351Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5362Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5363Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5368Epoch 13/15: [==============================] 63/63 batches, loss: 0.5356
[2025-05-02 17:48:28,771][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5356
[2025-05-02 17:48:29,218][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5896, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.4870Epoch 14/15: [                              ] 2/63 batches, loss: 0.4876Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5106Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5091Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5165Epoch 14/15: [==                            ] 6/63 batches, loss: 0.4991Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5156Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5185Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5168Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5151Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5202Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5152Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5139Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5145Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5123Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5089Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5131Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5138Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5131Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5150Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5164Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5201Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5205Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5217Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5248Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5250Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5244Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5243Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5257Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5247Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5279Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5301Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5314Epoch 14/15: [================              ] 34/63 batches, loss: 0.5317Epoch 14/15: [================              ] 35/63 batches, loss: 0.5294Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5297Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5316Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5320Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5302Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5295Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5282Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5282Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5275Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5296Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5299Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5297Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5288Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5300Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5298Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5311Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5314Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5298Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5297Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5297Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5305Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5312Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5304Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5302Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5303Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5311Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5317Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5318Epoch 14/15: [==============================] 63/63 batches, loss: 0.5329
[2025-05-02 17:48:30,564][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5329
[2025-05-02 17:48:30,963][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5869, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.6104Epoch 15/15: [                              ] 2/63 batches, loss: 0.6025Epoch 15/15: [=                             ] 3/63 batches, loss: 0.5715Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5538Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5479Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5443Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5389Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5423Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5400Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5390Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5372Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5281Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5261Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5295Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5210Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5174Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5264Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5305Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5302Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5324Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5334Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5371Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5367Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5354Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5398Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5390Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5387Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5364Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5367Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5386Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5365Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5361Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5373Epoch 15/15: [================              ] 34/63 batches, loss: 0.5375Epoch 15/15: [================              ] 35/63 batches, loss: 0.5356Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5357Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5354Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5347Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5368Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5376Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5378Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5379Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5364Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5363Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5344Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5336Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5319Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5316Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5312Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5322Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5321Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5315Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5317Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5321Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5328Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5322Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5313Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5314Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5303Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5302Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5288Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5282Epoch 15/15: [==============================] 63/63 batches, loss: 0.5271
[2025-05-02 17:48:32,412][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5271
[2025-05-02 17:48:32,867][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5853, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
[2025-05-02 17:48:33,064][src.training.lm_trainer][INFO] - Training completed in 27.79 seconds
[2025-05-02 17:48:33,065][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 17:48:35,134][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9939698492462311, 'f1': 0.9939393939393939, 'precision': 0.9979716024340771, 'recall': 0.9899396378269618}
[2025-05-02 17:48:35,134][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
[2025-05-02 17:48:35,134][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7272727272727273, 'f1': 0.676923076923077, 'precision': 0.5116279069767442, 'recall': 1.0}
[2025-05-02 17:48:36,384][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr1e-4/ar/model.pt
[2025-05-02 17:48:36,386][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▃▆▆▆▇███▇████
wandb:           best_val_f1 ▁▂▅▇▇▇▇████████
wandb:         best_val_loss ██▇▅▄▄▃▃▂▂▂▁▁▁▁
wandb:    best_val_precision ▁▄▆▇█▇█████████
wandb:       best_val_recall ▁▁▄▇▇▇▇███▇████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▂▂▂▂▃▃▃▃▃▃▃
wandb:            train_loss █▇▆▅▄▃▃▂▂▂▂▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▃▆▆▆▇███▇████
wandb:                val_f1 ▁▂▅▇▇▇▇████████
wandb:              val_loss ██▇▅▄▄▃▃▂▂▂▁▁▁▁
wandb:         val_precision ▁▄▆▇█▇█████████
wandb:            val_recall ▁▁▄▇▇▇▇███▇████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90476
wandb:         best_val_loss 0.58531
wandb:    best_val_precision 0.86364
wandb:       best_val_recall 0.95
wandb:                 epoch 15
wandb:   final_test_accuracy 0.72727
wandb:         final_test_f1 0.67692
wandb:  final_test_precision 0.51163
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99397
wandb:        final_train_f1 0.99394
wandb: final_train_precision 0.99797
wandb:    final_train_recall 0.98994
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90476
wandb:   final_val_precision 0.86364
wandb:      final_val_recall 0.95
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52712
wandb:            train_time 27.78859
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90476
wandb:              val_loss 0.58531
wandb:         val_precision 0.86364
wandb:            val_recall 0.95
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_174752-j79tuhul
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_174752-j79tuhul/logs
Experiment 1 completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr1e-4/ar/results.json
==============================================
Running experiment ID: 2
Language: ar, Task: question_type, Layer: 1
Hidden Size: 10, Dropout: 0.01, LR: 2e-5
==============================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 17:48:58,301][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr2e-5
experiment_name: probe_ar_question_type_layer1_h10_d0.01_lr2e-5
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.01
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 10
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 17:48:58,301][__main__][INFO] - Normalized task: question_type
[2025-05-02 17:48:58,301][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 17:48:58,301][__main__][INFO] - Determined Task Type: classification
[2025-05-02 17:48:58,305][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 17:48:58,305][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 17:49:00,938][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 17:49:02,916][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 17:49:02,917][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:49:03,046][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,108][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,261][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 17:49:03,267][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:49:03,267][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 17:49:03,268][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:49:03,296][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,370][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,382][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 17:49:03,383][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:49:03,383][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 17:49:03,384][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:49:03,441][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,507][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:49:03,518][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 17:49:03,519][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:49:03,519][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 17:49:03,530][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 17:49:03,530][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:49:03,530][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:49:03,530][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:49:03,531][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 17:49:03,531][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:49:03,531][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 17:49:03,531][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 17:49:03,531][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:49:03,532][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 17:49:03,532][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 17:49:03,532][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 17:49:03,532][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 17:49:03,533][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 17:49:08,133][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 17:49:08,134][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 17:49:08,134][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 17:49:08,134][src.models.model_factory][INFO] - Using provided probe_hidden_size: 10
[2025-05-02 17:49:08,136][src.models.model_factory][INFO] - Model has 9,367 trainable parameters out of 394,130,839 total parameters
[2025-05-02 17:49:08,136][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 9,367 trainable parameters
[2025-05-02 17:49:08,136][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=10, depth=2, activation=gelu, normalization=layer
[2025-05-02 17:49:08,136][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 10 hidden size
[2025-05-02 17:49:08,136][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 17:49:08,137][__main__][INFO] - Total parameters: 394,130,839
[2025-05-02 17:49:08,137][__main__][INFO] - Trainable parameters: 9,367 (0.00%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6690Epoch 1/15: [                              ] 2/63 batches, loss: 0.6954Epoch 1/15: [=                             ] 3/63 batches, loss: 0.6944Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7036Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7159Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7133Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7116Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7065Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7173Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7199Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7186Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7156Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7158Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7141Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7147Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7175Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7202Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7207Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7207Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7182Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7182Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7166Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7161Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7163Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7155Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7153Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7137Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7140Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7128Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7126Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7114Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7116Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7110Epoch 1/15: [================              ] 34/63 batches, loss: 0.7109Epoch 1/15: [================              ] 35/63 batches, loss: 0.7104Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7096Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7078Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7095Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7087Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7097Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7096Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7100Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7085Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7081Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7074Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7085Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7087Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7100Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7104Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7099Epoch 1/15: [========================      ] 51/63 batches, loss: 0.7089Epoch 1/15: [========================      ] 52/63 batches, loss: 0.7087Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.7087Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.7098Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.7103Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.7097Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.7090Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.7090Epoch 1/15: [============================  ] 59/63 batches, loss: 0.7085Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7081Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7077Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7084Epoch 1/15: [==============================] 63/63 batches, loss: 0.7066
[2025-05-02 17:49:13,373][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7066
[2025-05-02 17:49:13,785][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.7043, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6615Epoch 2/15: [                              ] 2/63 batches, loss: 0.6956Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6759Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6889Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6931Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6941Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6950Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6946Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6991Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6981Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6988Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6985Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6993Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6984Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6996Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.7004Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6991Epoch 2/15: [========                      ] 18/63 batches, loss: 0.7004Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.7009Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6998Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6997Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.7003Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.7025Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.7033Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.7026Epoch 2/15: [============                  ] 26/63 batches, loss: 0.7019Epoch 2/15: [============                  ] 27/63 batches, loss: 0.7031Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.7033Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.7029Epoch 2/15: [==============                ] 30/63 batches, loss: 0.7022Epoch 2/15: [==============                ] 31/63 batches, loss: 0.7033Epoch 2/15: [===============               ] 32/63 batches, loss: 0.7020Epoch 2/15: [===============               ] 33/63 batches, loss: 0.7035Epoch 2/15: [================              ] 34/63 batches, loss: 0.7036Epoch 2/15: [================              ] 35/63 batches, loss: 0.7038Epoch 2/15: [=================             ] 36/63 batches, loss: 0.7034Epoch 2/15: [=================             ] 37/63 batches, loss: 0.7024Epoch 2/15: [==================            ] 38/63 batches, loss: 0.7025Epoch 2/15: [==================            ] 39/63 batches, loss: 0.7037Epoch 2/15: [===================           ] 40/63 batches, loss: 0.7046Epoch 2/15: [===================           ] 41/63 batches, loss: 0.7038Epoch 2/15: [====================          ] 42/63 batches, loss: 0.7034Epoch 2/15: [====================          ] 43/63 batches, loss: 0.7036Epoch 2/15: [====================          ] 44/63 batches, loss: 0.7027Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.7026Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.7027Epoch 2/15: [======================        ] 47/63 batches, loss: 0.7019Epoch 2/15: [======================        ] 48/63 batches, loss: 0.7023Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.7021Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.7015Epoch 2/15: [========================      ] 51/63 batches, loss: 0.7018Epoch 2/15: [========================      ] 52/63 batches, loss: 0.7015Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.7013Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.7005Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.7006Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.7009Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.7001Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6999Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6995Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6998Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6993Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6992Epoch 2/15: [==============================] 63/63 batches, loss: 0.6984
[2025-05-02 17:49:15,103][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6984
[2025-05-02 17:49:15,525][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6998, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6990Epoch 3/15: [                              ] 2/63 batches, loss: 0.6976Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6986Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6975Epoch 3/15: [==                            ] 5/63 batches, loss: 0.7013Epoch 3/15: [==                            ] 6/63 batches, loss: 0.7018Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6960Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6973Epoch 3/15: [====                          ] 9/63 batches, loss: 0.7011Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6988Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6996Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6998Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6987Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6972Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6953Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6939Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6933Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6909Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6902Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6915Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6932Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6928Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6927Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6940Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6952Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6958Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6959Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6954Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6961Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6962Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6952Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6946Epoch 3/15: [================              ] 34/63 batches, loss: 0.6939Epoch 3/15: [================              ] 35/63 batches, loss: 0.6942Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6942Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6936Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6934Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6929Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6923Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6912Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6922Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6926Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6921Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6927Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6920Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6921Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6921Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6924Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6925Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6933Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6938Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6939Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6941Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6940Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6937Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6937Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6935Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6936Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6935Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6930Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6932Epoch 3/15: [==============================] 63/63 batches, loss: 0.6926
[2025-05-02 17:49:16,892][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6926
[2025-05-02 17:49:17,324][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6974, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6915Epoch 4/15: [                              ] 2/63 batches, loss: 0.6865Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6741Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6818Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6871Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6909Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6891Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6860Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6840Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6883Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6904Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6888Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6911Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6914Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6924Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6927Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6912Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6912Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6927Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6911Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6904Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6917Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6905Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6930Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6937Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6933Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6926Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6923Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6921Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6924Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6913Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6908Epoch 4/15: [================              ] 34/63 batches, loss: 0.6908Epoch 4/15: [================              ] 35/63 batches, loss: 0.6912Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6907Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6905Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6901Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6892Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6895Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6908Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6914Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6919Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6919Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6930Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6933Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6931Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6931Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6929Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6921Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6920Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6917Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6919Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6916Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6914Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6908Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6904Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6905Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6906Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6902Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6905Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6907Epoch 4/15: [==============================] 63/63 batches, loss: 0.6904
[2025-05-02 17:49:18,678][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6904
[2025-05-02 17:49:19,104][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6955, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6810Epoch 5/15: [                              ] 2/63 batches, loss: 0.6680Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6805Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6745Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6803Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6772Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6820Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6772Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6761Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6814Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6792Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6809Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6853Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6868Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6850Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6825Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6810Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6814Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6820Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6836Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6875Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6874Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6870Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6864Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6866Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6866Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6872Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6855Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6850Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6853Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6846Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6850Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6858Epoch 5/15: [================              ] 34/63 batches, loss: 0.6860Epoch 5/15: [================              ] 35/63 batches, loss: 0.6860Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6859Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6865Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6868Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6864Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6865Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6870Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6870Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6864Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6861Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6862Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6858Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6858Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6851Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6852Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6854Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6858Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6856Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6864Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6873Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6873Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6875Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6873Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6872Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6874Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6881Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6885Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6880Epoch 5/15: [==============================] 63/63 batches, loss: 0.6869
[2025-05-02 17:49:20,392][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6869
[2025-05-02 17:49:20,761][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6936, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.6910Epoch 6/15: [                              ] 2/63 batches, loss: 0.7049Epoch 6/15: [=                             ] 3/63 batches, loss: 0.7017Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6969Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6892Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6914Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6960Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6999Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6994Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6963Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6939Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6886Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6860Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6885Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6856Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6851Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6841Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6816Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6819Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6812Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6825Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6821Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6836Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6843Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6836Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6832Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6818Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6810Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6819Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6812Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6822Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6823Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6814Epoch 6/15: [================              ] 34/63 batches, loss: 0.6818Epoch 6/15: [================              ] 35/63 batches, loss: 0.6814Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6811Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6800Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6814Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6817Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6814Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6814Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6811Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6808Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6800Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6798Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6801Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6798Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6794Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6796Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6794Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6805Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6803Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6803Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6811Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6818Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6819Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6815Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6803Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6801Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6803Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6802Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6810Epoch 6/15: [==============================] 63/63 batches, loss: 0.6815
[2025-05-02 17:49:22,071][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6815
[2025-05-02 17:49:22,508][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6922, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.6532Epoch 7/15: [                              ] 2/63 batches, loss: 0.6648Epoch 7/15: [=                             ] 3/63 batches, loss: 0.6789Epoch 7/15: [=                             ] 4/63 batches, loss: 0.6710Epoch 7/15: [==                            ] 5/63 batches, loss: 0.6705Epoch 7/15: [==                            ] 6/63 batches, loss: 0.6733Epoch 7/15: [===                           ] 7/63 batches, loss: 0.6762Epoch 7/15: [===                           ] 8/63 batches, loss: 0.6751Epoch 7/15: [====                          ] 9/63 batches, loss: 0.6788Epoch 7/15: [====                          ] 10/63 batches, loss: 0.6783Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.6802Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.6807Epoch 7/15: [======                        ] 13/63 batches, loss: 0.6747Epoch 7/15: [======                        ] 14/63 batches, loss: 0.6755Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.6772Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.6789Epoch 7/15: [========                      ] 17/63 batches, loss: 0.6769Epoch 7/15: [========                      ] 18/63 batches, loss: 0.6767Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.6778Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.6775Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.6778Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.6781Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.6750Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.6753Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.6759Epoch 7/15: [============                  ] 26/63 batches, loss: 0.6751Epoch 7/15: [============                  ] 27/63 batches, loss: 0.6757Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.6763Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.6770Epoch 7/15: [==============                ] 30/63 batches, loss: 0.6760Epoch 7/15: [==============                ] 31/63 batches, loss: 0.6765Epoch 7/15: [===============               ] 32/63 batches, loss: 0.6773Epoch 7/15: [===============               ] 33/63 batches, loss: 0.6788Epoch 7/15: [================              ] 34/63 batches, loss: 0.6784Epoch 7/15: [================              ] 35/63 batches, loss: 0.6778Epoch 7/15: [=================             ] 36/63 batches, loss: 0.6777Epoch 7/15: [=================             ] 37/63 batches, loss: 0.6775Epoch 7/15: [==================            ] 38/63 batches, loss: 0.6779Epoch 7/15: [==================            ] 39/63 batches, loss: 0.6759Epoch 7/15: [===================           ] 40/63 batches, loss: 0.6760Epoch 7/15: [===================           ] 41/63 batches, loss: 0.6762Epoch 7/15: [====================          ] 42/63 batches, loss: 0.6757Epoch 7/15: [====================          ] 43/63 batches, loss: 0.6763Epoch 7/15: [====================          ] 44/63 batches, loss: 0.6774Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.6778Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.6773Epoch 7/15: [======================        ] 47/63 batches, loss: 0.6774Epoch 7/15: [======================        ] 48/63 batches, loss: 0.6772Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.6759Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.6766Epoch 7/15: [========================      ] 51/63 batches, loss: 0.6765Epoch 7/15: [========================      ] 52/63 batches, loss: 0.6771Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.6775Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.6770Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.6771Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.6770Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.6778Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.6787Epoch 7/15: [============================  ] 59/63 batches, loss: 0.6786Epoch 7/15: [============================  ] 60/63 batches, loss: 0.6776Epoch 7/15: [============================= ] 61/63 batches, loss: 0.6769Epoch 7/15: [============================= ] 62/63 batches, loss: 0.6772Epoch 7/15: [==============================] 63/63 batches, loss: 0.6769
[2025-05-02 17:49:23,808][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6769
[2025-05-02 17:49:24,229][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6906, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6752Epoch 8/15: [                              ] 2/63 batches, loss: 0.6498Epoch 8/15: [=                             ] 3/63 batches, loss: 0.6532Epoch 8/15: [=                             ] 4/63 batches, loss: 0.6621Epoch 8/15: [==                            ] 5/63 batches, loss: 0.6681Epoch 8/15: [==                            ] 6/63 batches, loss: 0.6778Epoch 8/15: [===                           ] 7/63 batches, loss: 0.6749Epoch 8/15: [===                           ] 8/63 batches, loss: 0.6750Epoch 8/15: [====                          ] 9/63 batches, loss: 0.6738Epoch 8/15: [====                          ] 10/63 batches, loss: 0.6753Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.6799Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.6803Epoch 8/15: [======                        ] 13/63 batches, loss: 0.6816Epoch 8/15: [======                        ] 14/63 batches, loss: 0.6804Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.6769Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.6752Epoch 8/15: [========                      ] 17/63 batches, loss: 0.6765Epoch 8/15: [========                      ] 18/63 batches, loss: 0.6761Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.6770Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.6786Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.6788Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.6800Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.6811Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.6784Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.6785Epoch 8/15: [============                  ] 26/63 batches, loss: 0.6768Epoch 8/15: [============                  ] 27/63 batches, loss: 0.6769Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.6757Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.6746Epoch 8/15: [==============                ] 30/63 batches, loss: 0.6740Epoch 8/15: [==============                ] 31/63 batches, loss: 0.6730Epoch 8/15: [===============               ] 32/63 batches, loss: 0.6715Epoch 8/15: [===============               ] 33/63 batches, loss: 0.6716Epoch 8/15: [================              ] 34/63 batches, loss: 0.6715Epoch 8/15: [================              ] 35/63 batches, loss: 0.6715Epoch 8/15: [=================             ] 36/63 batches, loss: 0.6719Epoch 8/15: [=================             ] 37/63 batches, loss: 0.6712Epoch 8/15: [==================            ] 38/63 batches, loss: 0.6720Epoch 8/15: [==================            ] 39/63 batches, loss: 0.6710Epoch 8/15: [===================           ] 40/63 batches, loss: 0.6710Epoch 8/15: [===================           ] 41/63 batches, loss: 0.6727Epoch 8/15: [====================          ] 42/63 batches, loss: 0.6735Epoch 8/15: [====================          ] 43/63 batches, loss: 0.6744Epoch 8/15: [====================          ] 44/63 batches, loss: 0.6731Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.6725Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.6711Epoch 8/15: [======================        ] 47/63 batches, loss: 0.6707Epoch 8/15: [======================        ] 48/63 batches, loss: 0.6713Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.6717Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.6709Epoch 8/15: [========================      ] 51/63 batches, loss: 0.6716Epoch 8/15: [========================      ] 52/63 batches, loss: 0.6713Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.6706Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.6713Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.6704Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.6691Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.6693Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.6696Epoch 8/15: [============================  ] 59/63 batches, loss: 0.6694Epoch 8/15: [============================  ] 60/63 batches, loss: 0.6698Epoch 8/15: [============================= ] 61/63 batches, loss: 0.6699Epoch 8/15: [============================= ] 62/63 batches, loss: 0.6694Epoch 8/15: [==============================] 63/63 batches, loss: 0.6682
[2025-05-02 17:49:25,641][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6682
[2025-05-02 17:49:26,077][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6895, Metrics: {'accuracy': 0.5227272727272727, 'f1': 0.08695652173913043, 'precision': 0.3333333333333333, 'recall': 0.05}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.6026Epoch 9/15: [                              ] 2/63 batches, loss: 0.6239Epoch 9/15: [=                             ] 3/63 batches, loss: 0.6304Epoch 9/15: [=                             ] 4/63 batches, loss: 0.6294Epoch 9/15: [==                            ] 5/63 batches, loss: 0.6461Epoch 9/15: [==                            ] 6/63 batches, loss: 0.6516Epoch 9/15: [===                           ] 7/63 batches, loss: 0.6568Epoch 9/15: [===                           ] 8/63 batches, loss: 0.6615Epoch 9/15: [====                          ] 9/63 batches, loss: 0.6612Epoch 9/15: [====                          ] 10/63 batches, loss: 0.6651Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.6699Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.6712Epoch 9/15: [======                        ] 13/63 batches, loss: 0.6702Epoch 9/15: [======                        ] 14/63 batches, loss: 0.6714Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.6700Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.6694Epoch 9/15: [========                      ] 17/63 batches, loss: 0.6678Epoch 9/15: [========                      ] 18/63 batches, loss: 0.6683Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.6677Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.6685Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.6723Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.6713Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.6677Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.6690Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.6684Epoch 9/15: [============                  ] 26/63 batches, loss: 0.6683Epoch 9/15: [============                  ] 27/63 batches, loss: 0.6687Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.6673Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.6681Epoch 9/15: [==============                ] 30/63 batches, loss: 0.6687Epoch 9/15: [==============                ] 31/63 batches, loss: 0.6660Epoch 9/15: [===============               ] 32/63 batches, loss: 0.6646Epoch 9/15: [===============               ] 33/63 batches, loss: 0.6629Epoch 9/15: [================              ] 34/63 batches, loss: 0.6621Epoch 9/15: [================              ] 35/63 batches, loss: 0.6631Epoch 9/15: [=================             ] 36/63 batches, loss: 0.6627Epoch 9/15: [=================             ] 37/63 batches, loss: 0.6644Epoch 9/15: [==================            ] 38/63 batches, loss: 0.6619Epoch 9/15: [==================            ] 39/63 batches, loss: 0.6609Epoch 9/15: [===================           ] 40/63 batches, loss: 0.6597Epoch 9/15: [===================           ] 41/63 batches, loss: 0.6608Epoch 9/15: [====================          ] 42/63 batches, loss: 0.6606Epoch 9/15: [====================          ] 43/63 batches, loss: 0.6611Epoch 9/15: [====================          ] 44/63 batches, loss: 0.6617Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.6602Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.6594Epoch 9/15: [======================        ] 47/63 batches, loss: 0.6593Epoch 9/15: [======================        ] 48/63 batches, loss: 0.6589Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.6590Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.6597Epoch 9/15: [========================      ] 51/63 batches, loss: 0.6596Epoch 9/15: [========================      ] 52/63 batches, loss: 0.6598Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.6598Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.6593Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.6583Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.6593Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.6604Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.6615Epoch 9/15: [============================  ] 59/63 batches, loss: 0.6621Epoch 9/15: [============================  ] 60/63 batches, loss: 0.6630Epoch 9/15: [============================= ] 61/63 batches, loss: 0.6624Epoch 9/15: [============================= ] 62/63 batches, loss: 0.6629Epoch 9/15: [==============================] 63/63 batches, loss: 0.6629
[2025-05-02 17:49:27,473][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6629
[2025-05-02 17:49:27,834][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6881, Metrics: {'accuracy': 0.5, 'f1': 0.08333333333333333, 'precision': 0.25, 'recall': 0.05}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.6767Epoch 10/15: [                              ] 2/63 batches, loss: 0.6824Epoch 10/15: [=                             ] 3/63 batches, loss: 0.6818Epoch 10/15: [=                             ] 4/63 batches, loss: 0.6892Epoch 10/15: [==                            ] 5/63 batches, loss: 0.6825Epoch 10/15: [==                            ] 6/63 batches, loss: 0.6817Epoch 10/15: [===                           ] 7/63 batches, loss: 0.6774Epoch 10/15: [===                           ] 8/63 batches, loss: 0.6793Epoch 10/15: [====                          ] 9/63 batches, loss: 0.6806Epoch 10/15: [====                          ] 10/63 batches, loss: 0.6670Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.6630Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.6648Epoch 10/15: [======                        ] 13/63 batches, loss: 0.6715Epoch 10/15: [======                        ] 14/63 batches, loss: 0.6693Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.6692Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.6723Epoch 10/15: [========                      ] 17/63 batches, loss: 0.6733Epoch 10/15: [========                      ] 18/63 batches, loss: 0.6738Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.6710Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.6708Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.6696Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.6713Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.6710Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.6708Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.6675Epoch 10/15: [============                  ] 26/63 batches, loss: 0.6658Epoch 10/15: [============                  ] 27/63 batches, loss: 0.6652Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.6660Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.6653Epoch 10/15: [==============                ] 30/63 batches, loss: 0.6665Epoch 10/15: [==============                ] 31/63 batches, loss: 0.6672Epoch 10/15: [===============               ] 32/63 batches, loss: 0.6673Epoch 10/15: [===============               ] 33/63 batches, loss: 0.6663Epoch 10/15: [================              ] 34/63 batches, loss: 0.6660Epoch 10/15: [================              ] 35/63 batches, loss: 0.6663Epoch 10/15: [=================             ] 36/63 batches, loss: 0.6656Epoch 10/15: [=================             ] 37/63 batches, loss: 0.6651Epoch 10/15: [==================            ] 38/63 batches, loss: 0.6641Epoch 10/15: [==================            ] 39/63 batches, loss: 0.6635Epoch 10/15: [===================           ] 40/63 batches, loss: 0.6636Epoch 10/15: [===================           ] 41/63 batches, loss: 0.6629Epoch 10/15: [====================          ] 42/63 batches, loss: 0.6622Epoch 10/15: [====================          ] 43/63 batches, loss: 0.6624Epoch 10/15: [====================          ] 44/63 batches, loss: 0.6621Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.6612Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.6597Epoch 10/15: [======================        ] 47/63 batches, loss: 0.6608Epoch 10/15: [======================        ] 48/63 batches, loss: 0.6597Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.6598Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.6581Epoch 10/15: [========================      ] 51/63 batches, loss: 0.6577Epoch 10/15: [========================      ] 52/63 batches, loss: 0.6580Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.6571Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.6570Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.6564Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.6559Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.6562Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.6560Epoch 10/15: [============================  ] 59/63 batches, loss: 0.6549Epoch 10/15: [============================  ] 60/63 batches, loss: 0.6556Epoch 10/15: [============================= ] 61/63 batches, loss: 0.6563Epoch 10/15: [============================= ] 62/63 batches, loss: 0.6574Epoch 10/15: [==============================] 63/63 batches, loss: 0.6586
[2025-05-02 17:49:29,217][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6586
[2025-05-02 17:49:29,625][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6851, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.2962962962962963, 'precision': 0.5714285714285714, 'recall': 0.2}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.6174Epoch 11/15: [                              ] 2/63 batches, loss: 0.6711Epoch 11/15: [=                             ] 3/63 batches, loss: 0.6607Epoch 11/15: [=                             ] 4/63 batches, loss: 0.6591Epoch 11/15: [==                            ] 5/63 batches, loss: 0.6552Epoch 11/15: [==                            ] 6/63 batches, loss: 0.6414Epoch 11/15: [===                           ] 7/63 batches, loss: 0.6426Epoch 11/15: [===                           ] 8/63 batches, loss: 0.6407Epoch 11/15: [====                          ] 9/63 batches, loss: 0.6384Epoch 11/15: [====                          ] 10/63 batches, loss: 0.6390Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.6357Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.6349Epoch 11/15: [======                        ] 13/63 batches, loss: 0.6367Epoch 11/15: [======                        ] 14/63 batches, loss: 0.6418Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.6436Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.6440Epoch 11/15: [========                      ] 17/63 batches, loss: 0.6419Epoch 11/15: [========                      ] 18/63 batches, loss: 0.6422Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.6394Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.6411Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.6446Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.6459Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.6452Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.6436Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.6424Epoch 11/15: [============                  ] 26/63 batches, loss: 0.6409Epoch 11/15: [============                  ] 27/63 batches, loss: 0.6400Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.6405Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.6408Epoch 11/15: [==============                ] 30/63 batches, loss: 0.6400Epoch 11/15: [==============                ] 31/63 batches, loss: 0.6394Epoch 11/15: [===============               ] 32/63 batches, loss: 0.6393Epoch 11/15: [===============               ] 33/63 batches, loss: 0.6406Epoch 11/15: [================              ] 34/63 batches, loss: 0.6409Epoch 11/15: [================              ] 35/63 batches, loss: 0.6402Epoch 11/15: [=================             ] 36/63 batches, loss: 0.6407Epoch 11/15: [=================             ] 37/63 batches, loss: 0.6400Epoch 11/15: [==================            ] 38/63 batches, loss: 0.6424Epoch 11/15: [==================            ] 39/63 batches, loss: 0.6437Epoch 11/15: [===================           ] 40/63 batches, loss: 0.6465Epoch 11/15: [===================           ] 41/63 batches, loss: 0.6470Epoch 11/15: [====================          ] 42/63 batches, loss: 0.6461Epoch 11/15: [====================          ] 43/63 batches, loss: 0.6470Epoch 11/15: [====================          ] 44/63 batches, loss: 0.6473Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.6486Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.6488Epoch 11/15: [======================        ] 47/63 batches, loss: 0.6499Epoch 11/15: [======================        ] 48/63 batches, loss: 0.6500Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.6497Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.6494Epoch 11/15: [========================      ] 51/63 batches, loss: 0.6495Epoch 11/15: [========================      ] 52/63 batches, loss: 0.6502Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.6507Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.6493Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.6496Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.6498Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.6506Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.6490Epoch 11/15: [============================  ] 59/63 batches, loss: 0.6493Epoch 11/15: [============================  ] 60/63 batches, loss: 0.6499Epoch 11/15: [============================= ] 61/63 batches, loss: 0.6494Epoch 11/15: [============================= ] 62/63 batches, loss: 0.6499Epoch 11/15: [==============================] 63/63 batches, loss: 0.6515
[2025-05-02 17:49:31,069][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.6515
[2025-05-02 17:49:31,507][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6815, Metrics: {'accuracy': 0.6136363636363636, 'f1': 0.41379310344827586, 'precision': 0.6666666666666666, 'recall': 0.3}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.6272Epoch 12/15: [                              ] 2/63 batches, loss: 0.6372Epoch 12/15: [=                             ] 3/63 batches, loss: 0.6668Epoch 12/15: [=                             ] 4/63 batches, loss: 0.6686Epoch 12/15: [==                            ] 5/63 batches, loss: 0.6617Epoch 12/15: [==                            ] 6/63 batches, loss: 0.6595Epoch 12/15: [===                           ] 7/63 batches, loss: 0.6581Epoch 12/15: [===                           ] 8/63 batches, loss: 0.6524Epoch 12/15: [====                          ] 9/63 batches, loss: 0.6468Epoch 12/15: [====                          ] 10/63 batches, loss: 0.6488Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.6436Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.6454Epoch 12/15: [======                        ] 13/63 batches, loss: 0.6448Epoch 12/15: [======                        ] 14/63 batches, loss: 0.6420Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.6432Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.6503Epoch 12/15: [========                      ] 17/63 batches, loss: 0.6494Epoch 12/15: [========                      ] 18/63 batches, loss: 0.6508Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.6499Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.6488Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.6515Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.6510Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.6510Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.6513Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.6509Epoch 12/15: [============                  ] 26/63 batches, loss: 0.6512Epoch 12/15: [============                  ] 27/63 batches, loss: 0.6503Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.6485Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.6471Epoch 12/15: [==============                ] 30/63 batches, loss: 0.6469Epoch 12/15: [==============                ] 31/63 batches, loss: 0.6446Epoch 12/15: [===============               ] 32/63 batches, loss: 0.6454Epoch 12/15: [===============               ] 33/63 batches, loss: 0.6462Epoch 12/15: [================              ] 34/63 batches, loss: 0.6465Epoch 12/15: [================              ] 35/63 batches, loss: 0.6474Epoch 12/15: [=================             ] 36/63 batches, loss: 0.6479Epoch 12/15: [=================             ] 37/63 batches, loss: 0.6470Epoch 12/15: [==================            ] 38/63 batches, loss: 0.6475Epoch 12/15: [==================            ] 39/63 batches, loss: 0.6467Epoch 12/15: [===================           ] 40/63 batches, loss: 0.6465Epoch 12/15: [===================           ] 41/63 batches, loss: 0.6446Epoch 12/15: [====================          ] 42/63 batches, loss: 0.6465Epoch 12/15: [====================          ] 43/63 batches, loss: 0.6480Epoch 12/15: [====================          ] 44/63 batches, loss: 0.6468Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.6468Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.6472Epoch 12/15: [======================        ] 47/63 batches, loss: 0.6468Epoch 12/15: [======================        ] 48/63 batches, loss: 0.6464Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.6444Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.6443Epoch 12/15: [========================      ] 51/63 batches, loss: 0.6438Epoch 12/15: [========================      ] 52/63 batches, loss: 0.6432Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.6429Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.6432Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.6419Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.6425Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.6416Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.6419Epoch 12/15: [============================  ] 59/63 batches, loss: 0.6422Epoch 12/15: [============================  ] 60/63 batches, loss: 0.6432Epoch 12/15: [============================= ] 61/63 batches, loss: 0.6434Epoch 12/15: [============================= ] 62/63 batches, loss: 0.6424Epoch 12/15: [==============================] 63/63 batches, loss: 0.6443
[2025-05-02 17:49:32,890][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.6443
[2025-05-02 17:49:33,292][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6778, Metrics: {'accuracy': 0.6136363636363636, 'f1': 0.45161290322580644, 'precision': 0.6363636363636364, 'recall': 0.35}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.6092Epoch 13/15: [                              ] 2/63 batches, loss: 0.6269Epoch 13/15: [=                             ] 3/63 batches, loss: 0.6283Epoch 13/15: [=                             ] 4/63 batches, loss: 0.6349Epoch 13/15: [==                            ] 5/63 batches, loss: 0.6334Epoch 13/15: [==                            ] 6/63 batches, loss: 0.6342Epoch 13/15: [===                           ] 7/63 batches, loss: 0.6409Epoch 13/15: [===                           ] 8/63 batches, loss: 0.6355Epoch 13/15: [====                          ] 9/63 batches, loss: 0.6362Epoch 13/15: [====                          ] 10/63 batches, loss: 0.6334Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.6296Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.6239Epoch 13/15: [======                        ] 13/63 batches, loss: 0.6269Epoch 13/15: [======                        ] 14/63 batches, loss: 0.6272Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.6297Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.6281Epoch 13/15: [========                      ] 17/63 batches, loss: 0.6315Epoch 13/15: [========                      ] 18/63 batches, loss: 0.6285Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.6310Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.6319Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.6317Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.6324Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.6324Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.6325Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.6302Epoch 13/15: [============                  ] 26/63 batches, loss: 0.6282Epoch 13/15: [============                  ] 27/63 batches, loss: 0.6284Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.6282Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.6296Epoch 13/15: [==============                ] 30/63 batches, loss: 0.6291Epoch 13/15: [==============                ] 31/63 batches, loss: 0.6274Epoch 13/15: [===============               ] 32/63 batches, loss: 0.6261Epoch 13/15: [===============               ] 33/63 batches, loss: 0.6290Epoch 13/15: [================              ] 34/63 batches, loss: 0.6302Epoch 13/15: [================              ] 35/63 batches, loss: 0.6322Epoch 13/15: [=================             ] 36/63 batches, loss: 0.6334Epoch 13/15: [=================             ] 37/63 batches, loss: 0.6335Epoch 13/15: [==================            ] 38/63 batches, loss: 0.6348Epoch 13/15: [==================            ] 39/63 batches, loss: 0.6339Epoch 13/15: [===================           ] 40/63 batches, loss: 0.6345Epoch 13/15: [===================           ] 41/63 batches, loss: 0.6340Epoch 13/15: [====================          ] 42/63 batches, loss: 0.6332Epoch 13/15: [====================          ] 43/63 batches, loss: 0.6338Epoch 13/15: [====================          ] 44/63 batches, loss: 0.6332Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.6325Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.6325Epoch 13/15: [======================        ] 47/63 batches, loss: 0.6327Epoch 13/15: [======================        ] 48/63 batches, loss: 0.6329Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.6326Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.6324Epoch 13/15: [========================      ] 51/63 batches, loss: 0.6328Epoch 13/15: [========================      ] 52/63 batches, loss: 0.6330Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.6336Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.6357Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.6359Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.6363Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.6360Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.6374Epoch 13/15: [============================  ] 59/63 batches, loss: 0.6377Epoch 13/15: [============================  ] 60/63 batches, loss: 0.6389Epoch 13/15: [============================= ] 61/63 batches, loss: 0.6383Epoch 13/15: [============================= ] 62/63 batches, loss: 0.6389Epoch 13/15: [==============================] 63/63 batches, loss: 0.6382
[2025-05-02 17:49:34,612][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.6382
[2025-05-02 17:49:35,057][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.6741, Metrics: {'accuracy': 0.6136363636363636, 'f1': 0.48484848484848486, 'precision': 0.6153846153846154, 'recall': 0.4}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5715Epoch 14/15: [                              ] 2/63 batches, loss: 0.5901Epoch 14/15: [=                             ] 3/63 batches, loss: 0.6131Epoch 14/15: [=                             ] 4/63 batches, loss: 0.6087Epoch 14/15: [==                            ] 5/63 batches, loss: 0.6168Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5992Epoch 14/15: [===                           ] 7/63 batches, loss: 0.6164Epoch 14/15: [===                           ] 8/63 batches, loss: 0.6185Epoch 14/15: [====                          ] 9/63 batches, loss: 0.6173Epoch 14/15: [====                          ] 10/63 batches, loss: 0.6188Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.6235Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.6203Epoch 14/15: [======                        ] 13/63 batches, loss: 0.6181Epoch 14/15: [======                        ] 14/63 batches, loss: 0.6171Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.6148Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.6132Epoch 14/15: [========                      ] 17/63 batches, loss: 0.6165Epoch 14/15: [========                      ] 18/63 batches, loss: 0.6186Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.6191Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.6210Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.6217Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.6251Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.6252Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.6259Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.6291Epoch 14/15: [============                  ] 26/63 batches, loss: 0.6291Epoch 14/15: [============                  ] 27/63 batches, loss: 0.6295Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.6298Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.6318Epoch 14/15: [==============                ] 30/63 batches, loss: 0.6305Epoch 14/15: [==============                ] 31/63 batches, loss: 0.6327Epoch 14/15: [===============               ] 32/63 batches, loss: 0.6344Epoch 14/15: [===============               ] 33/63 batches, loss: 0.6349Epoch 14/15: [================              ] 34/63 batches, loss: 0.6344Epoch 14/15: [================              ] 35/63 batches, loss: 0.6322Epoch 14/15: [=================             ] 36/63 batches, loss: 0.6324Epoch 14/15: [=================             ] 37/63 batches, loss: 0.6346Epoch 14/15: [==================            ] 38/63 batches, loss: 0.6345Epoch 14/15: [==================            ] 39/63 batches, loss: 0.6330Epoch 14/15: [===================           ] 40/63 batches, loss: 0.6329Epoch 14/15: [===================           ] 41/63 batches, loss: 0.6316Epoch 14/15: [====================          ] 42/63 batches, loss: 0.6312Epoch 14/15: [====================          ] 43/63 batches, loss: 0.6298Epoch 14/15: [====================          ] 44/63 batches, loss: 0.6320Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.6318Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.6314Epoch 14/15: [======================        ] 47/63 batches, loss: 0.6307Epoch 14/15: [======================        ] 48/63 batches, loss: 0.6316Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.6315Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.6327Epoch 14/15: [========================      ] 51/63 batches, loss: 0.6330Epoch 14/15: [========================      ] 52/63 batches, loss: 0.6312Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.6314Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.6312Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.6321Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.6328Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.6319Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.6315Epoch 14/15: [============================  ] 59/63 batches, loss: 0.6312Epoch 14/15: [============================  ] 60/63 batches, loss: 0.6319Epoch 14/15: [============================= ] 61/63 batches, loss: 0.6322Epoch 14/15: [============================= ] 62/63 batches, loss: 0.6324Epoch 14/15: [==============================] 63/63 batches, loss: 0.6331
[2025-05-02 17:49:36,401][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.6331
[2025-05-02 17:49:36,830][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.6700, Metrics: {'accuracy': 0.6363636363636364, 'f1': 0.5294117647058824, 'precision': 0.6428571428571429, 'recall': 0.45}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.6776Epoch 15/15: [                              ] 2/63 batches, loss: 0.6835Epoch 15/15: [=                             ] 3/63 batches, loss: 0.6555Epoch 15/15: [=                             ] 4/63 batches, loss: 0.6382Epoch 15/15: [==                            ] 5/63 batches, loss: 0.6379Epoch 15/15: [==                            ] 6/63 batches, loss: 0.6331Epoch 15/15: [===                           ] 7/63 batches, loss: 0.6318Epoch 15/15: [===                           ] 8/63 batches, loss: 0.6350Epoch 15/15: [====                          ] 9/63 batches, loss: 0.6344Epoch 15/15: [====                          ] 10/63 batches, loss: 0.6334Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.6309Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.6232Epoch 15/15: [======                        ] 13/63 batches, loss: 0.6243Epoch 15/15: [======                        ] 14/63 batches, loss: 0.6264Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.6199Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.6192Epoch 15/15: [========                      ] 17/63 batches, loss: 0.6273Epoch 15/15: [========                      ] 18/63 batches, loss: 0.6303Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.6303Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.6322Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.6341Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.6374Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.6360Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.6355Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.6399Epoch 15/15: [============                  ] 26/63 batches, loss: 0.6393Epoch 15/15: [============                  ] 27/63 batches, loss: 0.6384Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.6359Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.6361Epoch 15/15: [==============                ] 30/63 batches, loss: 0.6374Epoch 15/15: [==============                ] 31/63 batches, loss: 0.6358Epoch 15/15: [===============               ] 32/63 batches, loss: 0.6358Epoch 15/15: [===============               ] 33/63 batches, loss: 0.6367Epoch 15/15: [================              ] 34/63 batches, loss: 0.6360Epoch 15/15: [================              ] 35/63 batches, loss: 0.6343Epoch 15/15: [=================             ] 36/63 batches, loss: 0.6347Epoch 15/15: [=================             ] 37/63 batches, loss: 0.6344Epoch 15/15: [==================            ] 38/63 batches, loss: 0.6334Epoch 15/15: [==================            ] 39/63 batches, loss: 0.6347Epoch 15/15: [===================           ] 40/63 batches, loss: 0.6352Epoch 15/15: [===================           ] 41/63 batches, loss: 0.6355Epoch 15/15: [====================          ] 42/63 batches, loss: 0.6352Epoch 15/15: [====================          ] 43/63 batches, loss: 0.6338Epoch 15/15: [====================          ] 44/63 batches, loss: 0.6338Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.6322Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.6313Epoch 15/15: [======================        ] 47/63 batches, loss: 0.6300Epoch 15/15: [======================        ] 48/63 batches, loss: 0.6297Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.6292Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.6303Epoch 15/15: [========================      ] 51/63 batches, loss: 0.6299Epoch 15/15: [========================      ] 52/63 batches, loss: 0.6297Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.6298Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.6303Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.6307Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.6300Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.6295Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.6298Epoch 15/15: [============================  ] 59/63 batches, loss: 0.6289Epoch 15/15: [============================  ] 60/63 batches, loss: 0.6285Epoch 15/15: [============================= ] 61/63 batches, loss: 0.6273Epoch 15/15: [============================= ] 62/63 batches, loss: 0.6268Epoch 15/15: [==============================] 63/63 batches, loss: 0.6260
[2025-05-02 17:49:38,252][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.6260
[2025-05-02 17:49:38,668][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.6660, Metrics: {'accuracy': 0.6590909090909091, 'f1': 0.5714285714285714, 'precision': 0.6666666666666666, 'recall': 0.5}
[2025-05-02 17:49:38,883][src.training.lm_trainer][INFO] - Training completed in 27.83 seconds
[2025-05-02 17:49:38,883][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 17:49:40,986][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.8442211055276382, 'f1': 0.8248587570621468, 'precision': 0.9407216494845361, 'recall': 0.7344064386317908}
[2025-05-02 17:49:40,987][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.6590909090909091, 'f1': 0.5714285714285714, 'precision': 0.6666666666666666, 'recall': 0.5}
[2025-05-02 17:49:40,987][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6493506493506493, 'f1': 0.5423728813559322, 'precision': 0.43243243243243246, 'recall': 0.7272727272727273}
[2025-05-02 17:49:42,233][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr2e-5/ar/model.pt
[2025-05-02 17:49:42,237][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▃▃▃▃▃▃▃▂▁▄▆▆▆▇█
wandb:           best_val_f1 ▁▁▁▁▁▁▁▂▂▅▆▇▇▇█
wandb:         best_val_loss █▇▇▆▆▆▅▅▅▄▄▃▂▂▁
wandb:    best_val_precision ▁▁▁▁▁▁▁▅▄▇██▇██
wandb:       best_val_recall ▁▁▁▁▁▁▁▂▂▄▅▆▇▇█
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁▁▁▂▂▂
wandb:            train_loss █▇▇▇▆▆▅▅▄▄▃▃▂▂▁
wandb:            train_time ▁
wandb:          val_accuracy ▃▃▃▃▃▃▃▂▁▄▆▆▆▇█
wandb:                val_f1 ▁▁▁▁▁▁▁▂▂▅▆▇▇▇█
wandb:              val_loss █▇▇▆▆▆▅▅▅▄▄▃▂▂▁
wandb:         val_precision ▁▁▁▁▁▁▁▅▄▇██▇██
wandb:            val_recall ▁▁▁▁▁▁▁▂▂▄▅▆▇▇█
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.65909
wandb:           best_val_f1 0.57143
wandb:         best_val_loss 0.66605
wandb:    best_val_precision 0.66667
wandb:       best_val_recall 0.5
wandb:                 epoch 15
wandb:   final_test_accuracy 0.64935
wandb:         final_test_f1 0.54237
wandb:  final_test_precision 0.43243
wandb:     final_test_recall 0.72727
wandb:  final_train_accuracy 0.84422
wandb:        final_train_f1 0.82486
wandb: final_train_precision 0.94072
wandb:    final_train_recall 0.73441
wandb:    final_val_accuracy 0.65909
wandb:          final_val_f1 0.57143
wandb:   final_val_precision 0.66667
wandb:      final_val_recall 0.5
wandb:         learning_rate 2e-05
wandb:            train_loss 0.62603
wandb:            train_time 27.82736
wandb:          val_accuracy 0.65909
wandb:                val_f1 0.57143
wandb:              val_loss 0.66605
wandb:         val_precision 0.66667
wandb:            val_recall 0.5
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_174858-oj7jssyg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_174858-oj7jssyg/logs
Experiment 2 completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.01_lr2e-5/ar/results.json
==============================================
Running experiment ID: 3
Language: ar, Task: question_type, Layer: 1
Hidden Size: 10, Dropout: 0.1, LR: 1e-4
==============================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 17:50:04,642][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr1e-4
experiment_name: probe_ar_question_type_layer1_h10_d0.1_lr1e-4
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 10
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 17:50:04,643][__main__][INFO] - Normalized task: question_type
[2025-05-02 17:50:04,643][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 17:50:04,643][__main__][INFO] - Determined Task Type: classification
[2025-05-02 17:50:04,646][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 17:50:04,647][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 17:50:07,379][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 17:50:09,367][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 17:50:09,367][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:50:09,490][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,558][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,744][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 17:50:09,750][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:50:09,751][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 17:50:09,751][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:50:09,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,823][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,844][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 17:50:09,845][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:50:09,845][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 17:50:09,846][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:50:09,873][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,917][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:50:09,927][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 17:50:09,928][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:50:09,928][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 17:50:09,929][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 17:50:09,929][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:50:09,929][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:50:09,929][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:50:09,929][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:50:09,930][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 17:50:09,930][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:50:09,930][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 17:50:09,930][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:50:09,930][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:50:09,931][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 17:50:09,931][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 17:50:09,931][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 17:50:09,931][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 17:50:09,931][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 17:50:14,448][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 17:50:14,449][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 17:50:14,449][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 17:50:14,449][src.models.model_factory][INFO] - Using provided probe_hidden_size: 10
[2025-05-02 17:50:14,450][src.models.model_factory][INFO] - Model has 9,367 trainable parameters out of 394,130,839 total parameters
[2025-05-02 17:50:14,450][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 9,367 trainable parameters
[2025-05-02 17:50:14,450][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=10, depth=2, activation=gelu, normalization=layer
[2025-05-02 17:50:14,450][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 10 hidden size
[2025-05-02 17:50:14,450][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 17:50:14,451][__main__][INFO] - Total parameters: 394,130,839
[2025-05-02 17:50:14,451][__main__][INFO] - Trainable parameters: 9,367 (0.00%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6851Epoch 1/15: [                              ] 2/63 batches, loss: 0.7187Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7114Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7191Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7285Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7219Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7187Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7125Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7239Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7271Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7243Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7210Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7206Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7178Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7171Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7176Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7201Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7200Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7206Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7172Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7168Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7154Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7140Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7140Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7133Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7135Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7115Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7112Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7093Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7090Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7077Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7070Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7058Epoch 1/15: [================              ] 34/63 batches, loss: 0.7058Epoch 1/15: [================              ] 35/63 batches, loss: 0.7053Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7051Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7038Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7048Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7043Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7048Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7046Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7048Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7034Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7031Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7026Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7027Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7024Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7032Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7033Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7026Epoch 1/15: [========================      ] 51/63 batches, loss: 0.7019Epoch 1/15: [========================      ] 52/63 batches, loss: 0.7014Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.7016Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.7021Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.7030Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.7025Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.7019Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.7017Epoch 1/15: [============================  ] 59/63 batches, loss: 0.7013Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7010Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7011Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7015Epoch 1/15: [==============================] 63/63 batches, loss: 0.7003
[2025-05-02 17:50:19,972][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7003
[2025-05-02 17:50:20,378][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6954, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6688Epoch 2/15: [                              ] 2/63 batches, loss: 0.6899Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6703Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6783Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6806Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6804Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6815Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6801Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6857Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6856Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6869Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6872Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6863Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6862Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6859Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6868Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6855Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6856Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6853Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6852Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6849Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6852Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6861Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6859Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6858Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6858Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6867Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6871Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6866Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6865Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6874Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6869Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6879Epoch 2/15: [================              ] 34/63 batches, loss: 0.6884Epoch 2/15: [================              ] 35/63 batches, loss: 0.6886Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6889Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6880Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6880Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6892Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6899Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6893Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6893Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6897Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6892Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6894Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6894Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6890Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6891Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6888Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6884Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6889Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6890Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6888Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6883Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6890Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6896Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6886Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6885Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6881Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6886Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6878Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6875Epoch 2/15: [==============================] 63/63 batches, loss: 0.6862
[2025-05-02 17:50:21,727][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6862
[2025-05-02 17:50:22,115][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6918, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6720Epoch 3/15: [                              ] 2/63 batches, loss: 0.6780Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6796Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6788Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6809Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6801Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6781Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6819Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6872Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6848Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6859Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6849Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6848Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6833Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6806Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6788Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6785Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6747Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6744Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6757Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6775Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6769Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6775Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6768Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6776Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6783Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6784Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6772Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6762Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6761Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6762Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6755Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6744Epoch 3/15: [================              ] 34/63 batches, loss: 0.6740Epoch 3/15: [================              ] 35/63 batches, loss: 0.6740Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6741Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6731Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6731Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6720Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6714Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6697Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6704Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6705Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6694Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6701Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6689Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6689Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6688Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6686Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6689Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6707Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6717Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6718Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6719Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6716Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6710Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6709Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6705Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6705Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6699Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6688Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6685Epoch 3/15: [==============================] 63/63 batches, loss: 0.6681
[2025-05-02 17:50:23,446][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6681
[2025-05-02 17:50:23,840][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6829, Metrics: {'accuracy': 0.5227272727272727, 'f1': 0.16, 'precision': 0.4, 'recall': 0.1}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6542Epoch 4/15: [                              ] 2/63 batches, loss: 0.6577Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6294Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6462Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6537Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6552Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6508Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6469Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6429Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6491Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6524Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6495Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6520Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6536Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6554Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6560Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6539Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6548Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6565Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6535Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6517Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6543Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6527Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6572Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6558Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6572Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6560Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6554Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6559Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6559Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6558Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6533Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6519Epoch 4/15: [================              ] 34/63 batches, loss: 0.6526Epoch 4/15: [================              ] 35/63 batches, loss: 0.6536Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6525Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6532Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6524Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6522Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6530Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6555Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6559Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6572Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6564Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6579Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6572Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6569Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6564Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6565Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6547Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6545Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6536Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6544Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6534Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6529Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6518Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6510Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6511Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6512Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6508Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6513Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6514Epoch 4/15: [==============================] 63/63 batches, loss: 0.6508
[2025-05-02 17:50:25,173][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6508
[2025-05-02 17:50:25,559][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6703, Metrics: {'accuracy': 0.6136363636363636, 'f1': 0.48484848484848486, 'precision': 0.6153846153846154, 'recall': 0.4}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6118Epoch 5/15: [                              ] 2/63 batches, loss: 0.5997Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6265Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6200Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6275Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6223Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6206Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6090Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6110Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6174Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6133Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6178Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6236Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6276Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6219Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6180Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6150Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6174Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6192Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6220Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6288Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6273Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6271Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6239Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6241Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6232Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6242Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6213Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6206Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6229Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6216Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6233Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6246Epoch 5/15: [================              ] 34/63 batches, loss: 0.6239Epoch 5/15: [================              ] 35/63 batches, loss: 0.6251Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6245Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6250Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6263Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6252Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6253Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6262Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6260Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6253Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6242Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6239Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6231Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6235Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6220Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6228Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6228Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6239Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6234Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6240Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6255Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6254Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6254Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6256Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6257Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6259Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6269Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6274Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6263Epoch 5/15: [==============================] 63/63 batches, loss: 0.6243
[2025-05-02 17:50:26,852][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6243
[2025-05-02 17:50:27,202][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6532, Metrics: {'accuracy': 0.7727272727272727, 'f1': 0.7222222222222222, 'precision': 0.8125, 'recall': 0.65}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.6196Epoch 6/15: [                              ] 2/63 batches, loss: 0.6666Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6376Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6317Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6290Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6344Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6441Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6463Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6444Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6392Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6340Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6245Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6192Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6238Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6213Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6210Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6192Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6146Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6146Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6131Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6155Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6138Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6160Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6160Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6151Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6155Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6128Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6110Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6130Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6114Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6142Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6145Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6136Epoch 6/15: [================              ] 34/63 batches, loss: 0.6143Epoch 6/15: [================              ] 35/63 batches, loss: 0.6134Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6131Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6106Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6124Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6135Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6130Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6131Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6141Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6128Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6113Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6103Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6105Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6093Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6084Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6088Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6077Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6091Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6086Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6088Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6099Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6107Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6109Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6101Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6081Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6078Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6078Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6074Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6087Epoch 6/15: [==============================] 63/63 batches, loss: 0.6109
[2025-05-02 17:50:28,511][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6109
[2025-05-02 17:50:28,897][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6392, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8372093023255814, 'precision': 0.782608695652174, 'recall': 0.9}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5521Epoch 7/15: [                              ] 2/63 batches, loss: 0.5752Epoch 7/15: [=                             ] 3/63 batches, loss: 0.6069Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5917Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5921Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5983Epoch 7/15: [===                           ] 7/63 batches, loss: 0.6024Epoch 7/15: [===                           ] 8/63 batches, loss: 0.6007Epoch 7/15: [====                          ] 9/63 batches, loss: 0.6092Epoch 7/15: [====                          ] 10/63 batches, loss: 0.6072Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.6126Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.6117Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5996Epoch 7/15: [======                        ] 14/63 batches, loss: 0.6015Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.6026Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.6063Epoch 7/15: [========                      ] 17/63 batches, loss: 0.6031Epoch 7/15: [========                      ] 18/63 batches, loss: 0.6010Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.6015Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.6012Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.6022Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.6024Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5991Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.6001Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5986Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5967Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5982Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5990Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5998Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5975Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5981Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5991Epoch 7/15: [===============               ] 33/63 batches, loss: 0.6011Epoch 7/15: [================              ] 34/63 batches, loss: 0.6002Epoch 7/15: [================              ] 35/63 batches, loss: 0.5982Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5978Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5976Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5980Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5953Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5955Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5951Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5947Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5949Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5972Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5979Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5973Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5976Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5967Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5959Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5968Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5962Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5963Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5965Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5960Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5958Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5957Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5971Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5986Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5985Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5968Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5959Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5963Epoch 7/15: [==============================] 63/63 batches, loss: 0.5958
[2025-05-02 17:50:30,259][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5958
[2025-05-02 17:50:30,670][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6283, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8372093023255814, 'precision': 0.782608695652174, 'recall': 0.9}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5973Epoch 8/15: [                              ] 2/63 batches, loss: 0.5397Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5494Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5664Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5773Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5883Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5870Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5867Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5844Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5825Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5956Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5963Epoch 8/15: [======                        ] 13/63 batches, loss: 0.6002Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5968Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5915Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5875Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5892Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5886Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5909Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5946Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5940Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5965Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5983Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5952Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5952Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5925Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5919Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5908Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5899Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5888Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5878Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5868Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5869Epoch 8/15: [================              ] 34/63 batches, loss: 0.5874Epoch 8/15: [================              ] 35/63 batches, loss: 0.5869Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5865Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5849Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5874Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5860Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5871Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5886Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5902Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5922Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5895Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5883Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5866Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5861Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5872Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5881Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5876Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5888Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5884Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5874Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5888Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5873Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5856Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5860Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5866Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5858Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5860Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5859Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5855Epoch 8/15: [==============================] 63/63 batches, loss: 0.5828
[2025-05-02 17:50:31,991][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5828
[2025-05-02 17:50:32,417][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6213, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8571428571428571, 'precision': 0.8181818181818182, 'recall': 0.9}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.5151Epoch 9/15: [                              ] 2/63 batches, loss: 0.5346Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5356Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5343Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5560Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5589Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5683Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5760Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5719Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5743Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5805Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5846Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5838Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5860Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5847Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5862Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5850Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5853Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5838Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5852Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5910Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5895Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5846Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5858Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5843Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5843Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5833Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5811Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5817Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5811Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5768Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5748Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5724Epoch 9/15: [================              ] 34/63 batches, loss: 0.5721Epoch 9/15: [================              ] 35/63 batches, loss: 0.5729Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5732Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5755Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5718Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5711Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5696Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5708Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5710Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5717Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5730Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5716Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5707Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5707Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5695Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5693Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5699Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5702Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5706Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5701Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5700Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5686Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5696Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5715Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5726Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5730Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5739Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5737Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5740Epoch 9/15: [==============================] 63/63 batches, loss: 0.5744
[2025-05-02 17:50:33,733][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5744
[2025-05-02 17:50:34,134][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6142, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5907Epoch 10/15: [                              ] 2/63 batches, loss: 0.5977Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5951Epoch 10/15: [=                             ] 4/63 batches, loss: 0.6065Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5979Epoch 10/15: [==                            ] 6/63 batches, loss: 0.6003Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5953Epoch 10/15: [===                           ] 8/63 batches, loss: 0.6011Epoch 10/15: [====                          ] 9/63 batches, loss: 0.6001Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5859Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5789Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5829Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5904Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5877Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5868Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5909Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5917Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5929Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5893Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5898Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5878Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5896Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5891Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5892Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5830Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5802Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5798Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5795Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5798Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5817Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5823Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5809Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5793Epoch 10/15: [================              ] 34/63 batches, loss: 0.5799Epoch 10/15: [================              ] 35/63 batches, loss: 0.5809Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5792Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5789Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5781Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5780Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5785Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5767Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5759Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5760Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5756Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5749Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5728Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5733Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5720Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5721Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5705Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5696Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5697Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5688Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5684Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5673Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5671Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5672Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5668Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5657Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5667Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5672Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5687Epoch 10/15: [==============================] 63/63 batches, loss: 0.5708
[2025-05-02 17:50:35,484][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5708
[2025-05-02 17:50:35,897][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6106, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5186Epoch 11/15: [                              ] 2/63 batches, loss: 0.5954Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5778Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5697Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5630Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5472Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5525Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5490Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5498Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5497Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5494Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5495Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5545Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5581Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5615Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5615Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5587Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5588Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5564Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5568Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5653Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5669Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5649Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5627Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5626Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5614Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5608Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5606Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5603Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5591Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5570Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5576Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5587Epoch 11/15: [================              ] 34/63 batches, loss: 0.5583Epoch 11/15: [================              ] 35/63 batches, loss: 0.5574Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5580Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5566Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5589Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5600Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5639Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5646Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5636Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5640Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5642Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5659Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5661Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5668Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5669Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5667Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5656Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5659Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5665Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5674Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5657Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5658Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5663Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5673Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5653Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5656Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5664Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5657Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5664Epoch 11/15: [==============================] 63/63 batches, loss: 0.5670
[2025-05-02 17:50:37,277][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5670
[2025-05-02 17:50:37,660][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6040, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.5126Epoch 12/15: [                              ] 2/63 batches, loss: 0.5423Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5719Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5751Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5734Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5722Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5676Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5659Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5578Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5614Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5580Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5618Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5607Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5556Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5556Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5626Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5611Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5615Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5606Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5602Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5635Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5636Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5635Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5645Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5651Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5662Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5646Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5619Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5601Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5600Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5569Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5586Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5599Epoch 12/15: [================              ] 34/63 batches, loss: 0.5601Epoch 12/15: [================              ] 35/63 batches, loss: 0.5617Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5631Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5624Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5634Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5629Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5625Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5601Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5622Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5640Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5632Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5630Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5637Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5632Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5624Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5600Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5600Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5598Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5593Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5590Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5589Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5577Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5584Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5568Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5574Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5574Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5586Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5587Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5574Epoch 12/15: [==============================] 63/63 batches, loss: 0.5580
[2025-05-02 17:50:38,960][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5580
[2025-05-02 17:50:39,396][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5995, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5157Epoch 13/15: [                              ] 2/63 batches, loss: 0.5302Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5251Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5401Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5395Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5398Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5477Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5442Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5449Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5440Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5415Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5367Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5407Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5419Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5449Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5411Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5454Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5426Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5452Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5461Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5460Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5488Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5493Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5482Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5461Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5433Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5434Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5431Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5446Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5442Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5417Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5399Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5420Epoch 13/15: [================              ] 34/63 batches, loss: 0.5429Epoch 13/15: [================              ] 35/63 batches, loss: 0.5448Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5469Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5470Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5475Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5462Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5470Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5459Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5450Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5456Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5454Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5447Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5450Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5450Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5448Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5449Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5451Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5459Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5461Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5465Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5487Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5493Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5501Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5500Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5520Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5524Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5539Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5540Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5549Epoch 13/15: [==============================] 63/63 batches, loss: 0.5535
[2025-05-02 17:50:40,777][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5535
[2025-05-02 17:50:41,184][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5952, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5073Epoch 14/15: [                              ] 2/63 batches, loss: 0.5021Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5311Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5270Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5339Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5193Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5372Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5395Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5368Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5361Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5414Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5364Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5342Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5344Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5309Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5284Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5325Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5339Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5342Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5363Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5378Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5416Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5416Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5424Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5454Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5457Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5454Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5448Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5457Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5447Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5478Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5502Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5522Epoch 14/15: [================              ] 34/63 batches, loss: 0.5521Epoch 14/15: [================              ] 35/63 batches, loss: 0.5501Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5501Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5519Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5523Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5505Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5495Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5487Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5485Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5477Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5498Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5500Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5494Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5489Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5499Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5494Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5510Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5517Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5503Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5499Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5500Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5510Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5518Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5509Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5506Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5506Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5512Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5519Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5517Epoch 14/15: [==============================] 63/63 batches, loss: 0.5527
[2025-05-02 17:50:42,561][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5527
[2025-05-02 17:50:42,973][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5922, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5858Epoch 15/15: [                              ] 2/63 batches, loss: 0.5865Epoch 15/15: [=                             ] 3/63 batches, loss: 0.5627Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5474Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5466Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5442Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5430Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5503Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5490Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5480Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5465Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5380Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5373Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5420Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5338Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5316Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5410Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5446Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5436Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5453Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5465Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5507Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5500Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5484Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5533Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5526Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5526Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5511Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5514Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5539Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5519Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5514Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5531Epoch 15/15: [================              ] 34/63 batches, loss: 0.5529Epoch 15/15: [================              ] 35/63 batches, loss: 0.5515Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5515Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5512Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5503Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5520Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5528Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5529Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5531Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5519Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5514Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5493Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5485Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5472Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5467Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5470Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5475Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5471Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5467Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5469Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5476Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5482Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5473Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5462Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5464Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5456Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5454Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5440Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5434Epoch 15/15: [==============================] 63/63 batches, loss: 0.5425
[2025-05-02 17:50:44,415][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5425
[2025-05-02 17:50:44,846][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5895, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
[2025-05-02 17:50:45,043][src.training.lm_trainer][INFO] - Training completed in 27.46 seconds
[2025-05-02 17:50:45,043][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 17:50:47,036][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9909547738693467, 'f1': 0.9909182643794148, 'precision': 0.9939271255060729, 'recall': 0.9879275653923542}
[2025-05-02 17:50:47,037][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
[2025-05-02 17:50:47,037][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7012987012987013, 'f1': 0.6567164179104478, 'precision': 0.4888888888888889, 'recall': 1.0}
[2025-05-02 17:50:48,320][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr1e-4/ar/model.pt
[2025-05-02 17:50:48,321][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▃▆▇▇▇███████
wandb:           best_val_f1 ▁▁▂▅▇▇▇████████
wandb:         best_val_loss ██▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb:    best_val_precision ▁▁▄▆█▇▇████████
wandb:       best_val_recall ▁▁▂▄▆██████████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▂▂▂▂▃▃▃▃▃▃
wandb:            train_loss █▇▇▆▅▄▃▃▂▂▂▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▃▆▇▇▇███████
wandb:                val_f1 ▁▁▂▅▇▇▇████████
wandb:              val_loss ██▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb:         val_precision ▁▁▄▆█▇▇████████
wandb:            val_recall ▁▁▂▄▆██████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90476
wandb:         best_val_loss 0.58947
wandb:    best_val_precision 0.86364
wandb:       best_val_recall 0.95
wandb:                 epoch 15
wandb:   final_test_accuracy 0.7013
wandb:         final_test_f1 0.65672
wandb:  final_test_precision 0.48889
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99095
wandb:        final_train_f1 0.99092
wandb: final_train_precision 0.99393
wandb:    final_train_recall 0.98793
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90476
wandb:   final_val_precision 0.86364
wandb:      final_val_recall 0.95
wandb:         learning_rate 0.0001
wandb:            train_loss 0.54249
wandb:            train_time 27.45608
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90476
wandb:              val_loss 0.58947
wandb:         val_precision 0.86364
wandb:            val_recall 0.95
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_175004-5hfs1g81
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_175004-5hfs1g81/logs
Experiment 3 completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr1e-4/ar/results.json
==============================================
Running experiment ID: 4
Language: ar, Task: question_type, Layer: 1
Hidden Size: 10, Dropout: 0.1, LR: 2e-5
==============================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 17:51:09,923][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr2e-5
experiment_name: probe_ar_question_type_layer1_h10_d0.1_lr2e-5
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 10
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 17:51:09,923][__main__][INFO] - Normalized task: question_type
[2025-05-02 17:51:09,923][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 17:51:09,923][__main__][INFO] - Determined Task Type: classification
[2025-05-02 17:51:09,927][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 17:51:09,927][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 17:51:12,326][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 17:51:14,293][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 17:51:14,294][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:51:14,438][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,474][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,617][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 17:51:14,623][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:51:14,623][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 17:51:14,625][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:51:14,677][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,740][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,750][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 17:51:14,751][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:51:14,751][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 17:51:14,753][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 17:51:14,786][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,827][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 17:51:14,859][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 17:51:14,861][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 17:51:14,861][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 17:51:14,862][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 17:51:14,862][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:51:14,862][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:51:14,862][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:51:14,863][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 17:51:14,863][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:51:14,863][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 17:51:14,863][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 17:51:14,863][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 17:51:14,864][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 17:51:14,864][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 17:51:14,864][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 17:51:14,864][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 17:51:14,865][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 17:51:19,716][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 17:51:19,717][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 17:51:19,717][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 17:51:19,717][src.models.model_factory][INFO] - Using provided probe_hidden_size: 10
[2025-05-02 17:51:19,719][src.models.model_factory][INFO] - Model has 9,367 trainable parameters out of 394,130,839 total parameters
[2025-05-02 17:51:19,719][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 9,367 trainable parameters
[2025-05-02 17:51:19,719][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=10, depth=2, activation=gelu, normalization=layer
[2025-05-02 17:51:19,719][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 10 hidden size
[2025-05-02 17:51:19,719][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 17:51:19,720][__main__][INFO] - Total parameters: 394,130,839
[2025-05-02 17:51:19,720][__main__][INFO] - Trainable parameters: 9,367 (0.00%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6851Epoch 1/15: [                              ] 2/63 batches, loss: 0.7191Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7119Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7207Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7307Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7241Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7211Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7148Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7281Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7322Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7292Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7262Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7259Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7226Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7223Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7238Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7274Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7277Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7287Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7250Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7255Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7237Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7225Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7228Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7218Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7222Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7195Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7190Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7172Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7168Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7153Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7149Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7134Epoch 1/15: [================              ] 34/63 batches, loss: 0.7137Epoch 1/15: [================              ] 35/63 batches, loss: 0.7131Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7130Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7111Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7131Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7121Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7132Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7131Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7137Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7116Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7113Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7107Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7113Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7110Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7125Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7128Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7116Epoch 1/15: [========================      ] 51/63 batches, loss: 0.7107Epoch 1/15: [========================      ] 52/63 batches, loss: 0.7101Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.7103Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.7112Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.7125Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.7119Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.7112Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.7107Epoch 1/15: [============================  ] 59/63 batches, loss: 0.7104Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7099Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7099Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7110Epoch 1/15: [==============================] 63/63 batches, loss: 0.7094
[2025-05-02 17:51:25,606][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7094
[2025-05-02 17:51:26,015][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.7047, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6518Epoch 2/15: [                              ] 2/63 batches, loss: 0.6900Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6643Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6796Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6845Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6853Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6870Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6864Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6946Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6950Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6971Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6977Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6967Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6968Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6966Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6979Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6962Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6973Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6971Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6966Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6961Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6973Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6990Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6996Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6995Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6997Epoch 2/15: [============                  ] 27/63 batches, loss: 0.7010Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.7014Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.7009Epoch 2/15: [==============                ] 30/63 batches, loss: 0.7006Epoch 2/15: [==============                ] 31/63 batches, loss: 0.7020Epoch 2/15: [===============               ] 32/63 batches, loss: 0.7016Epoch 2/15: [===============               ] 33/63 batches, loss: 0.7029Epoch 2/15: [================              ] 34/63 batches, loss: 0.7036Epoch 2/15: [================              ] 35/63 batches, loss: 0.7039Epoch 2/15: [=================             ] 36/63 batches, loss: 0.7043Epoch 2/15: [=================             ] 37/63 batches, loss: 0.7032Epoch 2/15: [==================            ] 38/63 batches, loss: 0.7028Epoch 2/15: [==================            ] 39/63 batches, loss: 0.7042Epoch 2/15: [===================           ] 40/63 batches, loss: 0.7051Epoch 2/15: [===================           ] 41/63 batches, loss: 0.7045Epoch 2/15: [====================          ] 42/63 batches, loss: 0.7045Epoch 2/15: [====================          ] 43/63 batches, loss: 0.7049Epoch 2/15: [====================          ] 44/63 batches, loss: 0.7042Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.7048Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.7048Epoch 2/15: [======================        ] 47/63 batches, loss: 0.7039Epoch 2/15: [======================        ] 48/63 batches, loss: 0.7042Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.7038Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.7033Epoch 2/15: [========================      ] 51/63 batches, loss: 0.7039Epoch 2/15: [========================      ] 52/63 batches, loss: 0.7038Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.7036Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.7029Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.7036Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.7045Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.7037Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.7036Epoch 2/15: [============================  ] 59/63 batches, loss: 0.7031Epoch 2/15: [============================  ] 60/63 batches, loss: 0.7036Epoch 2/15: [============================= ] 61/63 batches, loss: 0.7029Epoch 2/15: [============================= ] 62/63 batches, loss: 0.7025Epoch 2/15: [==============================] 63/63 batches, loss: 0.7007
[2025-05-02 17:51:27,325][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.7007
[2025-05-02 17:51:27,708][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.7003, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6959Epoch 3/15: [                              ] 2/63 batches, loss: 0.7000Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6987Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6977Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6990Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6971Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6947Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6986Epoch 3/15: [====                          ] 9/63 batches, loss: 0.7046Epoch 3/15: [====                          ] 10/63 batches, loss: 0.7015Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.7029Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.7017Epoch 3/15: [======                        ] 13/63 batches, loss: 0.7024Epoch 3/15: [======                        ] 14/63 batches, loss: 0.7007Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6979Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6958Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6953Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6910Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6908Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6921Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6943Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6940Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6941Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6937Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6944Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6955Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6955Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6949Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6944Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6948Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6952Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6945Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6939Epoch 3/15: [================              ] 34/63 batches, loss: 0.6936Epoch 3/15: [================              ] 35/63 batches, loss: 0.6943Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6946Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6940Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6944Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6935Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6932Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6918Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6931Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6935Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6928Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6934Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6928Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6930Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6932Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6934Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6935Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6948Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6954Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6954Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6958Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6955Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6952Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6950Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6948Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6946Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6944Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6940Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6943Epoch 3/15: [==============================] 63/63 batches, loss: 0.6938
[2025-05-02 17:51:29,093][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6938
[2025-05-02 17:51:29,520][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6981, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6717Epoch 4/15: [                              ] 2/63 batches, loss: 0.6860Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6703Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6816Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6899Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6930Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6917Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6894Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6895Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6950Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6971Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6952Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6968Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6979Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6989Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6992Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6972Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6974Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6991Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6971Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6961Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6973Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6971Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.7007Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.7003Epoch 4/15: [============                  ] 26/63 batches, loss: 0.7008Epoch 4/15: [============                  ] 27/63 batches, loss: 0.7001Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6993Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6995Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6996Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6996Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6982Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6983Epoch 4/15: [================              ] 34/63 batches, loss: 0.6979Epoch 4/15: [================              ] 35/63 batches, loss: 0.6977Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6974Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6976Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6962Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6960Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6965Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6982Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6985Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6994Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6990Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.7000Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.7003Epoch 4/15: [======================        ] 47/63 batches, loss: 0.7000Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6999Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6997Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6986Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6988Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6981Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6987Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6981Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6984Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6980Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6975Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6972Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6973Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6969Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6973Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6975Epoch 4/15: [==============================] 63/63 batches, loss: 0.6973
[2025-05-02 17:51:30,868][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6973
[2025-05-02 17:51:31,297][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6965, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6824Epoch 5/15: [                              ] 2/63 batches, loss: 0.6788Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6886Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6869Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6888Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6870Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6912Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6870Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6844Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6885Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6864Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6880Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6908Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6927Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6913Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6881Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6856Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6852Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6864Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6879Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6915Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6916Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6922Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6915Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6915Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6907Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6920Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6908Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6903Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6907Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6897Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6910Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6918Epoch 5/15: [================              ] 34/63 batches, loss: 0.6922Epoch 5/15: [================              ] 35/63 batches, loss: 0.6923Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6923Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6929Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6933Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6921Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6926Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6930Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6918Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6915Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6919Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6917Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6915Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6906Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6905Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6905Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6910Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6907Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6912Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6917Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6922Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6922Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6919Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6919Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6921Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6927Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6929Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6924Epoch 5/15: [==============================] 63/63 batches, loss: 0.6913
[2025-05-02 17:51:32,642][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6913
[2025-05-02 17:51:33,067][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6951, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.7047Epoch 6/15: [                              ] 2/63 batches, loss: 0.7208Epoch 6/15: [=                             ] 3/63 batches, loss: 0.7144Epoch 6/15: [=                             ] 4/63 batches, loss: 0.7062Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6987Epoch 6/15: [==                            ] 6/63 batches, loss: 0.7003Epoch 6/15: [===                           ] 7/63 batches, loss: 0.7064Epoch 6/15: [===                           ] 8/63 batches, loss: 0.7056Epoch 6/15: [====                          ] 9/63 batches, loss: 0.7052Epoch 6/15: [====                          ] 10/63 batches, loss: 0.7019Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6999Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6942Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6933Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6963Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6942Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6940Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6933Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6904Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6902Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6885Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6894Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6893Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6899Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6902Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6905Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6901Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6896Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6893Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6899Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6888Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6897Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6898Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6896Epoch 6/15: [================              ] 34/63 batches, loss: 0.6900Epoch 6/15: [================              ] 35/63 batches, loss: 0.6899Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6894Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6876Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6886Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6885Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6883Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6883Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6885Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6882Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6872Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6867Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6869Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6864Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6863Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6868Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6866Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6875Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6873Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6874Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6883Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6894Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6893Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6890Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6881Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6878Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6878Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6879Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6885Epoch 6/15: [==============================] 63/63 batches, loss: 0.6892
[2025-05-02 17:51:34,367][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6892
[2025-05-02 17:51:34,806][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6940, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.6501Epoch 7/15: [                              ] 2/63 batches, loss: 0.6640Epoch 7/15: [=                             ] 3/63 batches, loss: 0.6793Epoch 7/15: [=                             ] 4/63 batches, loss: 0.6783Epoch 7/15: [==                            ] 5/63 batches, loss: 0.6769Epoch 7/15: [==                            ] 6/63 batches, loss: 0.6788Epoch 7/15: [===                           ] 7/63 batches, loss: 0.6844Epoch 7/15: [===                           ] 8/63 batches, loss: 0.6843Epoch 7/15: [====                          ] 9/63 batches, loss: 0.6872Epoch 7/15: [====                          ] 10/63 batches, loss: 0.6850Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.6865Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.6856Epoch 7/15: [======                        ] 13/63 batches, loss: 0.6800Epoch 7/15: [======                        ] 14/63 batches, loss: 0.6814Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.6824Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.6839Epoch 7/15: [========                      ] 17/63 batches, loss: 0.6827Epoch 7/15: [========                      ] 18/63 batches, loss: 0.6826Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.6826Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.6826Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.6840Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.6836Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.6819Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.6831Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.6834Epoch 7/15: [============                  ] 26/63 batches, loss: 0.6831Epoch 7/15: [============                  ] 27/63 batches, loss: 0.6843Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.6845Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.6847Epoch 7/15: [==============                ] 30/63 batches, loss: 0.6833Epoch 7/15: [==============                ] 31/63 batches, loss: 0.6842Epoch 7/15: [===============               ] 32/63 batches, loss: 0.6846Epoch 7/15: [===============               ] 33/63 batches, loss: 0.6862Epoch 7/15: [================              ] 34/63 batches, loss: 0.6857Epoch 7/15: [================              ] 35/63 batches, loss: 0.6853Epoch 7/15: [=================             ] 36/63 batches, loss: 0.6850Epoch 7/15: [=================             ] 37/63 batches, loss: 0.6847Epoch 7/15: [==================            ] 38/63 batches, loss: 0.6853Epoch 7/15: [==================            ] 39/63 batches, loss: 0.6839Epoch 7/15: [===================           ] 40/63 batches, loss: 0.6840Epoch 7/15: [===================           ] 41/63 batches, loss: 0.6846Epoch 7/15: [====================          ] 42/63 batches, loss: 0.6841Epoch 7/15: [====================          ] 43/63 batches, loss: 0.6851Epoch 7/15: [====================          ] 44/63 batches, loss: 0.6860Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.6865Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.6857Epoch 7/15: [======================        ] 47/63 batches, loss: 0.6857Epoch 7/15: [======================        ] 48/63 batches, loss: 0.6859Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.6852Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.6856Epoch 7/15: [========================      ] 51/63 batches, loss: 0.6851Epoch 7/15: [========================      ] 52/63 batches, loss: 0.6854Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.6859Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.6854Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.6856Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.6857Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.6866Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.6874Epoch 7/15: [============================  ] 59/63 batches, loss: 0.6871Epoch 7/15: [============================  ] 60/63 batches, loss: 0.6866Epoch 7/15: [============================= ] 61/63 batches, loss: 0.6863Epoch 7/15: [============================= ] 62/63 batches, loss: 0.6866Epoch 7/15: [==============================] 63/63 batches, loss: 0.6869
[2025-05-02 17:51:36,117][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6869
[2025-05-02 17:51:36,541][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6959Epoch 8/15: [                              ] 2/63 batches, loss: 0.6686Epoch 8/15: [=                             ] 3/63 batches, loss: 0.6771Epoch 8/15: [=                             ] 4/63 batches, loss: 0.6776Epoch 8/15: [==                            ] 5/63 batches, loss: 0.6836Epoch 8/15: [==                            ] 6/63 batches, loss: 0.6884Epoch 8/15: [===                           ] 7/63 batches, loss: 0.6852Epoch 8/15: [===                           ] 8/63 batches, loss: 0.6853Epoch 8/15: [====                          ] 9/63 batches, loss: 0.6819Epoch 8/15: [====                          ] 10/63 batches, loss: 0.6821Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.6874Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.6884Epoch 8/15: [======                        ] 13/63 batches, loss: 0.6902Epoch 8/15: [======                        ] 14/63 batches, loss: 0.6903Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.6888Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.6866Epoch 8/15: [========                      ] 17/63 batches, loss: 0.6882Epoch 8/15: [========                      ] 18/63 batches, loss: 0.6878Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.6892Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.6902Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.6891Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.6910Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.6913Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.6895Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.6898Epoch 8/15: [============                  ] 26/63 batches, loss: 0.6886Epoch 8/15: [============                  ] 27/63 batches, loss: 0.6884Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.6866Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.6852Epoch 8/15: [==============                ] 30/63 batches, loss: 0.6850Epoch 8/15: [==============                ] 31/63 batches, loss: 0.6851Epoch 8/15: [===============               ] 32/63 batches, loss: 0.6848Epoch 8/15: [===============               ] 33/63 batches, loss: 0.6849Epoch 8/15: [================              ] 34/63 batches, loss: 0.6850Epoch 8/15: [================              ] 35/63 batches, loss: 0.6850Epoch 8/15: [=================             ] 36/63 batches, loss: 0.6850Epoch 8/15: [=================             ] 37/63 batches, loss: 0.6841Epoch 8/15: [==================            ] 38/63 batches, loss: 0.6848Epoch 8/15: [==================            ] 39/63 batches, loss: 0.6840Epoch 8/15: [===================           ] 40/63 batches, loss: 0.6844Epoch 8/15: [===================           ] 41/63 batches, loss: 0.6850Epoch 8/15: [====================          ] 42/63 batches, loss: 0.6859Epoch 8/15: [====================          ] 43/63 batches, loss: 0.6864Epoch 8/15: [====================          ] 44/63 batches, loss: 0.6855Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.6853Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.6844Epoch 8/15: [======================        ] 47/63 batches, loss: 0.6841Epoch 8/15: [======================        ] 48/63 batches, loss: 0.6847Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.6850Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.6846Epoch 8/15: [========================      ] 51/63 batches, loss: 0.6851Epoch 8/15: [========================      ] 52/63 batches, loss: 0.6848Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.6845Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.6850Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.6843Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.6832Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.6834Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.6845Epoch 8/15: [============================  ] 59/63 batches, loss: 0.6842Epoch 8/15: [============================  ] 60/63 batches, loss: 0.6845Epoch 8/15: [============================= ] 61/63 batches, loss: 0.6847Epoch 8/15: [============================= ] 62/63 batches, loss: 0.6844Epoch 8/15: [==============================] 63/63 batches, loss: 0.6837
[2025-05-02 17:51:37,915][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6837
[2025-05-02 17:51:38,294][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.6282Epoch 9/15: [                              ] 2/63 batches, loss: 0.6709Epoch 9/15: [=                             ] 3/63 batches, loss: 0.6702Epoch 9/15: [=                             ] 4/63 batches, loss: 0.6672Epoch 9/15: [==                            ] 5/63 batches, loss: 0.6750Epoch 9/15: [==                            ] 6/63 batches, loss: 0.6768Epoch 9/15: [===                           ] 7/63 batches, loss: 0.6793Epoch 9/15: [===                           ] 8/63 batches, loss: 0.6803Epoch 9/15: [====                          ] 9/63 batches, loss: 0.6819Epoch 9/15: [====                          ] 10/63 batches, loss: 0.6843Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.6858Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.6853Epoch 9/15: [======                        ] 13/63 batches, loss: 0.6829Epoch 9/15: [======                        ] 14/63 batches, loss: 0.6830Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.6823Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.6825Epoch 9/15: [========                      ] 17/63 batches, loss: 0.6819Epoch 9/15: [========                      ] 18/63 batches, loss: 0.6816Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.6807Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.6819Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.6852Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.6845Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.6825Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.6836Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.6837Epoch 9/15: [============                  ] 26/63 batches, loss: 0.6844Epoch 9/15: [============                  ] 27/63 batches, loss: 0.6846Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.6839Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.6837Epoch 9/15: [==============                ] 30/63 batches, loss: 0.6839Epoch 9/15: [==============                ] 31/63 batches, loss: 0.6821Epoch 9/15: [===============               ] 32/63 batches, loss: 0.6812Epoch 9/15: [===============               ] 33/63 batches, loss: 0.6798Epoch 9/15: [================              ] 34/63 batches, loss: 0.6792Epoch 9/15: [================              ] 35/63 batches, loss: 0.6798Epoch 9/15: [=================             ] 36/63 batches, loss: 0.6795Epoch 9/15: [=================             ] 37/63 batches, loss: 0.6805Epoch 9/15: [==================            ] 38/63 batches, loss: 0.6786Epoch 9/15: [==================            ] 39/63 batches, loss: 0.6778Epoch 9/15: [===================           ] 40/63 batches, loss: 0.6769Epoch 9/15: [===================           ] 41/63 batches, loss: 0.6772Epoch 9/15: [====================          ] 42/63 batches, loss: 0.6768Epoch 9/15: [====================          ] 43/63 batches, loss: 0.6774Epoch 9/15: [====================          ] 44/63 batches, loss: 0.6777Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.6767Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.6759Epoch 9/15: [======================        ] 47/63 batches, loss: 0.6761Epoch 9/15: [======================        ] 48/63 batches, loss: 0.6754Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.6753Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.6759Epoch 9/15: [========================      ] 51/63 batches, loss: 0.6761Epoch 9/15: [========================      ] 52/63 batches, loss: 0.6764Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.6761Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.6757Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.6753Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.6760Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.6770Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.6784Epoch 9/15: [============================  ] 59/63 batches, loss: 0.6789Epoch 9/15: [============================  ] 60/63 batches, loss: 0.6794Epoch 9/15: [============================= ] 61/63 batches, loss: 0.6789Epoch 9/15: [============================= ] 62/63 batches, loss: 0.6789Epoch 9/15: [==============================] 63/63 batches, loss: 0.6787
[2025-05-02 17:51:39,611][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6787
[2025-05-02 17:51:40,051][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6913, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.7010Epoch 10/15: [                              ] 2/63 batches, loss: 0.7102Epoch 10/15: [=                             ] 3/63 batches, loss: 0.6979Epoch 10/15: [=                             ] 4/63 batches, loss: 0.7044Epoch 10/15: [==                            ] 5/63 batches, loss: 0.6943Epoch 10/15: [==                            ] 6/63 batches, loss: 0.6927Epoch 10/15: [===                           ] 7/63 batches, loss: 0.6912Epoch 10/15: [===                           ] 8/63 batches, loss: 0.6945Epoch 10/15: [====                          ] 9/63 batches, loss: 0.6930Epoch 10/15: [====                          ] 10/63 batches, loss: 0.6830Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.6801Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.6819Epoch 10/15: [======                        ] 13/63 batches, loss: 0.6861Epoch 10/15: [======                        ] 14/63 batches, loss: 0.6827Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.6846Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.6866Epoch 10/15: [========                      ] 17/63 batches, loss: 0.6881Epoch 10/15: [========                      ] 18/63 batches, loss: 0.6890Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.6868Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.6869Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.6862Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.6862Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.6854Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.6854Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.6820Epoch 10/15: [============                  ] 26/63 batches, loss: 0.6803Epoch 10/15: [============                  ] 27/63 batches, loss: 0.6796Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.6806Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.6809Epoch 10/15: [==============                ] 30/63 batches, loss: 0.6823Epoch 10/15: [==============                ] 31/63 batches, loss: 0.6824Epoch 10/15: [===============               ] 32/63 batches, loss: 0.6824Epoch 10/15: [===============               ] 33/63 batches, loss: 0.6817Epoch 10/15: [================              ] 34/63 batches, loss: 0.6825Epoch 10/15: [================              ] 35/63 batches, loss: 0.6833Epoch 10/15: [=================             ] 36/63 batches, loss: 0.6825Epoch 10/15: [=================             ] 37/63 batches, loss: 0.6826Epoch 10/15: [==================            ] 38/63 batches, loss: 0.6821Epoch 10/15: [==================            ] 39/63 batches, loss: 0.6816Epoch 10/15: [===================           ] 40/63 batches, loss: 0.6819Epoch 10/15: [===================           ] 41/63 batches, loss: 0.6815Epoch 10/15: [====================          ] 42/63 batches, loss: 0.6808Epoch 10/15: [====================          ] 43/63 batches, loss: 0.6812Epoch 10/15: [====================          ] 44/63 batches, loss: 0.6805Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.6802Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.6793Epoch 10/15: [======================        ] 47/63 batches, loss: 0.6796Epoch 10/15: [======================        ] 48/63 batches, loss: 0.6788Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.6784Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.6776Epoch 10/15: [========================      ] 51/63 batches, loss: 0.6769Epoch 10/15: [========================      ] 52/63 batches, loss: 0.6774Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.6765Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.6764Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.6757Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.6753Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.6757Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.6758Epoch 10/15: [============================  ] 59/63 batches, loss: 0.6745Epoch 10/15: [============================  ] 60/63 batches, loss: 0.6749Epoch 10/15: [============================= ] 61/63 batches, loss: 0.6754Epoch 10/15: [============================= ] 62/63 batches, loss: 0.6761Epoch 10/15: [==============================] 63/63 batches, loss: 0.6775
[2025-05-02 17:51:41,464][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6775
[2025-05-02 17:51:41,908][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6899, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.6319Epoch 11/15: [                              ] 2/63 batches, loss: 0.6920Epoch 11/15: [=                             ] 3/63 batches, loss: 0.6830Epoch 11/15: [=                             ] 4/63 batches, loss: 0.6755Epoch 11/15: [==                            ] 5/63 batches, loss: 0.6742Epoch 11/15: [==                            ] 6/63 batches, loss: 0.6676Epoch 11/15: [===                           ] 7/63 batches, loss: 0.6695Epoch 11/15: [===                           ] 8/63 batches, loss: 0.6680Epoch 11/15: [====                          ] 9/63 batches, loss: 0.6671Epoch 11/15: [====                          ] 10/63 batches, loss: 0.6672Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.6645Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.6645Epoch 11/15: [======                        ] 13/63 batches, loss: 0.6669Epoch 11/15: [======                        ] 14/63 batches, loss: 0.6700Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.6716Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.6725Epoch 11/15: [========                      ] 17/63 batches, loss: 0.6702Epoch 11/15: [========                      ] 18/63 batches, loss: 0.6697Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.6675Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.6672Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.6714Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.6725Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.6715Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.6708Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.6697Epoch 11/15: [============                  ] 26/63 batches, loss: 0.6691Epoch 11/15: [============                  ] 27/63 batches, loss: 0.6692Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.6691Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.6687Epoch 11/15: [==============                ] 30/63 batches, loss: 0.6680Epoch 11/15: [==============                ] 31/63 batches, loss: 0.6669Epoch 11/15: [===============               ] 32/63 batches, loss: 0.6676Epoch 11/15: [===============               ] 33/63 batches, loss: 0.6691Epoch 11/15: [================              ] 34/63 batches, loss: 0.6688Epoch 11/15: [================              ] 35/63 batches, loss: 0.6677Epoch 11/15: [=================             ] 36/63 batches, loss: 0.6684Epoch 11/15: [=================             ] 37/63 batches, loss: 0.6678Epoch 11/15: [==================            ] 38/63 batches, loss: 0.6693Epoch 11/15: [==================            ] 39/63 batches, loss: 0.6707Epoch 11/15: [===================           ] 40/63 batches, loss: 0.6731Epoch 11/15: [===================           ] 41/63 batches, loss: 0.6733Epoch 11/15: [====================          ] 42/63 batches, loss: 0.6721Epoch 11/15: [====================          ] 43/63 batches, loss: 0.6728Epoch 11/15: [====================          ] 44/63 batches, loss: 0.6728Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.6741Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.6746Epoch 11/15: [======================        ] 47/63 batches, loss: 0.6754Epoch 11/15: [======================        ] 48/63 batches, loss: 0.6754Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.6750Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.6746Epoch 11/15: [========================      ] 51/63 batches, loss: 0.6750Epoch 11/15: [========================      ] 52/63 batches, loss: 0.6756Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.6760Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.6749Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.6747Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.6752Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.6760Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.6747Epoch 11/15: [============================  ] 59/63 batches, loss: 0.6748Epoch 11/15: [============================  ] 60/63 batches, loss: 0.6753Epoch 11/15: [============================= ] 61/63 batches, loss: 0.6748Epoch 11/15: [============================= ] 62/63 batches, loss: 0.6753Epoch 11/15: [==============================] 63/63 batches, loss: 0.6762
[2025-05-02 17:51:43,230][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.6762
[2025-05-02 17:51:43,643][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6884, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.6663Epoch 12/15: [                              ] 2/63 batches, loss: 0.6680Epoch 12/15: [=                             ] 3/63 batches, loss: 0.6967Epoch 12/15: [=                             ] 4/63 batches, loss: 0.6944Epoch 12/15: [==                            ] 5/63 batches, loss: 0.6867Epoch 12/15: [==                            ] 6/63 batches, loss: 0.6805Epoch 12/15: [===                           ] 7/63 batches, loss: 0.6777Epoch 12/15: [===                           ] 8/63 batches, loss: 0.6728Epoch 12/15: [====                          ] 9/63 batches, loss: 0.6699Epoch 12/15: [====                          ] 10/63 batches, loss: 0.6714Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.6673Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.6685Epoch 12/15: [======                        ] 13/63 batches, loss: 0.6683Epoch 12/15: [======                        ] 14/63 batches, loss: 0.6652Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.6664Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.6739Epoch 12/15: [========                      ] 17/63 batches, loss: 0.6727Epoch 12/15: [========                      ] 18/63 batches, loss: 0.6729Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.6715Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.6708Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.6719Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.6712Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.6709Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.6719Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.6721Epoch 12/15: [============                  ] 26/63 batches, loss: 0.6724Epoch 12/15: [============                  ] 27/63 batches, loss: 0.6725Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.6710Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.6701Epoch 12/15: [==============                ] 30/63 batches, loss: 0.6699Epoch 12/15: [==============                ] 31/63 batches, loss: 0.6678Epoch 12/15: [===============               ] 32/63 batches, loss: 0.6686Epoch 12/15: [===============               ] 33/63 batches, loss: 0.6702Epoch 12/15: [================              ] 34/63 batches, loss: 0.6697Epoch 12/15: [================              ] 35/63 batches, loss: 0.6709Epoch 12/15: [=================             ] 36/63 batches, loss: 0.6714Epoch 12/15: [=================             ] 37/63 batches, loss: 0.6708Epoch 12/15: [==================            ] 38/63 batches, loss: 0.6714Epoch 12/15: [==================            ] 39/63 batches, loss: 0.6708Epoch 12/15: [===================           ] 40/63 batches, loss: 0.6701Epoch 12/15: [===================           ] 41/63 batches, loss: 0.6680Epoch 12/15: [====================          ] 42/63 batches, loss: 0.6692Epoch 12/15: [====================          ] 43/63 batches, loss: 0.6705Epoch 12/15: [====================          ] 44/63 batches, loss: 0.6698Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.6695Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.6701Epoch 12/15: [======================        ] 47/63 batches, loss: 0.6698Epoch 12/15: [======================        ] 48/63 batches, loss: 0.6695Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.6681Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.6678Epoch 12/15: [========================      ] 51/63 batches, loss: 0.6673Epoch 12/15: [========================      ] 52/63 batches, loss: 0.6672Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.6672Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.6675Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.6663Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.6672Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.6665Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.6670Epoch 12/15: [============================  ] 59/63 batches, loss: 0.6672Epoch 12/15: [============================  ] 60/63 batches, loss: 0.6683Epoch 12/15: [============================= ] 61/63 batches, loss: 0.6681Epoch 12/15: [============================= ] 62/63 batches, loss: 0.6674Epoch 12/15: [==============================] 63/63 batches, loss: 0.6690
[2025-05-02 17:51:44,973][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.6690
[2025-05-02 17:51:45,343][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6871, Metrics: {'accuracy': 0.5, 'f1': 0.08333333333333333, 'precision': 0.25, 'recall': 0.05}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.6522Epoch 13/15: [                              ] 2/63 batches, loss: 0.6562Epoch 13/15: [=                             ] 3/63 batches, loss: 0.6535Epoch 13/15: [=                             ] 4/63 batches, loss: 0.6572Epoch 13/15: [==                            ] 5/63 batches, loss: 0.6552Epoch 13/15: [==                            ] 6/63 batches, loss: 0.6527Epoch 13/15: [===                           ] 7/63 batches, loss: 0.6607Epoch 13/15: [===                           ] 8/63 batches, loss: 0.6570Epoch 13/15: [====                          ] 9/63 batches, loss: 0.6580Epoch 13/15: [====                          ] 10/63 batches, loss: 0.6577Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.6573Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.6531Epoch 13/15: [======                        ] 13/63 batches, loss: 0.6561Epoch 13/15: [======                        ] 14/63 batches, loss: 0.6561Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.6572Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.6540Epoch 13/15: [========                      ] 17/63 batches, loss: 0.6571Epoch 13/15: [========                      ] 18/63 batches, loss: 0.6564Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.6578Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.6586Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.6587Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.6605Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.6615Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.6623Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.6604Epoch 13/15: [============                  ] 26/63 batches, loss: 0.6592Epoch 13/15: [============                  ] 27/63 batches, loss: 0.6591Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.6578Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.6593Epoch 13/15: [==============                ] 30/63 batches, loss: 0.6584Epoch 13/15: [==============                ] 31/63 batches, loss: 0.6573Epoch 13/15: [===============               ] 32/63 batches, loss: 0.6558Epoch 13/15: [===============               ] 33/63 batches, loss: 0.6579Epoch 13/15: [================              ] 34/63 batches, loss: 0.6584Epoch 13/15: [================              ] 35/63 batches, loss: 0.6596Epoch 13/15: [=================             ] 36/63 batches, loss: 0.6614Epoch 13/15: [=================             ] 37/63 batches, loss: 0.6608Epoch 13/15: [==================            ] 38/63 batches, loss: 0.6614Epoch 13/15: [==================            ] 39/63 batches, loss: 0.6603Epoch 13/15: [===================           ] 40/63 batches, loss: 0.6608Epoch 13/15: [===================           ] 41/63 batches, loss: 0.6602Epoch 13/15: [====================          ] 42/63 batches, loss: 0.6591Epoch 13/15: [====================          ] 43/63 batches, loss: 0.6594Epoch 13/15: [====================          ] 44/63 batches, loss: 0.6591Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.6588Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.6584Epoch 13/15: [======================        ] 47/63 batches, loss: 0.6582Epoch 13/15: [======================        ] 48/63 batches, loss: 0.6583Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.6580Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.6582Epoch 13/15: [========================      ] 51/63 batches, loss: 0.6591Epoch 13/15: [========================      ] 52/63 batches, loss: 0.6595Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.6596Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.6618Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.6619Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.6625Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.6624Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.6640Epoch 13/15: [============================  ] 59/63 batches, loss: 0.6645Epoch 13/15: [============================  ] 60/63 batches, loss: 0.6654Epoch 13/15: [============================= ] 61/63 batches, loss: 0.6650Epoch 13/15: [============================= ] 62/63 batches, loss: 0.6655Epoch 13/15: [==============================] 63/63 batches, loss: 0.6648
[2025-05-02 17:51:46,712][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.6648
[2025-05-02 17:51:47,114][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.6855, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.23076923076923078, 'precision': 0.5, 'recall': 0.15}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5969Epoch 14/15: [                              ] 2/63 batches, loss: 0.6254Epoch 14/15: [=                             ] 3/63 batches, loss: 0.6502Epoch 14/15: [=                             ] 4/63 batches, loss: 0.6403Epoch 14/15: [==                            ] 5/63 batches, loss: 0.6478Epoch 14/15: [==                            ] 6/63 batches, loss: 0.6307Epoch 14/15: [===                           ] 7/63 batches, loss: 0.6480Epoch 14/15: [===                           ] 8/63 batches, loss: 0.6488Epoch 14/15: [====                          ] 9/63 batches, loss: 0.6477Epoch 14/15: [====                          ] 10/63 batches, loss: 0.6495Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.6542Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.6494Epoch 14/15: [======                        ] 13/63 batches, loss: 0.6470Epoch 14/15: [======                        ] 14/63 batches, loss: 0.6478Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.6464Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.6466Epoch 14/15: [========                      ] 17/63 batches, loss: 0.6495Epoch 14/15: [========                      ] 18/63 batches, loss: 0.6516Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.6516Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.6525Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.6533Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.6567Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.6562Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.6574Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.6602Epoch 14/15: [============                  ] 26/63 batches, loss: 0.6596Epoch 14/15: [============                  ] 27/63 batches, loss: 0.6602Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.6603Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.6614Epoch 14/15: [==============                ] 30/63 batches, loss: 0.6609Epoch 14/15: [==============                ] 31/63 batches, loss: 0.6623Epoch 14/15: [===============               ] 32/63 batches, loss: 0.6642Epoch 14/15: [===============               ] 33/63 batches, loss: 0.6655Epoch 14/15: [================              ] 34/63 batches, loss: 0.6652Epoch 14/15: [================              ] 35/63 batches, loss: 0.6638Epoch 14/15: [=================             ] 36/63 batches, loss: 0.6631Epoch 14/15: [=================             ] 37/63 batches, loss: 0.6649Epoch 14/15: [==================            ] 38/63 batches, loss: 0.6648Epoch 14/15: [==================            ] 39/63 batches, loss: 0.6629Epoch 14/15: [===================           ] 40/63 batches, loss: 0.6632Epoch 14/15: [===================           ] 41/63 batches, loss: 0.6624Epoch 14/15: [====================          ] 42/63 batches, loss: 0.6618Epoch 14/15: [====================          ] 43/63 batches, loss: 0.6610Epoch 14/15: [====================          ] 44/63 batches, loss: 0.6630Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.6630Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.6627Epoch 14/15: [======================        ] 47/63 batches, loss: 0.6628Epoch 14/15: [======================        ] 48/63 batches, loss: 0.6636Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.6633Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.6643Epoch 14/15: [========================      ] 51/63 batches, loss: 0.6655Epoch 14/15: [========================      ] 52/63 batches, loss: 0.6640Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.6637Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.6635Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.6640Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.6649Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.6640Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.6636Epoch 14/15: [============================  ] 59/63 batches, loss: 0.6631Epoch 14/15: [============================  ] 60/63 batches, loss: 0.6638Epoch 14/15: [============================= ] 61/63 batches, loss: 0.6645Epoch 14/15: [============================= ] 62/63 batches, loss: 0.6645Epoch 14/15: [==============================] 63/63 batches, loss: 0.6645
[2025-05-02 17:51:48,441][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.6645
[2025-05-02 17:51:48,843][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.6835, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.2962962962962963, 'precision': 0.5714285714285714, 'recall': 0.2}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.6662Epoch 15/15: [                              ] 2/63 batches, loss: 0.6766Epoch 15/15: [=                             ] 3/63 batches, loss: 0.6593Epoch 15/15: [=                             ] 4/63 batches, loss: 0.6486Epoch 15/15: [==                            ] 5/63 batches, loss: 0.6498Epoch 15/15: [==                            ] 6/63 batches, loss: 0.6471Epoch 15/15: [===                           ] 7/63 batches, loss: 0.6488Epoch 15/15: [===                           ] 8/63 batches, loss: 0.6533Epoch 15/15: [====                          ] 9/63 batches, loss: 0.6540Epoch 15/15: [====                          ] 10/63 batches, loss: 0.6539Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.6521Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.6449Epoch 15/15: [======                        ] 13/63 batches, loss: 0.6444Epoch 15/15: [======                        ] 14/63 batches, loss: 0.6470Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.6416Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.6421Epoch 15/15: [========                      ] 17/63 batches, loss: 0.6500Epoch 15/15: [========                      ] 18/63 batches, loss: 0.6514Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.6508Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.6528Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.6546Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.6583Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.6571Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.6563Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.6607Epoch 15/15: [============                  ] 26/63 batches, loss: 0.6602Epoch 15/15: [============                  ] 27/63 batches, loss: 0.6594Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.6576Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.6575Epoch 15/15: [==============                ] 30/63 batches, loss: 0.6593Epoch 15/15: [==============                ] 31/63 batches, loss: 0.6579Epoch 15/15: [===============               ] 32/63 batches, loss: 0.6584Epoch 15/15: [===============               ] 33/63 batches, loss: 0.6600Epoch 15/15: [================              ] 34/63 batches, loss: 0.6592Epoch 15/15: [================              ] 35/63 batches, loss: 0.6587Epoch 15/15: [=================             ] 36/63 batches, loss: 0.6593Epoch 15/15: [=================             ] 37/63 batches, loss: 0.6593Epoch 15/15: [==================            ] 38/63 batches, loss: 0.6584Epoch 15/15: [==================            ] 39/63 batches, loss: 0.6596Epoch 15/15: [===================           ] 40/63 batches, loss: 0.6601Epoch 15/15: [===================           ] 41/63 batches, loss: 0.6600Epoch 15/15: [====================          ] 42/63 batches, loss: 0.6598Epoch 15/15: [====================          ] 43/63 batches, loss: 0.6587Epoch 15/15: [====================          ] 44/63 batches, loss: 0.6586Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.6567Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.6562Epoch 15/15: [======================        ] 47/63 batches, loss: 0.6552Epoch 15/15: [======================        ] 48/63 batches, loss: 0.6546Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.6548Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.6551Epoch 15/15: [========================      ] 51/63 batches, loss: 0.6547Epoch 15/15: [========================      ] 52/63 batches, loss: 0.6546Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.6546Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.6556Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.6562Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.6554Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.6550Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.6555Epoch 15/15: [============================  ] 59/63 batches, loss: 0.6547Epoch 15/15: [============================  ] 60/63 batches, loss: 0.6542Epoch 15/15: [============================= ] 61/63 batches, loss: 0.6537Epoch 15/15: [============================= ] 62/63 batches, loss: 0.6532Epoch 15/15: [==============================] 63/63 batches, loss: 0.6525
[2025-05-02 17:51:50,258][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.6525
[2025-05-02 17:51:50,669][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.6809, Metrics: {'accuracy': 0.5909090909090909, 'f1': 0.35714285714285715, 'precision': 0.625, 'recall': 0.25}
[2025-05-02 17:51:50,868][src.training.lm_trainer][INFO] - Training completed in 27.58 seconds
[2025-05-02 17:51:50,868][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 17:51:52,999][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.7236180904522613, 'f1': 0.6328437917222964, 'precision': 0.9404761904761905, 'recall': 0.4768611670020121}
[2025-05-02 17:51:53,000][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5909090909090909, 'f1': 0.35714285714285715, 'precision': 0.625, 'recall': 0.25}
[2025-05-02 17:51:53,000][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7402597402597403, 'f1': 0.5454545454545454, 'precision': 0.5454545454545454, 'recall': 0.5454545454545454}
[2025-05-02 17:51:54,232][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr2e-5/ar/model.pt
[2025-05-02 17:51:54,233][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▅▅▅▅▅▅▅▅▅▅▁▁▅▆█
wandb:           best_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▃▆▇█
wandb:         best_val_loss █▇▆▆▅▅▅▄▄▄▃▃▂▂▁
wandb:    best_val_precision ▁▁▁▁▁▁▁▁▁▁▁▄▇▇█
wandb:       best_val_recall ▁▁▁▁▁▁▁▁▁▁▁▂▅▇█
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_loss █▇▆▇▆▆▅▅▄▄▄▃▃▂▁
wandb:            train_time ▁
wandb:          val_accuracy ▅▅▅▅▅▅▅▅▅▅▁▁▅▆█
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁▁▁▃▆▇█
wandb:              val_loss █▇▆▆▅▅▅▄▄▄▃▃▂▂▁
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁▁▄▇▇█
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁▁▂▅▇█
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.59091
wandb:           best_val_f1 0.35714
wandb:         best_val_loss 0.68094
wandb:    best_val_precision 0.625
wandb:       best_val_recall 0.25
wandb:                 epoch 15
wandb:   final_test_accuracy 0.74026
wandb:         final_test_f1 0.54545
wandb:  final_test_precision 0.54545
wandb:     final_test_recall 0.54545
wandb:  final_train_accuracy 0.72362
wandb:        final_train_f1 0.63284
wandb: final_train_precision 0.94048
wandb:    final_train_recall 0.47686
wandb:    final_val_accuracy 0.59091
wandb:          final_val_f1 0.35714
wandb:   final_val_precision 0.625
wandb:      final_val_recall 0.25
wandb:         learning_rate 2e-05
wandb:            train_loss 0.65246
wandb:            train_time 27.58339
wandb:          val_accuracy 0.59091
wandb:                val_f1 0.35714
wandb:              val_loss 0.68094
wandb:         val_precision 0.625
wandb:            val_recall 0.25
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_175110-5bpedvv6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_175110-5bpedvv6/logs
Experiment 4 completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/param_sweep_output/probe_ar_question_type_layer1_h10_d0.1_lr2e-5/ar/results.json
==============================================
Running experiment ID: 5
Language: ar, Task: question_type, Layer: 1
Hidden Size: 10, Dropout: 0.25, LR: 1e-4
==============================================
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
slurmstepd: error: *** JOB 64439803 ON s16g09 CANCELLED AT 2025-05-02T17:52:14 ***
