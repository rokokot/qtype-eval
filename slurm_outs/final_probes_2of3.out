SLURM_JOB_ID: 64444234
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Sun May  4 10:36:55 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 1
=======================
Experiment probe_layer1_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/en/en/results.json for layer 1
Experiment probe_layer1_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/en/en/results.json for layer 1
Experiment probe_layer1_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/id/id/results.json for layer 1
Experiment probe_layer1_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/id/id/results.json for layer 1
Experiment probe_layer1_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ru/ru/results.json for layer 1
=======================
PROBING LAYER 2
=======================
Experiment probe_layer2_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/en/en/results.json for layer 2
Experiment probe_layer2_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/en/en/results.json for layer 2
Experiment probe_layer2_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/id/id/results.json for layer 2
Experiment probe_layer2_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer2/ru/ru/results.json for layer 2
=======================
PROBING LAYER 3
=======================
Experiment probe_layer3_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/en/en/results.json for layer 3
Experiment probe_layer3_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/en/en/results.json for layer 3
Experiment probe_layer3_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/id/id/results.json for layer 3
Experiment probe_layer3_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/id/id/results.json for layer 3
Experiment probe_layer3_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer3/ru/ru/results.json for layer 3
=======================
PROBING LAYER 4
=======================
Experiment probe_layer4_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/en/en/results.json for layer 4
Experiment probe_layer4_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/en/en/results.json for layer 4
Experiment probe_layer4_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/id/id/results.json for layer 4
Experiment probe_layer4_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/id/id/results.json for layer 4
Experiment probe_layer4_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ru/ru/results.json for layer 4
=======================
PROBING LAYER 5
=======================
Experiment probe_layer5_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/en/en/results.json for layer 5
Experiment probe_layer5_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/en/en/results.json for layer 5
Experiment probe_layer5_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/id/id/results.json for layer 5
Experiment probe_layer5_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/id/id/results.json for layer 5
Experiment probe_layer5_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer5/ru/ru/results.json for layer 5
=======================
PROBING LAYER 6
=======================
Experiment probe_layer6_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/en/en/results.json for layer 6
Experiment probe_layer6_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/en/en/results.json for layer 6
Experiment probe_layer6_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/id/id/results.json for layer 6
Experiment probe_layer6_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/id/id/results.json for layer 6
Experiment probe_layer6_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ru/ru/results.json for layer 6
=======================
PROBING LAYER 7
=======================
Experiment probe_layer7_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/en/en/results.json for layer 7
Experiment probe_layer7_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/en/en/results.json for layer 7
Experiment probe_layer7_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/id/id/results.json for layer 7
Experiment probe_layer7_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/id/id/results.json for layer 7
Experiment probe_layer7_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer7/ru/ru/results.json for layer 7
=======================
PROBING LAYER 8
=======================
Experiment probe_layer8_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/en/en/results.json for layer 8
Experiment probe_layer8_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/en/en/results.json for layer 8
Experiment probe_layer8_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/id/id/results.json for layer 8
Experiment probe_layer8_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/id/id/results.json for layer 8
Experiment probe_layer8_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer8/ru/ru/results.json for layer 8
=======================
PROBING LAYER 9
=======================
Experiment probe_layer9_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/en/en/results.json for layer 9
Experiment probe_layer9_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/en/en/results.json for layer 9
Experiment probe_layer9_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/id/id/results.json for layer 9
Experiment probe_layer9_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/id/id/results.json for layer 9
Experiment probe_layer9_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ru/ru/results.json for layer 9
=======================
PROBING LAYER 10
=======================
Experiment probe_layer10_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/en/en/results.json for layer 10
Experiment probe_layer10_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/en/en/results.json for layer 10
Experiment probe_layer10_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/id/id/results.json for layer 10
Experiment probe_layer10_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/id/id/results.json for layer 10
Experiment probe_layer10_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/ja/ja/results.json for layer 10
Experiment probe_layer10_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/ja/ja/results.json for layer 10
Experiment probe_layer10_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/ko/ko/results.json for layer 10
Experiment probe_layer10_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/ko/ko/results.json for layer 10
Experiment probe_layer10_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer10/ru/ru/results.json for layer 10
Experiment probe_layer10_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer10/ru/ru/results.json for layer 10
=======================
PROBING LAYER 11
=======================
Experiment probe_layer11_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar/ar/results.json for layer 11
Experiment probe_layer11_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar/ar/results.json for layer 11
Experiment probe_layer11_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/en/en/results.json for layer 11
Experiment probe_layer11_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/en/en/results.json for layer 11
Experiment probe_layer11_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/fi/fi/results.json for layer 11
Experiment probe_layer11_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/fi/fi/results.json for layer 11
Experiment probe_layer11_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/id/id/results.json for layer 11
Experiment probe_layer11_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/id/id/results.json for layer 11
Experiment probe_layer11_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ja/ja/results.json for layer 11
Experiment probe_layer11_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ja/ja/results.json for layer 11
Experiment probe_layer11_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ko/ko/results.json for layer 11
Experiment probe_layer11_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ko/ko/results.json for layer 11
Experiment probe_layer11_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ru/ru/results.json for layer 11
Experiment probe_layer11_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ru/ru/results.json for layer 11
=======================
PROBING LAYER 12
=======================
Experiment probe_layer12_question_type_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/ar/ar/results.json for layer 12
Experiment probe_layer12_complexity_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/ar/ar/results.json for layer 12
Experiment probe_layer12_question_type_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/en/en/results.json for layer 12
Experiment probe_layer12_complexity_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/en/en/results.json for layer 12
Experiment probe_layer12_question_type_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/fi/fi/results.json for layer 12
Experiment probe_layer12_complexity_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/fi/fi/results.json for layer 12
Experiment probe_layer12_question_type_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/id/id/results.json for layer 12
Experiment probe_layer12_complexity_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/id/id/results.json for layer 12
Experiment probe_layer12_question_type_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/ja/ja/results.json for layer 12
Experiment probe_layer12_complexity_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/ja/ja/results.json for layer 12
Experiment probe_layer12_question_type_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/ko/ko/results.json for layer 12
Experiment probe_layer12_complexity_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/ko/ko/results.json for layer 12
Experiment probe_layer12_question_type_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer12/ru/ru/results.json for layer 12
Experiment probe_layer12_complexity_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer12/ru/ru/results.json for layer 12
Running control probing experiments...
=======================
PROBING LAYER 1 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer1_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/ar/ar/results.json for layer 1
Experiment probe_layer1_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/en/en/results.json for layer 1
Experiment probe_layer1_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/en/en/results.json for layer 1
Experiment probe_layer1_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/en/en/results.json for layer 1
Experiment probe_layer1_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/en/en/results.json for layer 1
Experiment probe_layer1_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/en/en/results.json for layer 1
Experiment probe_layer1_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/en/en/results.json for layer 1
Experiment probe_layer1_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/fi/fi/results.json for layer 1
Experiment probe_layer1_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/id/id/results.json for layer 1
Experiment probe_layer1_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/id/id/results.json for layer 1
Experiment probe_layer1_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/id/id/results.json for layer 1
Experiment probe_layer1_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/id/id/results.json for layer 1
Experiment probe_layer1_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/id/id/results.json for layer 1
Experiment probe_layer1_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/id/id/results.json for layer 1
Experiment probe_layer1_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/ja/ja/results.json for layer 1
Experiment probe_layer1_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/ko/ko/results.json for layer 1
Experiment probe_layer1_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer1/ru/ru/results.json for layer 1
Experiment probe_layer1_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer1/ru/ru/results.json for layer 1
=======================
PROBING LAYER 2 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer2_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/ar/ar/results.json for layer 2
Experiment probe_layer2_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/en/en/results.json for layer 2
Experiment probe_layer2_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/en/en/results.json for layer 2
Experiment probe_layer2_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/en/en/results.json for layer 2
Experiment probe_layer2_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/en/en/results.json for layer 2
Experiment probe_layer2_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/en/en/results.json for layer 2
Experiment probe_layer2_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/en/en/results.json for layer 2
Experiment probe_layer2_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/fi/fi/results.json for layer 2
Experiment probe_layer2_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/id/id/results.json for layer 2
Experiment probe_layer2_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/id/id/results.json for layer 2
Experiment probe_layer2_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/id/id/results.json for layer 2
Experiment probe_layer2_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/id/id/results.json for layer 2
Experiment probe_layer2_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/ja/ja/results.json for layer 2
Experiment probe_layer2_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/ko/ko/results.json for layer 2
Experiment probe_layer2_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer2/ru/ru/results.json for layer 2
Experiment probe_layer2_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer2/ru/ru/results.json for layer 2
=======================
PROBING LAYER 3 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer3_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/ar/ar/results.json for layer 3
Experiment probe_layer3_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/en/en/results.json for layer 3
Experiment probe_layer3_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/en/en/results.json for layer 3
Experiment probe_layer3_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/en/en/results.json for layer 3
Experiment probe_layer3_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/en/en/results.json for layer 3
Experiment probe_layer3_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/en/en/results.json for layer 3
Experiment probe_layer3_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/en/en/results.json for layer 3
Experiment probe_layer3_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/fi/fi/results.json for layer 3
Experiment probe_layer3_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/id/id/results.json for layer 3
Experiment probe_layer3_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/id/id/results.json for layer 3
Experiment probe_layer3_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/id/id/results.json for layer 3
Experiment probe_layer3_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/id/id/results.json for layer 3
Experiment probe_layer3_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/id/id/results.json for layer 3
Experiment probe_layer3_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/id/id/results.json for layer 3
Experiment probe_layer3_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/ja/ja/results.json for layer 3
Experiment probe_layer3_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/ko/ko/results.json for layer 3
Experiment probe_layer3_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer3/ru/ru/results.json for layer 3
Experiment probe_layer3_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer3/ru/ru/results.json for layer 3
=======================
PROBING LAYER 4 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer4_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/ar/ar/results.json for layer 4
Experiment probe_layer4_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/en/en/results.json for layer 4
Experiment probe_layer4_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/en/en/results.json for layer 4
Experiment probe_layer4_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/en/en/results.json for layer 4
Experiment probe_layer4_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/en/en/results.json for layer 4
Experiment probe_layer4_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/en/en/results.json for layer 4
Experiment probe_layer4_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/en/en/results.json for layer 4
Experiment probe_layer4_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/fi/fi/results.json for layer 4
Experiment probe_layer4_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/id/id/results.json for layer 4
Experiment probe_layer4_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/id/id/results.json for layer 4
Experiment probe_layer4_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/id/id/results.json for layer 4
Experiment probe_layer4_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/id/id/results.json for layer 4
Experiment probe_layer4_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/id/id/results.json for layer 4
Experiment probe_layer4_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/id/id/results.json for layer 4
Experiment probe_layer4_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/ja/ja/results.json for layer 4
Experiment probe_layer4_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/ko/ko/results.json for layer 4
Experiment probe_layer4_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer4/ru/ru/results.json for layer 4
Experiment probe_layer4_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer4/ru/ru/results.json for layer 4
=======================
PROBING LAYER 5 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer5_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/ar/ar/results.json for layer 5
Experiment probe_layer5_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/en/en/results.json for layer 5
Experiment probe_layer5_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/en/en/results.json for layer 5
Experiment probe_layer5_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/en/en/results.json for layer 5
Experiment probe_layer5_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/en/en/results.json for layer 5
Experiment probe_layer5_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/en/en/results.json for layer 5
Experiment probe_layer5_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/en/en/results.json for layer 5
Experiment probe_layer5_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/fi/fi/results.json for layer 5
Experiment probe_layer5_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/id/id/results.json for layer 5
Experiment probe_layer5_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/id/id/results.json for layer 5
Experiment probe_layer5_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/id/id/results.json for layer 5
Experiment probe_layer5_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/id/id/results.json for layer 5
Experiment probe_layer5_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/id/id/results.json for layer 5
Experiment probe_layer5_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/id/id/results.json for layer 5
Experiment probe_layer5_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/ja/ja/results.json for layer 5
Experiment probe_layer5_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/ko/ko/results.json for layer 5
Experiment probe_layer5_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer5/ru/ru/results.json for layer 5
Experiment probe_layer5_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer5/ru/ru/results.json for layer 5
=======================
PROBING LAYER 6 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer6_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/ar/ar/results.json for layer 6
Experiment probe_layer6_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/en/en/results.json for layer 6
Experiment probe_layer6_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/en/en/results.json for layer 6
Experiment probe_layer6_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/en/en/results.json for layer 6
Experiment probe_layer6_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/en/en/results.json for layer 6
Experiment probe_layer6_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/en/en/results.json for layer 6
Experiment probe_layer6_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/en/en/results.json for layer 6
Experiment probe_layer6_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/fi/fi/results.json for layer 6
Experiment probe_layer6_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/id/id/results.json for layer 6
Experiment probe_layer6_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/id/id/results.json for layer 6
Experiment probe_layer6_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/id/id/results.json for layer 6
Experiment probe_layer6_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/id/id/results.json for layer 6
Experiment probe_layer6_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/id/id/results.json for layer 6
Experiment probe_layer6_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/id/id/results.json for layer 6
Experiment probe_layer6_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/ja/ja/results.json for layer 6
Experiment probe_layer6_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/ko/ko/results.json for layer 6
Experiment probe_layer6_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer6/ru/ru/results.json for layer 6
Experiment probe_layer6_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer6/ru/ru/results.json for layer 6
=======================
PROBING LAYER 7 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer7_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/ar/ar/results.json for layer 7
Experiment probe_layer7_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/en/en/results.json for layer 7
Experiment probe_layer7_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/en/en/results.json for layer 7
Experiment probe_layer7_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/en/en/results.json for layer 7
Experiment probe_layer7_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/en/en/results.json for layer 7
Experiment probe_layer7_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/en/en/results.json for layer 7
Experiment probe_layer7_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/en/en/results.json for layer 7
Experiment probe_layer7_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/fi/fi/results.json for layer 7
Experiment probe_layer7_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/id/id/results.json for layer 7
Experiment probe_layer7_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/id/id/results.json for layer 7
Experiment probe_layer7_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/id/id/results.json for layer 7
Experiment probe_layer7_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/id/id/results.json for layer 7
Experiment probe_layer7_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/id/id/results.json for layer 7
Experiment probe_layer7_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/id/id/results.json for layer 7
Experiment probe_layer7_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/ja/ja/results.json for layer 7
Experiment probe_layer7_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/ko/ko/results.json for layer 7
Experiment probe_layer7_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer7/ru/ru/results.json for layer 7
Experiment probe_layer7_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer7/ru/ru/results.json for layer 7
=======================
PROBING LAYER 8 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer8_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/ar/ar/results.json for layer 8
Experiment probe_layer8_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/en/en/results.json for layer 8
Experiment probe_layer8_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/en/en/results.json for layer 8
Experiment probe_layer8_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/en/en/results.json for layer 8
Experiment probe_layer8_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/en/en/results.json for layer 8
Experiment probe_layer8_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/en/en/results.json for layer 8
Experiment probe_layer8_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/en/en/results.json for layer 8
Experiment probe_layer8_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/fi/fi/results.json for layer 8
Experiment probe_layer8_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/id/id/results.json for layer 8
Experiment probe_layer8_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/id/id/results.json for layer 8
Experiment probe_layer8_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/id/id/results.json for layer 8
Experiment probe_layer8_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/id/id/results.json for layer 8
Experiment probe_layer8_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/id/id/results.json for layer 8
Experiment probe_layer8_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/id/id/results.json for layer 8
Experiment probe_layer8_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/ja/ja/results.json for layer 8
Experiment probe_layer8_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/ko/ko/results.json for layer 8
Experiment probe_layer8_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer8/ru/ru/results.json for layer 8
Experiment probe_layer8_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer8/ru/ru/results.json for layer 8
=======================
PROBING LAYER 9 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer9_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/ar/ar/results.json for layer 9
Experiment probe_layer9_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/en/en/results.json for layer 9
Experiment probe_layer9_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/en/en/results.json for layer 9
Experiment probe_layer9_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/en/en/results.json for layer 9
Experiment probe_layer9_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/en/en/results.json for layer 9
Experiment probe_layer9_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/en/en/results.json for layer 9
Experiment probe_layer9_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/en/en/results.json for layer 9
Experiment probe_layer9_question_type_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_question_type_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_question_type_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_complexity_control1_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_complexity_control2_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_complexity_control3_fi already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/fi/fi/results.json for layer 9
Experiment probe_layer9_question_type_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/id/id/results.json for layer 9
Experiment probe_layer9_question_type_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/id/id/results.json for layer 9
Experiment probe_layer9_question_type_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/id/id/results.json for layer 9
Experiment probe_layer9_complexity_control1_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/id/id/results.json for layer 9
Experiment probe_layer9_complexity_control2_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/id/id/results.json for layer 9
Experiment probe_layer9_complexity_control3_id already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/id/id/results.json for layer 9
Experiment probe_layer9_question_type_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_question_type_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_question_type_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_complexity_control1_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_complexity_control2_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_complexity_control3_ja already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/ja/ja/results.json for layer 9
Experiment probe_layer9_question_type_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_question_type_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_question_type_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_complexity_control1_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_complexity_control2_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_complexity_control3_ko already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/ko/ko/results.json for layer 9
Experiment probe_layer9_question_type_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_question_type_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_question_type_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_complexity_control1_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_complexity_control2_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer9/ru/ru/results.json for layer 9
Experiment probe_layer9_complexity_control3_ru already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer9/ru/ru/results.json for layer 9
=======================
PROBING LAYER 10 (CONTROL EXPERIMENTS)
=======================
Experiment probe_layer10_question_type_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_question_type_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_question_type_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_complexity_control1_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_complexity_control2_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_complexity_control3_ar already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ar/ar/results.json for layer 10
Experiment probe_layer10_question_type_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/en/en/results.json for layer 10
Experiment probe_layer10_question_type_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/en/en/results.json for layer 10
Experiment probe_layer10_question_type_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/en/en/results.json for layer 10
Experiment probe_layer10_complexity_control1_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/en/en/results.json for layer 10
Experiment probe_layer10_complexity_control2_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/en/en/results.json for layer 10
Experiment probe_layer10_complexity_control3_en already completed successfully. Extracting metrics...
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/en/en/results.json for layer 10
Experiment probe_layer10_question_type_control1_fi already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/fi/fi/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_question_type_control2_fi already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/fi/fi/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_question_type_control3_fi already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/fi/fi/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/fi/fi/results.json for layer 10
Experiment probe_layer10_complexity_control1_fi already completed successfully. Extracting metrics...
Warning: No test metrics found in /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/fi/fi/results.json
Failed to extract metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_fi"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:38:33,825][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/fi
experiment_name: probe_layer10_complexity_control2_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:38:33,825][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:38:33,825][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:38:33,826][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:38:33,831][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-04 10:38:33,831][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:38:36,657][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:38:38,945][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:38:38,946][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:38:39,086][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:38:39,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:38:39,353][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-04 10:38:39,362][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:38:39,362][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-04 10:38:39,363][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:38:39,415][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:38:39,468][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:38:39,492][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-04 10:38:39,493][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:38:39,493][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-04 10:38:39,494][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:38:39,525][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:38:39,565][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:38:39,590][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-04 10:38:39,592][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:38:39,592][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-04 10:38:39,593][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-04 10:38:39,593][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:38:39,593][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:38:39,593][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:38:39,593][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:38:39,593][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:38:39,594][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Sample label: 0.6458609104156494
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:38:39,594][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:38:39,594][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:38:39,594][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:38:39,595][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:38:39,595][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-04 10:38:39,595][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:38:39,596][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:38:39,596][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:38:39,596][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:38:44,473][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:38:44,474][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:38:44,474][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:38:44,474][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:38:44,477][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:38:44,477][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:38:44,477][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:38:44,477][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:38:44,478][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-04 10:38:44,478][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:38:44,479][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6625Epoch 1/15: [                              ] 2/75 batches, loss: 0.8841Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8861Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8130Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7840Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6863Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6460Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6530Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6205Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6272Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5969Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5681Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5583Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5426Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5237Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5110Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5173Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.5111Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4994Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4968Epoch 1/15: [========                      ] 21/75 batches, loss: 0.5015Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4988Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4864Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4776Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4702Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4672Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4739Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4731Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4658Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4625Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4544Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4539Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4471Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4444Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4414Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4432Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4378Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4347Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4312Epoch 1/15: [================              ] 40/75 batches, loss: 0.4315Epoch 1/15: [================              ] 41/75 batches, loss: 0.4300Epoch 1/15: [================              ] 42/75 batches, loss: 0.4277Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4242Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4212Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4204Epoch 1/15: [==================            ] 46/75 batches, loss: 0.4147Epoch 1/15: [==================            ] 47/75 batches, loss: 0.4117Epoch 1/15: [===================           ] 48/75 batches, loss: 0.4075Epoch 1/15: [===================           ] 49/75 batches, loss: 0.4034Epoch 1/15: [====================          ] 50/75 batches, loss: 0.4015Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3979Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3951Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3941Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3956Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3934Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3893Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3862Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3854Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3825Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3781Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3742Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3715Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3687Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3674Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3650Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3627Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3596Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3568Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3575Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3544Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3508Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3476Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3465Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3433Epoch 1/15: [==============================] 75/75 batches, loss: 0.3405
[2025-05-04 10:38:49,955][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3405
[2025-05-04 10:38:50,182][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0752, Metrics: {'mse': 0.07509806007146835, 'rmse': 0.2740402526481618, 'r2': -0.14547646045684814}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2214Epoch 2/15: [                              ] 2/75 batches, loss: 0.1954Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2033Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2072Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1943Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1775Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1858Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1920Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1798Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1894Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1888Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1821Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1806Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1786Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1787Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1747Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1714Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1751Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1751Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1725Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1730Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1708Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1726Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1702Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1716Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1699Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1789Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1772Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1777Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1793Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1764Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1776Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1749Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1757Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1776Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1747Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1735Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1725Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1736Epoch 2/15: [================              ] 40/75 batches, loss: 0.1714Epoch 2/15: [================              ] 41/75 batches, loss: 0.1706Epoch 2/15: [================              ] 42/75 batches, loss: 0.1681Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1684Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1672Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1663Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1694Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1693Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1694Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1703Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1690Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1677Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1665Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1673Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1660Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1673Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1680Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1660Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1643Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1640Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1632Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1623Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1612Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1605Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1597Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1598Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1592Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1591Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1584Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1572Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1559Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1563Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1554Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1546Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1543Epoch 2/15: [==============================] 75/75 batches, loss: 0.1531
[2025-05-04 10:38:52,886][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1531
[2025-05-04 10:38:53,124][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1168, Metrics: {'mse': 0.11701660603284836, 'rmse': 0.34207690075894975, 'r2': -0.7848632335662842}
[2025-05-04 10:38:53,124][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1475Epoch 3/15: [                              ] 2/75 batches, loss: 0.1250Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1308Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1305Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1259Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1155Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1106Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1154Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1146Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1231Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1186Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1149Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1178Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1180Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1189Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1171Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1188Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1199Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1183Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1175Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1154Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1143Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1126Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1154Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1138Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1132Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1110Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1112Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1117Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1096Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1079Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1096Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1096Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1084Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1071Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1063Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1051Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1052Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1056Epoch 3/15: [================              ] 40/75 batches, loss: 0.1050Epoch 3/15: [================              ] 41/75 batches, loss: 0.1057Epoch 3/15: [================              ] 42/75 batches, loss: 0.1054Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1053Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1049Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1045Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1059Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1056Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1060Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1056Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1052Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1044Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1041Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1038Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1033Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1029Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1026Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1026Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1025Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1026Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1019Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1014Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1013Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1001Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1000Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0993Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0993Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0992Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0996Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0998Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0995Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0993Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0988Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0988Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0983Epoch 3/15: [==============================] 75/75 batches, loss: 0.0990
[2025-05-04 10:38:55,383][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0990
[2025-05-04 10:38:55,619][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1055, Metrics: {'mse': 0.10559704899787903, 'rmse': 0.3249569956130796, 'r2': -0.6106798648834229}
[2025-05-04 10:38:55,620][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0698Epoch 4/15: [                              ] 2/75 batches, loss: 0.0962Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0883Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0884Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0852Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0880Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0922Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0913Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1002Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0941Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0999Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1005Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0972Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1002Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0983Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0993Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1014Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1013Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0990Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0998Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1023Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1004Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1001Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1024Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1014Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1005Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0995Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0986Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0962Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0947Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0939Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0917Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0926Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0934Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0923Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0920Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0905Epoch 4/15: [================              ] 40/75 batches, loss: 0.0906Epoch 4/15: [================              ] 41/75 batches, loss: 0.0903Epoch 4/15: [================              ] 42/75 batches, loss: 0.0891Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0892Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0878Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0877Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0886Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0887Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0889Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0891Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0893Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0888Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0889Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0877Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0870Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0871Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0881Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0870Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0873Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0878Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0877Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0882Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0879Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0875Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0871Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0872Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0865Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0861Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0862Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0864Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0863Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0857Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0850Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0848Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0842Epoch 4/15: [==============================] 75/75 batches, loss: 0.0842
[2025-05-04 10:38:57,899][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0842
[2025-05-04 10:38:58,151][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1163, Metrics: {'mse': 0.11648093163967133, 'rmse': 0.341293028993666, 'r2': -0.7766925096511841}
[2025-05-04 10:38:58,152][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0604Epoch 5/15: [                              ] 2/75 batches, loss: 0.0714Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0747Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0765Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0764Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0781Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0855Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0865Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0846Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0810Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0787Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0748Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0725Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0744Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0729Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0718Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0710Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0692Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0727Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0711Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0708Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0694Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0701Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0707Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0719Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0712Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0708Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0715Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0709Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0707Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0704Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0710Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0707Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0708Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0715Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0719Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0724Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0717Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0717Epoch 5/15: [================              ] 40/75 batches, loss: 0.0741Epoch 5/15: [================              ] 41/75 batches, loss: 0.0740Epoch 5/15: [================              ] 42/75 batches, loss: 0.0735Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0741Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0744Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0737Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0747Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0737Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0732Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0734Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0725Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0722Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0717Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0715Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0714Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0707Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0705Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0700Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0700Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0695Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0690Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0683Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0681Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0682Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0679Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0681Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0679Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0681Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0679Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0675Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0672Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0674Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0680Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0676Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0672Epoch 5/15: [==============================] 75/75 batches, loss: 0.0665
[2025-05-04 10:39:00,416][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0665
[2025-05-04 10:39:00,653][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0990, Metrics: {'mse': 0.09899085015058517, 'rmse': 0.3146281140498814, 'r2': -0.5099149942398071}
[2025-05-04 10:39:00,653][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:39:00,653][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:39:00,653][src.training.lm_trainer][INFO] - Training completed in 13.99 seconds
[2025-05-04 10:39:00,654][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:39:03,525][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0343533456325531, 'rmse': 0.18534655549147144, 'r2': -0.6999833583831787}
[2025-05-04 10:39:03,525][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07509806007146835, 'rmse': 0.2740402526481618, 'r2': -0.14547646045684814}
[2025-05-04 10:39:03,525][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050482336431741714, 'rmse': 0.22468274618168105, 'r2': -0.27845442295074463}
[2025-05-04 10:39:05,259][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/fi/fi/model.pt
[2025-05-04 10:39:05,261][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▂▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▆█▅
wandb:          val_mse ▁█▆█▅
wandb:           val_r2 █▁▃▁▄
wandb:         val_rmse ▁█▆█▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07515
wandb:     best_val_mse 0.0751
wandb:      best_val_r2 -0.14548
wandb:    best_val_rmse 0.27404
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05048
wandb:    final_test_r2 -0.27845
wandb:  final_test_rmse 0.22468
wandb:  final_train_mse 0.03435
wandb:   final_train_r2 -0.69998
wandb: final_train_rmse 0.18535
wandb:    final_val_mse 0.0751
wandb:     final_val_r2 -0.14548
wandb:   final_val_rmse 0.27404
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06652
wandb:       train_time 13.99382
wandb:         val_loss 0.09897
wandb:          val_mse 0.09899
wandb:           val_r2 -0.50991
wandb:         val_rmse 0.31463
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_103833-lrprp8p7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_103833-lrprp8p7/logs
Experiment probe_layer10_complexity_control2_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_fi
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[fi]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_fi"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/fi"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:39:19,088][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/fi
experiment_name: probe_layer10_complexity_control3_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - fi
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:39:19,088][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:39:19,088][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:39:19,088][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:39:19,093][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['fi']
[2025-05-04 10:39:19,094][__main__][INFO] - Processing language: fi
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:39:20,823][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:39:23,046][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:39:23,046][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:39:23,083][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:39:23,125][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:39:23,262][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-05-04 10:39:23,272][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:39:23,272][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-05-04 10:39:23,274][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:39:23,293][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:39:23,346][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:39:23,368][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-05-04 10:39:23,369][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:39:23,370][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-05-04 10:39:23,370][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:39:23,390][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:39:23,421][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:39:23,434][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-05-04 10:39:23,436][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:39:23,436][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-05-04 10:39:23,437][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:39:23,438][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:39:23,438][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-05-04 10:39:23,438][src.data.datasets][INFO] - Sample label: 0.3422375023365021
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:39:23,439][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:39:23,439][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-05-04 10:39:23,439][src.data.datasets][INFO] - Sample label: 1.0
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:39:23,440][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:39:23,440][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-05-04 10:39:23,440][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:39:23,441][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:39:23,441][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:39:23,441][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:39:27,691][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:39:27,692][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:39:27,692][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:39:27,692][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:39:27,695][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:39:27,695][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:39:27,696][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:39:27,696][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:39:27,696][__main__][INFO] - Successfully created lm_probe model for fi
[2025-05-04 10:39:27,697][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:39:27,697][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6673Epoch 1/15: [                              ] 2/75 batches, loss: 0.9061Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8736Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8396Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7970Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7175Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6765Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6675Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6313Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6295Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5986Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5699Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5544Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5353Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5116Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4964Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4970Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4868Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4758Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4706Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4777Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4809Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4699Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4605Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4583Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4524Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4576Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4551Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4487Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4452Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4413Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4407Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4346Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4320Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4280Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4320Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4257Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4230Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4189Epoch 1/15: [================              ] 40/75 batches, loss: 0.4183Epoch 1/15: [================              ] 41/75 batches, loss: 0.4159Epoch 1/15: [================              ] 42/75 batches, loss: 0.4131Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4082Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4064Epoch 1/15: [==================            ] 45/75 batches, loss: 0.4062Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3997Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3970Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3918Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3875Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3850Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3806Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3793Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3783Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3786Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3766Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3729Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3710Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3692Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3659Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3634Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3601Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3566Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3536Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3513Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3481Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3474Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3437Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3411Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3404Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3377Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3344Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3322Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3297Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3267Epoch 1/15: [==============================] 75/75 batches, loss: 0.3243
[2025-05-04 10:39:32,888][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3243
[2025-05-04 10:39:33,105][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0777, Metrics: {'mse': 0.07765040546655655, 'rmse': 0.2786582233966128, 'r2': -0.18440747261047363}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2069Epoch 2/15: [                              ] 2/75 batches, loss: 0.1890Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1990Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2138Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1995Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1783Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1887Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1989Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1910Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2000Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1979Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1937Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1908Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1879Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1836Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1803Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1760Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1824Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1834Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1773Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1768Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1765Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1767Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1741Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1762Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1760Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1855Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1838Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1845Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1863Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1832Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1829Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1812Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1827Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1836Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1808Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1778Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1753Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1759Epoch 2/15: [================              ] 40/75 batches, loss: 0.1746Epoch 2/15: [================              ] 41/75 batches, loss: 0.1738Epoch 2/15: [================              ] 42/75 batches, loss: 0.1718Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1721Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1706Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1708Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1718Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1705Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1689Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1700Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1684Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1667Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1659Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1661Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1653Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1662Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1659Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1642Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1627Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1610Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1602Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1594Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1581Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1566Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1561Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1574Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1573Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1572Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1563Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1546Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1534Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1528Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1514Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1511Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1503Epoch 2/15: [==============================] 75/75 batches, loss: 0.1490
[2025-05-04 10:39:35,774][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1490
[2025-05-04 10:39:36,017][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1284, Metrics: {'mse': 0.12857870757579803, 'rmse': 0.35857873274330987, 'r2': -0.9612207412719727}
[2025-05-04 10:39:36,017][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1394Epoch 3/15: [                              ] 2/75 batches, loss: 0.1190Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1464Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1400Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1312Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1218Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1165Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1163Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1166Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1232Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1216Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1228Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1232Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1253Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1252Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1217Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1221Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1213Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1185Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1170Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1149Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1120Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1106Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1116Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1108Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1098Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1080Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1076Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1093Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1092Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1078Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1090Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1084Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1085Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1066Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1071Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1061Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1064Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1057Epoch 3/15: [================              ] 40/75 batches, loss: 0.1047Epoch 3/15: [================              ] 41/75 batches, loss: 0.1048Epoch 3/15: [================              ] 42/75 batches, loss: 0.1045Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1040Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1044Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1038Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1027Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1025Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1022Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1016Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1007Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0998Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0992Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0990Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0980Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0977Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0969Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0975Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0980Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0984Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0975Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0970Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0967Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0958Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0959Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0962Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0957Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0958Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0958Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0965Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0963Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0960Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0965Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0963Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0963Epoch 3/15: [==============================] 75/75 batches, loss: 0.0966
[2025-05-04 10:39:38,295][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0966
[2025-05-04 10:39:38,545][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1055, Metrics: {'mse': 0.10557249933481216, 'rmse': 0.3249192197067021, 'r2': -0.6103053092956543}
[2025-05-04 10:39:38,545][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.1305Epoch 4/15: [                              ] 2/75 batches, loss: 0.1238Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0982Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1064Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1049Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1009Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1060Epoch 4/15: [===                           ] 8/75 batches, loss: 0.1021Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1036Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0991Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1083Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1120Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1091Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1035Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1075Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1072Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1057Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1038Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1051Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1028Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1034Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1072Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1053Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1048Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1058Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1042Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1058Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1046Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1044Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1037Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1014Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1002Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0991Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0985Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0974Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0974Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0963Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0964Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0952Epoch 4/15: [================              ] 40/75 batches, loss: 0.0951Epoch 4/15: [================              ] 41/75 batches, loss: 0.0947Epoch 4/15: [================              ] 42/75 batches, loss: 0.0938Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0940Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0937Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0938Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0941Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0936Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0944Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0940Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0940Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0936Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0937Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0928Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0921Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0915Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0917Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0903Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0894Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0895Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0890Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0895Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0889Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0885Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0888Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0882Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0881Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0876Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0880Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0879Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0883Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0878Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0878Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0874Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0868Epoch 4/15: [==============================] 75/75 batches, loss: 0.0861
[2025-05-04 10:39:40,830][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0861
[2025-05-04 10:39:41,061][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1169, Metrics: {'mse': 0.11707070469856262, 'rmse': 0.34215596545809723, 'r2': -0.7856884002685547}
[2025-05-04 10:39:41,062][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0819Epoch 5/15: [                              ] 2/75 batches, loss: 0.0851Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0824Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0790Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0749Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0727Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0779Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0828Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0802Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0754Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0781Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0764Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0735Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0732Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0730Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0721Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0729Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0717Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0724Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0711Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0715Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0703Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0697Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0710Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0721Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0708Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0711Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0711Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0699Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0694Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0701Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0702Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0697Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0700Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0694Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0698Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0714Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0705Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0703Epoch 5/15: [================              ] 40/75 batches, loss: 0.0705Epoch 5/15: [================              ] 41/75 batches, loss: 0.0707Epoch 5/15: [================              ] 42/75 batches, loss: 0.0705Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0695Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0694Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0691Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0697Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0690Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0687Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0688Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0683Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0680Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0674Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0676Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0674Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0668Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0667Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0667Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0664Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0663Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0664Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0656Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0657Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0663Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0662Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0659Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0655Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0653Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0650Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0644Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0651Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0650Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0656Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0656Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0652Epoch 5/15: [==============================] 75/75 batches, loss: 0.0645
[2025-05-04 10:39:43,321][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0645
[2025-05-04 10:39:43,560][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0995, Metrics: {'mse': 0.0995156466960907, 'rmse': 0.3154610066174435, 'r2': -0.5179197788238525}
[2025-05-04 10:39:43,560][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:39:43,560][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:39:43,561][src.training.lm_trainer][INFO] - Training completed in 13.62 seconds
[2025-05-04 10:39:43,561][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:39:46,389][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03202337026596069, 'rmse': 0.17895074815702977, 'r2': -0.5846841335296631}
[2025-05-04 10:39:46,389][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07765040546655655, 'rmse': 0.2786582233966128, 'r2': -0.18440747261047363}
[2025-05-04 10:39:46,389][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.050216205418109894, 'rmse': 0.22408972626631032, 'r2': -0.2717146873474121}
[2025-05-04 10:39:48,071][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/fi/fi/model.pt
[2025-05-04 10:39:48,072][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▃▂
wandb:       train_loss █▃▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁█▅▆▄
wandb:          val_mse ▁█▅▆▄
wandb:           val_r2 █▁▄▃▅
wandb:         val_rmse ▁█▅▇▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07769
wandb:     best_val_mse 0.07765
wandb:      best_val_r2 -0.18441
wandb:    best_val_rmse 0.27866
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05022
wandb:    final_test_r2 -0.27171
wandb:  final_test_rmse 0.22409
wandb:  final_train_mse 0.03202
wandb:   final_train_r2 -0.58468
wandb: final_train_rmse 0.17895
wandb:    final_val_mse 0.07765
wandb:     final_val_r2 -0.18441
wandb:   final_val_rmse 0.27866
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06453
wandb:       train_time 13.62076
wandb:         val_loss 0.09949
wandb:          val_mse 0.09952
wandb:           val_r2 -0.51792
wandb:         val_rmse 0.31546
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_103919-ylubpg6d
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_103919-ylubpg6d/logs
Experiment probe_layer10_complexity_control3_fi completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/fi/fi/results.json for layer 10
Running experiment: probe_layer10_question_type_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:40:04,611][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/id
experiment_name: probe_layer10_question_type_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:40:04,611][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:40:04,611][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:40:04,611][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:40:04,617][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-04 10:40:04,617][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:40:06,926][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:40:09,137][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:40:09,137][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:09,270][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:40:09,315][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:40:09,432][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:40:09,439][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:09,439][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:40:09,440][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:09,465][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:09,511][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:09,539][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:40:09,540][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:09,540][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:40:09,541][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:09,568][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:09,605][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:09,620][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:40:09,621][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:09,622][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:40:09,623][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:09,624][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-04 10:40:09,624][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:40:09,624][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:09,625][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:40:09,625][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:09,625][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:09,626][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:40:09,626][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:40:09,626][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:40:09,627][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:40:09,627][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:40:14,671][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:40:14,672][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:40:14,672][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:40:14,672][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:40:14,677][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:40:14,678][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:40:14,678][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:40:14,678][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:40:14,678][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:40:14,679][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:40:14,679][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:40:14,680][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.7677Epoch 1/15: [=                             ] 2/60 batches, loss: 0.7429Epoch 1/15: [=                             ] 3/60 batches, loss: 0.7366Epoch 1/15: [==                            ] 4/60 batches, loss: 0.7312Epoch 1/15: [==                            ] 5/60 batches, loss: 0.7302Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7245Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7200Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7171Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7141Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7116Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7098Epoch 1/15: [======                        ] 12/60 batches, loss: 0.7083Epoch 1/15: [======                        ] 13/60 batches, loss: 0.7071Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.7062Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.7053Epoch 1/15: [========                      ] 16/60 batches, loss: 0.7047Epoch 1/15: [========                      ] 17/60 batches, loss: 0.7040Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.7034Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.7029Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.7025Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.7021Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.7017Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.7013Epoch 1/15: [============                  ] 24/60 batches, loss: 0.7010Epoch 1/15: [============                  ] 25/60 batches, loss: 0.7006Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.7003Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.7000Epoch 1/15: [==============                ] 28/60 batches, loss: 0.6998Epoch 1/15: [==============                ] 29/60 batches, loss: 0.6996Epoch 1/15: [===============               ] 30/60 batches, loss: 0.6994Epoch 1/15: [===============               ] 31/60 batches, loss: 0.6993Epoch 1/15: [================              ] 32/60 batches, loss: 0.6990Epoch 1/15: [================              ] 33/60 batches, loss: 0.6988Epoch 1/15: [=================             ] 34/60 batches, loss: 0.6986Epoch 1/15: [=================             ] 35/60 batches, loss: 0.6985Epoch 1/15: [==================            ] 36/60 batches, loss: 0.6984Epoch 1/15: [==================            ] 37/60 batches, loss: 0.6982Epoch 1/15: [===================           ] 38/60 batches, loss: 0.6981Epoch 1/15: [===================           ] 39/60 batches, loss: 0.6980Epoch 1/15: [====================          ] 40/60 batches, loss: 0.6978Epoch 1/15: [====================          ] 41/60 batches, loss: 0.6977Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.6977Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.6975Epoch 1/15: [======================        ] 44/60 batches, loss: 0.6974Epoch 1/15: [======================        ] 45/60 batches, loss: 0.6973Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.6972Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.6972Epoch 1/15: [========================      ] 48/60 batches, loss: 0.6971Epoch 1/15: [========================      ] 49/60 batches, loss: 0.6970Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.6969Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.6969Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.6968Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.6967Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.6967Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.6966Epoch 1/15: [============================  ] 56/60 batches, loss: 0.6965Epoch 1/15: [============================  ] 57/60 batches, loss: 0.6965Epoch 1/15: [============================= ] 58/60 batches, loss: 0.6964Epoch 1/15: [============================= ] 59/60 batches, loss: 0.6964Epoch 1/15: [==============================] 60/60 batches, loss: 0.6963
[2025-05-04 10:40:19,485][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6963
[2025-05-04 10:40:19,725][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.6931Epoch 2/15: [=                             ] 2/60 batches, loss: 0.6937Epoch 2/15: [=                             ] 3/60 batches, loss: 0.6938Epoch 2/15: [==                            ] 4/60 batches, loss: 0.6938Epoch 2/15: [==                            ] 5/60 batches, loss: 0.6937Epoch 2/15: [===                           ] 6/60 batches, loss: 0.6937Epoch 2/15: [===                           ] 7/60 batches, loss: 0.6936Epoch 2/15: [====                          ] 8/60 batches, loss: 0.6935Epoch 2/15: [====                          ] 9/60 batches, loss: 0.6935Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.6935Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.6935Epoch 2/15: [======                        ] 12/60 batches, loss: 0.6935Epoch 2/15: [======                        ] 13/60 batches, loss: 0.6935Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.6935Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.6935Epoch 2/15: [========                      ] 16/60 batches, loss: 0.6935Epoch 2/15: [========                      ] 17/60 batches, loss: 0.6935Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.6935Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.6935Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.6935Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.6935Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.6934Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.6934Epoch 2/15: [============                  ] 24/60 batches, loss: 0.6935Epoch 2/15: [============                  ] 25/60 batches, loss: 0.6934Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.6934Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.6934Epoch 2/15: [==============                ] 28/60 batches, loss: 0.6934Epoch 2/15: [==============                ] 29/60 batches, loss: 0.6934Epoch 2/15: [===============               ] 30/60 batches, loss: 0.6934Epoch 2/15: [===============               ] 31/60 batches, loss: 0.6934Epoch 2/15: [================              ] 32/60 batches, loss: 0.6934Epoch 2/15: [================              ] 33/60 batches, loss: 0.6934Epoch 2/15: [=================             ] 34/60 batches, loss: 0.6934Epoch 2/15: [=================             ] 35/60 batches, loss: 0.6934Epoch 2/15: [==================            ] 36/60 batches, loss: 0.6934Epoch 2/15: [==================            ] 37/60 batches, loss: 0.6934Epoch 2/15: [===================           ] 38/60 batches, loss: 0.6933Epoch 2/15: [===================           ] 39/60 batches, loss: 0.6933Epoch 2/15: [====================          ] 40/60 batches, loss: 0.6933Epoch 2/15: [====================          ] 41/60 batches, loss: 0.6933Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.6933Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.6933Epoch 2/15: [======================        ] 44/60 batches, loss: 0.6933Epoch 2/15: [======================        ] 45/60 batches, loss: 0.6933Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.6933Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.6933Epoch 2/15: [========================      ] 48/60 batches, loss: 0.6933Epoch 2/15: [========================      ] 49/60 batches, loss: 0.6933Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.6933Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.6933Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.6933Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.6933Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.6933Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.6933Epoch 2/15: [============================  ] 56/60 batches, loss: 0.6933Epoch 2/15: [============================  ] 57/60 batches, loss: 0.6933Epoch 2/15: [============================= ] 58/60 batches, loss: 0.6933Epoch 2/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 2/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:40:21,990][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:40:22,256][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:22,257][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.6931Epoch 3/15: [=                             ] 2/60 batches, loss: 0.6934Epoch 3/15: [=                             ] 3/60 batches, loss: 0.6932Epoch 3/15: [==                            ] 4/60 batches, loss: 0.6931Epoch 3/15: [==                            ] 5/60 batches, loss: 0.6931Epoch 3/15: [===                           ] 6/60 batches, loss: 0.6932Epoch 3/15: [===                           ] 7/60 batches, loss: 0.6932Epoch 3/15: [====                          ] 8/60 batches, loss: 0.6932Epoch 3/15: [====                          ] 9/60 batches, loss: 0.6933Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.6932Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.6933Epoch 3/15: [======                        ] 12/60 batches, loss: 0.6932Epoch 3/15: [======                        ] 13/60 batches, loss: 0.6932Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.6932Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.6932Epoch 3/15: [========                      ] 16/60 batches, loss: 0.6932Epoch 3/15: [========                      ] 17/60 batches, loss: 0.6931Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.6931Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.6931Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.6931Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.6931Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.6931Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.6931Epoch 3/15: [============                  ] 24/60 batches, loss: 0.6932Epoch 3/15: [============                  ] 25/60 batches, loss: 0.6932Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.6932Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.6932Epoch 3/15: [==============                ] 28/60 batches, loss: 0.6931Epoch 3/15: [==============                ] 29/60 batches, loss: 0.6932Epoch 3/15: [===============               ] 30/60 batches, loss: 0.6932Epoch 3/15: [===============               ] 31/60 batches, loss: 0.6932Epoch 3/15: [================              ] 32/60 batches, loss: 0.6932Epoch 3/15: [================              ] 33/60 batches, loss: 0.6932Epoch 3/15: [=================             ] 34/60 batches, loss: 0.6932Epoch 3/15: [=================             ] 35/60 batches, loss: 0.6932Epoch 3/15: [==================            ] 36/60 batches, loss: 0.6932Epoch 3/15: [==================            ] 37/60 batches, loss: 0.6932Epoch 3/15: [===================           ] 38/60 batches, loss: 0.6932Epoch 3/15: [===================           ] 39/60 batches, loss: 0.6932Epoch 3/15: [====================          ] 40/60 batches, loss: 0.6932Epoch 3/15: [====================          ] 41/60 batches, loss: 0.6932Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.6932Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.6932Epoch 3/15: [======================        ] 44/60 batches, loss: 0.6932Epoch 3/15: [======================        ] 45/60 batches, loss: 0.6932Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.6932Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.6932Epoch 3/15: [========================      ] 48/60 batches, loss: 0.6932Epoch 3/15: [========================      ] 49/60 batches, loss: 0.6932Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.6932Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.6932Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.6932Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.6932Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 3/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 3/15: [============================  ] 57/60 batches, loss: 0.6931Epoch 3/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 3/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 3/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:40:24,087][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-04 10:40:24,352][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:24,352][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.6926Epoch 4/15: [=                             ] 2/60 batches, loss: 0.6930Epoch 4/15: [=                             ] 3/60 batches, loss: 0.6930Epoch 4/15: [==                            ] 4/60 batches, loss: 0.6929Epoch 4/15: [==                            ] 5/60 batches, loss: 0.6929Epoch 4/15: [===                           ] 6/60 batches, loss: 0.6928Epoch 4/15: [===                           ] 7/60 batches, loss: 0.6929Epoch 4/15: [====                          ] 8/60 batches, loss: 0.6929Epoch 4/15: [====                          ] 9/60 batches, loss: 0.6929Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.6929Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.6930Epoch 4/15: [======                        ] 12/60 batches, loss: 0.6931Epoch 4/15: [======                        ] 13/60 batches, loss: 0.6931Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.6931Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.6931Epoch 4/15: [========                      ] 16/60 batches, loss: 0.6931Epoch 4/15: [========                      ] 17/60 batches, loss: 0.6932Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.6931Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.6932Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.6931Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 24/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 25/60 batches, loss: 0.6931Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.6931Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.6932Epoch 4/15: [==============                ] 28/60 batches, loss: 0.6931Epoch 4/15: [==============                ] 29/60 batches, loss: 0.6931Epoch 4/15: [===============               ] 30/60 batches, loss: 0.6931Epoch 4/15: [===============               ] 31/60 batches, loss: 0.6931Epoch 4/15: [================              ] 32/60 batches, loss: 0.6931Epoch 4/15: [================              ] 33/60 batches, loss: 0.6931Epoch 4/15: [=================             ] 34/60 batches, loss: 0.6931Epoch 4/15: [=================             ] 35/60 batches, loss: 0.6931Epoch 4/15: [==================            ] 36/60 batches, loss: 0.6931Epoch 4/15: [==================            ] 37/60 batches, loss: 0.6931Epoch 4/15: [===================           ] 38/60 batches, loss: 0.6931Epoch 4/15: [===================           ] 39/60 batches, loss: 0.6931Epoch 4/15: [====================          ] 40/60 batches, loss: 0.6931Epoch 4/15: [====================          ] 41/60 batches, loss: 0.6931Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.6931Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.6931Epoch 4/15: [======================        ] 44/60 batches, loss: 0.6931Epoch 4/15: [======================        ] 45/60 batches, loss: 0.6931Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.6931Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.6931Epoch 4/15: [========================      ] 48/60 batches, loss: 0.6931Epoch 4/15: [========================      ] 49/60 batches, loss: 0.6931Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.6931Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.6931Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.6932Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 57/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 4/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:40:26,183][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:40:26,442][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:26,443][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:40:26,443][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-04 10:40:26,443][src.training.lm_trainer][INFO] - Training completed in 9.58 seconds
[2025-05-04 10:40:26,443][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:40:28,956][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5209643605870021, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:28,956][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:28,957][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:40:30,641][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/id/id/model.pt
[2025-05-04 10:40:30,643][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁██▇
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69296
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.52096
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69318
wandb:            train_time 9.5767
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69304
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104004-61sb1akt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104004-61sb1akt/logs
Experiment probe_layer10_question_type_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_question_type_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:40:44,570][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/id
experiment_name: probe_layer10_question_type_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:40:44,570][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:40:44,570][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:40:44,570][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:40:44,575][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-04 10:40:44,575][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:40:46,075][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:40:48,346][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:40:48,346][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:48,385][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:40:48,427][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:40:48,597][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:40:48,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:48,606][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:40:48,608][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:48,629][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:48,674][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:48,698][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:40:48,699][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:48,699][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:40:48,700][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:40:48,726][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:48,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:40:48,772][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:40:48,773][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:40:48,774][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:40:48,775][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:48,776][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-04 10:40:48,776][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:40:48,776][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:48,777][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:40:48,777][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:40:48,777][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:40:48,778][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:40:48,778][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:40:48,778][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:40:48,779][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:40:48,779][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:40:52,963][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:40:52,964][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:40:52,964][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:40:52,965][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:40:52,970][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:40:52,971][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:40:52,971][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:40:52,971][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:40:52,971][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:40:52,972][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:40:52,972][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:40:52,973][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.7294Epoch 1/15: [=                             ] 2/60 batches, loss: 0.7191Epoch 1/15: [=                             ] 3/60 batches, loss: 0.7059Epoch 1/15: [==                            ] 4/60 batches, loss: 0.7091Epoch 1/15: [==                            ] 5/60 batches, loss: 0.7074Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7051Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7054Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7048Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7025Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7013Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7031Epoch 1/15: [======                        ] 12/60 batches, loss: 0.7032Epoch 1/15: [======                        ] 13/60 batches, loss: 0.7030Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.7024Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.7016Epoch 1/15: [========                      ] 16/60 batches, loss: 0.7010Epoch 1/15: [========                      ] 17/60 batches, loss: 0.7006Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.7001Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.6996Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.6993Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.6991Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.6987Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.6985Epoch 1/15: [============                  ] 24/60 batches, loss: 0.6983Epoch 1/15: [============                  ] 25/60 batches, loss: 0.6981Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.6979Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.6977Epoch 1/15: [==============                ] 28/60 batches, loss: 0.6975Epoch 1/15: [==============                ] 29/60 batches, loss: 0.6973Epoch 1/15: [===============               ] 30/60 batches, loss: 0.6972Epoch 1/15: [===============               ] 31/60 batches, loss: 0.6971Epoch 1/15: [================              ] 32/60 batches, loss: 0.6971Epoch 1/15: [================              ] 33/60 batches, loss: 0.6970Epoch 1/15: [=================             ] 34/60 batches, loss: 0.6970Epoch 1/15: [=================             ] 35/60 batches, loss: 0.6969Epoch 1/15: [==================            ] 36/60 batches, loss: 0.6968Epoch 1/15: [==================            ] 37/60 batches, loss: 0.6967Epoch 1/15: [===================           ] 38/60 batches, loss: 0.6966Epoch 1/15: [===================           ] 39/60 batches, loss: 0.6965Epoch 1/15: [====================          ] 40/60 batches, loss: 0.6964Epoch 1/15: [====================          ] 41/60 batches, loss: 0.6963Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.6962Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.6961Epoch 1/15: [======================        ] 44/60 batches, loss: 0.6961Epoch 1/15: [======================        ] 45/60 batches, loss: 0.6960Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.6960Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.6959Epoch 1/15: [========================      ] 48/60 batches, loss: 0.6959Epoch 1/15: [========================      ] 49/60 batches, loss: 0.6959Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.6958Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.6957Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.6957Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.6956Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.6956Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.6955Epoch 1/15: [============================  ] 56/60 batches, loss: 0.6955Epoch 1/15: [============================  ] 57/60 batches, loss: 0.6955Epoch 1/15: [============================= ] 58/60 batches, loss: 0.6954Epoch 1/15: [============================= ] 59/60 batches, loss: 0.6954Epoch 1/15: [==============================] 60/60 batches, loss: 0.6954
[2025-05-04 10:40:57,516][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6954
[2025-05-04 10:40:57,746][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.6930Epoch 2/15: [=                             ] 2/60 batches, loss: 0.6932Epoch 2/15: [=                             ] 3/60 batches, loss: 0.6933Epoch 2/15: [==                            ] 4/60 batches, loss: 0.6934Epoch 2/15: [==                            ] 5/60 batches, loss: 0.6934Epoch 2/15: [===                           ] 6/60 batches, loss: 0.6933Epoch 2/15: [===                           ] 7/60 batches, loss: 0.6932Epoch 2/15: [====                          ] 8/60 batches, loss: 0.6932Epoch 2/15: [====                          ] 9/60 batches, loss: 0.6932Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.6932Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.6932Epoch 2/15: [======                        ] 12/60 batches, loss: 0.6931Epoch 2/15: [======                        ] 13/60 batches, loss: 0.6931Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.6931Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.6929Epoch 2/15: [========                      ] 16/60 batches, loss: 0.6930Epoch 2/15: [========                      ] 17/60 batches, loss: 0.6930Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.6930Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.6930Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.6930Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.6931Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.6931Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.6931Epoch 2/15: [============                  ] 24/60 batches, loss: 0.6931Epoch 2/15: [============                  ] 25/60 batches, loss: 0.6931Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.6931Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.6931Epoch 2/15: [==============                ] 28/60 batches, loss: 0.6931Epoch 2/15: [==============                ] 29/60 batches, loss: 0.6931Epoch 2/15: [===============               ] 30/60 batches, loss: 0.6931Epoch 2/15: [===============               ] 31/60 batches, loss: 0.6931Epoch 2/15: [================              ] 32/60 batches, loss: 0.6931Epoch 2/15: [================              ] 33/60 batches, loss: 0.6931Epoch 2/15: [=================             ] 34/60 batches, loss: 0.6931Epoch 2/15: [=================             ] 35/60 batches, loss: 0.6931Epoch 2/15: [==================            ] 36/60 batches, loss: 0.6931Epoch 2/15: [==================            ] 37/60 batches, loss: 0.6931Epoch 2/15: [===================           ] 38/60 batches, loss: 0.6931Epoch 2/15: [===================           ] 39/60 batches, loss: 0.6931Epoch 2/15: [====================          ] 40/60 batches, loss: 0.6931Epoch 2/15: [====================          ] 41/60 batches, loss: 0.6931Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.6931Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.6931Epoch 2/15: [======================        ] 44/60 batches, loss: 0.6931Epoch 2/15: [======================        ] 45/60 batches, loss: 0.6931Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.6931Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.6931Epoch 2/15: [========================      ] 48/60 batches, loss: 0.6931Epoch 2/15: [========================      ] 49/60 batches, loss: 0.6931Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.6931Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.6931Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.6931Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.6931Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.6931Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.6931Epoch 2/15: [============================  ] 56/60 batches, loss: 0.6931Epoch 2/15: [============================  ] 57/60 batches, loss: 0.6931Epoch 2/15: [============================= ] 58/60 batches, loss: 0.6931Epoch 2/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 2/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:40:59,996][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:41:00,247][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:00,248][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.6932Epoch 3/15: [=                             ] 2/60 batches, loss: 0.6932Epoch 3/15: [=                             ] 3/60 batches, loss: 0.6931Epoch 3/15: [==                            ] 4/60 batches, loss: 0.6932Epoch 3/15: [==                            ] 5/60 batches, loss: 0.6931Epoch 3/15: [===                           ] 6/60 batches, loss: 0.6932Epoch 3/15: [===                           ] 7/60 batches, loss: 0.6933Epoch 3/15: [====                          ] 8/60 batches, loss: 0.6932Epoch 3/15: [====                          ] 9/60 batches, loss: 0.6931Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.6931Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.6932Epoch 3/15: [======                        ] 12/60 batches, loss: 0.6931Epoch 3/15: [======                        ] 13/60 batches, loss: 0.6931Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.6931Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.6930Epoch 3/15: [========                      ] 16/60 batches, loss: 0.6930Epoch 3/15: [========                      ] 17/60 batches, loss: 0.6930Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.6930Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.6930Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.6931Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.6931Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.6931Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.6930Epoch 3/15: [============                  ] 24/60 batches, loss: 0.6930Epoch 3/15: [============                  ] 25/60 batches, loss: 0.6931Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.6931Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.6930Epoch 3/15: [==============                ] 28/60 batches, loss: 0.6930Epoch 3/15: [==============                ] 29/60 batches, loss: 0.6930Epoch 3/15: [===============               ] 30/60 batches, loss: 0.6930Epoch 3/15: [===============               ] 31/60 batches, loss: 0.6930Epoch 3/15: [================              ] 32/60 batches, loss: 0.6930Epoch 3/15: [================              ] 33/60 batches, loss: 0.6930Epoch 3/15: [=================             ] 34/60 batches, loss: 0.6930Epoch 3/15: [=================             ] 35/60 batches, loss: 0.6930Epoch 3/15: [==================            ] 36/60 batches, loss: 0.6930Epoch 3/15: [==================            ] 37/60 batches, loss: 0.6931Epoch 3/15: [===================           ] 38/60 batches, loss: 0.6931Epoch 3/15: [===================           ] 39/60 batches, loss: 0.6931Epoch 3/15: [====================          ] 40/60 batches, loss: 0.6931Epoch 3/15: [====================          ] 41/60 batches, loss: 0.6931Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.6931Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.6931Epoch 3/15: [======================        ] 44/60 batches, loss: 0.6931Epoch 3/15: [======================        ] 45/60 batches, loss: 0.6931Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.6931Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.6931Epoch 3/15: [========================      ] 48/60 batches, loss: 0.6931Epoch 3/15: [========================      ] 49/60 batches, loss: 0.6931Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.6931Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.6931Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.6931Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.6931Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 3/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 3/15: [============================  ] 57/60 batches, loss: 0.6932Epoch 3/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 3/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 3/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:41:02,096][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-04 10:41:02,386][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:02,386][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.6924Epoch 4/15: [=                             ] 2/60 batches, loss: 0.6928Epoch 4/15: [=                             ] 3/60 batches, loss: 0.6930Epoch 4/15: [==                            ] 4/60 batches, loss: 0.6931Epoch 4/15: [==                            ] 5/60 batches, loss: 0.6932Epoch 4/15: [===                           ] 6/60 batches, loss: 0.6933Epoch 4/15: [===                           ] 7/60 batches, loss: 0.6933Epoch 4/15: [====                          ] 8/60 batches, loss: 0.6933Epoch 4/15: [====                          ] 9/60 batches, loss: 0.6933Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.6933Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.6933Epoch 4/15: [======                        ] 12/60 batches, loss: 0.6932Epoch 4/15: [======                        ] 13/60 batches, loss: 0.6933Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.6933Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.6933Epoch 4/15: [========                      ] 16/60 batches, loss: 0.6932Epoch 4/15: [========                      ] 17/60 batches, loss: 0.6932Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.6932Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.6932Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.6932Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 24/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 25/60 batches, loss: 0.6932Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.6932Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.6932Epoch 4/15: [==============                ] 28/60 batches, loss: 0.6932Epoch 4/15: [==============                ] 29/60 batches, loss: 0.6932Epoch 4/15: [===============               ] 30/60 batches, loss: 0.6932Epoch 4/15: [===============               ] 31/60 batches, loss: 0.6932Epoch 4/15: [================              ] 32/60 batches, loss: 0.6932Epoch 4/15: [================              ] 33/60 batches, loss: 0.6932Epoch 4/15: [=================             ] 34/60 batches, loss: 0.6932Epoch 4/15: [=================             ] 35/60 batches, loss: 0.6932Epoch 4/15: [==================            ] 36/60 batches, loss: 0.6932Epoch 4/15: [==================            ] 37/60 batches, loss: 0.6932Epoch 4/15: [===================           ] 38/60 batches, loss: 0.6932Epoch 4/15: [===================           ] 39/60 batches, loss: 0.6932Epoch 4/15: [====================          ] 40/60 batches, loss: 0.6932Epoch 4/15: [====================          ] 41/60 batches, loss: 0.6932Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.6932Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.6932Epoch 4/15: [======================        ] 44/60 batches, loss: 0.6932Epoch 4/15: [======================        ] 45/60 batches, loss: 0.6932Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.6932Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.6932Epoch 4/15: [========================      ] 48/60 batches, loss: 0.6932Epoch 4/15: [========================      ] 49/60 batches, loss: 0.6932Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.6932Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.6932Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.6932Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 57/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 4/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:41:04,227][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:41:04,495][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:04,496][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:41:04,496][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-04 10:41:04,496][src.training.lm_trainer][INFO] - Training completed in 9.75 seconds
[2025-05-04 10:41:04,496][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:41:06,973][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5209643605870021, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:06,973][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:06,973][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:08,671][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/id/id/model.pt
[2025-05-04 10:41:08,672][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▃▅█
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69297
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.52096
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69317
wandb:            train_time 9.74938
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.6931
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104044-240dcb0p
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104044-240dcb0p/logs
Experiment probe_layer10_question_type_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_question_type_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:41:25,763][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/id
experiment_name: probe_layer10_question_type_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:41:25,763][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:41:25,763][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:41:25,764][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:41:25,769][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['id']
[2025-05-04 10:41:25,769][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:41:27,726][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:41:30,015][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:41:30,015][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:41:30,146][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:41:30,196][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:41:30,406][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:41:30,413][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:41:30,414][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:41:30,425][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:41:30,466][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:41:30,507][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:41:30,519][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:41:30,521][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:41:30,521][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:41:30,522][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:41:30,562][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:41:30,623][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:41:30,635][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:41:30,637][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:41:30,637][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:41:30,638][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:41:30,639][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:41:30,639][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:41:30,639][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:41:30,639][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:41:30,639][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-05-04 10:41:30,640][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:41:30,640][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:41:30,640][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:41:30,640][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:41:30,641][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:41:30,641][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:41:30,641][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:41:30,642][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:41:30,642][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:41:30,642][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:41:35,638][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:41:35,639][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:41:35,640][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:41:35,640][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:41:35,645][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:41:35,646][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:41:35,646][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:41:35,646][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:41:35,646][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:41:35,647][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:41:35,647][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:41:35,648][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.6878Epoch 1/15: [=                             ] 2/60 batches, loss: 0.7273Epoch 1/15: [=                             ] 3/60 batches, loss: 0.7128Epoch 1/15: [==                            ] 4/60 batches, loss: 0.7203Epoch 1/15: [==                            ] 5/60 batches, loss: 0.7177Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7132Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7119Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7106Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7083Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7078Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7055Epoch 1/15: [======                        ] 12/60 batches, loss: 0.7062Epoch 1/15: [======                        ] 13/60 batches, loss: 0.7048Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.7041Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.7033Epoch 1/15: [========                      ] 16/60 batches, loss: 0.7026Epoch 1/15: [========                      ] 17/60 batches, loss: 0.7022Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.7011Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.7011Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.7008Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.7008Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.7004Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.7003Epoch 1/15: [============                  ] 24/60 batches, loss: 0.7000Epoch 1/15: [============                  ] 25/60 batches, loss: 0.6996Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.6994Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.6991Epoch 1/15: [==============                ] 28/60 batches, loss: 0.6989Epoch 1/15: [==============                ] 29/60 batches, loss: 0.6987Epoch 1/15: [===============               ] 30/60 batches, loss: 0.6984Epoch 1/15: [===============               ] 31/60 batches, loss: 0.6983Epoch 1/15: [================              ] 32/60 batches, loss: 0.6981Epoch 1/15: [================              ] 33/60 batches, loss: 0.6980Epoch 1/15: [=================             ] 34/60 batches, loss: 0.6978Epoch 1/15: [=================             ] 35/60 batches, loss: 0.6978Epoch 1/15: [==================            ] 36/60 batches, loss: 0.6977Epoch 1/15: [==================            ] 37/60 batches, loss: 0.6976Epoch 1/15: [===================           ] 38/60 batches, loss: 0.6975Epoch 1/15: [===================           ] 39/60 batches, loss: 0.6974Epoch 1/15: [====================          ] 40/60 batches, loss: 0.6973Epoch 1/15: [====================          ] 41/60 batches, loss: 0.6972Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.6971Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.6970Epoch 1/15: [======================        ] 44/60 batches, loss: 0.6969Epoch 1/15: [======================        ] 45/60 batches, loss: 0.6969Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.6968Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.6967Epoch 1/15: [========================      ] 48/60 batches, loss: 0.6967Epoch 1/15: [========================      ] 49/60 batches, loss: 0.6966Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.6965Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.6964Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.6963Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.6962Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.6962Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.6961Epoch 1/15: [============================  ] 56/60 batches, loss: 0.6961Epoch 1/15: [============================  ] 57/60 batches, loss: 0.6960Epoch 1/15: [============================= ] 58/60 batches, loss: 0.6960Epoch 1/15: [============================= ] 59/60 batches, loss: 0.6959Epoch 1/15: [==============================] 60/60 batches, loss: 0.6959
[2025-05-04 10:41:40,850][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6959
[2025-05-04 10:41:41,087][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.6933Epoch 2/15: [=                             ] 2/60 batches, loss: 0.6935Epoch 2/15: [=                             ] 3/60 batches, loss: 0.6936Epoch 2/15: [==                            ] 4/60 batches, loss: 0.6934Epoch 2/15: [==                            ] 5/60 batches, loss: 0.6935Epoch 2/15: [===                           ] 6/60 batches, loss: 0.6937Epoch 2/15: [===                           ] 7/60 batches, loss: 0.6936Epoch 2/15: [====                          ] 8/60 batches, loss: 0.6934Epoch 2/15: [====                          ] 9/60 batches, loss: 0.6934Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.6934Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.6933Epoch 2/15: [======                        ] 12/60 batches, loss: 0.6933Epoch 2/15: [======                        ] 13/60 batches, loss: 0.6933Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.6933Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.6933Epoch 2/15: [========                      ] 16/60 batches, loss: 0.6933Epoch 2/15: [========                      ] 17/60 batches, loss: 0.6933Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.6933Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.6932Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.6932Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.6932Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.6932Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.6933Epoch 2/15: [============                  ] 24/60 batches, loss: 0.6934Epoch 2/15: [============                  ] 25/60 batches, loss: 0.6934Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.6934Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.6934Epoch 2/15: [==============                ] 28/60 batches, loss: 0.6934Epoch 2/15: [==============                ] 29/60 batches, loss: 0.6935Epoch 2/15: [===============               ] 30/60 batches, loss: 0.6935Epoch 2/15: [===============               ] 31/60 batches, loss: 0.6935Epoch 2/15: [================              ] 32/60 batches, loss: 0.6934Epoch 2/15: [================              ] 33/60 batches, loss: 0.6934Epoch 2/15: [=================             ] 34/60 batches, loss: 0.6934Epoch 2/15: [=================             ] 35/60 batches, loss: 0.6934Epoch 2/15: [==================            ] 36/60 batches, loss: 0.6933Epoch 2/15: [==================            ] 37/60 batches, loss: 0.6933Epoch 2/15: [===================           ] 38/60 batches, loss: 0.6933Epoch 2/15: [===================           ] 39/60 batches, loss: 0.6933Epoch 2/15: [====================          ] 40/60 batches, loss: 0.6933Epoch 2/15: [====================          ] 41/60 batches, loss: 0.6933Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.6932Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.6932Epoch 2/15: [======================        ] 44/60 batches, loss: 0.6932Epoch 2/15: [======================        ] 45/60 batches, loss: 0.6932Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.6932Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.6932Epoch 2/15: [========================      ] 48/60 batches, loss: 0.6932Epoch 2/15: [========================      ] 49/60 batches, loss: 0.6932Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.6932Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.6932Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.6932Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.6932Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 2/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 2/15: [============================  ] 57/60 batches, loss: 0.6932Epoch 2/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 2/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 2/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:41:43,329][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:41:43,579][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:43,580][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.6929Epoch 3/15: [=                             ] 2/60 batches, loss: 0.6941Epoch 3/15: [=                             ] 3/60 batches, loss: 0.6938Epoch 3/15: [==                            ] 4/60 batches, loss: 0.6935Epoch 3/15: [==                            ] 5/60 batches, loss: 0.6935Epoch 3/15: [===                           ] 6/60 batches, loss: 0.6935Epoch 3/15: [===                           ] 7/60 batches, loss: 0.6935Epoch 3/15: [====                          ] 8/60 batches, loss: 0.6933Epoch 3/15: [====                          ] 9/60 batches, loss: 0.6933Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.6933Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.6931Epoch 3/15: [======                        ] 12/60 batches, loss: 0.6932Epoch 3/15: [======                        ] 13/60 batches, loss: 0.6932Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.6932Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.6933Epoch 3/15: [========                      ] 16/60 batches, loss: 0.6934Epoch 3/15: [========                      ] 17/60 batches, loss: 0.6933Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.6933Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.6933Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.6933Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.6933Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.6933Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.6933Epoch 3/15: [============                  ] 24/60 batches, loss: 0.6933Epoch 3/15: [============                  ] 25/60 batches, loss: 0.6933Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.6933Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.6933Epoch 3/15: [==============                ] 28/60 batches, loss: 0.6933Epoch 3/15: [==============                ] 29/60 batches, loss: 0.6933Epoch 3/15: [===============               ] 30/60 batches, loss: 0.6933Epoch 3/15: [===============               ] 31/60 batches, loss: 0.6933Epoch 3/15: [================              ] 32/60 batches, loss: 0.6933Epoch 3/15: [================              ] 33/60 batches, loss: 0.6933Epoch 3/15: [=================             ] 34/60 batches, loss: 0.6933Epoch 3/15: [=================             ] 35/60 batches, loss: 0.6933Epoch 3/15: [==================            ] 36/60 batches, loss: 0.6933Epoch 3/15: [==================            ] 37/60 batches, loss: 0.6933Epoch 3/15: [===================           ] 38/60 batches, loss: 0.6933Epoch 3/15: [===================           ] 39/60 batches, loss: 0.6933Epoch 3/15: [====================          ] 40/60 batches, loss: 0.6933Epoch 3/15: [====================          ] 41/60 batches, loss: 0.6933Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.6933Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.6933Epoch 3/15: [======================        ] 44/60 batches, loss: 0.6933Epoch 3/15: [======================        ] 45/60 batches, loss: 0.6933Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.6933Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.6933Epoch 3/15: [========================      ] 48/60 batches, loss: 0.6933Epoch 3/15: [========================      ] 49/60 batches, loss: 0.6933Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.6933Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.6933Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.6933Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.6933Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.6933Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.6933Epoch 3/15: [============================  ] 56/60 batches, loss: 0.6933Epoch 3/15: [============================  ] 57/60 batches, loss: 0.6933Epoch 3/15: [============================= ] 58/60 batches, loss: 0.6933Epoch 3/15: [============================= ] 59/60 batches, loss: 0.6933Epoch 3/15: [==============================] 60/60 batches, loss: 0.6933
[2025-05-04 10:41:45,419][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6933
[2025-05-04 10:41:45,667][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:45,668][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.6931Epoch 4/15: [=                             ] 2/60 batches, loss: 0.6930Epoch 4/15: [=                             ] 3/60 batches, loss: 0.6930Epoch 4/15: [==                            ] 4/60 batches, loss: 0.6931Epoch 4/15: [==                            ] 5/60 batches, loss: 0.6932Epoch 4/15: [===                           ] 6/60 batches, loss: 0.6931Epoch 4/15: [===                           ] 7/60 batches, loss: 0.6932Epoch 4/15: [====                          ] 8/60 batches, loss: 0.6932Epoch 4/15: [====                          ] 9/60 batches, loss: 0.6932Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.6932Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.6932Epoch 4/15: [======                        ] 12/60 batches, loss: 0.6934Epoch 4/15: [======                        ] 13/60 batches, loss: 0.6934Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.6934Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.6934Epoch 4/15: [========                      ] 16/60 batches, loss: 0.6933Epoch 4/15: [========                      ] 17/60 batches, loss: 0.6933Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.6932Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.6932Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.6932Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.6932Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 24/60 batches, loss: 0.6932Epoch 4/15: [============                  ] 25/60 batches, loss: 0.6932Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.6932Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.6932Epoch 4/15: [==============                ] 28/60 batches, loss: 0.6932Epoch 4/15: [==============                ] 29/60 batches, loss: 0.6932Epoch 4/15: [===============               ] 30/60 batches, loss: 0.6932Epoch 4/15: [===============               ] 31/60 batches, loss: 0.6932Epoch 4/15: [================              ] 32/60 batches, loss: 0.6932Epoch 4/15: [================              ] 33/60 batches, loss: 0.6932Epoch 4/15: [=================             ] 34/60 batches, loss: 0.6932Epoch 4/15: [=================             ] 35/60 batches, loss: 0.6932Epoch 4/15: [==================            ] 36/60 batches, loss: 0.6932Epoch 4/15: [==================            ] 37/60 batches, loss: 0.6932Epoch 4/15: [===================           ] 38/60 batches, loss: 0.6932Epoch 4/15: [===================           ] 39/60 batches, loss: 0.6932Epoch 4/15: [====================          ] 40/60 batches, loss: 0.6932Epoch 4/15: [====================          ] 41/60 batches, loss: 0.6932Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.6932Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.6932Epoch 4/15: [======================        ] 44/60 batches, loss: 0.6932Epoch 4/15: [======================        ] 45/60 batches, loss: 0.6932Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.6932Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.6932Epoch 4/15: [========================      ] 48/60 batches, loss: 0.6932Epoch 4/15: [========================      ] 49/60 batches, loss: 0.6932Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.6932Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.6932Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.6932Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.6932Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 56/60 batches, loss: 0.6932Epoch 4/15: [============================  ] 57/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 58/60 batches, loss: 0.6932Epoch 4/15: [============================= ] 59/60 batches, loss: 0.6932Epoch 4/15: [==============================] 60/60 batches, loss: 0.6932
[2025-05-04 10:41:47,497][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:41:47,754][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:47,754][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:41:47,754][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-04 10:41:47,755][src.training.lm_trainer][INFO] - Training completed in 9.67 seconds
[2025-05-04 10:41:47,755][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:41:50,246][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5209643605870021, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:50,246][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:50,246][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:41:51,914][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/id/id/model.pt
[2025-05-04 10:41:51,916][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▄▇█
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69302
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.52096
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69319
wandb:            train_time 9.67266
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.6931
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104125-wt3rsvsv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104125-wt3rsvsv/logs
Experiment probe_layer10_question_type_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_complexity_control1_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control1_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:42:07,444][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/id
experiment_name: probe_layer10_complexity_control1_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:42:07,444][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:42:07,444][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:42:07,444][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:42:07,450][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-04 10:42:07,451][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:42:09,430][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:42:11,712][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:42:11,713][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:42:11,762][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:42:11,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:42:11,936][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:42:11,943][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:42:11,943][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:42:11,946][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:42:11,986][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:42:12,068][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:42:12,086][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:42:12,087][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:42:12,088][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:42:12,089][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:42:12,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:42:12,213][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:42:12,227][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:42:12,229][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:42:12,229][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:42:12,230][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:42:12,231][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:42:12,232][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:42:12,232][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Sample label: 0.41827768087387085
[2025-05-04 10:42:12,232][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:42:12,233][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:42:12,233][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:42:12,233][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:42:12,234][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:42:12,234][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-04 10:42:12,234][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:42:12,234][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-04 10:42:12,234][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:42:12,234][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:42:12,234][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:42:12,235][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:42:12,235][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:42:16,874][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:42:16,875][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:42:16,875][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:42:16,875][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:42:16,878][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:42:16,879][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:42:16,879][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:42:16,879][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:42:16,879][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:42:16,880][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:42:16,880][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5026Epoch 1/15: [=                             ] 2/60 batches, loss: 0.8226Epoch 1/15: [=                             ] 3/60 batches, loss: 0.9819Epoch 1/15: [==                            ] 4/60 batches, loss: 0.9015Epoch 1/15: [==                            ] 5/60 batches, loss: 0.8549Epoch 1/15: [===                           ] 6/60 batches, loss: 0.7654Epoch 1/15: [===                           ] 7/60 batches, loss: 0.6895Epoch 1/15: [====                          ] 8/60 batches, loss: 0.6538Epoch 1/15: [====                          ] 9/60 batches, loss: 0.6356Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.6731Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.6447Epoch 1/15: [======                        ] 12/60 batches, loss: 0.6014Epoch 1/15: [======                        ] 13/60 batches, loss: 0.6020Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.5874Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.5658Epoch 1/15: [========                      ] 16/60 batches, loss: 0.5776Epoch 1/15: [========                      ] 17/60 batches, loss: 0.5662Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.5612Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.5508Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.5387Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.5361Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.5357Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.5331Epoch 1/15: [============                  ] 24/60 batches, loss: 0.5195Epoch 1/15: [============                  ] 25/60 batches, loss: 0.5132Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.5125Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.5087Epoch 1/15: [==============                ] 28/60 batches, loss: 0.5015Epoch 1/15: [==============                ] 29/60 batches, loss: 0.4971Epoch 1/15: [===============               ] 30/60 batches, loss: 0.4939Epoch 1/15: [===============               ] 31/60 batches, loss: 0.4886Epoch 1/15: [================              ] 32/60 batches, loss: 0.4911Epoch 1/15: [================              ] 33/60 batches, loss: 0.4812Epoch 1/15: [=================             ] 34/60 batches, loss: 0.4797Epoch 1/15: [=================             ] 35/60 batches, loss: 0.4763Epoch 1/15: [==================            ] 36/60 batches, loss: 0.4755Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4708Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4645Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4577Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4529Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4452Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4394Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.4362Epoch 1/15: [======================        ] 44/60 batches, loss: 0.4355Epoch 1/15: [======================        ] 45/60 batches, loss: 0.4365Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.4350Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.4294Epoch 1/15: [========================      ] 48/60 batches, loss: 0.4240Epoch 1/15: [========================      ] 49/60 batches, loss: 0.4227Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.4199Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.4163Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.4149Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.4144Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.4147Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.4147Epoch 1/15: [============================  ] 56/60 batches, loss: 0.4138Epoch 1/15: [============================  ] 57/60 batches, loss: 0.4108Epoch 1/15: [============================= ] 58/60 batches, loss: 0.4095Epoch 1/15: [============================= ] 59/60 batches, loss: 0.4052Epoch 1/15: [==============================] 60/60 batches, loss: 0.4018
[2025-05-04 10:42:21,518][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.4018
[2025-05-04 10:42:21,755][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0996, Metrics: {'mse': 0.09587430953979492, 'rmse': 0.3096357691543322, 'r2': -1.2931413650512695}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3510Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2538Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2329Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2346Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2131Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2015Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2147Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2247Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2322Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2384Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2490Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2421Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2365Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2335Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2288Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2219Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2170Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2137Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2167Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2174Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2113Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2156Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2159Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2123Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2180Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2194Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2143Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2148Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2109Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2144Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2099Epoch 2/15: [================              ] 32/60 batches, loss: 0.2060Epoch 2/15: [================              ] 33/60 batches, loss: 0.2055Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2053Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2067Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2095Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2085Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2087Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2077Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2071Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2082Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2074Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2052Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2059Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2051Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2028Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2021Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1996Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2005Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2022Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1998Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1994Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1979Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1981Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1963Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1964Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1946Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1944Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1932Epoch 2/15: [==============================] 60/60 batches, loss: 0.1910
[2025-05-04 10:42:24,011][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1910
[2025-05-04 10:42:24,274][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0868, Metrics: {'mse': 0.08433052152395248, 'rmse': 0.2903971789187224, 'r2': -1.0170345306396484}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0933Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1440Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1327Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1563Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1465Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1345Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1320Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1352Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1375Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1505Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1462Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1449Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1483Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1502Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1466Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1498Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1494Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1508Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1534Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1508Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1486Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1486Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1487Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1508Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1527Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1536Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1525Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1497Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1520Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1496Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1520Epoch 3/15: [================              ] 32/60 batches, loss: 0.1518Epoch 3/15: [================              ] 33/60 batches, loss: 0.1526Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1513Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1485Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1504Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1501Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1491Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1493Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1466Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1475Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1480Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1481Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1499Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1501Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1498Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1499Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1486Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1486Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1478Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1470Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1454Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1440Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1441Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1435Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1429Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1424Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1420Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1420Epoch 3/15: [==============================] 60/60 batches, loss: 0.1407
[2025-05-04 10:42:26,551][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1407
[2025-05-04 10:42:26,810][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0949, Metrics: {'mse': 0.09229471534490585, 'rmse': 0.3038004531677098, 'r2': -1.2075238227844238}
[2025-05-04 10:42:26,811][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1041Epoch 4/15: [=                             ] 2/60 batches, loss: 0.0880Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1122Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1100Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1141Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1141Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1224Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1198Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1255Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1261Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1263Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1255Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1282Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1246Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1207Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1278Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1298Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1287Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1310Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1286Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1302Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1329Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1313Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1314Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1302Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1325Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1315Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1323Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1318Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1306Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1300Epoch 4/15: [================              ] 32/60 batches, loss: 0.1273Epoch 4/15: [================              ] 33/60 batches, loss: 0.1263Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1246Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1228Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1222Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1204Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1198Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1190Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1173Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1158Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1154Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1143Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1149Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1150Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1152Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1147Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1163Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1168Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1182Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1181Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1182Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1175Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1173Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1171Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1163Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1164Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1157Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1149Epoch 4/15: [==============================] 60/60 batches, loss: 0.1141
[2025-05-04 10:42:28,658][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1141
[2025-05-04 10:42:28,914][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0973, Metrics: {'mse': 0.0947815328836441, 'rmse': 0.30786609570338225, 'r2': -1.2670040130615234}
[2025-05-04 10:42:28,914][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1229Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1226Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1402Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1265Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1410Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1313Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1367Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1394Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1318Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1245Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1250Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1214Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1210Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1198Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1196Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1212Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1201Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1227Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1207Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1178Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1171Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1164Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1167Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1154Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1136Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1127Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1114Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1093Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1090Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1112Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1109Epoch 5/15: [================              ] 32/60 batches, loss: 0.1105Epoch 5/15: [================              ] 33/60 batches, loss: 0.1096Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1098Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1099Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1100Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1108Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1092Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1088Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1068Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1074Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1080Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1072Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1063Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1050Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1051Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1048Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1048Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1048Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1045Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1037Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1035Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1045Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1052Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1047Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1035Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1036Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1033Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1032Epoch 5/15: [==============================] 60/60 batches, loss: 0.1019
[2025-05-04 10:42:30,772][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1019
[2025-05-04 10:42:31,033][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1150, Metrics: {'mse': 0.11224360018968582, 'rmse': 0.3350277603269404, 'r2': -1.6846654415130615}
[2025-05-04 10:42:31,033][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.0849Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0663Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0743Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0687Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0663Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0651Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0677Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0725Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0705Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0756Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0740Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0750Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0760Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0776Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0767Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0801Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0785Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0799Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0794Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0779Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0795Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0794Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0798Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0782Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0786Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0804Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0796Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0802Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0803Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0800Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0803Epoch 6/15: [================              ] 32/60 batches, loss: 0.0806Epoch 6/15: [================              ] 33/60 batches, loss: 0.0816Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0805Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0804Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0808Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0812Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0819Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0811Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0821Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0834Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0824Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0826Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0817Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0817Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0814Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0810Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0815Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0818Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0823Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0842Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0847Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0842Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0843Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0838Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0838Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0842Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0836Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0834Epoch 6/15: [==============================] 60/60 batches, loss: 0.0837
[2025-05-04 10:42:32,887][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0837
[2025-05-04 10:42:33,147][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0862, Metrics: {'mse': 0.0842924565076828, 'rmse': 0.29033163194471734, 'r2': -1.0161242485046387}
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0795Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0996Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0972Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0893Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0955Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0916Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0875Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0898Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0866Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0830Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0840Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0899Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0895Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0893Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0891Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0872Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0852Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0859Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0841Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0866Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0869Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0882Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0884Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0881Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0889Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0893Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0893Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0884Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0872Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0891Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0887Epoch 7/15: [================              ] 32/60 batches, loss: 0.0883Epoch 7/15: [================              ] 33/60 batches, loss: 0.0886Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0876Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0873Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0887Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0876Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0887Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0883Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0882Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0879Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0879Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0876Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0885Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0876Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0867Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0884Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0881Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0893Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0884Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0880Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0889Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0876Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0871Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0867Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0865Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0865Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0870Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0866Epoch 7/15: [==============================] 60/60 batches, loss: 0.0862
[2025-05-04 10:42:35,364][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0862
[2025-05-04 10:42:35,637][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0797, Metrics: {'mse': 0.0777730643749237, 'rmse': 0.27887822499242154, 'r2': -0.8601921796798706}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0740Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0715Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0735Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0697Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0691Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0744Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0748Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0721Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0720Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0745Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0759Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0754Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0773Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0794Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0766Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0749Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0786Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0774Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0783Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0779Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0760Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0765Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0765Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0758Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0773Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0766Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0772Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0797Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0797Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0793Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0798Epoch 8/15: [================              ] 32/60 batches, loss: 0.0794Epoch 8/15: [================              ] 33/60 batches, loss: 0.0783Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0778Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0777Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0778Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0769Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0795Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0800Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0789Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0785Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0780Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0779Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0776Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0776Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0774Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0769Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0767Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0757Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0750Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0750Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0750Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0751Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0751Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0756Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0754Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0749Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0748Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0745Epoch 8/15: [==============================] 60/60 batches, loss: 0.0743
[2025-05-04 10:42:37,931][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0743
[2025-05-04 10:42:38,202][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1093, Metrics: {'mse': 0.10647928714752197, 'rmse': 0.3263116411461932, 'r2': -1.5467934608459473}
[2025-05-04 10:42:38,203][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1594Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1249Epoch 9/15: [=                             ] 3/60 batches, loss: 0.1010Epoch 9/15: [==                            ] 4/60 batches, loss: 0.0896Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0931Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0835Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0778Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0828Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0784Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0741Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0766Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0755Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0756Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0752Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0747Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0739Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0748Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0755Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0758Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0747Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0746Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0762Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0759Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0749Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0740Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0734Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0736Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0761Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0764Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0772Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0769Epoch 9/15: [================              ] 32/60 batches, loss: 0.0758Epoch 9/15: [================              ] 33/60 batches, loss: 0.0742Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0736Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0730Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0731Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0730Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0736Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0727Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0732Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0734Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0732Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0734Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0730Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0726Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0720Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0717Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0712Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0709Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0717Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0714Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0712Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0710Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0705Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0704Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0703Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0699Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0699Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0699Epoch 9/15: [==============================] 60/60 batches, loss: 0.0691
[2025-05-04 10:42:40,058][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0691
[2025-05-04 10:42:40,314][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0805, Metrics: {'mse': 0.0783456563949585, 'rmse': 0.27990294102591795, 'r2': -0.8738874197006226}
[2025-05-04 10:42:40,315][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.1001Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0975Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0824Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0900Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0843Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0877Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0885Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0839Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0803Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0782Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0741Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0731Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0745Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0737Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0720Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0702Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0696Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0685Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0687Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0682Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0667Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0658Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0653Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0660Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0655Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0653Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0645Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0668Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0658Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0669Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0663Epoch 10/15: [================              ] 32/60 batches, loss: 0.0670Epoch 10/15: [================              ] 33/60 batches, loss: 0.0676Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0674Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0667Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0661Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0661Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0654Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0664Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0659Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0655Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0652Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0653Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0646Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0650Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0649Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0647Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0644Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0640Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0637Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0632Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0631Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0628Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0634Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0636Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0635Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0637Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0635Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0633Epoch 10/15: [==============================] 60/60 batches, loss: 0.0629
[2025-05-04 10:42:42,168][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0629
[2025-05-04 10:42:42,432][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0813, Metrics: {'mse': 0.07882124185562134, 'rmse': 0.2807512098916429, 'r2': -0.8852626085281372}
[2025-05-04 10:42:42,432][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0500Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0455Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0538Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0483Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0501Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0561Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0604Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0607Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0621Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0613Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0615Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0628Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0625Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0629Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0625Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0623Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0615Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0629Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0633Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0624Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0617Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0618Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0619Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0616Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0613Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0610Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0614Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0599Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0591Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0604Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0611Epoch 11/15: [================              ] 32/60 batches, loss: 0.0599Epoch 11/15: [================              ] 33/60 batches, loss: 0.0599Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0591Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0589Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0591Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0594Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0591Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0603Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0596Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0595Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0602Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0605Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0620Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0625Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0621Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0626Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0625Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0626Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0627Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0626Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0627Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0624Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0624Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0627Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0627Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0631Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0629Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0628Epoch 11/15: [==============================] 60/60 batches, loss: 0.0623
[2025-05-04 10:42:44,294][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0623
[2025-05-04 10:42:44,574][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0858, Metrics: {'mse': 0.08334771543741226, 'rmse': 0.28870004405509236, 'r2': -0.9935277700424194}
[2025-05-04 10:42:44,574][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:42:44,574][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 11
[2025-05-04 10:42:44,575][src.training.lm_trainer][INFO] - Training completed in 25.80 seconds
[2025-05-04 10:42:44,575][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:42:47,082][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.040546953678131104, 'rmse': 0.20136274153410583, 'r2': -0.11740422248840332}
[2025-05-04 10:42:47,082][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.0777730643749237, 'rmse': 0.27887822499242154, 'r2': -0.8601921796798706}
[2025-05-04 10:42:47,082][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04923596978187561, 'rmse': 0.22189179746415957, 'r2': -0.20753026008605957}
[2025-05-04 10:42:48,740][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/id/id/model.pt
[2025-05-04 10:42:48,741][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▁
wandb:     best_val_mse █▄▄▁
wandb:      best_val_r2 ▁▅▅█
wandb:    best_val_rmse █▄▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▄▃▃▁▄▅▂▅▅
wandb:       train_loss █▄▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅▂▄▄█▂▁▇▁▁▂
wandb:          val_mse ▅▂▄▄█▂▁▇▁▁▂
wandb:           val_r2 ▄▇▅▅▁▇█▂██▇
wandb:         val_rmse ▅▂▄▅█▂▁▇▁▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07967
wandb:     best_val_mse 0.07777
wandb:      best_val_r2 -0.86019
wandb:    best_val_rmse 0.27888
wandb: early_stop_epoch 11
wandb:            epoch 11
wandb:   final_test_mse 0.04924
wandb:    final_test_r2 -0.20753
wandb:  final_test_rmse 0.22189
wandb:  final_train_mse 0.04055
wandb:   final_train_r2 -0.1174
wandb: final_train_rmse 0.20136
wandb:    final_val_mse 0.07777
wandb:     final_val_r2 -0.86019
wandb:   final_val_rmse 0.27888
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06232
wandb:       train_time 25.79527
wandb:         val_loss 0.08583
wandb:          val_mse 0.08335
wandb:           val_r2 -0.99353
wandb:         val_rmse 0.2887
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104207-oq0mpnxe
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104207-oq0mpnxe/logs
Experiment probe_layer10_complexity_control1_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:43:06,414][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/id
experiment_name: probe_layer10_complexity_control2_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:43:06,414][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:43:06,414][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:43:06,414][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:43:06,419][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-04 10:43:06,420][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:43:08,928][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:43:11,226][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:43:11,226][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:43:11,353][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:43:11,406][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:43:11,587][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:43:11,594][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:43:11,594][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:43:11,596][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:43:11,660][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:43:11,727][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:43:11,741][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:43:11,743][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:43:11,743][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:43:11,744][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:43:11,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:43:11,844][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:43:11,879][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:43:11,881][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:43:11,882][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:43:11,883][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:43:11,883][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:43:11,884][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:43:11,884][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-05-04 10:43:11,884][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:43:11,885][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:43:11,885][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:43:11,885][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:43:11,886][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:43:11,886][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:43:11,886][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:43:11,887][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:43:11,887][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:43:16,958][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:43:16,959][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:43:16,959][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:43:16,959][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:43:16,962][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:43:16,963][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:43:16,963][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:43:16,963][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:43:16,963][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:43:16,964][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:43:16,964][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.4982Epoch 1/15: [=                             ] 2/60 batches, loss: 0.8876Epoch 1/15: [=                             ] 3/60 batches, loss: 1.0691Epoch 1/15: [==                            ] 4/60 batches, loss: 1.0077Epoch 1/15: [==                            ] 5/60 batches, loss: 0.9662Epoch 1/15: [===                           ] 6/60 batches, loss: 0.8675Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7761Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7428Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7152Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7307Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.6971Epoch 1/15: [======                        ] 12/60 batches, loss: 0.6535Epoch 1/15: [======                        ] 13/60 batches, loss: 0.6383Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.6323Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.6093Epoch 1/15: [========                      ] 16/60 batches, loss: 0.6204Epoch 1/15: [========                      ] 17/60 batches, loss: 0.6104Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.6118Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.5953Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.5815Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.5809Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.5798Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.5713Epoch 1/15: [============                  ] 24/60 batches, loss: 0.5591Epoch 1/15: [============                  ] 25/60 batches, loss: 0.5491Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.5514Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.5513Epoch 1/15: [==============                ] 28/60 batches, loss: 0.5395Epoch 1/15: [==============                ] 29/60 batches, loss: 0.5336Epoch 1/15: [===============               ] 30/60 batches, loss: 0.5315Epoch 1/15: [===============               ] 31/60 batches, loss: 0.5245Epoch 1/15: [================              ] 32/60 batches, loss: 0.5228Epoch 1/15: [================              ] 33/60 batches, loss: 0.5134Epoch 1/15: [=================             ] 34/60 batches, loss: 0.5096Epoch 1/15: [=================             ] 35/60 batches, loss: 0.5064Epoch 1/15: [==================            ] 36/60 batches, loss: 0.5038Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4997Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4938Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4863Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4821Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4741Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4688Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.4640Epoch 1/15: [======================        ] 44/60 batches, loss: 0.4645Epoch 1/15: [======================        ] 45/60 batches, loss: 0.4605Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.4591Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.4537Epoch 1/15: [========================      ] 48/60 batches, loss: 0.4465Epoch 1/15: [========================      ] 49/60 batches, loss: 0.4435Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.4423Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.4388Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.4378Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.4343Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.4350Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.4364Epoch 1/15: [============================  ] 56/60 batches, loss: 0.4363Epoch 1/15: [============================  ] 57/60 batches, loss: 0.4320Epoch 1/15: [============================= ] 58/60 batches, loss: 0.4305Epoch 1/15: [============================= ] 59/60 batches, loss: 0.4259Epoch 1/15: [==============================] 60/60 batches, loss: 0.4230
[2025-05-04 10:43:22,299][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.4230
[2025-05-04 10:43:22,535][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0976, Metrics: {'mse': 0.09406629204750061, 'rmse': 0.30670228569004926, 'r2': -1.2498970031738281}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.2326Epoch 2/15: [=                             ] 2/60 batches, loss: 0.1759Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2050Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2114Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2096Epoch 2/15: [===                           ] 6/60 batches, loss: 0.1947Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2139Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2146Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2141Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2092Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2219Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2150Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2110Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2106Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2157Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2161Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2148Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2117Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2094Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2093Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2083Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2108Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2076Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2033Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2074Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2102Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2041Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2090Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2060Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2065Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2051Epoch 2/15: [================              ] 32/60 batches, loss: 0.2052Epoch 2/15: [================              ] 33/60 batches, loss: 0.2047Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2052Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2101Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2117Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2107Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2109Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2104Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2133Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2143Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2146Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2124Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2132Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2104Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2090Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.2090Epoch 2/15: [========================      ] 48/60 batches, loss: 0.2072Epoch 2/15: [========================      ] 49/60 batches, loss: 0.2079Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.2079Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.2070Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.2064Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.2047Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.2036Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.2018Epoch 2/15: [============================  ] 56/60 batches, loss: 0.2008Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1989Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1987Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1971Epoch 2/15: [==============================] 60/60 batches, loss: 0.1950
[2025-05-04 10:43:24,795][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1950
[2025-05-04 10:43:25,044][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1080, Metrics: {'mse': 0.10491856187582016, 'rmse': 0.3239113487913323, 'r2': -1.5094635486602783}
[2025-05-04 10:43:25,045][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.2156Epoch 3/15: [=                             ] 2/60 batches, loss: 0.2230Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1984Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1910Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1794Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1567Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1661Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1620Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1583Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1638Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1619Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1580Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1561Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1578Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1526Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1491Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1469Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1464Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1446Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1438Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1390Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1381Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1363Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1379Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1376Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1378Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1357Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1320Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1326Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1315Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1340Epoch 3/15: [================              ] 32/60 batches, loss: 0.1344Epoch 3/15: [================              ] 33/60 batches, loss: 0.1372Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1373Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1372Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1396Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1402Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1400Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1397Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1382Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1370Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1373Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1377Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1370Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1359Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1354Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1356Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1353Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1357Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1342Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1342Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1344Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1342Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1344Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1335Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1329Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1323Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1322Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1322Epoch 3/15: [==============================] 60/60 batches, loss: 0.1310
[2025-05-04 10:43:26,891][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1310
[2025-05-04 10:43:27,155][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0862, Metrics: {'mse': 0.08369310200214386, 'rmse': 0.2892976010998775, 'r2': -1.00178861618042}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1091Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1235Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1326Epoch 4/15: [==                            ] 4/60 batches, loss: 0.1259Epoch 4/15: [==                            ] 5/60 batches, loss: 0.1189Epoch 4/15: [===                           ] 6/60 batches, loss: 0.1124Epoch 4/15: [===                           ] 7/60 batches, loss: 0.1317Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1344Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1332Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1298Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1260Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1246Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1308Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1304Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1265Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1318Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1308Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1312Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1316Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1287Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1281Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1295Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1284Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1293Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1294Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1291Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1267Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1296Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1309Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1300Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1300Epoch 4/15: [================              ] 32/60 batches, loss: 0.1282Epoch 4/15: [================              ] 33/60 batches, loss: 0.1267Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1274Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1263Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1241Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1231Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1231Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1243Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1233Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1227Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1221Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1207Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1198Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1195Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1191Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1180Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1182Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1191Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1192Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1203Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1204Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1207Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1203Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1204Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1199Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1189Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1186Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1188Epoch 4/15: [==============================] 60/60 batches, loss: 0.1187
[2025-05-04 10:43:29,433][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1187
[2025-05-04 10:43:29,688][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0792, Metrics: {'mse': 0.07696115970611572, 'rmse': 0.2774187443308684, 'r2': -0.8407727479934692}
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1012Epoch 5/15: [=                             ] 2/60 batches, loss: 0.0947Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1042Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1223Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1194Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1145Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1217Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1215Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1212Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1182Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1202Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1157Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1174Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1152Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1138Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1180Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1188Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1194Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1196Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1184Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1206Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1186Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1194Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1173Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1143Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1127Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1134Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1102Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1113Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1116Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1104Epoch 5/15: [================              ] 32/60 batches, loss: 0.1100Epoch 5/15: [================              ] 33/60 batches, loss: 0.1097Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1092Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1092Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1102Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1104Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1086Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1081Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1072Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1078Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1086Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1076Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1075Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1062Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1055Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.1054Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1068Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1074Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1072Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.1069Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.1067Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1063Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1076Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1078Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1071Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1077Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1081Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1084Epoch 5/15: [==============================] 60/60 batches, loss: 0.1073
[2025-05-04 10:43:31,915][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1073
[2025-05-04 10:43:32,189][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0982, Metrics: {'mse': 0.09538307785987854, 'rmse': 0.30884150928895315, 'r2': -1.2813918590545654}
[2025-05-04 10:43:32,190][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1141Epoch 6/15: [=                             ] 2/60 batches, loss: 0.0714Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0675Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0684Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0678Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0646Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0701Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0691Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0679Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0780Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0752Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0760Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0786Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0763Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0743Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0778Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0759Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0743Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0766Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0753Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0777Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0804Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0791Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0786Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0798Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0783Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0771Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0768Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0762Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0766Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0757Epoch 6/15: [================              ] 32/60 batches, loss: 0.0778Epoch 6/15: [================              ] 33/60 batches, loss: 0.0782Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0784Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0784Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0796Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0804Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0808Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0809Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0815Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0828Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0824Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0820Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0819Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0830Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0819Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0819Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0823Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0813Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0811Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0822Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0825Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0826Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0819Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0816Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0813Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0815Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0810Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0820Epoch 6/15: [==============================] 60/60 batches, loss: 0.0815
[2025-05-04 10:43:34,047][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0815
[2025-05-04 10:43:34,305][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0824, Metrics: {'mse': 0.07994634658098221, 'rmse': 0.28274784982556844, 'r2': -0.9121730327606201}
[2025-05-04 10:43:34,306][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0637Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0633Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0580Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0725Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0731Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0799Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0818Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0772Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0777Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0766Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0722Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0762Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0765Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0756Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0847Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0836Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0829Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0843Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0835Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0828Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0833Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0845Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0856Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0879Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0868Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0852Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0852Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0860Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0846Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0865Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0858Epoch 7/15: [================              ] 32/60 batches, loss: 0.0861Epoch 7/15: [================              ] 33/60 batches, loss: 0.0857Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0847Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0837Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0826Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0819Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0821Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0829Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0833Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0833Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0843Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0840Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0848Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0843Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0847Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0846Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0847Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0853Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0849Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0842Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0833Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0828Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0825Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0820Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0820Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0824Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0827Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0826Epoch 7/15: [==============================] 60/60 batches, loss: 0.0825
[2025-05-04 10:43:36,176][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0825
[2025-05-04 10:43:36,430][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0924, Metrics: {'mse': 0.08987338095903397, 'rmse': 0.29978889398881003, 'r2': -1.1496098041534424}
[2025-05-04 10:43:36,431][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.1001Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0782Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0636Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0552Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0602Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0637Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0614Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0628Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0607Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0666Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0709Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0704Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0735Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0736Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0734Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0751Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0742Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0712Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0707Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0689Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0680Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0708Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0736Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0738Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0723Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0712Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0715Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0719Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0721Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0728Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0735Epoch 8/15: [================              ] 32/60 batches, loss: 0.0724Epoch 8/15: [================              ] 33/60 batches, loss: 0.0719Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0724Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0712Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0717Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0720Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0752Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0755Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0750Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0744Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0755Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0764Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0757Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0759Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0753Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0753Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0753Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0745Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0751Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0743Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0737Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0734Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0742Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0748Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0743Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0742Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0738Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0735Epoch 8/15: [==============================] 60/60 batches, loss: 0.0732
[2025-05-04 10:43:38,287][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0732
[2025-05-04 10:43:38,537][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0986, Metrics: {'mse': 0.09572847932577133, 'rmse': 0.3094001928340888, 'r2': -1.2896533012390137}
[2025-05-04 10:43:38,538][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:43:38,538][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-04 10:43:38,538][src.training.lm_trainer][INFO] - Training completed in 18.90 seconds
[2025-05-04 10:43:38,538][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:43:41,018][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.039794083684682846, 'rmse': 0.1994845449770053, 'r2': -0.0966564416885376}
[2025-05-04 10:43:41,018][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07696115970611572, 'rmse': 0.2774187443308684, 'r2': -0.8407727479934692}
[2025-05-04 10:43:41,018][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05173225328326225, 'rmse': 0.2274472538485841, 'r2': -0.2687525749206543}
[2025-05-04 10:43:42,664][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/id/id/model.pt
[2025-05-04 10:43:42,665][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▄▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▄▄▂▄▃
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▃▁▆▂▄▆
wandb:          val_mse ▅█▃▁▆▂▄▆
wandb:           val_r2 ▄▁▆█▃▇▅▃
wandb:         val_rmse ▅█▃▁▆▂▄▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07922
wandb:     best_val_mse 0.07696
wandb:      best_val_r2 -0.84077
wandb:    best_val_rmse 0.27742
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.05173
wandb:    final_test_r2 -0.26875
wandb:  final_test_rmse 0.22745
wandb:  final_train_mse 0.03979
wandb:   final_train_r2 -0.09666
wandb: final_train_rmse 0.19948
wandb:    final_val_mse 0.07696
wandb:     final_val_r2 -0.84077
wandb:   final_val_rmse 0.27742
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07322
wandb:       train_time 18.90042
wandb:         val_loss 0.09863
wandb:          val_mse 0.09573
wandb:           val_r2 -1.28965
wandb:         val_rmse 0.3094
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104306-1dpjos9l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104306-1dpjos9l/logs
Experiment probe_layer10_complexity_control2_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_id
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[id]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_id"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/id"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:43:56,572][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/id
experiment_name: probe_layer10_complexity_control3_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - id
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:43:56,572][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:43:56,572][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:43:56,572][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:43:56,579][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['id']
[2025-05-04 10:43:56,579][__main__][INFO] - Processing language: id
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:43:58,516][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:44:00,833][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:44:00,833][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:44:00,920][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:44:00,965][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:44:01,094][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-05-04 10:44:01,101][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:44:01,101][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-05-04 10:44:01,103][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:44:01,155][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:44:01,211][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:44:01,222][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-05-04 10:44:01,224][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:44:01,224][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-05-04 10:44:01,225][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:44:01,255][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:44:01,298][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:44:01,320][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-05-04 10:44:01,322][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:44:01,322][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-05-04 10:44:01,323][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-05-04 10:44:01,324][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:44:01,324][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:44:01,324][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:44:01,324][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:44:01,324][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:44:01,325][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Sample label: 0.12975981831550598
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:44:01,325][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:44:01,325][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:44:01,326][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Sample label: 0.0
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:44:01,326][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:44:01,326][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-05-04 10:44:01,326][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-05-04 10:44:01,327][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-05-04 10:44:01,327][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-05-04 10:44:01,327][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:44:01,327][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:44:01,327][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:44:01,327][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:44:06,073][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:44:06,074][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:44:06,074][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:44:06,074][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:44:06,077][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:44:06,077][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:44:06,077][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:44:06,078][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:44:06,078][__main__][INFO] - Successfully created lm_probe model for id
[2025-05-04 10:44:06,079][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:44:06,079][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/60 batches, loss: 0.5581Epoch 1/15: [=                             ] 2/60 batches, loss: 0.9616Epoch 1/15: [=                             ] 3/60 batches, loss: 1.0601Epoch 1/15: [==                            ] 4/60 batches, loss: 1.0313Epoch 1/15: [==                            ] 5/60 batches, loss: 0.9711Epoch 1/15: [===                           ] 6/60 batches, loss: 0.8529Epoch 1/15: [===                           ] 7/60 batches, loss: 0.7725Epoch 1/15: [====                          ] 8/60 batches, loss: 0.7399Epoch 1/15: [====                          ] 9/60 batches, loss: 0.7162Epoch 1/15: [=====                         ] 10/60 batches, loss: 0.7338Epoch 1/15: [=====                         ] 11/60 batches, loss: 0.7006Epoch 1/15: [======                        ] 12/60 batches, loss: 0.6573Epoch 1/15: [======                        ] 13/60 batches, loss: 0.6401Epoch 1/15: [=======                       ] 14/60 batches, loss: 0.6238Epoch 1/15: [=======                       ] 15/60 batches, loss: 0.6019Epoch 1/15: [========                      ] 16/60 batches, loss: 0.6158Epoch 1/15: [========                      ] 17/60 batches, loss: 0.6053Epoch 1/15: [=========                     ] 18/60 batches, loss: 0.6037Epoch 1/15: [=========                     ] 19/60 batches, loss: 0.5843Epoch 1/15: [==========                    ] 20/60 batches, loss: 0.5786Epoch 1/15: [==========                    ] 21/60 batches, loss: 0.5757Epoch 1/15: [===========                   ] 22/60 batches, loss: 0.5754Epoch 1/15: [===========                   ] 23/60 batches, loss: 0.5666Epoch 1/15: [============                  ] 24/60 batches, loss: 0.5518Epoch 1/15: [============                  ] 25/60 batches, loss: 0.5431Epoch 1/15: [=============                 ] 26/60 batches, loss: 0.5473Epoch 1/15: [=============                 ] 27/60 batches, loss: 0.5457Epoch 1/15: [==============                ] 28/60 batches, loss: 0.5366Epoch 1/15: [==============                ] 29/60 batches, loss: 0.5308Epoch 1/15: [===============               ] 30/60 batches, loss: 0.5290Epoch 1/15: [===============               ] 31/60 batches, loss: 0.5221Epoch 1/15: [================              ] 32/60 batches, loss: 0.5237Epoch 1/15: [================              ] 33/60 batches, loss: 0.5116Epoch 1/15: [=================             ] 34/60 batches, loss: 0.5083Epoch 1/15: [=================             ] 35/60 batches, loss: 0.5051Epoch 1/15: [==================            ] 36/60 batches, loss: 0.5040Epoch 1/15: [==================            ] 37/60 batches, loss: 0.4992Epoch 1/15: [===================           ] 38/60 batches, loss: 0.4949Epoch 1/15: [===================           ] 39/60 batches, loss: 0.4903Epoch 1/15: [====================          ] 40/60 batches, loss: 0.4867Epoch 1/15: [====================          ] 41/60 batches, loss: 0.4788Epoch 1/15: [=====================         ] 42/60 batches, loss: 0.4740Epoch 1/15: [=====================         ] 43/60 batches, loss: 0.4704Epoch 1/15: [======================        ] 44/60 batches, loss: 0.4685Epoch 1/15: [======================        ] 45/60 batches, loss: 0.4660Epoch 1/15: [=======================       ] 46/60 batches, loss: 0.4644Epoch 1/15: [=======================       ] 47/60 batches, loss: 0.4611Epoch 1/15: [========================      ] 48/60 batches, loss: 0.4563Epoch 1/15: [========================      ] 49/60 batches, loss: 0.4524Epoch 1/15: [=========================     ] 50/60 batches, loss: 0.4491Epoch 1/15: [=========================     ] 51/60 batches, loss: 0.4461Epoch 1/15: [==========================    ] 52/60 batches, loss: 0.4436Epoch 1/15: [==========================    ] 53/60 batches, loss: 0.4411Epoch 1/15: [===========================   ] 54/60 batches, loss: 0.4415Epoch 1/15: [===========================   ] 55/60 batches, loss: 0.4417Epoch 1/15: [============================  ] 56/60 batches, loss: 0.4414Epoch 1/15: [============================  ] 57/60 batches, loss: 0.4379Epoch 1/15: [============================= ] 58/60 batches, loss: 0.4366Epoch 1/15: [============================= ] 59/60 batches, loss: 0.4325Epoch 1/15: [==============================] 60/60 batches, loss: 0.4294
[2025-05-04 10:44:11,538][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.4294
[2025-05-04 10:44:11,779][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1089, Metrics: {'mse': 0.1048884466290474, 'rmse': 0.3238648585892693, 'r2': -1.5087432861328125}
Epoch 2/15: [Epoch 2/15: [                              ] 1/60 batches, loss: 0.3021Epoch 2/15: [=                             ] 2/60 batches, loss: 0.2216Epoch 2/15: [=                             ] 3/60 batches, loss: 0.2349Epoch 2/15: [==                            ] 4/60 batches, loss: 0.2252Epoch 2/15: [==                            ] 5/60 batches, loss: 0.2171Epoch 2/15: [===                           ] 6/60 batches, loss: 0.2123Epoch 2/15: [===                           ] 7/60 batches, loss: 0.2299Epoch 2/15: [====                          ] 8/60 batches, loss: 0.2292Epoch 2/15: [====                          ] 9/60 batches, loss: 0.2237Epoch 2/15: [=====                         ] 10/60 batches, loss: 0.2206Epoch 2/15: [=====                         ] 11/60 batches, loss: 0.2307Epoch 2/15: [======                        ] 12/60 batches, loss: 0.2279Epoch 2/15: [======                        ] 13/60 batches, loss: 0.2257Epoch 2/15: [=======                       ] 14/60 batches, loss: 0.2299Epoch 2/15: [=======                       ] 15/60 batches, loss: 0.2268Epoch 2/15: [========                      ] 16/60 batches, loss: 0.2229Epoch 2/15: [========                      ] 17/60 batches, loss: 0.2178Epoch 2/15: [=========                     ] 18/60 batches, loss: 0.2160Epoch 2/15: [=========                     ] 19/60 batches, loss: 0.2171Epoch 2/15: [==========                    ] 20/60 batches, loss: 0.2175Epoch 2/15: [==========                    ] 21/60 batches, loss: 0.2148Epoch 2/15: [===========                   ] 22/60 batches, loss: 0.2156Epoch 2/15: [===========                   ] 23/60 batches, loss: 0.2118Epoch 2/15: [============                  ] 24/60 batches, loss: 0.2072Epoch 2/15: [============                  ] 25/60 batches, loss: 0.2102Epoch 2/15: [=============                 ] 26/60 batches, loss: 0.2158Epoch 2/15: [=============                 ] 27/60 batches, loss: 0.2110Epoch 2/15: [==============                ] 28/60 batches, loss: 0.2096Epoch 2/15: [==============                ] 29/60 batches, loss: 0.2069Epoch 2/15: [===============               ] 30/60 batches, loss: 0.2058Epoch 2/15: [===============               ] 31/60 batches, loss: 0.2024Epoch 2/15: [================              ] 32/60 batches, loss: 0.2021Epoch 2/15: [================              ] 33/60 batches, loss: 0.2040Epoch 2/15: [=================             ] 34/60 batches, loss: 0.2070Epoch 2/15: [=================             ] 35/60 batches, loss: 0.2117Epoch 2/15: [==================            ] 36/60 batches, loss: 0.2111Epoch 2/15: [==================            ] 37/60 batches, loss: 0.2091Epoch 2/15: [===================           ] 38/60 batches, loss: 0.2071Epoch 2/15: [===================           ] 39/60 batches, loss: 0.2075Epoch 2/15: [====================          ] 40/60 batches, loss: 0.2075Epoch 2/15: [====================          ] 41/60 batches, loss: 0.2067Epoch 2/15: [=====================         ] 42/60 batches, loss: 0.2077Epoch 2/15: [=====================         ] 43/60 batches, loss: 0.2061Epoch 2/15: [======================        ] 44/60 batches, loss: 0.2057Epoch 2/15: [======================        ] 45/60 batches, loss: 0.2034Epoch 2/15: [=======================       ] 46/60 batches, loss: 0.2006Epoch 2/15: [=======================       ] 47/60 batches, loss: 0.1994Epoch 2/15: [========================      ] 48/60 batches, loss: 0.1976Epoch 2/15: [========================      ] 49/60 batches, loss: 0.1981Epoch 2/15: [=========================     ] 50/60 batches, loss: 0.1982Epoch 2/15: [=========================     ] 51/60 batches, loss: 0.1979Epoch 2/15: [==========================    ] 52/60 batches, loss: 0.1966Epoch 2/15: [==========================    ] 53/60 batches, loss: 0.1967Epoch 2/15: [===========================   ] 54/60 batches, loss: 0.1955Epoch 2/15: [===========================   ] 55/60 batches, loss: 0.1942Epoch 2/15: [============================  ] 56/60 batches, loss: 0.1937Epoch 2/15: [============================  ] 57/60 batches, loss: 0.1918Epoch 2/15: [============================= ] 58/60 batches, loss: 0.1924Epoch 2/15: [============================= ] 59/60 batches, loss: 0.1912Epoch 2/15: [==============================] 60/60 batches, loss: 0.1894
[2025-05-04 10:44:14,042][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1894
[2025-05-04 10:44:14,287][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0887, Metrics: {'mse': 0.08608651161193848, 'rmse': 0.2934050299704122, 'r2': -1.059034824371338}
Epoch 3/15: [Epoch 3/15: [                              ] 1/60 batches, loss: 0.0822Epoch 3/15: [=                             ] 2/60 batches, loss: 0.1514Epoch 3/15: [=                             ] 3/60 batches, loss: 0.1433Epoch 3/15: [==                            ] 4/60 batches, loss: 0.1424Epoch 3/15: [==                            ] 5/60 batches, loss: 0.1468Epoch 3/15: [===                           ] 6/60 batches, loss: 0.1331Epoch 3/15: [===                           ] 7/60 batches, loss: 0.1304Epoch 3/15: [====                          ] 8/60 batches, loss: 0.1333Epoch 3/15: [====                          ] 9/60 batches, loss: 0.1392Epoch 3/15: [=====                         ] 10/60 batches, loss: 0.1514Epoch 3/15: [=====                         ] 11/60 batches, loss: 0.1511Epoch 3/15: [======                        ] 12/60 batches, loss: 0.1484Epoch 3/15: [======                        ] 13/60 batches, loss: 0.1465Epoch 3/15: [=======                       ] 14/60 batches, loss: 0.1473Epoch 3/15: [=======                       ] 15/60 batches, loss: 0.1452Epoch 3/15: [========                      ] 16/60 batches, loss: 0.1441Epoch 3/15: [========                      ] 17/60 batches, loss: 0.1455Epoch 3/15: [=========                     ] 18/60 batches, loss: 0.1471Epoch 3/15: [=========                     ] 19/60 batches, loss: 0.1480Epoch 3/15: [==========                    ] 20/60 batches, loss: 0.1481Epoch 3/15: [==========                    ] 21/60 batches, loss: 0.1435Epoch 3/15: [===========                   ] 22/60 batches, loss: 0.1430Epoch 3/15: [===========                   ] 23/60 batches, loss: 0.1449Epoch 3/15: [============                  ] 24/60 batches, loss: 0.1446Epoch 3/15: [============                  ] 25/60 batches, loss: 0.1433Epoch 3/15: [=============                 ] 26/60 batches, loss: 0.1442Epoch 3/15: [=============                 ] 27/60 batches, loss: 0.1425Epoch 3/15: [==============                ] 28/60 batches, loss: 0.1400Epoch 3/15: [==============                ] 29/60 batches, loss: 0.1409Epoch 3/15: [===============               ] 30/60 batches, loss: 0.1398Epoch 3/15: [===============               ] 31/60 batches, loss: 0.1442Epoch 3/15: [================              ] 32/60 batches, loss: 0.1434Epoch 3/15: [================              ] 33/60 batches, loss: 0.1441Epoch 3/15: [=================             ] 34/60 batches, loss: 0.1430Epoch 3/15: [=================             ] 35/60 batches, loss: 0.1413Epoch 3/15: [==================            ] 36/60 batches, loss: 0.1439Epoch 3/15: [==================            ] 37/60 batches, loss: 0.1416Epoch 3/15: [===================           ] 38/60 batches, loss: 0.1407Epoch 3/15: [===================           ] 39/60 batches, loss: 0.1434Epoch 3/15: [====================          ] 40/60 batches, loss: 0.1418Epoch 3/15: [====================          ] 41/60 batches, loss: 0.1420Epoch 3/15: [=====================         ] 42/60 batches, loss: 0.1421Epoch 3/15: [=====================         ] 43/60 batches, loss: 0.1423Epoch 3/15: [======================        ] 44/60 batches, loss: 0.1427Epoch 3/15: [======================        ] 45/60 batches, loss: 0.1436Epoch 3/15: [=======================       ] 46/60 batches, loss: 0.1441Epoch 3/15: [=======================       ] 47/60 batches, loss: 0.1442Epoch 3/15: [========================      ] 48/60 batches, loss: 0.1435Epoch 3/15: [========================      ] 49/60 batches, loss: 0.1454Epoch 3/15: [=========================     ] 50/60 batches, loss: 0.1446Epoch 3/15: [=========================     ] 51/60 batches, loss: 0.1438Epoch 3/15: [==========================    ] 52/60 batches, loss: 0.1426Epoch 3/15: [==========================    ] 53/60 batches, loss: 0.1421Epoch 3/15: [===========================   ] 54/60 batches, loss: 0.1425Epoch 3/15: [===========================   ] 55/60 batches, loss: 0.1421Epoch 3/15: [============================  ] 56/60 batches, loss: 0.1411Epoch 3/15: [============================  ] 57/60 batches, loss: 0.1416Epoch 3/15: [============================= ] 58/60 batches, loss: 0.1417Epoch 3/15: [============================= ] 59/60 batches, loss: 0.1414Epoch 3/15: [==============================] 60/60 batches, loss: 0.1407
[2025-05-04 10:44:16,570][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1407
[2025-05-04 10:44:16,927][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0860, Metrics: {'mse': 0.08343258500099182, 'rmse': 0.28884699236964856, 'r2': -0.9955575466156006}
Epoch 4/15: [Epoch 4/15: [                              ] 1/60 batches, loss: 0.1593Epoch 4/15: [=                             ] 2/60 batches, loss: 0.1053Epoch 4/15: [=                             ] 3/60 batches, loss: 0.1037Epoch 4/15: [==                            ] 4/60 batches, loss: 0.0945Epoch 4/15: [==                            ] 5/60 batches, loss: 0.0924Epoch 4/15: [===                           ] 6/60 batches, loss: 0.0870Epoch 4/15: [===                           ] 7/60 batches, loss: 0.0971Epoch 4/15: [====                          ] 8/60 batches, loss: 0.1006Epoch 4/15: [====                          ] 9/60 batches, loss: 0.1042Epoch 4/15: [=====                         ] 10/60 batches, loss: 0.1068Epoch 4/15: [=====                         ] 11/60 batches, loss: 0.1129Epoch 4/15: [======                        ] 12/60 batches, loss: 0.1129Epoch 4/15: [======                        ] 13/60 batches, loss: 0.1161Epoch 4/15: [=======                       ] 14/60 batches, loss: 0.1133Epoch 4/15: [=======                       ] 15/60 batches, loss: 0.1122Epoch 4/15: [========                      ] 16/60 batches, loss: 0.1178Epoch 4/15: [========                      ] 17/60 batches, loss: 0.1155Epoch 4/15: [=========                     ] 18/60 batches, loss: 0.1144Epoch 4/15: [=========                     ] 19/60 batches, loss: 0.1142Epoch 4/15: [==========                    ] 20/60 batches, loss: 0.1127Epoch 4/15: [==========                    ] 21/60 batches, loss: 0.1123Epoch 4/15: [===========                   ] 22/60 batches, loss: 0.1146Epoch 4/15: [===========                   ] 23/60 batches, loss: 0.1159Epoch 4/15: [============                  ] 24/60 batches, loss: 0.1193Epoch 4/15: [============                  ] 25/60 batches, loss: 0.1207Epoch 4/15: [=============                 ] 26/60 batches, loss: 0.1242Epoch 4/15: [=============                 ] 27/60 batches, loss: 0.1224Epoch 4/15: [==============                ] 28/60 batches, loss: 0.1244Epoch 4/15: [==============                ] 29/60 batches, loss: 0.1239Epoch 4/15: [===============               ] 30/60 batches, loss: 0.1232Epoch 4/15: [===============               ] 31/60 batches, loss: 0.1242Epoch 4/15: [================              ] 32/60 batches, loss: 0.1217Epoch 4/15: [================              ] 33/60 batches, loss: 0.1219Epoch 4/15: [=================             ] 34/60 batches, loss: 0.1212Epoch 4/15: [=================             ] 35/60 batches, loss: 0.1214Epoch 4/15: [==================            ] 36/60 batches, loss: 0.1208Epoch 4/15: [==================            ] 37/60 batches, loss: 0.1194Epoch 4/15: [===================           ] 38/60 batches, loss: 0.1191Epoch 4/15: [===================           ] 39/60 batches, loss: 0.1174Epoch 4/15: [====================          ] 40/60 batches, loss: 0.1160Epoch 4/15: [====================          ] 41/60 batches, loss: 0.1159Epoch 4/15: [=====================         ] 42/60 batches, loss: 0.1149Epoch 4/15: [=====================         ] 43/60 batches, loss: 0.1145Epoch 4/15: [======================        ] 44/60 batches, loss: 0.1141Epoch 4/15: [======================        ] 45/60 batches, loss: 0.1145Epoch 4/15: [=======================       ] 46/60 batches, loss: 0.1132Epoch 4/15: [=======================       ] 47/60 batches, loss: 0.1135Epoch 4/15: [========================      ] 48/60 batches, loss: 0.1140Epoch 4/15: [========================      ] 49/60 batches, loss: 0.1147Epoch 4/15: [=========================     ] 50/60 batches, loss: 0.1152Epoch 4/15: [=========================     ] 51/60 batches, loss: 0.1165Epoch 4/15: [==========================    ] 52/60 batches, loss: 0.1174Epoch 4/15: [==========================    ] 53/60 batches, loss: 0.1164Epoch 4/15: [===========================   ] 54/60 batches, loss: 0.1175Epoch 4/15: [===========================   ] 55/60 batches, loss: 0.1184Epoch 4/15: [============================  ] 56/60 batches, loss: 0.1174Epoch 4/15: [============================  ] 57/60 batches, loss: 0.1167Epoch 4/15: [============================= ] 58/60 batches, loss: 0.1159Epoch 4/15: [============================= ] 59/60 batches, loss: 0.1166Epoch 4/15: [==============================] 60/60 batches, loss: 0.1162
[2025-05-04 10:44:19,133][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1162
[2025-05-04 10:44:19,394][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1027, Metrics: {'mse': 0.09992895275354385, 'rmse': 0.3161154104967739, 'r2': -1.3901212215423584}
[2025-05-04 10:44:19,395][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/60 batches, loss: 0.1254Epoch 5/15: [=                             ] 2/60 batches, loss: 0.1271Epoch 5/15: [=                             ] 3/60 batches, loss: 0.1293Epoch 5/15: [==                            ] 4/60 batches, loss: 0.1266Epoch 5/15: [==                            ] 5/60 batches, loss: 0.1241Epoch 5/15: [===                           ] 6/60 batches, loss: 0.1152Epoch 5/15: [===                           ] 7/60 batches, loss: 0.1292Epoch 5/15: [====                          ] 8/60 batches, loss: 0.1260Epoch 5/15: [====                          ] 9/60 batches, loss: 0.1221Epoch 5/15: [=====                         ] 10/60 batches, loss: 0.1214Epoch 5/15: [=====                         ] 11/60 batches, loss: 0.1214Epoch 5/15: [======                        ] 12/60 batches, loss: 0.1201Epoch 5/15: [======                        ] 13/60 batches, loss: 0.1226Epoch 5/15: [=======                       ] 14/60 batches, loss: 0.1200Epoch 5/15: [=======                       ] 15/60 batches, loss: 0.1151Epoch 5/15: [========                      ] 16/60 batches, loss: 0.1180Epoch 5/15: [========                      ] 17/60 batches, loss: 0.1175Epoch 5/15: [=========                     ] 18/60 batches, loss: 0.1179Epoch 5/15: [=========                     ] 19/60 batches, loss: 0.1182Epoch 5/15: [==========                    ] 20/60 batches, loss: 0.1145Epoch 5/15: [==========                    ] 21/60 batches, loss: 0.1167Epoch 5/15: [===========                   ] 22/60 batches, loss: 0.1160Epoch 5/15: [===========                   ] 23/60 batches, loss: 0.1170Epoch 5/15: [============                  ] 24/60 batches, loss: 0.1150Epoch 5/15: [============                  ] 25/60 batches, loss: 0.1126Epoch 5/15: [=============                 ] 26/60 batches, loss: 0.1110Epoch 5/15: [=============                 ] 27/60 batches, loss: 0.1097Epoch 5/15: [==============                ] 28/60 batches, loss: 0.1074Epoch 5/15: [==============                ] 29/60 batches, loss: 0.1078Epoch 5/15: [===============               ] 30/60 batches, loss: 0.1098Epoch 5/15: [===============               ] 31/60 batches, loss: 0.1077Epoch 5/15: [================              ] 32/60 batches, loss: 0.1068Epoch 5/15: [================              ] 33/60 batches, loss: 0.1058Epoch 5/15: [=================             ] 34/60 batches, loss: 0.1040Epoch 5/15: [=================             ] 35/60 batches, loss: 0.1021Epoch 5/15: [==================            ] 36/60 batches, loss: 0.1035Epoch 5/15: [==================            ] 37/60 batches, loss: 0.1019Epoch 5/15: [===================           ] 38/60 batches, loss: 0.1015Epoch 5/15: [===================           ] 39/60 batches, loss: 0.1008Epoch 5/15: [====================          ] 40/60 batches, loss: 0.1004Epoch 5/15: [====================          ] 41/60 batches, loss: 0.1003Epoch 5/15: [=====================         ] 42/60 batches, loss: 0.1029Epoch 5/15: [=====================         ] 43/60 batches, loss: 0.1037Epoch 5/15: [======================        ] 44/60 batches, loss: 0.1031Epoch 5/15: [======================        ] 45/60 batches, loss: 0.1019Epoch 5/15: [=======================       ] 46/60 batches, loss: 0.1010Epoch 5/15: [=======================       ] 47/60 batches, loss: 0.0997Epoch 5/15: [========================      ] 48/60 batches, loss: 0.1000Epoch 5/15: [========================      ] 49/60 batches, loss: 0.1004Epoch 5/15: [=========================     ] 50/60 batches, loss: 0.1000Epoch 5/15: [=========================     ] 51/60 batches, loss: 0.0993Epoch 5/15: [==========================    ] 52/60 batches, loss: 0.0995Epoch 5/15: [==========================    ] 53/60 batches, loss: 0.1006Epoch 5/15: [===========================   ] 54/60 batches, loss: 0.1013Epoch 5/15: [===========================   ] 55/60 batches, loss: 0.1019Epoch 5/15: [============================  ] 56/60 batches, loss: 0.1017Epoch 5/15: [============================  ] 57/60 batches, loss: 0.1019Epoch 5/15: [============================= ] 58/60 batches, loss: 0.1031Epoch 5/15: [============================= ] 59/60 batches, loss: 0.1029Epoch 5/15: [==============================] 60/60 batches, loss: 0.1023
[2025-05-04 10:44:21,270][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1023
[2025-05-04 10:44:21,544][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1015, Metrics: {'mse': 0.09873322397470474, 'rmse': 0.3142184335374116, 'r2': -1.3615214824676514}
[2025-05-04 10:44:21,545][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/60 batches, loss: 0.1564Epoch 6/15: [=                             ] 2/60 batches, loss: 0.1134Epoch 6/15: [=                             ] 3/60 batches, loss: 0.0865Epoch 6/15: [==                            ] 4/60 batches, loss: 0.0726Epoch 6/15: [==                            ] 5/60 batches, loss: 0.0727Epoch 6/15: [===                           ] 6/60 batches, loss: 0.0727Epoch 6/15: [===                           ] 7/60 batches, loss: 0.0779Epoch 6/15: [====                          ] 8/60 batches, loss: 0.0789Epoch 6/15: [====                          ] 9/60 batches, loss: 0.0774Epoch 6/15: [=====                         ] 10/60 batches, loss: 0.0780Epoch 6/15: [=====                         ] 11/60 batches, loss: 0.0768Epoch 6/15: [======                        ] 12/60 batches, loss: 0.0782Epoch 6/15: [======                        ] 13/60 batches, loss: 0.0746Epoch 6/15: [=======                       ] 14/60 batches, loss: 0.0750Epoch 6/15: [=======                       ] 15/60 batches, loss: 0.0740Epoch 6/15: [========                      ] 16/60 batches, loss: 0.0764Epoch 6/15: [========                      ] 17/60 batches, loss: 0.0769Epoch 6/15: [=========                     ] 18/60 batches, loss: 0.0790Epoch 6/15: [=========                     ] 19/60 batches, loss: 0.0800Epoch 6/15: [==========                    ] 20/60 batches, loss: 0.0802Epoch 6/15: [==========                    ] 21/60 batches, loss: 0.0821Epoch 6/15: [===========                   ] 22/60 batches, loss: 0.0828Epoch 6/15: [===========                   ] 23/60 batches, loss: 0.0831Epoch 6/15: [============                  ] 24/60 batches, loss: 0.0819Epoch 6/15: [============                  ] 25/60 batches, loss: 0.0822Epoch 6/15: [=============                 ] 26/60 batches, loss: 0.0824Epoch 6/15: [=============                 ] 27/60 batches, loss: 0.0816Epoch 6/15: [==============                ] 28/60 batches, loss: 0.0828Epoch 6/15: [==============                ] 29/60 batches, loss: 0.0843Epoch 6/15: [===============               ] 30/60 batches, loss: 0.0828Epoch 6/15: [===============               ] 31/60 batches, loss: 0.0827Epoch 6/15: [================              ] 32/60 batches, loss: 0.0830Epoch 6/15: [================              ] 33/60 batches, loss: 0.0861Epoch 6/15: [=================             ] 34/60 batches, loss: 0.0858Epoch 6/15: [=================             ] 35/60 batches, loss: 0.0862Epoch 6/15: [==================            ] 36/60 batches, loss: 0.0857Epoch 6/15: [==================            ] 37/60 batches, loss: 0.0860Epoch 6/15: [===================           ] 38/60 batches, loss: 0.0869Epoch 6/15: [===================           ] 39/60 batches, loss: 0.0877Epoch 6/15: [====================          ] 40/60 batches, loss: 0.0872Epoch 6/15: [====================          ] 41/60 batches, loss: 0.0881Epoch 6/15: [=====================         ] 42/60 batches, loss: 0.0872Epoch 6/15: [=====================         ] 43/60 batches, loss: 0.0867Epoch 6/15: [======================        ] 44/60 batches, loss: 0.0855Epoch 6/15: [======================        ] 45/60 batches, loss: 0.0859Epoch 6/15: [=======================       ] 46/60 batches, loss: 0.0848Epoch 6/15: [=======================       ] 47/60 batches, loss: 0.0850Epoch 6/15: [========================      ] 48/60 batches, loss: 0.0856Epoch 6/15: [========================      ] 49/60 batches, loss: 0.0857Epoch 6/15: [=========================     ] 50/60 batches, loss: 0.0848Epoch 6/15: [=========================     ] 51/60 batches, loss: 0.0858Epoch 6/15: [==========================    ] 52/60 batches, loss: 0.0858Epoch 6/15: [==========================    ] 53/60 batches, loss: 0.0861Epoch 6/15: [===========================   ] 54/60 batches, loss: 0.0853Epoch 6/15: [===========================   ] 55/60 batches, loss: 0.0856Epoch 6/15: [============================  ] 56/60 batches, loss: 0.0846Epoch 6/15: [============================  ] 57/60 batches, loss: 0.0847Epoch 6/15: [============================= ] 58/60 batches, loss: 0.0847Epoch 6/15: [============================= ] 59/60 batches, loss: 0.0856Epoch 6/15: [==============================] 60/60 batches, loss: 0.0864
[2025-05-04 10:44:23,394][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0864
[2025-05-04 10:44:23,659][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1077, Metrics: {'mse': 0.10473015904426575, 'rmse': 0.3236203934307382, 'r2': -1.5049571990966797}
[2025-05-04 10:44:23,660][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/60 batches, loss: 0.0290Epoch 7/15: [=                             ] 2/60 batches, loss: 0.0595Epoch 7/15: [=                             ] 3/60 batches, loss: 0.0724Epoch 7/15: [==                            ] 4/60 batches, loss: 0.0751Epoch 7/15: [==                            ] 5/60 batches, loss: 0.0881Epoch 7/15: [===                           ] 6/60 batches, loss: 0.0851Epoch 7/15: [===                           ] 7/60 batches, loss: 0.0821Epoch 7/15: [====                          ] 8/60 batches, loss: 0.0842Epoch 7/15: [====                          ] 9/60 batches, loss: 0.0837Epoch 7/15: [=====                         ] 10/60 batches, loss: 0.0814Epoch 7/15: [=====                         ] 11/60 batches, loss: 0.0795Epoch 7/15: [======                        ] 12/60 batches, loss: 0.0805Epoch 7/15: [======                        ] 13/60 batches, loss: 0.0821Epoch 7/15: [=======                       ] 14/60 batches, loss: 0.0809Epoch 7/15: [=======                       ] 15/60 batches, loss: 0.0818Epoch 7/15: [========                      ] 16/60 batches, loss: 0.0831Epoch 7/15: [========                      ] 17/60 batches, loss: 0.0834Epoch 7/15: [=========                     ] 18/60 batches, loss: 0.0831Epoch 7/15: [=========                     ] 19/60 batches, loss: 0.0830Epoch 7/15: [==========                    ] 20/60 batches, loss: 0.0831Epoch 7/15: [==========                    ] 21/60 batches, loss: 0.0826Epoch 7/15: [===========                   ] 22/60 batches, loss: 0.0814Epoch 7/15: [===========                   ] 23/60 batches, loss: 0.0815Epoch 7/15: [============                  ] 24/60 batches, loss: 0.0815Epoch 7/15: [============                  ] 25/60 batches, loss: 0.0807Epoch 7/15: [=============                 ] 26/60 batches, loss: 0.0808Epoch 7/15: [=============                 ] 27/60 batches, loss: 0.0807Epoch 7/15: [==============                ] 28/60 batches, loss: 0.0802Epoch 7/15: [==============                ] 29/60 batches, loss: 0.0805Epoch 7/15: [===============               ] 30/60 batches, loss: 0.0815Epoch 7/15: [===============               ] 31/60 batches, loss: 0.0813Epoch 7/15: [================              ] 32/60 batches, loss: 0.0806Epoch 7/15: [================              ] 33/60 batches, loss: 0.0816Epoch 7/15: [=================             ] 34/60 batches, loss: 0.0804Epoch 7/15: [=================             ] 35/60 batches, loss: 0.0799Epoch 7/15: [==================            ] 36/60 batches, loss: 0.0806Epoch 7/15: [==================            ] 37/60 batches, loss: 0.0794Epoch 7/15: [===================           ] 38/60 batches, loss: 0.0814Epoch 7/15: [===================           ] 39/60 batches, loss: 0.0811Epoch 7/15: [====================          ] 40/60 batches, loss: 0.0802Epoch 7/15: [====================          ] 41/60 batches, loss: 0.0814Epoch 7/15: [=====================         ] 42/60 batches, loss: 0.0813Epoch 7/15: [=====================         ] 43/60 batches, loss: 0.0802Epoch 7/15: [======================        ] 44/60 batches, loss: 0.0805Epoch 7/15: [======================        ] 45/60 batches, loss: 0.0797Epoch 7/15: [=======================       ] 46/60 batches, loss: 0.0797Epoch 7/15: [=======================       ] 47/60 batches, loss: 0.0799Epoch 7/15: [========================      ] 48/60 batches, loss: 0.0801Epoch 7/15: [========================      ] 49/60 batches, loss: 0.0810Epoch 7/15: [=========================     ] 50/60 batches, loss: 0.0812Epoch 7/15: [=========================     ] 51/60 batches, loss: 0.0813Epoch 7/15: [==========================    ] 52/60 batches, loss: 0.0811Epoch 7/15: [==========================    ] 53/60 batches, loss: 0.0802Epoch 7/15: [===========================   ] 54/60 batches, loss: 0.0798Epoch 7/15: [===========================   ] 55/60 batches, loss: 0.0796Epoch 7/15: [============================  ] 56/60 batches, loss: 0.0795Epoch 7/15: [============================  ] 57/60 batches, loss: 0.0806Epoch 7/15: [============================= ] 58/60 batches, loss: 0.0804Epoch 7/15: [============================= ] 59/60 batches, loss: 0.0803Epoch 7/15: [==============================] 60/60 batches, loss: 0.0811
[2025-05-04 10:44:25,533][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0811
[2025-05-04 10:44:25,789][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0859, Metrics: {'mse': 0.0835282951593399, 'rmse': 0.2890126211073487, 'r2': -0.9978468418121338}
Epoch 8/15: [Epoch 8/15: [                              ] 1/60 batches, loss: 0.0592Epoch 8/15: [=                             ] 2/60 batches, loss: 0.0577Epoch 8/15: [=                             ] 3/60 batches, loss: 0.0751Epoch 8/15: [==                            ] 4/60 batches, loss: 0.0662Epoch 8/15: [==                            ] 5/60 batches, loss: 0.0665Epoch 8/15: [===                           ] 6/60 batches, loss: 0.0671Epoch 8/15: [===                           ] 7/60 batches, loss: 0.0693Epoch 8/15: [====                          ] 8/60 batches, loss: 0.0680Epoch 8/15: [====                          ] 9/60 batches, loss: 0.0730Epoch 8/15: [=====                         ] 10/60 batches, loss: 0.0718Epoch 8/15: [=====                         ] 11/60 batches, loss: 0.0745Epoch 8/15: [======                        ] 12/60 batches, loss: 0.0729Epoch 8/15: [======                        ] 13/60 batches, loss: 0.0722Epoch 8/15: [=======                       ] 14/60 batches, loss: 0.0780Epoch 8/15: [=======                       ] 15/60 batches, loss: 0.0784Epoch 8/15: [========                      ] 16/60 batches, loss: 0.0788Epoch 8/15: [========                      ] 17/60 batches, loss: 0.0795Epoch 8/15: [=========                     ] 18/60 batches, loss: 0.0775Epoch 8/15: [=========                     ] 19/60 batches, loss: 0.0782Epoch 8/15: [==========                    ] 20/60 batches, loss: 0.0771Epoch 8/15: [==========                    ] 21/60 batches, loss: 0.0762Epoch 8/15: [===========                   ] 22/60 batches, loss: 0.0788Epoch 8/15: [===========                   ] 23/60 batches, loss: 0.0820Epoch 8/15: [============                  ] 24/60 batches, loss: 0.0819Epoch 8/15: [============                  ] 25/60 batches, loss: 0.0824Epoch 8/15: [=============                 ] 26/60 batches, loss: 0.0821Epoch 8/15: [=============                 ] 27/60 batches, loss: 0.0821Epoch 8/15: [==============                ] 28/60 batches, loss: 0.0823Epoch 8/15: [==============                ] 29/60 batches, loss: 0.0820Epoch 8/15: [===============               ] 30/60 batches, loss: 0.0813Epoch 8/15: [===============               ] 31/60 batches, loss: 0.0831Epoch 8/15: [================              ] 32/60 batches, loss: 0.0817Epoch 8/15: [================              ] 33/60 batches, loss: 0.0830Epoch 8/15: [=================             ] 34/60 batches, loss: 0.0818Epoch 8/15: [=================             ] 35/60 batches, loss: 0.0813Epoch 8/15: [==================            ] 36/60 batches, loss: 0.0821Epoch 8/15: [==================            ] 37/60 batches, loss: 0.0822Epoch 8/15: [===================           ] 38/60 batches, loss: 0.0853Epoch 8/15: [===================           ] 39/60 batches, loss: 0.0848Epoch 8/15: [====================          ] 40/60 batches, loss: 0.0842Epoch 8/15: [====================          ] 41/60 batches, loss: 0.0838Epoch 8/15: [=====================         ] 42/60 batches, loss: 0.0836Epoch 8/15: [=====================         ] 43/60 batches, loss: 0.0831Epoch 8/15: [======================        ] 44/60 batches, loss: 0.0830Epoch 8/15: [======================        ] 45/60 batches, loss: 0.0821Epoch 8/15: [=======================       ] 46/60 batches, loss: 0.0810Epoch 8/15: [=======================       ] 47/60 batches, loss: 0.0805Epoch 8/15: [========================      ] 48/60 batches, loss: 0.0804Epoch 8/15: [========================      ] 49/60 batches, loss: 0.0797Epoch 8/15: [=========================     ] 50/60 batches, loss: 0.0798Epoch 8/15: [=========================     ] 51/60 batches, loss: 0.0790Epoch 8/15: [==========================    ] 52/60 batches, loss: 0.0792Epoch 8/15: [==========================    ] 53/60 batches, loss: 0.0793Epoch 8/15: [===========================   ] 54/60 batches, loss: 0.0796Epoch 8/15: [===========================   ] 55/60 batches, loss: 0.0796Epoch 8/15: [============================  ] 56/60 batches, loss: 0.0790Epoch 8/15: [============================  ] 57/60 batches, loss: 0.0783Epoch 8/15: [============================= ] 58/60 batches, loss: 0.0779Epoch 8/15: [============================= ] 59/60 batches, loss: 0.0771Epoch 8/15: [==============================] 60/60 batches, loss: 0.0772
[2025-05-04 10:44:28,073][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0772
[2025-05-04 10:44:28,351][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0957, Metrics: {'mse': 0.09296467900276184, 'rmse': 0.3049010970835655, 'r2': -1.223548173904419}
[2025-05-04 10:44:28,352][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/60 batches, loss: 0.1692Epoch 9/15: [=                             ] 2/60 batches, loss: 0.1265Epoch 9/15: [=                             ] 3/60 batches, loss: 0.1142Epoch 9/15: [==                            ] 4/60 batches, loss: 0.1065Epoch 9/15: [==                            ] 5/60 batches, loss: 0.0972Epoch 9/15: [===                           ] 6/60 batches, loss: 0.0883Epoch 9/15: [===                           ] 7/60 batches, loss: 0.0889Epoch 9/15: [====                          ] 8/60 batches, loss: 0.0870Epoch 9/15: [====                          ] 9/60 batches, loss: 0.0807Epoch 9/15: [=====                         ] 10/60 batches, loss: 0.0751Epoch 9/15: [=====                         ] 11/60 batches, loss: 0.0746Epoch 9/15: [======                        ] 12/60 batches, loss: 0.0755Epoch 9/15: [======                        ] 13/60 batches, loss: 0.0732Epoch 9/15: [=======                       ] 14/60 batches, loss: 0.0716Epoch 9/15: [=======                       ] 15/60 batches, loss: 0.0746Epoch 9/15: [========                      ] 16/60 batches, loss: 0.0726Epoch 9/15: [========                      ] 17/60 batches, loss: 0.0726Epoch 9/15: [=========                     ] 18/60 batches, loss: 0.0757Epoch 9/15: [=========                     ] 19/60 batches, loss: 0.0769Epoch 9/15: [==========                    ] 20/60 batches, loss: 0.0763Epoch 9/15: [==========                    ] 21/60 batches, loss: 0.0763Epoch 9/15: [===========                   ] 22/60 batches, loss: 0.0774Epoch 9/15: [===========                   ] 23/60 batches, loss: 0.0757Epoch 9/15: [============                  ] 24/60 batches, loss: 0.0751Epoch 9/15: [============                  ] 25/60 batches, loss: 0.0739Epoch 9/15: [=============                 ] 26/60 batches, loss: 0.0738Epoch 9/15: [=============                 ] 27/60 batches, loss: 0.0735Epoch 9/15: [==============                ] 28/60 batches, loss: 0.0730Epoch 9/15: [==============                ] 29/60 batches, loss: 0.0737Epoch 9/15: [===============               ] 30/60 batches, loss: 0.0744Epoch 9/15: [===============               ] 31/60 batches, loss: 0.0737Epoch 9/15: [================              ] 32/60 batches, loss: 0.0741Epoch 9/15: [================              ] 33/60 batches, loss: 0.0740Epoch 9/15: [=================             ] 34/60 batches, loss: 0.0739Epoch 9/15: [=================             ] 35/60 batches, loss: 0.0740Epoch 9/15: [==================            ] 36/60 batches, loss: 0.0741Epoch 9/15: [==================            ] 37/60 batches, loss: 0.0745Epoch 9/15: [===================           ] 38/60 batches, loss: 0.0757Epoch 9/15: [===================           ] 39/60 batches, loss: 0.0761Epoch 9/15: [====================          ] 40/60 batches, loss: 0.0759Epoch 9/15: [====================          ] 41/60 batches, loss: 0.0754Epoch 9/15: [=====================         ] 42/60 batches, loss: 0.0749Epoch 9/15: [=====================         ] 43/60 batches, loss: 0.0741Epoch 9/15: [======================        ] 44/60 batches, loss: 0.0736Epoch 9/15: [======================        ] 45/60 batches, loss: 0.0730Epoch 9/15: [=======================       ] 46/60 batches, loss: 0.0722Epoch 9/15: [=======================       ] 47/60 batches, loss: 0.0720Epoch 9/15: [========================      ] 48/60 batches, loss: 0.0717Epoch 9/15: [========================      ] 49/60 batches, loss: 0.0713Epoch 9/15: [=========================     ] 50/60 batches, loss: 0.0716Epoch 9/15: [=========================     ] 51/60 batches, loss: 0.0714Epoch 9/15: [==========================    ] 52/60 batches, loss: 0.0713Epoch 9/15: [==========================    ] 53/60 batches, loss: 0.0711Epoch 9/15: [===========================   ] 54/60 batches, loss: 0.0704Epoch 9/15: [===========================   ] 55/60 batches, loss: 0.0700Epoch 9/15: [============================  ] 56/60 batches, loss: 0.0705Epoch 9/15: [============================  ] 57/60 batches, loss: 0.0698Epoch 9/15: [============================= ] 58/60 batches, loss: 0.0697Epoch 9/15: [============================= ] 59/60 batches, loss: 0.0694Epoch 9/15: [==============================] 60/60 batches, loss: 0.0690
[2025-05-04 10:44:30,229][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0690
[2025-05-04 10:44:30,507][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0880, Metrics: {'mse': 0.08539039641618729, 'rmse': 0.2922163520684414, 'r2': -1.0423848628997803}
[2025-05-04 10:44:30,507][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 10/15: [Epoch 10/15: [                              ] 1/60 batches, loss: 0.0882Epoch 10/15: [=                             ] 2/60 batches, loss: 0.0900Epoch 10/15: [=                             ] 3/60 batches, loss: 0.0821Epoch 10/15: [==                            ] 4/60 batches, loss: 0.0802Epoch 10/15: [==                            ] 5/60 batches, loss: 0.0742Epoch 10/15: [===                           ] 6/60 batches, loss: 0.0720Epoch 10/15: [===                           ] 7/60 batches, loss: 0.0710Epoch 10/15: [====                          ] 8/60 batches, loss: 0.0705Epoch 10/15: [====                          ] 9/60 batches, loss: 0.0690Epoch 10/15: [=====                         ] 10/60 batches, loss: 0.0675Epoch 10/15: [=====                         ] 11/60 batches, loss: 0.0685Epoch 10/15: [======                        ] 12/60 batches, loss: 0.0690Epoch 10/15: [======                        ] 13/60 batches, loss: 0.0696Epoch 10/15: [=======                       ] 14/60 batches, loss: 0.0699Epoch 10/15: [=======                       ] 15/60 batches, loss: 0.0719Epoch 10/15: [========                      ] 16/60 batches, loss: 0.0704Epoch 10/15: [========                      ] 17/60 batches, loss: 0.0694Epoch 10/15: [=========                     ] 18/60 batches, loss: 0.0675Epoch 10/15: [=========                     ] 19/60 batches, loss: 0.0673Epoch 10/15: [==========                    ] 20/60 batches, loss: 0.0677Epoch 10/15: [==========                    ] 21/60 batches, loss: 0.0685Epoch 10/15: [===========                   ] 22/60 batches, loss: 0.0709Epoch 10/15: [===========                   ] 23/60 batches, loss: 0.0705Epoch 10/15: [============                  ] 24/60 batches, loss: 0.0709Epoch 10/15: [============                  ] 25/60 batches, loss: 0.0705Epoch 10/15: [=============                 ] 26/60 batches, loss: 0.0695Epoch 10/15: [=============                 ] 27/60 batches, loss: 0.0696Epoch 10/15: [==============                ] 28/60 batches, loss: 0.0695Epoch 10/15: [==============                ] 29/60 batches, loss: 0.0704Epoch 10/15: [===============               ] 30/60 batches, loss: 0.0708Epoch 10/15: [===============               ] 31/60 batches, loss: 0.0713Epoch 10/15: [================              ] 32/60 batches, loss: 0.0706Epoch 10/15: [================              ] 33/60 batches, loss: 0.0698Epoch 10/15: [=================             ] 34/60 batches, loss: 0.0703Epoch 10/15: [=================             ] 35/60 batches, loss: 0.0700Epoch 10/15: [==================            ] 36/60 batches, loss: 0.0694Epoch 10/15: [==================            ] 37/60 batches, loss: 0.0688Epoch 10/15: [===================           ] 38/60 batches, loss: 0.0684Epoch 10/15: [===================           ] 39/60 batches, loss: 0.0693Epoch 10/15: [====================          ] 40/60 batches, loss: 0.0693Epoch 10/15: [====================          ] 41/60 batches, loss: 0.0690Epoch 10/15: [=====================         ] 42/60 batches, loss: 0.0684Epoch 10/15: [=====================         ] 43/60 batches, loss: 0.0684Epoch 10/15: [======================        ] 44/60 batches, loss: 0.0695Epoch 10/15: [======================        ] 45/60 batches, loss: 0.0691Epoch 10/15: [=======================       ] 46/60 batches, loss: 0.0689Epoch 10/15: [=======================       ] 47/60 batches, loss: 0.0685Epoch 10/15: [========================      ] 48/60 batches, loss: 0.0681Epoch 10/15: [========================      ] 49/60 batches, loss: 0.0674Epoch 10/15: [=========================     ] 50/60 batches, loss: 0.0680Epoch 10/15: [=========================     ] 51/60 batches, loss: 0.0689Epoch 10/15: [==========================    ] 52/60 batches, loss: 0.0688Epoch 10/15: [==========================    ] 53/60 batches, loss: 0.0682Epoch 10/15: [===========================   ] 54/60 batches, loss: 0.0681Epoch 10/15: [===========================   ] 55/60 batches, loss: 0.0677Epoch 10/15: [============================  ] 56/60 batches, loss: 0.0673Epoch 10/15: [============================  ] 57/60 batches, loss: 0.0671Epoch 10/15: [============================= ] 58/60 batches, loss: 0.0665Epoch 10/15: [============================= ] 59/60 batches, loss: 0.0668Epoch 10/15: [==============================] 60/60 batches, loss: 0.0666
[2025-05-04 10:44:32,386][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.0666
[2025-05-04 10:44:32,662][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0844, Metrics: {'mse': 0.08167149126529694, 'rmse': 0.28578224448922107, 'r2': -0.9534354209899902}
Epoch 11/15: [Epoch 11/15: [                              ] 1/60 batches, loss: 0.0268Epoch 11/15: [=                             ] 2/60 batches, loss: 0.0507Epoch 11/15: [=                             ] 3/60 batches, loss: 0.0602Epoch 11/15: [==                            ] 4/60 batches, loss: 0.0665Epoch 11/15: [==                            ] 5/60 batches, loss: 0.0720Epoch 11/15: [===                           ] 6/60 batches, loss: 0.0775Epoch 11/15: [===                           ] 7/60 batches, loss: 0.0754Epoch 11/15: [====                          ] 8/60 batches, loss: 0.0793Epoch 11/15: [====                          ] 9/60 batches, loss: 0.0803Epoch 11/15: [=====                         ] 10/60 batches, loss: 0.0772Epoch 11/15: [=====                         ] 11/60 batches, loss: 0.0744Epoch 11/15: [======                        ] 12/60 batches, loss: 0.0753Epoch 11/15: [======                        ] 13/60 batches, loss: 0.0719Epoch 11/15: [=======                       ] 14/60 batches, loss: 0.0717Epoch 11/15: [=======                       ] 15/60 batches, loss: 0.0702Epoch 11/15: [========                      ] 16/60 batches, loss: 0.0682Epoch 11/15: [========                      ] 17/60 batches, loss: 0.0665Epoch 11/15: [=========                     ] 18/60 batches, loss: 0.0667Epoch 11/15: [=========                     ] 19/60 batches, loss: 0.0659Epoch 11/15: [==========                    ] 20/60 batches, loss: 0.0657Epoch 11/15: [==========                    ] 21/60 batches, loss: 0.0655Epoch 11/15: [===========                   ] 22/60 batches, loss: 0.0656Epoch 11/15: [===========                   ] 23/60 batches, loss: 0.0653Epoch 11/15: [============                  ] 24/60 batches, loss: 0.0647Epoch 11/15: [============                  ] 25/60 batches, loss: 0.0654Epoch 11/15: [=============                 ] 26/60 batches, loss: 0.0647Epoch 11/15: [=============                 ] 27/60 batches, loss: 0.0650Epoch 11/15: [==============                ] 28/60 batches, loss: 0.0639Epoch 11/15: [==============                ] 29/60 batches, loss: 0.0629Epoch 11/15: [===============               ] 30/60 batches, loss: 0.0633Epoch 11/15: [===============               ] 31/60 batches, loss: 0.0636Epoch 11/15: [================              ] 32/60 batches, loss: 0.0633Epoch 11/15: [================              ] 33/60 batches, loss: 0.0631Epoch 11/15: [=================             ] 34/60 batches, loss: 0.0630Epoch 11/15: [=================             ] 35/60 batches, loss: 0.0620Epoch 11/15: [==================            ] 36/60 batches, loss: 0.0615Epoch 11/15: [==================            ] 37/60 batches, loss: 0.0623Epoch 11/15: [===================           ] 38/60 batches, loss: 0.0621Epoch 11/15: [===================           ] 39/60 batches, loss: 0.0624Epoch 11/15: [====================          ] 40/60 batches, loss: 0.0631Epoch 11/15: [====================          ] 41/60 batches, loss: 0.0624Epoch 11/15: [=====================         ] 42/60 batches, loss: 0.0622Epoch 11/15: [=====================         ] 43/60 batches, loss: 0.0624Epoch 11/15: [======================        ] 44/60 batches, loss: 0.0624Epoch 11/15: [======================        ] 45/60 batches, loss: 0.0624Epoch 11/15: [=======================       ] 46/60 batches, loss: 0.0624Epoch 11/15: [=======================       ] 47/60 batches, loss: 0.0625Epoch 11/15: [========================      ] 48/60 batches, loss: 0.0621Epoch 11/15: [========================      ] 49/60 batches, loss: 0.0620Epoch 11/15: [=========================     ] 50/60 batches, loss: 0.0627Epoch 11/15: [=========================     ] 51/60 batches, loss: 0.0626Epoch 11/15: [==========================    ] 52/60 batches, loss: 0.0629Epoch 11/15: [==========================    ] 53/60 batches, loss: 0.0625Epoch 11/15: [===========================   ] 54/60 batches, loss: 0.0633Epoch 11/15: [===========================   ] 55/60 batches, loss: 0.0639Epoch 11/15: [============================  ] 56/60 batches, loss: 0.0637Epoch 11/15: [============================  ] 57/60 batches, loss: 0.0643Epoch 11/15: [============================= ] 58/60 batches, loss: 0.0640Epoch 11/15: [============================= ] 59/60 batches, loss: 0.0635Epoch 11/15: [==============================] 60/60 batches, loss: 0.0635
[2025-05-04 10:44:34,927][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.0635
[2025-05-04 10:44:35,193][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0871, Metrics: {'mse': 0.0843215063214302, 'rmse': 0.2903816563101571, 'r2': -1.0168190002441406}
[2025-05-04 10:44:35,194][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/60 batches, loss: 0.0959Epoch 12/15: [=                             ] 2/60 batches, loss: 0.0704Epoch 12/15: [=                             ] 3/60 batches, loss: 0.0657Epoch 12/15: [==                            ] 4/60 batches, loss: 0.0614Epoch 12/15: [==                            ] 5/60 batches, loss: 0.0626Epoch 12/15: [===                           ] 6/60 batches, loss: 0.0602Epoch 12/15: [===                           ] 7/60 batches, loss: 0.0595Epoch 12/15: [====                          ] 8/60 batches, loss: 0.0560Epoch 12/15: [====                          ] 9/60 batches, loss: 0.0559Epoch 12/15: [=====                         ] 10/60 batches, loss: 0.0558Epoch 12/15: [=====                         ] 11/60 batches, loss: 0.0562Epoch 12/15: [======                        ] 12/60 batches, loss: 0.0571Epoch 12/15: [======                        ] 13/60 batches, loss: 0.0548Epoch 12/15: [=======                       ] 14/60 batches, loss: 0.0549Epoch 12/15: [=======                       ] 15/60 batches, loss: 0.0552Epoch 12/15: [========                      ] 16/60 batches, loss: 0.0576Epoch 12/15: [========                      ] 17/60 batches, loss: 0.0567Epoch 12/15: [=========                     ] 18/60 batches, loss: 0.0562Epoch 12/15: [=========                     ] 19/60 batches, loss: 0.0541Epoch 12/15: [==========                    ] 20/60 batches, loss: 0.0533Epoch 12/15: [==========                    ] 21/60 batches, loss: 0.0534Epoch 12/15: [===========                   ] 22/60 batches, loss: 0.0542Epoch 12/15: [===========                   ] 23/60 batches, loss: 0.0566Epoch 12/15: [============                  ] 24/60 batches, loss: 0.0588Epoch 12/15: [============                  ] 25/60 batches, loss: 0.0582Epoch 12/15: [=============                 ] 26/60 batches, loss: 0.0581Epoch 12/15: [=============                 ] 27/60 batches, loss: 0.0586Epoch 12/15: [==============                ] 28/60 batches, loss: 0.0584Epoch 12/15: [==============                ] 29/60 batches, loss: 0.0596Epoch 12/15: [===============               ] 30/60 batches, loss: 0.0599Epoch 12/15: [===============               ] 31/60 batches, loss: 0.0600Epoch 12/15: [================              ] 32/60 batches, loss: 0.0599Epoch 12/15: [================              ] 33/60 batches, loss: 0.0602Epoch 12/15: [=================             ] 34/60 batches, loss: 0.0595Epoch 12/15: [=================             ] 35/60 batches, loss: 0.0592Epoch 12/15: [==================            ] 36/60 batches, loss: 0.0583Epoch 12/15: [==================            ] 37/60 batches, loss: 0.0579Epoch 12/15: [===================           ] 38/60 batches, loss: 0.0586Epoch 12/15: [===================           ] 39/60 batches, loss: 0.0584Epoch 12/15: [====================          ] 40/60 batches, loss: 0.0586Epoch 12/15: [====================          ] 41/60 batches, loss: 0.0594Epoch 12/15: [=====================         ] 42/60 batches, loss: 0.0595Epoch 12/15: [=====================         ] 43/60 batches, loss: 0.0599Epoch 12/15: [======================        ] 44/60 batches, loss: 0.0595Epoch 12/15: [======================        ] 45/60 batches, loss: 0.0598Epoch 12/15: [=======================       ] 46/60 batches, loss: 0.0603Epoch 12/15: [=======================       ] 47/60 batches, loss: 0.0597Epoch 12/15: [========================      ] 48/60 batches, loss: 0.0597Epoch 12/15: [========================      ] 49/60 batches, loss: 0.0591Epoch 12/15: [=========================     ] 50/60 batches, loss: 0.0594Epoch 12/15: [=========================     ] 51/60 batches, loss: 0.0595Epoch 12/15: [==========================    ] 52/60 batches, loss: 0.0596Epoch 12/15: [==========================    ] 53/60 batches, loss: 0.0596Epoch 12/15: [===========================   ] 54/60 batches, loss: 0.0599Epoch 12/15: [===========================   ] 55/60 batches, loss: 0.0595Epoch 12/15: [============================  ] 56/60 batches, loss: 0.0589Epoch 12/15: [============================  ] 57/60 batches, loss: 0.0585Epoch 12/15: [============================= ] 58/60 batches, loss: 0.0588Epoch 12/15: [============================= ] 59/60 batches, loss: 0.0588Epoch 12/15: [==============================] 60/60 batches, loss: 0.0586
[2025-05-04 10:44:37,065][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.0586
[2025-05-04 10:44:37,333][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0784, Metrics: {'mse': 0.07593013346195221, 'rmse': 0.27555422962087195, 'r2': -0.8161123991012573}
Epoch 13/15: [Epoch 13/15: [                              ] 1/60 batches, loss: 0.0625Epoch 13/15: [=                             ] 2/60 batches, loss: 0.0564Epoch 13/15: [=                             ] 3/60 batches, loss: 0.0687Epoch 13/15: [==                            ] 4/60 batches, loss: 0.0612Epoch 13/15: [==                            ] 5/60 batches, loss: 0.0605Epoch 13/15: [===                           ] 6/60 batches, loss: 0.0538Epoch 13/15: [===                           ] 7/60 batches, loss: 0.0573Epoch 13/15: [====                          ] 8/60 batches, loss: 0.0605Epoch 13/15: [====                          ] 9/60 batches, loss: 0.0623Epoch 13/15: [=====                         ] 10/60 batches, loss: 0.0616Epoch 13/15: [=====                         ] 11/60 batches, loss: 0.0611Epoch 13/15: [======                        ] 12/60 batches, loss: 0.0610Epoch 13/15: [======                        ] 13/60 batches, loss: 0.0615Epoch 13/15: [=======                       ] 14/60 batches, loss: 0.0629Epoch 13/15: [=======                       ] 15/60 batches, loss: 0.0608Epoch 13/15: [========                      ] 16/60 batches, loss: 0.0614Epoch 13/15: [========                      ] 17/60 batches, loss: 0.0624Epoch 13/15: [=========                     ] 18/60 batches, loss: 0.0612Epoch 13/15: [=========                     ] 19/60 batches, loss: 0.0626Epoch 13/15: [==========                    ] 20/60 batches, loss: 0.0618Epoch 13/15: [==========                    ] 21/60 batches, loss: 0.0609Epoch 13/15: [===========                   ] 22/60 batches, loss: 0.0609Epoch 13/15: [===========                   ] 23/60 batches, loss: 0.0608Epoch 13/15: [============                  ] 24/60 batches, loss: 0.0610Epoch 13/15: [============                  ] 25/60 batches, loss: 0.0610Epoch 13/15: [=============                 ] 26/60 batches, loss: 0.0600Epoch 13/15: [=============                 ] 27/60 batches, loss: 0.0597Epoch 13/15: [==============                ] 28/60 batches, loss: 0.0591Epoch 13/15: [==============                ] 29/60 batches, loss: 0.0591Epoch 13/15: [===============               ] 30/60 batches, loss: 0.0593Epoch 13/15: [===============               ] 31/60 batches, loss: 0.0590Epoch 13/15: [================              ] 32/60 batches, loss: 0.0593Epoch 13/15: [================              ] 33/60 batches, loss: 0.0594Epoch 13/15: [=================             ] 34/60 batches, loss: 0.0590Epoch 13/15: [=================             ] 35/60 batches, loss: 0.0597Epoch 13/15: [==================            ] 36/60 batches, loss: 0.0606Epoch 13/15: [==================            ] 37/60 batches, loss: 0.0606Epoch 13/15: [===================           ] 38/60 batches, loss: 0.0599Epoch 13/15: [===================           ] 39/60 batches, loss: 0.0599Epoch 13/15: [====================          ] 40/60 batches, loss: 0.0599Epoch 13/15: [====================          ] 41/60 batches, loss: 0.0592Epoch 13/15: [=====================         ] 42/60 batches, loss: 0.0589Epoch 13/15: [=====================         ] 43/60 batches, loss: 0.0593Epoch 13/15: [======================        ] 44/60 batches, loss: 0.0593Epoch 13/15: [======================        ] 45/60 batches, loss: 0.0590Epoch 13/15: [=======================       ] 46/60 batches, loss: 0.0592Epoch 13/15: [=======================       ] 47/60 batches, loss: 0.0602Epoch 13/15: [========================      ] 48/60 batches, loss: 0.0596Epoch 13/15: [========================      ] 49/60 batches, loss: 0.0594Epoch 13/15: [=========================     ] 50/60 batches, loss: 0.0588Epoch 13/15: [=========================     ] 51/60 batches, loss: 0.0584Epoch 13/15: [==========================    ] 52/60 batches, loss: 0.0585Epoch 13/15: [==========================    ] 53/60 batches, loss: 0.0584Epoch 13/15: [===========================   ] 54/60 batches, loss: 0.0580Epoch 13/15: [===========================   ] 55/60 batches, loss: 0.0581Epoch 13/15: [============================  ] 56/60 batches, loss: 0.0584Epoch 13/15: [============================  ] 57/60 batches, loss: 0.0582Epoch 13/15: [============================= ] 58/60 batches, loss: 0.0584Epoch 13/15: [============================= ] 59/60 batches, loss: 0.0584Epoch 13/15: [==============================] 60/60 batches, loss: 0.0586
[2025-05-04 10:44:39,558][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.0586
[2025-05-04 10:44:39,827][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0816, Metrics: {'mse': 0.07907992601394653, 'rmse': 0.28121153250524156, 'r2': -0.8914498090744019}
[2025-05-04 10:44:39,828][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 14/15: [Epoch 14/15: [                              ] 1/60 batches, loss: 0.0423Epoch 14/15: [=                             ] 2/60 batches, loss: 0.0342Epoch 14/15: [=                             ] 3/60 batches, loss: 0.0570Epoch 14/15: [==                            ] 4/60 batches, loss: 0.0567Epoch 14/15: [==                            ] 5/60 batches, loss: 0.0666Epoch 14/15: [===                           ] 6/60 batches, loss: 0.0661Epoch 14/15: [===                           ] 7/60 batches, loss: 0.0640Epoch 14/15: [====                          ] 8/60 batches, loss: 0.0632Epoch 14/15: [====                          ] 9/60 batches, loss: 0.0652Epoch 14/15: [=====                         ] 10/60 batches, loss: 0.0636Epoch 14/15: [=====                         ] 11/60 batches, loss: 0.0614Epoch 14/15: [======                        ] 12/60 batches, loss: 0.0613Epoch 14/15: [======                        ] 13/60 batches, loss: 0.0619Epoch 14/15: [=======                       ] 14/60 batches, loss: 0.0606Epoch 14/15: [=======                       ] 15/60 batches, loss: 0.0597Epoch 14/15: [========                      ] 16/60 batches, loss: 0.0580Epoch 14/15: [========                      ] 17/60 batches, loss: 0.0566Epoch 14/15: [=========                     ] 18/60 batches, loss: 0.0570Epoch 14/15: [=========                     ] 19/60 batches, loss: 0.0567Epoch 14/15: [==========                    ] 20/60 batches, loss: 0.0559Epoch 14/15: [==========                    ] 21/60 batches, loss: 0.0565Epoch 14/15: [===========                   ] 22/60 batches, loss: 0.0566Epoch 14/15: [===========                   ] 23/60 batches, loss: 0.0554Epoch 14/15: [============                  ] 24/60 batches, loss: 0.0547Epoch 14/15: [============                  ] 25/60 batches, loss: 0.0550Epoch 14/15: [=============                 ] 26/60 batches, loss: 0.0556Epoch 14/15: [=============                 ] 27/60 batches, loss: 0.0557Epoch 14/15: [==============                ] 28/60 batches, loss: 0.0550Epoch 14/15: [==============                ] 29/60 batches, loss: 0.0555Epoch 14/15: [===============               ] 30/60 batches, loss: 0.0565Epoch 14/15: [===============               ] 31/60 batches, loss: 0.0564Epoch 14/15: [================              ] 32/60 batches, loss: 0.0568Epoch 14/15: [================              ] 33/60 batches, loss: 0.0565Epoch 14/15: [=================             ] 34/60 batches, loss: 0.0563Epoch 14/15: [=================             ] 35/60 batches, loss: 0.0559Epoch 14/15: [==================            ] 36/60 batches, loss: 0.0557Epoch 14/15: [==================            ] 37/60 batches, loss: 0.0557Epoch 14/15: [===================           ] 38/60 batches, loss: 0.0558Epoch 14/15: [===================           ] 39/60 batches, loss: 0.0554Epoch 14/15: [====================          ] 40/60 batches, loss: 0.0547Epoch 14/15: [====================          ] 41/60 batches, loss: 0.0545Epoch 14/15: [=====================         ] 42/60 batches, loss: 0.0540Epoch 14/15: [=====================         ] 43/60 batches, loss: 0.0542Epoch 14/15: [======================        ] 44/60 batches, loss: 0.0538Epoch 14/15: [======================        ] 45/60 batches, loss: 0.0534Epoch 14/15: [=======================       ] 46/60 batches, loss: 0.0534Epoch 14/15: [=======================       ] 47/60 batches, loss: 0.0532Epoch 14/15: [========================      ] 48/60 batches, loss: 0.0532Epoch 14/15: [========================      ] 49/60 batches, loss: 0.0528Epoch 14/15: [=========================     ] 50/60 batches, loss: 0.0527Epoch 14/15: [=========================     ] 51/60 batches, loss: 0.0527Epoch 14/15: [==========================    ] 52/60 batches, loss: 0.0528Epoch 14/15: [==========================    ] 53/60 batches, loss: 0.0525Epoch 14/15: [===========================   ] 54/60 batches, loss: 0.0526Epoch 14/15: [===========================   ] 55/60 batches, loss: 0.0522Epoch 14/15: [============================  ] 56/60 batches, loss: 0.0518Epoch 14/15: [============================  ] 57/60 batches, loss: 0.0514Epoch 14/15: [============================= ] 58/60 batches, loss: 0.0510Epoch 14/15: [============================= ] 59/60 batches, loss: 0.0515Epoch 14/15: [==============================] 60/60 batches, loss: 0.0513
[2025-05-04 10:44:41,709][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0513
[2025-05-04 10:44:41,987][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0849, Metrics: {'mse': 0.08245071768760681, 'rmse': 0.28714233001702627, 'r2': -0.9720731973648071}
[2025-05-04 10:44:41,988][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/60 batches, loss: 0.0376Epoch 15/15: [=                             ] 2/60 batches, loss: 0.0490Epoch 15/15: [=                             ] 3/60 batches, loss: 0.0651Epoch 15/15: [==                            ] 4/60 batches, loss: 0.0689Epoch 15/15: [==                            ] 5/60 batches, loss: 0.0632Epoch 15/15: [===                           ] 6/60 batches, loss: 0.0642Epoch 15/15: [===                           ] 7/60 batches, loss: 0.0600Epoch 15/15: [====                          ] 8/60 batches, loss: 0.0567Epoch 15/15: [====                          ] 9/60 batches, loss: 0.0580Epoch 15/15: [=====                         ] 10/60 batches, loss: 0.0580Epoch 15/15: [=====                         ] 11/60 batches, loss: 0.0578Epoch 15/15: [======                        ] 12/60 batches, loss: 0.0586Epoch 15/15: [======                        ] 13/60 batches, loss: 0.0573Epoch 15/15: [=======                       ] 14/60 batches, loss: 0.0568Epoch 15/15: [=======                       ] 15/60 batches, loss: 0.0547Epoch 15/15: [========                      ] 16/60 batches, loss: 0.0541Epoch 15/15: [========                      ] 17/60 batches, loss: 0.0540Epoch 15/15: [=========                     ] 18/60 batches, loss: 0.0539Epoch 15/15: [=========                     ] 19/60 batches, loss: 0.0542Epoch 15/15: [==========                    ] 20/60 batches, loss: 0.0532Epoch 15/15: [==========                    ] 21/60 batches, loss: 0.0528Epoch 15/15: [===========                   ] 22/60 batches, loss: 0.0528Epoch 15/15: [===========                   ] 23/60 batches, loss: 0.0525Epoch 15/15: [============                  ] 24/60 batches, loss: 0.0526Epoch 15/15: [============                  ] 25/60 batches, loss: 0.0521Epoch 15/15: [=============                 ] 26/60 batches, loss: 0.0512Epoch 15/15: [=============                 ] 27/60 batches, loss: 0.0523Epoch 15/15: [==============                ] 28/60 batches, loss: 0.0534Epoch 15/15: [==============                ] 29/60 batches, loss: 0.0546Epoch 15/15: [===============               ] 30/60 batches, loss: 0.0546Epoch 15/15: [===============               ] 31/60 batches, loss: 0.0539Epoch 15/15: [================              ] 32/60 batches, loss: 0.0534Epoch 15/15: [================              ] 33/60 batches, loss: 0.0541Epoch 15/15: [=================             ] 34/60 batches, loss: 0.0533Epoch 15/15: [=================             ] 35/60 batches, loss: 0.0530Epoch 15/15: [==================            ] 36/60 batches, loss: 0.0525Epoch 15/15: [==================            ] 37/60 batches, loss: 0.0523Epoch 15/15: [===================           ] 38/60 batches, loss: 0.0519Epoch 15/15: [===================           ] 39/60 batches, loss: 0.0520Epoch 15/15: [====================          ] 40/60 batches, loss: 0.0517Epoch 15/15: [====================          ] 41/60 batches, loss: 0.0517Epoch 15/15: [=====================         ] 42/60 batches, loss: 0.0524Epoch 15/15: [=====================         ] 43/60 batches, loss: 0.0521Epoch 15/15: [======================        ] 44/60 batches, loss: 0.0517Epoch 15/15: [======================        ] 45/60 batches, loss: 0.0516Epoch 15/15: [=======================       ] 46/60 batches, loss: 0.0511Epoch 15/15: [=======================       ] 47/60 batches, loss: 0.0511Epoch 15/15: [========================      ] 48/60 batches, loss: 0.0511Epoch 15/15: [========================      ] 49/60 batches, loss: 0.0517Epoch 15/15: [=========================     ] 50/60 batches, loss: 0.0514Epoch 15/15: [=========================     ] 51/60 batches, loss: 0.0517Epoch 15/15: [==========================    ] 52/60 batches, loss: 0.0516Epoch 15/15: [==========================    ] 53/60 batches, loss: 0.0517Epoch 15/15: [===========================   ] 54/60 batches, loss: 0.0519Epoch 15/15: [===========================   ] 55/60 batches, loss: 0.0516Epoch 15/15: [============================  ] 56/60 batches, loss: 0.0516Epoch 15/15: [============================  ] 57/60 batches, loss: 0.0513Epoch 15/15: [============================= ] 58/60 batches, loss: 0.0508Epoch 15/15: [============================= ] 59/60 batches, loss: 0.0511Epoch 15/15: [==============================] 60/60 batches, loss: 0.0512
[2025-05-04 10:44:44,628][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0512
[2025-05-04 10:44:44,887][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0784, Metrics: {'mse': 0.07602406293153763, 'rmse': 0.2757246143011857, 'r2': -0.8183590173721313}
[2025-05-04 10:44:45,321][src.training.lm_trainer][INFO] - Training completed in 36.65 seconds
[2025-05-04 10:44:45,321][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:44:47,850][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03887993097305298, 'rmse': 0.19717994566652305, 'r2': -0.071463942527771}
[2025-05-04 10:44:47,850][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07602406293153763, 'rmse': 0.2757246143011857, 'r2': -0.8183590173721313}
[2025-05-04 10:44:47,850][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04925757646560669, 'rmse': 0.2219404795561339, 'r2': -0.20806026458740234}
[2025-05-04 10:44:49,547][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/id/id/model.pt
[2025-05-04 10:44:49,549][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▃▂▁▁
wandb:     best_val_mse █▃▃▃▂▁▁
wandb:      best_val_r2 ▁▆▆▆▇██
wandb:    best_val_rmse █▄▃▃▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▂▂▁▄▃▃▄▄▄▄▄
wandb:       train_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▃▇▆█▃▅▃▂▃▁▂▂▁
wandb:          val_mse █▃▃▇▇█▃▅▃▂▃▁▂▃▁
wandb:           val_r2 ▁▆▆▂▂▁▆▄▆▇▆█▇▆█
wandb:         val_rmse █▄▃▇▇█▃▅▃▂▃▁▂▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0784
wandb:     best_val_mse 0.07602
wandb:      best_val_r2 -0.81836
wandb:    best_val_rmse 0.27572
wandb:            epoch 15
wandb:   final_test_mse 0.04926
wandb:    final_test_r2 -0.20806
wandb:  final_test_rmse 0.22194
wandb:  final_train_mse 0.03888
wandb:   final_train_r2 -0.07146
wandb: final_train_rmse 0.19718
wandb:    final_val_mse 0.07602
wandb:     final_val_r2 -0.81836
wandb:   final_val_rmse 0.27572
wandb:    learning_rate 0.0001
wandb:       train_loss 0.05115
wandb:       train_time 36.65371
wandb:         val_loss 0.0784
wandb:          val_mse 0.07602
wandb:           val_r2 -0.81836
wandb:         val_rmse 0.27572
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104356-mig00bvs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104356-mig00bvs/logs
Experiment probe_layer10_complexity_control3_id completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/id/id/results.json for layer 10
Running experiment: probe_layer10_question_type_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:45:10,624][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ja
experiment_name: probe_layer10_question_type_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:45:10,624][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:45:10,624][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:45:10,624][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:45:10,630][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-04 10:45:10,630][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:45:13,348][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:45:15,638][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:45:15,638][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:45:15,761][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:45:15,826][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:45:16,037][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:45:16,046][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:45:16,046][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:45:16,048][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:45:16,110][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:45:16,154][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:45:16,179][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:45:16,181][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:45:16,181][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:45:16,182][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:45:16,205][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:45:16,299][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:45:16,334][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:45:16,335][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:45:16,335][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:45:16,345][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:45:16,346][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:45:16,346][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:45:16,346][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:45:16,347][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-04 10:45:16,347][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:45:16,347][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:45:16,348][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-04 10:45:16,348][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:45:16,348][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-04 10:45:16,348][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:45:16,348][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:45:16,349][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:45:16,349][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:45:16,349][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:45:16,349][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:45:16,349][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:45:21,976][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:45:21,977][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:45:21,977][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:45:21,977][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:45:21,983][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:45:21,983][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:45:21,983][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:45:21,983][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:45:21,983][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:45:21,984][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:45:21,984][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:45:21,985][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7017Epoch 1/15: [                              ] 2/75 batches, loss: 0.7053Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7112Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7023Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6975Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6999Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7001Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6995Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6964Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6970Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6965Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6951Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6940Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6936Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6928Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6923Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6925Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6925Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6911Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6923Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6921Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6922Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6928Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6923Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6927Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6937Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6940Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6942Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6944Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6944Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6943Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6942Epoch 1/15: [================              ] 40/75 batches, loss: 0.6941Epoch 1/15: [================              ] 41/75 batches, loss: 0.6942Epoch 1/15: [================              ] 42/75 batches, loss: 0.6942Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6941Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6940Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6941Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6941Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6941Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6940Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6940Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6940Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6940Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6939Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6938Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6938Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6938Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6939Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6938Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6938Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6938Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6938Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6937Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6937Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6937Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6938Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6938Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6939Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6939Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6938Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6938Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6938Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6937Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6938Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6938Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6938Epoch 1/15: [==============================] 75/75 batches, loss: 0.6939
[2025-05-04 10:45:27,690][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6939
[2025-05-04 10:45:27,885][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6921, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6925Epoch 2/15: [                              ] 2/75 batches, loss: 0.6932Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6934Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6928Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6924Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6925Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6925Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6926Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6926Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6926Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6926Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6926Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6928Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6927Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6929Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6927Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6928Epoch 2/15: [================              ] 40/75 batches, loss: 0.6928Epoch 2/15: [================              ] 41/75 batches, loss: 0.6927Epoch 2/15: [================              ] 42/75 batches, loss: 0.6927Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6928Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6928Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6927Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6927Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6926Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6926Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6926Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6926Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6927Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6928Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6927Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6927Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6927Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6928Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6928Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6929Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6928Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6928Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6928Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6929Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:45:30,579][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:45:30,789][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6911, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6913Epoch 3/15: [                              ] 2/75 batches, loss: 0.6914Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6929Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6924Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6926Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6926Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6927Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6924Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6927Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6927Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6929Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6931Epoch 3/15: [================              ] 41/75 batches, loss: 0.6931Epoch 3/15: [================              ] 42/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:45:33,495][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-04 10:45:33,729][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6922, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:33,730][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6944Epoch 4/15: [                              ] 2/75 batches, loss: 0.6944Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6939Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6937Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6936Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6937Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6936Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6936Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6936Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6935Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6934Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6934Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6933Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6934Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6934Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6933Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6934Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6933Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6934Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6933Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6933Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6933Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6933Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6933Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 4/15: [================              ] 40/75 batches, loss: 0.6932Epoch 4/15: [================              ] 41/75 batches, loss: 0.6932Epoch 4/15: [================              ] 42/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 4/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-04 10:45:36,028][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6930
[2025-05-04 10:45:36,246][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6913, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:36,246][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6933Epoch 5/15: [                              ] 2/75 batches, loss: 0.6910Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6919Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6925Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6924Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6919Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6915Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6915Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6906Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6905Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6914Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6913Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6913Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6916Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6923Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6924Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6921Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6928Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6938Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6936Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6938Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6938Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6940Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6940Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6940Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6940Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6940Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6939Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6939Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6939Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6939Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6938Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6938Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6938Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6938Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6938Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6938Epoch 5/15: [================              ] 40/75 batches, loss: 0.6938Epoch 5/15: [================              ] 41/75 batches, loss: 0.6938Epoch 5/15: [================              ] 42/75 batches, loss: 0.6937Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6937Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6937Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6937Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6937Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6937Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6937Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6937Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6936Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6936Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6936Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6936Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6936Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6936Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6936Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6936Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6936Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6936Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6936Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6935Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6935Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6935Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6935Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6935Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6935Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6935Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6935Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6935Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6935Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6935Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6935Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6935Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6935Epoch 5/15: [==============================] 75/75 batches, loss: 0.6935
[2025-05-04 10:45:38,523][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6935
[2025-05-04 10:45:38,746][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:38,747][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:45:38,747][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:45:38,747][src.training.lm_trainer][INFO] - Training completed in 14.18 seconds
[2025-05-04 10:45:38,747][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:45:41,586][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4995801847187238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:41,586][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:41,586][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.40217391304347827, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:45:43,387][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ja/ja/model.pt
[2025-05-04 10:45:43,388][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁
wandb:           best_val_f1 ▁▁
wandb:         best_val_loss █▁
wandb:    best_val_precision ▁▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁
wandb:            train_loss █▃▂▁▅
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁
wandb:              val_loss ▅▁▅▂█
wandb:         val_precision ▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.47826
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69106
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.40217
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.47826
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69345
wandb:            train_time 14.17528
wandb:          val_accuracy 0.47826
wandb:                val_f1 0
wandb:              val_loss 0.69307
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104510-6z50g2me
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104510-6z50g2me/logs
Experiment probe_layer10_question_type_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_question_type_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:45:58,040][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ja
experiment_name: probe_layer10_question_type_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:45:58,041][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:45:58,041][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:45:58,041][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:45:58,045][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-04 10:45:58,046][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:45:59,979][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:46:02,218][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:46:02,218][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:02,269][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:46:02,339][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:46:02,433][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:46:02,442][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:02,443][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:46:02,444][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:02,491][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:02,559][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:02,573][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:46:02,574][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:02,574][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:46:02,575][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:02,618][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:02,653][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:02,667][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:46:02,669][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:02,669][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:46:02,670][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:46:02,670][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:02,670][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:02,670][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:02,671][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-04 10:46:02,671][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:02,671][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:02,672][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-04 10:46:02,672][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:02,672][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:02,672][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-04 10:46:02,672][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-04 10:46:02,673][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:46:02,673][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:46:02,673][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:46:02,673][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:46:02,673][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:46:02,673][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:46:02,673][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:46:07,215][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:46:07,217][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:46:07,217][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:46:07,217][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:46:07,223][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:46:07,223][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:46:07,223][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:46:07,223][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:46:07,223][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:46:07,224][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:46:07,224][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:46:07,225][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7573Epoch 1/15: [                              ] 2/75 batches, loss: 0.7094Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7055Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6997Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7000Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7072Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7046Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7039Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7025Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7008Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7019Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7013Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7006Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7000Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6992Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6993Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6992Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6986Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6988Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6984Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6982Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6981Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6977Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6974Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6975Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6974Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6972Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6971Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6969Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6969Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6968Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6968Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6966Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6965Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6965Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6964Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6963Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6963Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6962Epoch 1/15: [================              ] 40/75 batches, loss: 0.6961Epoch 1/15: [================              ] 41/75 batches, loss: 0.6960Epoch 1/15: [================              ] 42/75 batches, loss: 0.6959Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6959Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6958Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6958Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6957Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6957Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6956Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6955Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6955Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6954Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6954Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6953Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6953Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6952Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6951Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6950Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6950Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6949Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6949Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6948Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6948Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6948Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6947Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6947Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6947Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6947Epoch 1/15: [==============================] 75/75 batches, loss: 0.6947
[2025-05-04 10:46:13,291][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6947
[2025-05-04 10:46:13,500][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6938Epoch 2/15: [                              ] 2/75 batches, loss: 0.6935Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6927Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6927Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6929Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 2/15: [================              ] 40/75 batches, loss: 0.6930Epoch 2/15: [================              ] 41/75 batches, loss: 0.6930Epoch 2/15: [================              ] 42/75 batches, loss: 0.6930Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 2/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:46:16,172][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6931
[2025-05-04 10:46:16,400][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6942Epoch 3/15: [                              ] 2/75 batches, loss: 0.6936Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6935Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6934Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6936Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6936Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6935Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6934Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6931Epoch 3/15: [================              ] 41/75 batches, loss: 0.6931Epoch 3/15: [================              ] 42/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:46:19,143][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-04 10:46:19,367][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:19,368][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6936Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6930Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6930Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6928Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6927Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6927Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6928Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6928Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6928Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6929Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 4/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:46:21,633][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:46:21,838][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:21,839][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6926Epoch 5/15: [                              ] 2/75 batches, loss: 0.6928Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6933Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6933Epoch 5/15: [================              ] 40/75 batches, loss: 0.6933Epoch 5/15: [================              ] 41/75 batches, loss: 0.6933Epoch 5/15: [================              ] 42/75 batches, loss: 0.6933Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6933Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6933Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6933Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6933Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6933Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6933Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6933Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6933Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 5/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:46:24,121][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6932
[2025-05-04 10:46:24,389][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:24,389][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:46:24,389][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:46:24,390][src.training.lm_trainer][INFO] - Training completed in 14.45 seconds
[2025-05-04 10:46:24,390][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:46:27,263][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.4995801847187238, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:27,264][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:27,264][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.40217391304347827, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:46:28,899][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ja/ja/model.pt
[2025-05-04 10:46:28,900][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁
wandb:           best_val_f1 ▁▁
wandb:         best_val_loss █▁
wandb:    best_val_precision ▁▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁
wandb:            train_loss █▁▁▁▂
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁
wandb:              val_loss ▂▁▁▆█
wandb:         val_precision ▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.47826
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69296
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.40217
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.49958
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.47826
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69324
wandb:            train_time 14.45146
wandb:          val_accuracy 0.47826
wandb:                val_f1 0
wandb:              val_loss 0.69307
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104558-119rzuho
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104558-119rzuho/logs
Experiment probe_layer10_question_type_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_question_type_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:46:50,177][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ja
experiment_name: probe_layer10_question_type_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:46:50,177][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:46:50,177][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:46:50,177][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:46:50,182][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ja']
[2025-05-04 10:46:50,183][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:46:53,567][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:46:55,813][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:46:55,814][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:56,137][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:46:56,205][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:46:56,432][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:46:56,440][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:56,441][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:46:56,442][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:56,520][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:56,594][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:56,637][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:46:56,638][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:56,638][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:46:56,639][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:46:56,677][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:56,728][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:46:56,742][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:46:56,744][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:46:56,744][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:46:56,745][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:46:56,746][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:56,746][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:56,746][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:56,746][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:56,747][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-05-04 10:46:56,747][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:56,747][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-05-04 10:46:56,747][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-05-04 10:46:56,747][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:46:56,748][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-05-04 10:46:56,748][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:46:56,748][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:46:56,749][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:46:56,749][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:46:56,749][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:47:02,807][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:47:02,807][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:47:02,807][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:47:02,808][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:47:02,814][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:47:02,814][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:47:02,814][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:47:02,814][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:47:02,814][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:47:02,815][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:47:02,815][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:47:02,816][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6624Epoch 1/15: [                              ] 2/75 batches, loss: 0.7408Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7340Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7127Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7127Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7098Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7125Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7138Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7112Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7095Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7107Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7105Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7092Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7082Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7069Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7059Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7052Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7046Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7040Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7035Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7029Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7023Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7020Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7015Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.7012Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.7009Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.7006Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.7002Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.7000Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6998Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6995Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6993Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6992Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6990Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6988Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6988Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6986Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6986Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6984Epoch 1/15: [================              ] 40/75 batches, loss: 0.6983Epoch 1/15: [================              ] 41/75 batches, loss: 0.6982Epoch 1/15: [================              ] 42/75 batches, loss: 0.6981Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6980Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6979Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6978Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6976Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6975Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6974Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6973Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6972Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6972Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6971Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6970Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6970Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6969Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6968Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6967Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6967Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6966Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6966Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6966Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6965Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6965Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6964Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6964Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6963Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6963Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6962Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6961Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6961Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6961Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6960Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6960Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6959Epoch 1/15: [==============================] 75/75 batches, loss: 0.6959
[2025-05-04 10:47:08,557][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6959
[2025-05-04 10:47:08,747][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6942Epoch 2/15: [                              ] 2/75 batches, loss: 0.6937Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6934Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6934Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6934Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6934Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6933Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6929Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6928Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6927Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6927Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6928Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6928Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6928Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6928Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 2/15: [================              ] 40/75 batches, loss: 0.6930Epoch 2/15: [================              ] 41/75 batches, loss: 0.6930Epoch 2/15: [================              ] 42/75 batches, loss: 0.6930Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 2/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:47:11,424][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6931
[2025-05-04 10:47:11,625][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6927, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6921Epoch 3/15: [                              ] 2/75 batches, loss: 0.6923Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6929Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6926Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6927Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6933Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6933Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6933Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6930Epoch 3/15: [================              ] 41/75 batches, loss: 0.6930Epoch 3/15: [================              ] 42/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 3/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-04 10:47:14,299][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6930
[2025-05-04 10:47:14,515][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6925, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6926Epoch 4/15: [                              ] 2/75 batches, loss: 0.6919Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6917Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6918Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6916Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6918Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6918Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6917Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6919Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6918Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6920Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6917Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6915Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6919Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6918Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6919Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6915Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6914Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6915Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6916Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6918Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6918Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6922Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6924Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6924Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6924Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6928Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6924Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6924Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6922Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6923Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6923Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6924Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6924Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6927Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6927Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6927Epoch 4/15: [================              ] 40/75 batches, loss: 0.6933Epoch 4/15: [================              ] 41/75 batches, loss: 0.6933Epoch 4/15: [================              ] 42/75 batches, loss: 0.6935Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6938Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6938Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6928Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6929Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6928Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6929Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6928Epoch 4/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:47:17,152][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6929
[2025-05-04 10:47:17,380][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6920, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6929Epoch 5/15: [                              ] 2/75 batches, loss: 0.6930Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6916Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6916Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6937Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6928Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6908Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6918Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6919Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6928Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6921Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6921Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6922Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6923Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6923Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6933Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6933Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6933Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6933Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6933Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6928Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6929Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 5/15: [================              ] 40/75 batches, loss: 0.6929Epoch 5/15: [================              ] 41/75 batches, loss: 0.6929Epoch 5/15: [================              ] 42/75 batches, loss: 0.6930Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6927Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6927Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6928Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6928Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 5/15: [==============================] 75/75 batches, loss: 0.6930
[2025-05-04 10:47:20,026][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6930
[2025-05-04 10:47:20,237][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6918, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6934Epoch 6/15: [                              ] 2/75 batches, loss: 0.6940Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6934Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6937Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6947Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6944Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6927Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6927Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6923Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6919Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6911Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6914Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6914Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6920Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6920Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6927Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6934Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6939Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6939Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6939Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6939Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6939Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6937Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6937Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6936Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6937Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6937Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6937Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6937Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6936Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6936Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6936Epoch 6/15: [================              ] 40/75 batches, loss: 0.6935Epoch 6/15: [================              ] 41/75 batches, loss: 0.6935Epoch 6/15: [================              ] 42/75 batches, loss: 0.6935Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6935Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6935Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6934Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6933Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6934Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6934Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6934Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6934Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6934Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6934Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6934Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6934Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6934Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6934Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6934Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6934Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6933Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6933Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6933Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6933Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 6/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-04 10:47:22,901][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6933
[2025-05-04 10:47:23,106][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6924, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:47:23,107][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6954Epoch 7/15: [                              ] 2/75 batches, loss: 0.6954Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6939Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6930Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6921Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6922Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6921Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6922Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6918Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6919Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6920Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6921Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6922Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6918Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6915Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6914Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6917Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6914Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6916Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6918Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6917Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6915Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6916Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6916Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6920Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6920Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6923Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6922Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6923Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6920Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6922Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6921Epoch 7/15: [================              ] 40/75 batches, loss: 0.6922Epoch 7/15: [================              ] 41/75 batches, loss: 0.6916Epoch 7/15: [================              ] 42/75 batches, loss: 0.6919Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6918Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6917Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6927Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6924Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6924Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6922Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6922Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6926Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6926Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6926Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6927Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6926Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6926Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6927Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6927Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6928Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6928Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6928Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6928Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6928Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6929Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 7/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:47:25,403][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6932
[2025-05-04 10:47:25,620][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6915, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6932Epoch 8/15: [                              ] 2/75 batches, loss: 0.6931Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6929Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6933Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6929Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6929Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6929Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6928Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6929Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6929Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6929Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6930Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 8/15: [================              ] 40/75 batches, loss: 0.6929Epoch 8/15: [================              ] 41/75 batches, loss: 0.6929Epoch 8/15: [================              ] 42/75 batches, loss: 0.6929Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6928Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6928Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6929Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6929Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6928Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6928Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6928Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6928Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6929Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6928Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6929Epoch 8/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:47:28,315][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6929
[2025-05-04 10:47:28,535][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6891, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6928Epoch 9/15: [                              ] 2/75 batches, loss: 0.6924Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6924Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6929Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6928Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6922Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6918Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6919Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6916Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6922Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6925Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6928Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6933Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6933Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6928Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6929Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 9/15: [================              ] 40/75 batches, loss: 0.6930Epoch 9/15: [================              ] 41/75 batches, loss: 0.6932Epoch 9/15: [================              ] 42/75 batches, loss: 0.6930Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6928Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6927Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6925Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6926Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6922Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6921Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6916Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6918Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6914Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6919Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6927Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6928Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6925Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6927Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6926Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6925Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6925Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6925Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6924Epoch 9/15: [==============================] 75/75 batches, loss: 0.6924
[2025-05-04 10:47:31,200][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6924
[2025-05-04 10:47:31,433][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6884, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.6885Epoch 10/15: [                              ] 2/75 batches, loss: 0.6896Epoch 10/15: [=                             ] 3/75 batches, loss: 0.6914Epoch 10/15: [=                             ] 4/75 batches, loss: 0.6908Epoch 10/15: [==                            ] 5/75 batches, loss: 0.6908Epoch 10/15: [==                            ] 6/75 batches, loss: 0.6919Epoch 10/15: [==                            ] 7/75 batches, loss: 0.6918Epoch 10/15: [===                           ] 8/75 batches, loss: 0.6922Epoch 10/15: [===                           ] 9/75 batches, loss: 0.6923Epoch 10/15: [====                          ] 10/75 batches, loss: 0.6924Epoch 10/15: [====                          ] 11/75 batches, loss: 0.6925Epoch 10/15: [====                          ] 12/75 batches, loss: 0.6925Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.6917Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.6919Epoch 10/15: [======                        ] 15/75 batches, loss: 0.6916Epoch 10/15: [======                        ] 16/75 batches, loss: 0.6921Epoch 10/15: [======                        ] 17/75 batches, loss: 0.6920Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.6919Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.6916Epoch 10/15: [========                      ] 20/75 batches, loss: 0.6917Epoch 10/15: [========                      ] 21/75 batches, loss: 0.6922Epoch 10/15: [========                      ] 22/75 batches, loss: 0.6921Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.6923Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.6927Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.6927Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.6924Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.6923Epoch 10/15: [============                  ] 30/75 batches, loss: 0.6922Epoch 10/15: [============                  ] 31/75 batches, loss: 0.6920Epoch 10/15: [============                  ] 32/75 batches, loss: 0.6918Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.6912Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.6915Epoch 10/15: [==============                ] 35/75 batches, loss: 0.6918Epoch 10/15: [==============                ] 36/75 batches, loss: 0.6921Epoch 10/15: [==============                ] 37/75 batches, loss: 0.6925Epoch 10/15: [===============               ] 38/75 batches, loss: 0.6928Epoch 10/15: [===============               ] 39/75 batches, loss: 0.6925Epoch 10/15: [================              ] 40/75 batches, loss: 0.6924Epoch 10/15: [================              ] 41/75 batches, loss: 0.6925Epoch 10/15: [================              ] 42/75 batches, loss: 0.6923Epoch 10/15: [=================             ] 43/75 batches, loss: 0.6922Epoch 10/15: [=================             ] 44/75 batches, loss: 0.6923Epoch 10/15: [==================            ] 45/75 batches, loss: 0.6925Epoch 10/15: [==================            ] 46/75 batches, loss: 0.6924Epoch 10/15: [==================            ] 47/75 batches, loss: 0.6926Epoch 10/15: [===================           ] 48/75 batches, loss: 0.6924Epoch 10/15: [===================           ] 49/75 batches, loss: 0.6923Epoch 10/15: [====================          ] 50/75 batches, loss: 0.6922Epoch 10/15: [====================          ] 51/75 batches, loss: 0.6923Epoch 10/15: [====================          ] 52/75 batches, loss: 0.6925Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.6926Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.6925Epoch 10/15: [======================        ] 55/75 batches, loss: 0.6924Epoch 10/15: [======================        ] 56/75 batches, loss: 0.6925Epoch 10/15: [======================        ] 57/75 batches, loss: 0.6925Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.6925Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.6924Epoch 10/15: [========================      ] 60/75 batches, loss: 0.6924Epoch 10/15: [========================      ] 61/75 batches, loss: 0.6924Epoch 10/15: [========================      ] 62/75 batches, loss: 0.6924Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.6924Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.6923Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.6922Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.6920Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.6918Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.6918Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.6915Epoch 10/15: [============================  ] 70/75 batches, loss: 0.6916Epoch 10/15: [============================  ] 71/75 batches, loss: 0.6919Epoch 10/15: [============================  ] 72/75 batches, loss: 0.6921Epoch 10/15: [============================= ] 73/75 batches, loss: 0.6911Epoch 10/15: [============================= ] 74/75 batches, loss: 0.6912Epoch 10/15: [==============================] 75/75 batches, loss: 0.6906
[2025-05-04 10:47:34,140][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6906
[2025-05-04 10:47:34,375][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6589, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.42424242424242425, 'precision': 0.7777777777777778, 'recall': 0.2916666666666667}
Epoch 11/15: [Epoch 11/15: [                              ] 1/75 batches, loss: 0.6955Epoch 11/15: [                              ] 2/75 batches, loss: 0.7396Epoch 11/15: [=                             ] 3/75 batches, loss: 0.7123Epoch 11/15: [=                             ] 4/75 batches, loss: 0.7124Epoch 11/15: [==                            ] 5/75 batches, loss: 0.7101Epoch 11/15: [==                            ] 6/75 batches, loss: 0.7090Epoch 11/15: [==                            ] 7/75 batches, loss: 0.7063Epoch 11/15: [===                           ] 8/75 batches, loss: 0.7039Epoch 11/15: [===                           ] 9/75 batches, loss: 0.7023Epoch 11/15: [====                          ] 10/75 batches, loss: 0.7011Epoch 11/15: [====                          ] 11/75 batches, loss: 0.7002Epoch 11/15: [====                          ] 12/75 batches, loss: 0.6990Epoch 11/15: [=====                         ] 13/75 batches, loss: 0.6984Epoch 11/15: [=====                         ] 14/75 batches, loss: 0.6979Epoch 11/15: [======                        ] 15/75 batches, loss: 0.6965Epoch 11/15: [======                        ] 16/75 batches, loss: 0.6963Epoch 11/15: [======                        ] 17/75 batches, loss: 0.6963Epoch 11/15: [=======                       ] 18/75 batches, loss: 0.6962Epoch 11/15: [=======                       ] 19/75 batches, loss: 0.6962Epoch 11/15: [========                      ] 20/75 batches, loss: 0.6958Epoch 11/15: [========                      ] 21/75 batches, loss: 0.6956Epoch 11/15: [========                      ] 22/75 batches, loss: 0.6954Epoch 11/15: [=========                     ] 23/75 batches, loss: 0.6953Epoch 11/15: [=========                     ] 24/75 batches, loss: 0.6953Epoch 11/15: [==========                    ] 25/75 batches, loss: 0.6951Epoch 11/15: [==========                    ] 26/75 batches, loss: 0.6951Epoch 11/15: [==========                    ] 27/75 batches, loss: 0.6950Epoch 11/15: [===========                   ] 28/75 batches, loss: 0.6949Epoch 11/15: [===========                   ] 29/75 batches, loss: 0.6949Epoch 11/15: [============                  ] 30/75 batches, loss: 0.6948Epoch 11/15: [============                  ] 31/75 batches, loss: 0.6948Epoch 11/15: [============                  ] 32/75 batches, loss: 0.6948Epoch 11/15: [=============                 ] 33/75 batches, loss: 0.6947Epoch 11/15: [=============                 ] 34/75 batches, loss: 0.6947Epoch 11/15: [==============                ] 35/75 batches, loss: 0.6946Epoch 11/15: [==============                ] 36/75 batches, loss: 0.6945Epoch 11/15: [==============                ] 37/75 batches, loss: 0.6945Epoch 11/15: [===============               ] 38/75 batches, loss: 0.6944Epoch 11/15: [===============               ] 39/75 batches, loss: 0.6943Epoch 11/15: [================              ] 40/75 batches, loss: 0.6942Epoch 11/15: [================              ] 41/75 batches, loss: 0.6942Epoch 11/15: [================              ] 42/75 batches, loss: 0.6942Epoch 11/15: [=================             ] 43/75 batches, loss: 0.6941Epoch 11/15: [=================             ] 44/75 batches, loss: 0.6939Epoch 11/15: [==================            ] 45/75 batches, loss: 0.6940Epoch 11/15: [==================            ] 46/75 batches, loss: 0.6939Epoch 11/15: [==================            ] 47/75 batches, loss: 0.6939Epoch 11/15: [===================           ] 48/75 batches, loss: 0.6938Epoch 11/15: [===================           ] 49/75 batches, loss: 0.6935Epoch 11/15: [====================          ] 50/75 batches, loss: 0.6935Epoch 11/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 11/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 11/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 11/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 11/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 11/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 11/15: [======================        ] 57/75 batches, loss: 0.6925Epoch 11/15: [=======================       ] 58/75 batches, loss: 0.6915Epoch 11/15: [=======================       ] 59/75 batches, loss: 0.6912Epoch 11/15: [========================      ] 60/75 batches, loss: 0.6915Epoch 11/15: [========================      ] 61/75 batches, loss: 0.6917Epoch 11/15: [========================      ] 62/75 batches, loss: 0.6910Epoch 11/15: [=========================     ] 63/75 batches, loss: 0.6905Epoch 11/15: [=========================     ] 64/75 batches, loss: 0.6919Epoch 11/15: [==========================    ] 65/75 batches, loss: 0.6926Epoch 11/15: [==========================    ] 66/75 batches, loss: 0.6933Epoch 11/15: [==========================    ] 67/75 batches, loss: 0.6941Epoch 11/15: [===========================   ] 68/75 batches, loss: 0.6934Epoch 11/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 11/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 11/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 11/15: [============================  ] 72/75 batches, loss: 0.6935Epoch 11/15: [============================= ] 73/75 batches, loss: 0.6935Epoch 11/15: [============================= ] 74/75 batches, loss: 0.6934Epoch 11/15: [==============================] 75/75 batches, loss: 0.6935
[2025-05-04 10:47:37,064][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.6935
[2025-05-04 10:47:37,287][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6767, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:47:37,288][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/75 batches, loss: 0.6993Epoch 12/15: [                              ] 2/75 batches, loss: 0.6953Epoch 12/15: [=                             ] 3/75 batches, loss: 0.6954Epoch 12/15: [=                             ] 4/75 batches, loss: 0.6940Epoch 12/15: [==                            ] 5/75 batches, loss: 0.6941Epoch 12/15: [==                            ] 6/75 batches, loss: 0.6945Epoch 12/15: [==                            ] 7/75 batches, loss: 0.6943Epoch 12/15: [===                           ] 8/75 batches, loss: 0.6935Epoch 12/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 12/15: [====                          ] 10/75 batches, loss: 0.6938Epoch 12/15: [====                          ] 11/75 batches, loss: 0.6940Epoch 12/15: [====                          ] 12/75 batches, loss: 0.6939Epoch 12/15: [=====                         ] 13/75 batches, loss: 0.6936Epoch 12/15: [=====                         ] 14/75 batches, loss: 0.6936Epoch 12/15: [======                        ] 15/75 batches, loss: 0.6936Epoch 12/15: [======                        ] 16/75 batches, loss: 0.6935Epoch 12/15: [======                        ] 17/75 batches, loss: 0.6936Epoch 12/15: [=======                       ] 18/75 batches, loss: 0.6934Epoch 12/15: [=======                       ] 19/75 batches, loss: 0.6933Epoch 12/15: [========                      ] 20/75 batches, loss: 0.6933Epoch 12/15: [========                      ] 21/75 batches, loss: 0.6933Epoch 12/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 12/15: [=========                     ] 23/75 batches, loss: 0.6934Epoch 12/15: [=========                     ] 24/75 batches, loss: 0.6940Epoch 12/15: [==========                    ] 25/75 batches, loss: 0.6940Epoch 12/15: [==========                    ] 26/75 batches, loss: 0.6939Epoch 12/15: [==========                    ] 27/75 batches, loss: 0.6938Epoch 12/15: [===========                   ] 28/75 batches, loss: 0.6938Epoch 12/15: [===========                   ] 29/75 batches, loss: 0.6938Epoch 12/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 12/15: [============                  ] 31/75 batches, loss: 0.6940Epoch 12/15: [============                  ] 32/75 batches, loss: 0.6939Epoch 12/15: [=============                 ] 33/75 batches, loss: 0.6939Epoch 12/15: [=============                 ] 34/75 batches, loss: 0.6939Epoch 12/15: [==============                ] 35/75 batches, loss: 0.6939Epoch 12/15: [==============                ] 36/75 batches, loss: 0.6938Epoch 12/15: [==============                ] 37/75 batches, loss: 0.6938Epoch 12/15: [===============               ] 38/75 batches, loss: 0.6938Epoch 12/15: [===============               ] 39/75 batches, loss: 0.6938Epoch 12/15: [================              ] 40/75 batches, loss: 0.6937Epoch 12/15: [================              ] 41/75 batches, loss: 0.6937Epoch 12/15: [================              ] 42/75 batches, loss: 0.6937Epoch 12/15: [=================             ] 43/75 batches, loss: 0.6937Epoch 12/15: [=================             ] 44/75 batches, loss: 0.6937Epoch 12/15: [==================            ] 45/75 batches, loss: 0.6937Epoch 12/15: [==================            ] 46/75 batches, loss: 0.6936Epoch 12/15: [==================            ] 47/75 batches, loss: 0.6936Epoch 12/15: [===================           ] 48/75 batches, loss: 0.6936Epoch 12/15: [===================           ] 49/75 batches, loss: 0.6936Epoch 12/15: [====================          ] 50/75 batches, loss: 0.6936Epoch 12/15: [====================          ] 51/75 batches, loss: 0.6936Epoch 12/15: [====================          ] 52/75 batches, loss: 0.6936Epoch 12/15: [=====================         ] 53/75 batches, loss: 0.6936Epoch 12/15: [=====================         ] 54/75 batches, loss: 0.6935Epoch 12/15: [======================        ] 55/75 batches, loss: 0.6936Epoch 12/15: [======================        ] 56/75 batches, loss: 0.6935Epoch 12/15: [======================        ] 57/75 batches, loss: 0.6935Epoch 12/15: [=======================       ] 58/75 batches, loss: 0.6935Epoch 12/15: [=======================       ] 59/75 batches, loss: 0.6935Epoch 12/15: [========================      ] 60/75 batches, loss: 0.6935Epoch 12/15: [========================      ] 61/75 batches, loss: 0.6935Epoch 12/15: [========================      ] 62/75 batches, loss: 0.6935Epoch 12/15: [=========================     ] 63/75 batches, loss: 0.6935Epoch 12/15: [=========================     ] 64/75 batches, loss: 0.6935Epoch 12/15: [==========================    ] 65/75 batches, loss: 0.6935Epoch 12/15: [==========================    ] 66/75 batches, loss: 0.6934Epoch 12/15: [==========================    ] 67/75 batches, loss: 0.6934Epoch 12/15: [===========================   ] 68/75 batches, loss: 0.6934Epoch 12/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 12/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 12/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 12/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 12/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 12/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 12/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-04 10:47:39,608][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.6933
[2025-05-04 10:47:39,829][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6922, Metrics: {'accuracy': 0.4782608695652174, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:47:39,830][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 13/15: [Epoch 13/15: [                              ] 1/75 batches, loss: 0.6933Epoch 13/15: [                              ] 2/75 batches, loss: 0.6932Epoch 13/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 13/15: [=                             ] 4/75 batches, loss: 0.6932Epoch 13/15: [==                            ] 5/75 batches, loss: 0.6933Epoch 13/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 13/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 13/15: [===                           ] 8/75 batches, loss: 0.6929Epoch 13/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 13/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 13/15: [====                          ] 11/75 batches, loss: 0.6927Epoch 13/15: [====                          ] 12/75 batches, loss: 0.6926Epoch 13/15: [=====                         ] 13/75 batches, loss: 0.6927Epoch 13/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 13/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 13/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 13/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 13/15: [=======                       ] 18/75 batches, loss: 0.6927Epoch 13/15: [=======                       ] 19/75 batches, loss: 0.6927Epoch 13/15: [========                      ] 20/75 batches, loss: 0.6926Epoch 13/15: [========                      ] 21/75 batches, loss: 0.6924Epoch 13/15: [========                      ] 22/75 batches, loss: 0.6925Epoch 13/15: [=========                     ] 23/75 batches, loss: 0.6926Epoch 13/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 13/15: [==========                    ] 25/75 batches, loss: 0.6926Epoch 13/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 13/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 13/15: [===========                   ] 28/75 batches, loss: 0.6926Epoch 13/15: [===========                   ] 29/75 batches, loss: 0.6925Epoch 13/15: [============                  ] 30/75 batches, loss: 0.6925Epoch 13/15: [============                  ] 31/75 batches, loss: 0.6925Epoch 13/15: [============                  ] 32/75 batches, loss: 0.6923Epoch 13/15: [=============                 ] 33/75 batches, loss: 0.6923Epoch 13/15: [=============                 ] 34/75 batches, loss: 0.6923Epoch 13/15: [==============                ] 35/75 batches, loss: 0.6922Epoch 13/15: [==============                ] 36/75 batches, loss: 0.6923Epoch 13/15: [==============                ] 37/75 batches, loss: 0.6921Epoch 13/15: [===============               ] 38/75 batches, loss: 0.6921Epoch 13/15: [===============               ] 39/75 batches, loss: 0.6921Epoch 13/15: [================              ] 40/75 batches, loss: 0.6921Epoch 13/15: [================              ] 41/75 batches, loss: 0.6921Epoch 13/15: [================              ] 42/75 batches, loss: 0.6921Epoch 13/15: [=================             ] 43/75 batches, loss: 0.6922Epoch 13/15: [=================             ] 44/75 batches, loss: 0.6922Epoch 13/15: [==================            ] 45/75 batches, loss: 0.6922Epoch 13/15: [==================            ] 46/75 batches, loss: 0.6922Epoch 13/15: [==================            ] 47/75 batches, loss: 0.6921Epoch 13/15: [===================           ] 48/75 batches, loss: 0.6921Epoch 13/15: [===================           ] 49/75 batches, loss: 0.6921Epoch 13/15: [====================          ] 50/75 batches, loss: 0.6922Epoch 13/15: [====================          ] 51/75 batches, loss: 0.6921Epoch 13/15: [====================          ] 52/75 batches, loss: 0.6921Epoch 13/15: [=====================         ] 53/75 batches, loss: 0.6919Epoch 13/15: [=====================         ] 54/75 batches, loss: 0.6921Epoch 13/15: [======================        ] 55/75 batches, loss: 0.6920Epoch 13/15: [======================        ] 56/75 batches, loss: 0.6920Epoch 13/15: [======================        ] 57/75 batches, loss: 0.6920Epoch 13/15: [=======================       ] 58/75 batches, loss: 0.6920Epoch 13/15: [=======================       ] 59/75 batches, loss: 0.6925Epoch 13/15: [========================      ] 60/75 batches, loss: 0.6926Epoch 13/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 13/15: [========================      ] 62/75 batches, loss: 0.6929Epoch 13/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 13/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 13/15: [==========================    ] 65/75 batches, loss: 0.6929Epoch 13/15: [==========================    ] 66/75 batches, loss: 0.6929Epoch 13/15: [==========================    ] 67/75 batches, loss: 0.6929Epoch 13/15: [===========================   ] 68/75 batches, loss: 0.6927Epoch 13/15: [===========================   ] 69/75 batches, loss: 0.6924Epoch 13/15: [============================  ] 70/75 batches, loss: 0.6925Epoch 13/15: [============================  ] 71/75 batches, loss: 0.6925Epoch 13/15: [============================  ] 72/75 batches, loss: 0.6923Epoch 13/15: [============================= ] 73/75 batches, loss: 0.6924Epoch 13/15: [============================= ] 74/75 batches, loss: 0.6923Epoch 13/15: [==============================] 75/75 batches, loss: 0.6924
[2025-05-04 10:47:42,106][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.6924
[2025-05-04 10:47:42,315][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.6634, Metrics: {'accuracy': 0.5217391304347826, 'f1': 0.15384615384615385, 'precision': 1.0, 'recall': 0.08333333333333333}
[2025-05-04 10:47:42,316][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:47:42,316][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 13
[2025-05-04 10:47:42,316][src.training.lm_trainer][INFO] - Training completed in 36.78 seconds
[2025-05-04 10:47:42,316][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:47:45,238][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5474391267842149, 'f1': 0.35294117647058826, 'precision': 0.620253164556962, 'recall': 0.24664429530201343}
[2025-05-04 10:47:45,239][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5869565217391305, 'f1': 0.42424242424242425, 'precision': 0.7777777777777778, 'recall': 0.2916666666666667}
[2025-05-04 10:47:45,239][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.391304347826087, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:47:46,936][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ja/ja/model.pt
[2025-05-04 10:47:46,937][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁▁▁▁█
wandb:           best_val_f1 ▁▁▁▁▁▁▁▁█
wandb:         best_val_loss ██████▇▇▁
wandb:    best_val_precision ▁▁▁▁▁▁▁▁█
wandb:       best_val_recall ▁▁▁▁▁▁▁▁█
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁▂▁▁
wandb:            train_loss █▄▄▄▄▅▅▄▃▁▅▅▃
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁█▁▁▄
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁█▁▁▄
wandb:              val_loss ███████▇▇▁▅█▂
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▆▁▁█
wandb:            val_recall ▁▁▁▁▁▁▁▁▁█▁▁▃
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.58696
wandb:           best_val_f1 0.42424
wandb:         best_val_loss 0.65889
wandb:    best_val_precision 0.77778
wandb:       best_val_recall 0.29167
wandb:      early_stop_epoch 13
wandb:                 epoch 13
wandb:   final_test_accuracy 0.3913
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.54744
wandb:        final_train_f1 0.35294
wandb: final_train_precision 0.62025
wandb:    final_train_recall 0.24664
wandb:    final_val_accuracy 0.58696
wandb:          final_val_f1 0.42424
wandb:   final_val_precision 0.77778
wandb:      final_val_recall 0.29167
wandb:         learning_rate 0.0001
wandb:            train_loss 0.6924
wandb:            train_time 36.78165
wandb:          val_accuracy 0.52174
wandb:                val_f1 0.15385
wandb:              val_loss 0.66344
wandb:         val_precision 1
wandb:            val_recall 0.08333
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104650-rrvx9fp1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104650-rrvx9fp1/logs
Experiment probe_layer10_question_type_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_complexity_control1_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control1_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:48:08,925][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ja
experiment_name: probe_layer10_complexity_control1_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:48:08,925][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:48:08,925][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:48:08,925][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:48:08,931][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-04 10:48:08,932][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:48:12,118][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:48:14,390][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:48:14,390][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:48:14,516][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:48:14,552][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:48:14,736][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:48:14,745][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:48:14,745][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:48:14,746][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:48:14,788][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:48:14,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:48:14,867][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:48:14,868][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:48:14,868][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:48:14,869][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:48:14,957][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:48:15,054][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:48:15,071][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:48:15,072][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:48:15,072][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:48:15,074][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:48:15,075][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:48:15,075][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:48:15,075][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:48:15,075][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:48:15,075][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:48:15,075][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-04 10:48:15,075][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Sample label: 0.5826417803764343
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:48:15,076][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:48:15,076][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:48:15,076][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:48:15,077][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:48:15,077][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:48:15,077][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:48:15,078][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:48:15,078][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:48:15,078][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:48:21,239][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:48:21,240][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:48:21,240][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:48:21,240][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:48:21,243][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:48:21,243][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:48:21,244][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:48:21,244][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:48:21,244][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:48:21,245][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:48:21,245][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3825Epoch 1/15: [                              ] 2/75 batches, loss: 0.6551Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6367Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6265Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5688Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5170Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5345Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5259Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5341Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5506Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5100Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4938Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4937Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4912Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4845Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4917Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4855Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4799Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4788Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4824Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4705Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4727Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4739Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4663Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4652Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4605Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4613Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4561Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4496Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4436Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4365Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4323Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4253Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4208Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4229Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4210Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4132Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4088Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4068Epoch 1/15: [================              ] 40/75 batches, loss: 0.4124Epoch 1/15: [================              ] 41/75 batches, loss: 0.4066Epoch 1/15: [================              ] 42/75 batches, loss: 0.4037Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3979Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3949Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3953Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3913Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3876Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3860Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3828Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3788Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3759Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3733Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3710Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3692Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3677Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3679Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3648Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3642Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3606Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3583Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3542Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3511Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3505Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3493Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3462Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3449Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3421Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3395Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3394Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3370Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3348Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3332Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3309Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3296Epoch 1/15: [==============================] 75/75 batches, loss: 0.3299
[2025-05-04 10:48:27,120][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3299
[2025-05-04 10:48:27,336][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0618, Metrics: {'mse': 0.06217161938548088, 'rmse': 0.24934237382659385, 'r2': -0.013268351554870605}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1936Epoch 2/15: [                              ] 2/75 batches, loss: 0.2345Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2385Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2420Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2269Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2126Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2011Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1922Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1953Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1969Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1863Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1858Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1822Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1778Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1777Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1715Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1736Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1731Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1723Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1737Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1736Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1684Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1672Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1661Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1628Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1679Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1741Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1735Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1720Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1715Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1699Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1701Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1723Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1711Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1709Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1699Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1693Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1684Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1664Epoch 2/15: [================              ] 40/75 batches, loss: 0.1672Epoch 2/15: [================              ] 41/75 batches, loss: 0.1653Epoch 2/15: [================              ] 42/75 batches, loss: 0.1652Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1637Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1652Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1653Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1634Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1633Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1639Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1641Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1625Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1632Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1626Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1647Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1641Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1629Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1623Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1620Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1615Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1615Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1598Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1585Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1591Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1587Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1592Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1574Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1564Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1560Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1559Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1544Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1538Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1534Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1534Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1532Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1520Epoch 2/15: [==============================] 75/75 batches, loss: 0.1515
[2025-05-04 10:48:30,043][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1515
[2025-05-04 10:48:30,236][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0751, Metrics: {'mse': 0.07488859444856644, 'rmse': 0.27365780538578915, 'r2': -0.2205287218093872}
[2025-05-04 10:48:30,237][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.2083Epoch 3/15: [                              ] 2/75 batches, loss: 0.1435Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1422Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1312Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1461Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1507Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1419Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1392Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1323Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1248Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1277Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1236Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1242Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1220Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1219Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1235Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1218Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1260Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1242Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1228Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1240Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1229Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1236Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1234Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1217Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1210Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1213Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1267Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1253Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1261Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1258Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1267Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1274Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1257Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1243Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1254Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1270Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1253Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1252Epoch 3/15: [================              ] 40/75 batches, loss: 0.1247Epoch 3/15: [================              ] 41/75 batches, loss: 0.1258Epoch 3/15: [================              ] 42/75 batches, loss: 0.1258Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1252Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1237Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1251Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1230Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1223Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1221Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1234Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1225Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1212Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1204Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1189Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1182Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1177Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1167Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1161Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1165Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1174Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1183Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1179Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1172Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1172Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1169Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1178Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1169Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1173Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1178Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1172Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1169Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1170Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1167Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1168Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1163Epoch 3/15: [==============================] 75/75 batches, loss: 0.1163
[2025-05-04 10:48:32,508][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1163
[2025-05-04 10:48:32,733][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0826, Metrics: {'mse': 0.08240079134702682, 'rmse': 0.2870553802788354, 'r2': -0.34296202659606934}
[2025-05-04 10:48:32,733][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0424Epoch 4/15: [                              ] 2/75 batches, loss: 0.0645Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0829Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0795Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0768Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0849Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0867Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0875Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1003Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0977Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1048Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1046Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1050Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1026Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1006Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1010Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1022Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1015Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1018Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0999Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0991Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1001Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0979Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0978Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0980Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0962Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0977Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0977Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0977Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0964Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0969Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0961Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0953Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0939Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0934Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0928Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0922Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0905Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0906Epoch 4/15: [================              ] 40/75 batches, loss: 0.0942Epoch 4/15: [================              ] 41/75 batches, loss: 0.0945Epoch 4/15: [================              ] 42/75 batches, loss: 0.0944Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0953Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0962Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0965Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0965Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0961Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0964Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0959Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0951Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0951Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0952Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0948Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0945Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0942Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0937Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0932Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0939Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0941Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0942Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0936Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0935Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0934Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0929Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0929Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0928Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0930Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0936Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0940Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0938Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0934Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0930Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0926Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0929Epoch 4/15: [==============================] 75/75 batches, loss: 0.0933
[2025-05-04 10:48:35,050][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0933
[2025-05-04 10:48:35,260][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0796, Metrics: {'mse': 0.07947086542844772, 'rmse': 0.281905774024669, 'r2': -0.29521024227142334}
[2025-05-04 10:48:35,261][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1496Epoch 5/15: [                              ] 2/75 batches, loss: 0.1307Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0971Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0943Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0863Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0918Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0919Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0858Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0853Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0912Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0959Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0973Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0934Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0961Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0967Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0929Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0911Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0907Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0907Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0913Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0911Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0920Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0928Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0919Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0915Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0909Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0906Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0908Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0906Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0903Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0889Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0886Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0899Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0911Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0915Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0910Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0916Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0916Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0908Epoch 5/15: [================              ] 40/75 batches, loss: 0.0907Epoch 5/15: [================              ] 41/75 batches, loss: 0.0901Epoch 5/15: [================              ] 42/75 batches, loss: 0.0914Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0905Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0898Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0886Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0891Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0894Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0889Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0885Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0884Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0883Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0882Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0881Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0882Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0881Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0876Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0875Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0878Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0880Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0886Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0885Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0881Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0879Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0879Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0888Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0889Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0883Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0882Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0879Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0876Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0873Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0876Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0876Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0878Epoch 5/15: [==============================] 75/75 batches, loss: 0.0882
[2025-05-04 10:48:37,596][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0882
[2025-05-04 10:48:37,811][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0809, Metrics: {'mse': 0.08071509003639221, 'rmse': 0.28410401270730445, 'r2': -0.3154885768890381}
[2025-05-04 10:48:37,811][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:48:37,811][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:48:37,811][src.training.lm_trainer][INFO] - Training completed in 13.87 seconds
[2025-05-04 10:48:37,812][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:48:40,675][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.05725177004933357, 'rmse': 0.239273421109269, 'r2': -0.42896008491516113}
[2025-05-04 10:48:40,676][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06217161938548088, 'rmse': 0.24934237382659385, 'r2': -0.013268351554870605}
[2025-05-04 10:48:40,676][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0576142743229866, 'rmse': 0.24002973633070257, 'r2': -0.1067206859588623}
[2025-05-04 10:48:42,335][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ja/ja/model.pt
[2025-05-04 10:48:42,336][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▂▁▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▅█▇▇
wandb:          val_mse ▁▅█▇▇
wandb:           val_r2 █▄▁▂▂
wandb:         val_rmse ▁▆█▇▇
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0618
wandb:     best_val_mse 0.06217
wandb:      best_val_r2 -0.01327
wandb:    best_val_rmse 0.24934
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05761
wandb:    final_test_r2 -0.10672
wandb:  final_test_rmse 0.24003
wandb:  final_train_mse 0.05725
wandb:   final_train_r2 -0.42896
wandb: final_train_rmse 0.23927
wandb:    final_val_mse 0.06217
wandb:     final_val_r2 -0.01327
wandb:   final_val_rmse 0.24934
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08823
wandb:       train_time 13.86839
wandb:         val_loss 0.08085
wandb:          val_mse 0.08072
wandb:           val_r2 -0.31549
wandb:         val_rmse 0.2841
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104808-j6dhsuw9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104808-j6dhsuw9/logs
Experiment probe_layer10_complexity_control1_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:48:59,706][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ja
experiment_name: probe_layer10_complexity_control2_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:48:59,706][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:48:59,706][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:48:59,707][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:48:59,711][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-04 10:48:59,712][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:49:02,271][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:49:04,754][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:49:04,754][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:04,880][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:49:04,966][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:49:05,072][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:49:05,081][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:05,082][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:49:05,083][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:05,121][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:05,166][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:05,179][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:49:05,181][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:05,181][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:49:05,182][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:05,241][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:05,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:05,349][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:49:05,351][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:05,351][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:49:05,352][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:49:05,352][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:05,352][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:05,353][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:05,353][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Sample label: 0.5349239110946655
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:05,353][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:05,354][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:05,354][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:05,354][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:05,354][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:05,355][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-04 10:49:05,355][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:49:05,355][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-04 10:49:05,355][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:49:05,355][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:49:05,355][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:49:05,355][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:49:05,355][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:49:10,301][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:49:10,302][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:49:10,302][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:49:10,302][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:49:10,305][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:49:10,305][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:49:10,305][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:49:10,306][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:49:10,306][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:49:10,307][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:49:10,307][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.4375Epoch 1/15: [                              ] 2/75 batches, loss: 0.5862Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6089Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6307Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5691Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5088Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5108Epoch 1/15: [===                           ] 8/75 batches, loss: 0.4982Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5090Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5299Epoch 1/15: [====                          ] 11/75 batches, loss: 0.4978Epoch 1/15: [====                          ] 12/75 batches, loss: 0.4890Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4934Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4853Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4805Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4786Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4825Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4775Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4769Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4696Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4606Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4613Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4568Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4487Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4428Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4365Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4372Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4308Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4279Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4231Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4181Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4134Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4069Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4038Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4126Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4110Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4054Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4003Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3956Epoch 1/15: [================              ] 40/75 batches, loss: 0.4026Epoch 1/15: [================              ] 41/75 batches, loss: 0.3967Epoch 1/15: [================              ] 42/75 batches, loss: 0.3941Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3907Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3906Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3910Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3887Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3836Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3814Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3786Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3743Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3721Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3706Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3670Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3627Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3608Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3605Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3578Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3567Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3537Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3504Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3466Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3440Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3416Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3415Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3397Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3380Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3369Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3343Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3333Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3302Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3288Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3277Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3263Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3246Epoch 1/15: [==============================] 75/75 batches, loss: 0.3231
[2025-05-04 10:49:17,437][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3231
[2025-05-04 10:49:17,643][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0632, Metrics: {'mse': 0.06354910135269165, 'rmse': 0.2520894709278665, 'r2': -0.035718560218811035}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2544Epoch 2/15: [                              ] 2/75 batches, loss: 0.1971Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2170Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2077Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1879Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1746Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1773Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1811Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1876Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1918Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1815Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1750Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1786Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1756Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1749Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1703Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1728Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1680Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1635Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1617Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1599Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1572Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1558Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1534Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1523Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1569Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1649Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1629Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1638Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1626Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1630Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1617Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1643Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1647Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1639Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1628Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1621Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1620Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1609Epoch 2/15: [================              ] 40/75 batches, loss: 0.1601Epoch 2/15: [================              ] 41/75 batches, loss: 0.1587Epoch 2/15: [================              ] 42/75 batches, loss: 0.1576Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1584Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1599Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1576Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1557Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1541Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1566Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1562Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1545Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1556Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1557Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1581Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1577Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1573Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1568Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1568Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1557Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1552Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1544Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1534Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1538Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1540Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1551Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1539Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1530Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1522Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1519Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1506Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1514Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1509Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1514Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1506Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1505Epoch 2/15: [==============================] 75/75 batches, loss: 0.1495
[2025-05-04 10:49:20,306][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1495
[2025-05-04 10:49:20,542][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0708, Metrics: {'mse': 0.07059626281261444, 'rmse': 0.2656995724735259, 'r2': -0.1505725383758545}
[2025-05-04 10:49:20,543][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1312Epoch 3/15: [                              ] 2/75 batches, loss: 0.1311Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1260Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1248Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1539Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1623Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1494Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1464Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1385Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1328Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1394Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1433Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1384Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1362Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1353Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1346Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1342Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1362Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1315Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1319Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1320Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1296Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1264Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1260Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1235Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1251Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1224Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1264Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1240Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1216Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1215Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1236Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1239Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1227Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1222Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1238Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1232Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1226Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1214Epoch 3/15: [================              ] 40/75 batches, loss: 0.1219Epoch 3/15: [================              ] 41/75 batches, loss: 0.1220Epoch 3/15: [================              ] 42/75 batches, loss: 0.1201Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1197Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1186Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1202Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1186Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1184Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1184Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1188Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1178Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1166Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1161Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1148Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1145Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1138Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1129Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1140Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1145Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1147Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1153Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1151Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1142Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1134Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1139Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1137Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1129Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1126Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1129Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1132Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1134Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1133Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1134Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1135Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1129Epoch 3/15: [==============================] 75/75 batches, loss: 0.1120
[2025-05-04 10:49:22,846][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1120
[2025-05-04 10:49:23,054][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0831, Metrics: {'mse': 0.08285067230463028, 'rmse': 0.28783792714760553, 'r2': -0.35029399394989014}
[2025-05-04 10:49:23,055][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0594Epoch 4/15: [                              ] 2/75 batches, loss: 0.1282Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1273Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1174Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1064Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0986Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1021Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0928Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0999Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1015Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1052Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1036Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1020Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0998Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0997Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1032Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1037Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1034Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1027Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1010Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1020Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1033Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1036Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1027Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1020Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1011Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1005Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1023Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1029Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1034Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1024Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1024Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1026Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1025Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1023Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1009Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0994Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0978Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0985Epoch 4/15: [================              ] 40/75 batches, loss: 0.1010Epoch 4/15: [================              ] 41/75 batches, loss: 0.1004Epoch 4/15: [================              ] 42/75 batches, loss: 0.0999Epoch 4/15: [=================             ] 43/75 batches, loss: 0.1002Epoch 4/15: [=================             ] 44/75 batches, loss: 0.1009Epoch 4/15: [==================            ] 45/75 batches, loss: 0.1003Epoch 4/15: [==================            ] 46/75 batches, loss: 0.1001Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0998Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0999Epoch 4/15: [===================           ] 49/75 batches, loss: 0.1009Epoch 4/15: [====================          ] 50/75 batches, loss: 0.1005Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0998Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0988Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0994Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0994Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0992Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0990Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0986Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0994Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0996Epoch 4/15: [========================      ] 60/75 batches, loss: 0.1003Epoch 4/15: [========================      ] 61/75 batches, loss: 0.1006Epoch 4/15: [========================      ] 62/75 batches, loss: 0.1009Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.1015Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.1013Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.1012Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.1011Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.1010Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.1020Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.1018Epoch 4/15: [============================  ] 70/75 batches, loss: 0.1020Epoch 4/15: [============================  ] 71/75 batches, loss: 0.1014Epoch 4/15: [============================  ] 72/75 batches, loss: 0.1010Epoch 4/15: [============================= ] 73/75 batches, loss: 0.1011Epoch 4/15: [============================= ] 74/75 batches, loss: 0.1006Epoch 4/15: [==============================] 75/75 batches, loss: 0.1007
[2025-05-04 10:49:25,337][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1007
[2025-05-04 10:49:25,544][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0809, Metrics: {'mse': 0.08068962395191193, 'rmse': 0.2840591909301861, 'r2': -0.3150733709335327}
[2025-05-04 10:49:25,545][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.1225Epoch 5/15: [                              ] 2/75 batches, loss: 0.1051Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0990Epoch 5/15: [=                             ] 4/75 batches, loss: 0.1071Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0972Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0867Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0917Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0958Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0940Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0964Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0971Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0978Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0996Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0973Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0989Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0968Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0948Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0955Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0970Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0973Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0957Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0939Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0930Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0927Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0931Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0908Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0906Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0900Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0899Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0889Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0894Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0897Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0894Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0917Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0915Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0912Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0911Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0921Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0933Epoch 5/15: [================              ] 40/75 batches, loss: 0.0925Epoch 5/15: [================              ] 41/75 batches, loss: 0.0915Epoch 5/15: [================              ] 42/75 batches, loss: 0.0921Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0914Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0908Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0900Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0899Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0894Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0895Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0890Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0885Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0878Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0878Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0873Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0869Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0866Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0868Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0867Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0865Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0866Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0887Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0895Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0898Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0905Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0900Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0905Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0904Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0904Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0900Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0896Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0892Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0892Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0891Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0887Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0882Epoch 5/15: [==============================] 75/75 batches, loss: 0.0883
[2025-05-04 10:49:27,815][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0883
[2025-05-04 10:49:28,020][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0907, Metrics: {'mse': 0.09043025970458984, 'rmse': 0.3007162444973498, 'r2': -0.4738255739212036}
[2025-05-04 10:49:28,021][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:49:28,021][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:49:28,021][src.training.lm_trainer][INFO] - Training completed in 13.83 seconds
[2025-05-04 10:49:28,021][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:49:30,831][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.06467796117067337, 'rmse': 0.2543186213604371, 'r2': -0.6143122911453247}
[2025-05-04 10:49:30,831][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06354910135269165, 'rmse': 0.2520894709278665, 'r2': -0.035718560218811035}
[2025-05-04 10:49:30,832][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05564209818840027, 'rmse': 0.2358857736032427, 'r2': -0.06883680820465088}
[2025-05-04 10:49:32,508][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ja/ja/model.pt
[2025-05-04 10:49:32,509][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▄▃▁▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▃▆▆█
wandb:          val_mse ▁▃▆▅█
wandb:           val_r2 █▆▃▄▁
wandb:         val_rmse ▁▃▆▆█
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06318
wandb:     best_val_mse 0.06355
wandb:      best_val_r2 -0.03572
wandb:    best_val_rmse 0.25209
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.05564
wandb:    final_test_r2 -0.06884
wandb:  final_test_rmse 0.23589
wandb:  final_train_mse 0.06468
wandb:   final_train_r2 -0.61431
wandb: final_train_rmse 0.25432
wandb:    final_val_mse 0.06355
wandb:     final_val_r2 -0.03572
wandb:   final_val_rmse 0.25209
wandb:    learning_rate 0.0001
wandb:       train_loss 0.08831
wandb:       train_time 13.82694
wandb:         val_loss 0.0907
wandb:          val_mse 0.09043
wandb:           val_r2 -0.47383
wandb:         val_rmse 0.30072
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104859-4x1pf87g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104859-4x1pf87g/logs
Experiment probe_layer10_complexity_control2_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_ja
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ja]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_ja"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ja"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:49:51,593][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ja
experiment_name: probe_layer10_complexity_control3_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ja
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:49:51,593][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:49:51,593][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:49:51,593][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:49:51,599][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ja']
[2025-05-04 10:49:51,600][__main__][INFO] - Processing language: ja
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:49:54,589][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:49:56,934][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:49:56,934][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:57,209][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:49:57,319][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:49:57,451][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-05-04 10:49:57,459][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:57,460][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-05-04 10:49:57,461][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:57,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:57,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:57,678][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-05-04 10:49:57,679][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:57,680][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-05-04 10:49:57,681][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:49:57,753][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:57,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:49:57,843][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-05-04 10:49:57,844][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:49:57,845][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-05-04 10:49:57,846][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-05-04 10:49:57,847][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:57,847][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:57,847][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:57,847][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:57,847][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:57,848][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Sample label: 0.2807745635509491
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:57,848][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:57,848][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:57,848][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:49:57,849][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:49:57,849][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-05-04 10:49:57,849][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-05-04 10:49:57,850][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:49:57,850][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:49:57,850][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:49:57,850][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:50:03,433][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:50:03,434][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:50:03,434][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:50:03,435][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:50:03,437][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:50:03,438][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:50:03,438][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:50:03,438][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:50:03,438][__main__][INFO] - Successfully created lm_probe model for ja
[2025-05-04 10:50:03,439][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:50:03,439][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3928Epoch 1/15: [                              ] 2/75 batches, loss: 0.6698Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6574Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6477Epoch 1/15: [==                            ] 5/75 batches, loss: 0.5925Epoch 1/15: [==                            ] 6/75 batches, loss: 0.5358Epoch 1/15: [==                            ] 7/75 batches, loss: 0.5575Epoch 1/15: [===                           ] 8/75 batches, loss: 0.5427Epoch 1/15: [===                           ] 9/75 batches, loss: 0.5555Epoch 1/15: [====                          ] 10/75 batches, loss: 0.5548Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5197Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5020Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.4964Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.4828Epoch 1/15: [======                        ] 15/75 batches, loss: 0.4843Epoch 1/15: [======                        ] 16/75 batches, loss: 0.4873Epoch 1/15: [======                        ] 17/75 batches, loss: 0.4839Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4811Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4793Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4750Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4682Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4721Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4658Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4553Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4496Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4411Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4429Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4376Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4357Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4340Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4266Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4264Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4193Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4162Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4165Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4158Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4106Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4059Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4013Epoch 1/15: [================              ] 40/75 batches, loss: 0.4061Epoch 1/15: [================              ] 41/75 batches, loss: 0.3997Epoch 1/15: [================              ] 42/75 batches, loss: 0.3965Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3923Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3927Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3918Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3877Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3822Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3802Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3775Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3733Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3735Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3700Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3670Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3662Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3668Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3661Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3628Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3611Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3580Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3545Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3500Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3473Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3446Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3432Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3418Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3403Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3384Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3353Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3342Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3318Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3295Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3289Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3270Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3268Epoch 1/15: [==============================] 75/75 batches, loss: 0.3247
[2025-05-04 10:50:09,251][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3247
[2025-05-04 10:50:09,434][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0572, Metrics: {'mse': 0.05731366574764252, 'rmse': 0.23940272710986923, 'r2': 0.06590616703033447}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2600Epoch 2/15: [                              ] 2/75 batches, loss: 0.2630Epoch 2/15: [=                             ] 3/75 batches, loss: 0.2576Epoch 2/15: [=                             ] 4/75 batches, loss: 0.2447Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2166Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1985Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1892Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1872Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1925Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1861Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1759Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1710Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1679Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1675Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1700Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1652Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1644Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1593Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1588Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1588Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1556Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1559Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1543Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1514Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1487Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1527Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1581Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1603Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1584Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1588Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1595Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1588Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1587Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1570Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1580Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1559Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1584Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1592Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1584Epoch 2/15: [================              ] 40/75 batches, loss: 0.1592Epoch 2/15: [================              ] 41/75 batches, loss: 0.1573Epoch 2/15: [================              ] 42/75 batches, loss: 0.1563Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1540Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1559Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1545Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1537Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1536Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1555Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1555Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1545Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1550Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1547Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1560Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1554Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1552Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1550Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1545Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1535Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1535Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1536Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1524Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1530Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1519Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1536Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1533Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1531Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1525Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1521Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1510Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1502Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1499Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1506Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1494Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1489Epoch 2/15: [==============================] 75/75 batches, loss: 0.1486
[2025-05-04 10:50:12,107][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1486
[2025-05-04 10:50:12,316][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0672, Metrics: {'mse': 0.06710412353277206, 'rmse': 0.25904463617834683, 'r2': -0.09365808963775635}
[2025-05-04 10:50:12,317][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1644Epoch 3/15: [                              ] 2/75 batches, loss: 0.1343Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1331Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1220Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1422Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1405Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1308Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1357Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1333Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1261Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1349Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1300Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1249Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1213Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1227Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1244Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1220Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1207Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1204Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1200Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1188Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1195Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1185Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1179Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1150Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1168Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1165Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1172Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1168Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1173Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1155Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1181Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1230Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1216Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1197Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1199Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1185Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1173Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1170Epoch 3/15: [================              ] 40/75 batches, loss: 0.1189Epoch 3/15: [================              ] 41/75 batches, loss: 0.1202Epoch 3/15: [================              ] 42/75 batches, loss: 0.1201Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1197Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1197Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1225Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1205Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1200Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1210Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1218Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1211Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1209Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1203Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1188Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1181Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1171Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1161Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1159Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1158Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1173Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1177Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1169Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1171Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1168Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1164Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1161Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1162Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1167Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1166Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1167Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1164Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1160Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1155Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1159Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1150Epoch 3/15: [==============================] 75/75 batches, loss: 0.1141
[2025-05-04 10:50:14,581][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1141
[2025-05-04 10:50:14,797][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0755, Metrics: {'mse': 0.07536915689706802, 'rmse': 0.27453443663239774, 'r2': -0.22836089134216309}
[2025-05-04 10:50:14,797][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0996Epoch 4/15: [                              ] 2/75 batches, loss: 0.1277Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1064Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1050Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1015Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0945Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0931Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0872Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1016Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1077Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1101Epoch 4/15: [====                          ] 12/75 batches, loss: 0.1094Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.1068Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.1057Epoch 4/15: [======                        ] 15/75 batches, loss: 0.1065Epoch 4/15: [======                        ] 16/75 batches, loss: 0.1065Epoch 4/15: [======                        ] 17/75 batches, loss: 0.1066Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.1106Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.1097Epoch 4/15: [========                      ] 20/75 batches, loss: 0.1098Epoch 4/15: [========                      ] 21/75 batches, loss: 0.1118Epoch 4/15: [========                      ] 22/75 batches, loss: 0.1118Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.1091Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.1067Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.1067Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.1064Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.1082Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.1120Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.1098Epoch 4/15: [============                  ] 30/75 batches, loss: 0.1084Epoch 4/15: [============                  ] 31/75 batches, loss: 0.1077Epoch 4/15: [============                  ] 32/75 batches, loss: 0.1059Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.1055Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.1048Epoch 4/15: [==============                ] 35/75 batches, loss: 0.1036Epoch 4/15: [==============                ] 36/75 batches, loss: 0.1023Epoch 4/15: [==============                ] 37/75 batches, loss: 0.1018Epoch 4/15: [===============               ] 38/75 batches, loss: 0.1014Epoch 4/15: [===============               ] 39/75 batches, loss: 0.1019Epoch 4/15: [================              ] 40/75 batches, loss: 0.1028Epoch 4/15: [================              ] 41/75 batches, loss: 0.1018Epoch 4/15: [================              ] 42/75 batches, loss: 0.1002Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0992Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0987Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0984Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0986Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0976Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0968Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0974Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0970Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0965Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0954Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0962Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0959Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0958Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0952Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0951Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0948Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0947Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0950Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0949Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0955Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0951Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0946Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0952Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0961Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0962Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0966Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0973Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0966Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0961Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0955Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0952Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0949Epoch 4/15: [==============================] 75/75 batches, loss: 0.0948
[2025-05-04 10:50:17,063][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0948
[2025-05-04 10:50:17,258][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0821, Metrics: {'mse': 0.08188624680042267, 'rmse': 0.2861577306319413, 'r2': -0.3345760107040405}
[2025-05-04 10:50:17,259][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0634Epoch 5/15: [                              ] 2/75 batches, loss: 0.0571Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0604Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0701Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0741Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0909Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0947Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0960Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0962Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0976Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0981Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0953Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0932Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0923Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0945Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0936Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0926Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0948Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0926Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0926Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0909Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0905Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0892Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0879Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0876Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0890Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0881Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0877Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0883Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0880Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0880Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0880Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0871Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0890Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0893Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0896Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0895Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0901Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0923Epoch 5/15: [================              ] 40/75 batches, loss: 0.0922Epoch 5/15: [================              ] 41/75 batches, loss: 0.0924Epoch 5/15: [================              ] 42/75 batches, loss: 0.0949Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0942Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0927Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0923Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0917Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0910Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0906Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0908Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0901Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0900Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0908Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0904Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0898Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0892Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0898Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0894Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0897Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0897Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0898Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0908Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0910Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0911Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0910Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0921Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0922Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0924Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0917Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0911Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0919Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0922Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0923Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0922Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0925Epoch 5/15: [==============================] 75/75 batches, loss: 0.0917
[2025-05-04 10:50:19,530][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0917
[2025-05-04 10:50:19,729][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0764, Metrics: {'mse': 0.07624717056751251, 'rmse': 0.27612890208653007, 'r2': -0.24267077445983887}
[2025-05-04 10:50:19,729][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:50:19,729][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:50:19,729][src.training.lm_trainer][INFO] - Training completed in 13.68 seconds
[2025-05-04 10:50:19,730][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:50:22,537][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.049024444073438644, 'rmse': 0.22141464286139398, 'r2': -0.22361254692077637}
[2025-05-04 10:50:22,537][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.05731366574764252, 'rmse': 0.23940272710986923, 'r2': 0.06590616703033447}
[2025-05-04 10:50:22,537][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.06395196169614792, 'rmse': 0.2528872509561285, 'r2': -0.22846221923828125}
[2025-05-04 10:50:24,255][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ja/ja/model.pt
[2025-05-04 10:50:24,256][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▃▂▁
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▄▆█▆
wandb:          val_mse ▁▄▆█▆
wandb:           val_r2 █▅▃▁▃
wandb:         val_rmse ▁▄▆█▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05723
wandb:     best_val_mse 0.05731
wandb:      best_val_r2 0.06591
wandb:    best_val_rmse 0.2394
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.06395
wandb:    final_test_r2 -0.22846
wandb:  final_test_rmse 0.25289
wandb:  final_train_mse 0.04902
wandb:   final_train_r2 -0.22361
wandb: final_train_rmse 0.22141
wandb:    final_val_mse 0.05731
wandb:     final_val_r2 0.06591
wandb:   final_val_rmse 0.2394
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0917
wandb:       train_time 13.68344
wandb:         val_loss 0.07635
wandb:          val_mse 0.07625
wandb:           val_r2 -0.24267
wandb:         val_rmse 0.27613
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104951-1x8isz64
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_104951-1x8isz64/logs
Experiment probe_layer10_complexity_control3_ja completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ja/ja/results.json for layer 10
Running experiment: probe_layer10_question_type_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:50:41,268][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ko
experiment_name: probe_layer10_question_type_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:50:41,268][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:50:41,268][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:50:41,268][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:50:41,274][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-05-04 10:50:41,274][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:50:43,287][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:50:45,500][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:50:45,501][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:50:45,590][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:50:45,625][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:50:45,754][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:50:45,760][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:50:45,761][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:50:45,762][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:50:45,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:50:45,866][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:50:45,913][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:50:45,915][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:50:45,915][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:50:45,916][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:50:45,948][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:50:46,025][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:50:46,039][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:50:46,040][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:50:46,040][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:50:46,041][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:50:46,042][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:50:46,042][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:50:46,042][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:50:46,042][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:50:46,043][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-05-04 10:50:46,043][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:50:46,043][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:50:46,043][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:50:46,043][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:50:46,044][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:50:46,044][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:50:46,044][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:50:46,045][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:50:46,045][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:50:46,045][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:50:50,700][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:50:50,701][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:50:50,701][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:50:50,701][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:50:50,707][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:50:50,708][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:50:50,708][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:50:50,708][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:50:50,708][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:50:50,709][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:50:50,709][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:50:50,710][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.7667Epoch 1/15: [=                             ] 2/47 batches, loss: 0.7252Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7322Epoch 1/15: [==                            ] 4/47 batches, loss: 0.7216Epoch 1/15: [===                           ] 5/47 batches, loss: 0.7144Epoch 1/15: [===                           ] 6/47 batches, loss: 0.7150Epoch 1/15: [====                          ] 7/47 batches, loss: 0.7115Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.7106Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.7087Epoch 1/15: [======                        ] 10/47 batches, loss: 0.7079Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.7056Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.7043Epoch 1/15: [========                      ] 13/47 batches, loss: 0.7040Epoch 1/15: [========                      ] 14/47 batches, loss: 0.7027Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.7020Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.7015Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.7010Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.7001Epoch 1/15: [============                  ] 19/47 batches, loss: 0.7005Epoch 1/15: [============                  ] 20/47 batches, loss: 0.7001Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.6993Epoch 1/15: [==============                ] 22/47 batches, loss: 0.6988Epoch 1/15: [==============                ] 23/47 batches, loss: 0.6982Epoch 1/15: [===============               ] 24/47 batches, loss: 0.6980Epoch 1/15: [===============               ] 25/47 batches, loss: 0.6977Epoch 1/15: [================              ] 26/47 batches, loss: 0.6973Epoch 1/15: [=================             ] 27/47 batches, loss: 0.6978Epoch 1/15: [=================             ] 28/47 batches, loss: 0.6977Epoch 1/15: [==================            ] 29/47 batches, loss: 0.6975Epoch 1/15: [===================           ] 30/47 batches, loss: 0.6980Epoch 1/15: [===================           ] 31/47 batches, loss: 0.6980Epoch 1/15: [====================          ] 32/47 batches, loss: 0.6979Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.6979Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.6978Epoch 1/15: [======================        ] 35/47 batches, loss: 0.6979Epoch 1/15: [======================        ] 36/47 batches, loss: 0.6975Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.6974Epoch 1/15: [========================      ] 38/47 batches, loss: 0.6971Epoch 1/15: [========================      ] 39/47 batches, loss: 0.6971Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.6970Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.6970Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.6969Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.6968Epoch 1/15: [============================  ] 44/47 batches, loss: 0.6967Epoch 1/15: [============================  ] 45/47 batches, loss: 0.6967Epoch 1/15: [============================= ] 46/47 batches, loss: 0.6966Epoch 1/15: [==============================] 47/47 batches, loss: 0.6964
[2025-05-04 10:50:54,739][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6964
[2025-05-04 10:50:54,979][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.6905Epoch 2/15: [=                             ] 2/47 batches, loss: 0.6921Epoch 2/15: [=                             ] 3/47 batches, loss: 0.6925Epoch 2/15: [==                            ] 4/47 batches, loss: 0.6926Epoch 2/15: [===                           ] 5/47 batches, loss: 0.6928Epoch 2/15: [===                           ] 6/47 batches, loss: 0.6928Epoch 2/15: [====                          ] 7/47 batches, loss: 0.6928Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.6937Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.6944Epoch 2/15: [======                        ] 10/47 batches, loss: 0.6943Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.6943Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.6942Epoch 2/15: [========                      ] 13/47 batches, loss: 0.6941Epoch 2/15: [========                      ] 14/47 batches, loss: 0.6939Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.6940Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.6941Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.6939Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.6939Epoch 2/15: [============                  ] 19/47 batches, loss: 0.6938Epoch 2/15: [============                  ] 20/47 batches, loss: 0.6940Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.6938Epoch 2/15: [==============                ] 22/47 batches, loss: 0.6938Epoch 2/15: [==============                ] 23/47 batches, loss: 0.6937Epoch 2/15: [===============               ] 24/47 batches, loss: 0.6937Epoch 2/15: [===============               ] 25/47 batches, loss: 0.6936Epoch 2/15: [================              ] 26/47 batches, loss: 0.6936Epoch 2/15: [=================             ] 27/47 batches, loss: 0.6934Epoch 2/15: [=================             ] 28/47 batches, loss: 0.6934Epoch 2/15: [==================            ] 29/47 batches, loss: 0.6933Epoch 2/15: [===================           ] 30/47 batches, loss: 0.6933Epoch 2/15: [===================           ] 31/47 batches, loss: 0.6933Epoch 2/15: [====================          ] 32/47 batches, loss: 0.6931Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.6931Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.6931Epoch 2/15: [======================        ] 35/47 batches, loss: 0.6931Epoch 2/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 2/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 2/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.6933Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.6933Epoch 2/15: [============================  ] 44/47 batches, loss: 0.6933Epoch 2/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 2/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 2/15: [==============================] 47/47 batches, loss: 0.6931
[2025-05-04 10:50:56,842][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6931
[2025-05-04 10:50:57,093][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.6947Epoch 3/15: [=                             ] 2/47 batches, loss: 0.6926Epoch 3/15: [=                             ] 3/47 batches, loss: 0.6927Epoch 3/15: [==                            ] 4/47 batches, loss: 0.6920Epoch 3/15: [===                           ] 5/47 batches, loss: 0.6923Epoch 3/15: [===                           ] 6/47 batches, loss: 0.6928Epoch 3/15: [====                          ] 7/47 batches, loss: 0.6929Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.6935Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.6938Epoch 3/15: [======                        ] 10/47 batches, loss: 0.6941Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.6940Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.6946Epoch 3/15: [========                      ] 13/47 batches, loss: 0.6945Epoch 3/15: [========                      ] 14/47 batches, loss: 0.6944Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.6944Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.6941Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.6939Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.6941Epoch 3/15: [============                  ] 19/47 batches, loss: 0.6940Epoch 3/15: [============                  ] 20/47 batches, loss: 0.6940Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.6940Epoch 3/15: [==============                ] 22/47 batches, loss: 0.6940Epoch 3/15: [==============                ] 23/47 batches, loss: 0.6940Epoch 3/15: [===============               ] 24/47 batches, loss: 0.6939Epoch 3/15: [===============               ] 25/47 batches, loss: 0.6939Epoch 3/15: [================              ] 26/47 batches, loss: 0.6938Epoch 3/15: [=================             ] 27/47 batches, loss: 0.6938Epoch 3/15: [=================             ] 28/47 batches, loss: 0.6938Epoch 3/15: [==================            ] 29/47 batches, loss: 0.6938Epoch 3/15: [===================           ] 30/47 batches, loss: 0.6938Epoch 3/15: [===================           ] 31/47 batches, loss: 0.6938Epoch 3/15: [====================          ] 32/47 batches, loss: 0.6938Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.6938Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.6938Epoch 3/15: [======================        ] 35/47 batches, loss: 0.6938Epoch 3/15: [======================        ] 36/47 batches, loss: 0.6938Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.6937Epoch 3/15: [========================      ] 38/47 batches, loss: 0.6937Epoch 3/15: [========================      ] 39/47 batches, loss: 0.6937Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.6937Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.6937Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.6938Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.6938Epoch 3/15: [============================  ] 44/47 batches, loss: 0.6937Epoch 3/15: [============================  ] 45/47 batches, loss: 0.6937Epoch 3/15: [============================= ] 46/47 batches, loss: 0.6937Epoch 3/15: [==============================] 47/47 batches, loss: 0.6937
[2025-05-04 10:50:58,983][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6937
[2025-05-04 10:50:59,240][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:50:59,240][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.6937Epoch 4/15: [=                             ] 2/47 batches, loss: 0.6934Epoch 4/15: [=                             ] 3/47 batches, loss: 0.6937Epoch 4/15: [==                            ] 4/47 batches, loss: 0.6934Epoch 4/15: [===                           ] 5/47 batches, loss: 0.6933Epoch 4/15: [===                           ] 6/47 batches, loss: 0.6932Epoch 4/15: [====                          ] 7/47 batches, loss: 0.6932Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.6935Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.6934Epoch 4/15: [======                        ] 10/47 batches, loss: 0.6934Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.6933Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.6933Epoch 4/15: [========                      ] 13/47 batches, loss: 0.6933Epoch 4/15: [========                      ] 14/47 batches, loss: 0.6933Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.6933Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.6933Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.6933Epoch 4/15: [==============                ] 22/47 batches, loss: 0.6933Epoch 4/15: [==============                ] 23/47 batches, loss: 0.6933Epoch 4/15: [===============               ] 24/47 batches, loss: 0.6933Epoch 4/15: [===============               ] 25/47 batches, loss: 0.6933Epoch 4/15: [================              ] 26/47 batches, loss: 0.6933Epoch 4/15: [=================             ] 27/47 batches, loss: 0.6933Epoch 4/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 4/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 4/15: [===================           ] 30/47 batches, loss: 0.6933Epoch 4/15: [===================           ] 31/47 batches, loss: 0.6933Epoch 4/15: [====================          ] 32/47 batches, loss: 0.6933Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.6933Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.6933Epoch 4/15: [======================        ] 35/47 batches, loss: 0.6933Epoch 4/15: [======================        ] 36/47 batches, loss: 0.6933Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.6933Epoch 4/15: [========================      ] 38/47 batches, loss: 0.6933Epoch 4/15: [========================      ] 39/47 batches, loss: 0.6933Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.6933Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.6933Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.6933Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.6933Epoch 4/15: [============================  ] 44/47 batches, loss: 0.6933Epoch 4/15: [============================  ] 45/47 batches, loss: 0.6933Epoch 4/15: [============================= ] 46/47 batches, loss: 0.6933Epoch 4/15: [==============================] 47/47 batches, loss: 0.6933
[2025-05-04 10:51:00,695][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6933
[2025-05-04 10:51:00,962][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:00,963][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.6930Epoch 5/15: [=                             ] 2/47 batches, loss: 0.6932Epoch 5/15: [=                             ] 3/47 batches, loss: 0.6930Epoch 5/15: [==                            ] 4/47 batches, loss: 0.6930Epoch 5/15: [===                           ] 5/47 batches, loss: 0.6930Epoch 5/15: [===                           ] 6/47 batches, loss: 0.6931Epoch 5/15: [====                          ] 7/47 batches, loss: 0.6931Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.6931Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.6931Epoch 5/15: [======                        ] 10/47 batches, loss: 0.6931Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.6931Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.6931Epoch 5/15: [========                      ] 13/47 batches, loss: 0.6931Epoch 5/15: [========                      ] 14/47 batches, loss: 0.6931Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.6931Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.6931Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.6931Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 5/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 5/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 5/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 5/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 5/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 5/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 5/15: [================              ] 26/47 batches, loss: 0.6932Epoch 5/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 5/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 5/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 5/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 5/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 5/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 5/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 5/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 5/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 5/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 5/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 5/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 5/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 5/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:51:02,428][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6932
[2025-05-04 10:51:02,689][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:02,690][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:51:02,690][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:51:02,690][src.training.lm_trainer][INFO] - Training completed in 10.16 seconds
[2025-05-04 10:51:02,690][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:51:04,839][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5385656292286874, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:04,840][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:04,840][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:06,483][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ko/ko/model.pt
[2025-05-04 10:51:06,485][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁
wandb:           best_val_f1 ▁▁
wandb:         best_val_loss █▁
wandb:    best_val_precision ▁▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁
wandb:            train_loss █▁▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁
wandb:              val_loss ▃▁▄▇█
wandb:         val_precision ▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.6931
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 5
wandb:                 epoch 5
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.53857
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69316
wandb:            train_time 10.16183
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69312
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105041-twpix21e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105041-twpix21e/logs
Experiment probe_layer10_question_type_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_question_type_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:51:23,819][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ko
experiment_name: probe_layer10_question_type_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:51:23,819][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:51:23,819][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:51:23,820][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:51:23,824][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-05-04 10:51:23,825][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:51:26,612][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:51:28,889][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:51:28,889][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:51:29,028][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:51:29,092][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:51:29,274][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:51:29,280][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:51:29,280][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:51:29,281][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:51:29,360][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:51:29,427][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:51:29,442][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:51:29,443][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:51:29,443][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:51:29,444][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:51:29,479][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:51:29,555][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:51:29,589][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:51:29,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:51:29,591][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:51:29,592][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:51:29,593][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:51:29,593][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:51:29,593][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:51:29,593][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:51:29,593][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-05-04 10:51:29,593][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-05-04 10:51:29,593][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:51:29,594][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:51:29,594][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:51:29,594][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:51:29,595][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:51:29,595][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:51:29,595][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:51:29,596][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:51:29,596][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:51:34,970][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:51:34,971][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:51:34,971][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:51:34,971][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:51:34,976][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:51:34,977][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:51:34,977][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:51:34,977][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:51:34,977][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:51:34,978][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:51:34,978][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:51:34,979][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.6607Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6895Epoch 1/15: [=                             ] 3/47 batches, loss: 0.6864Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6902Epoch 1/15: [===                           ] 5/47 batches, loss: 0.6991Epoch 1/15: [===                           ] 6/47 batches, loss: 0.7018Epoch 1/15: [====                          ] 7/47 batches, loss: 0.6993Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.6970Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.6973Epoch 1/15: [======                        ] 10/47 batches, loss: 0.6985Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.6971Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.6984Epoch 1/15: [========                      ] 13/47 batches, loss: 0.6993Epoch 1/15: [========                      ] 14/47 batches, loss: 0.7001Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.6999Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.6996Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.6992Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.6992Epoch 1/15: [============                  ] 19/47 batches, loss: 0.6983Epoch 1/15: [============                  ] 20/47 batches, loss: 0.6981Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.6977Epoch 1/15: [==============                ] 22/47 batches, loss: 0.6974Epoch 1/15: [==============                ] 23/47 batches, loss: 0.6972Epoch 1/15: [===============               ] 24/47 batches, loss: 0.6971Epoch 1/15: [===============               ] 25/47 batches, loss: 0.6970Epoch 1/15: [================              ] 26/47 batches, loss: 0.6965Epoch 1/15: [=================             ] 27/47 batches, loss: 0.6961Epoch 1/15: [=================             ] 28/47 batches, loss: 0.6960Epoch 1/15: [==================            ] 29/47 batches, loss: 0.6959Epoch 1/15: [===================           ] 30/47 batches, loss: 0.6958Epoch 1/15: [===================           ] 31/47 batches, loss: 0.6956Epoch 1/15: [====================          ] 32/47 batches, loss: 0.6955Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.6955Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.6955Epoch 1/15: [======================        ] 35/47 batches, loss: 0.6956Epoch 1/15: [======================        ] 36/47 batches, loss: 0.6952Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.6951Epoch 1/15: [========================      ] 38/47 batches, loss: 0.6953Epoch 1/15: [========================      ] 39/47 batches, loss: 0.6952Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.6951Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.6952Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.6952Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.6951Epoch 1/15: [============================  ] 44/47 batches, loss: 0.6951Epoch 1/15: [============================  ] 45/47 batches, loss: 0.6950Epoch 1/15: [============================= ] 46/47 batches, loss: 0.6949Epoch 1/15: [==============================] 47/47 batches, loss: 0.6947
[2025-05-04 10:51:40,086][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6947
[2025-05-04 10:51:40,317][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.6961Epoch 2/15: [=                             ] 2/47 batches, loss: 0.6950Epoch 2/15: [=                             ] 3/47 batches, loss: 0.6945Epoch 2/15: [==                            ] 4/47 batches, loss: 0.6944Epoch 2/15: [===                           ] 5/47 batches, loss: 0.6941Epoch 2/15: [===                           ] 6/47 batches, loss: 0.6940Epoch 2/15: [====                          ] 7/47 batches, loss: 0.6938Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.6938Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.6952Epoch 2/15: [======                        ] 10/47 batches, loss: 0.6945Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.6942Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.6941Epoch 2/15: [========                      ] 13/47 batches, loss: 0.6940Epoch 2/15: [========                      ] 14/47 batches, loss: 0.6940Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.6943Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.6945Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.6946Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.6945Epoch 2/15: [============                  ] 19/47 batches, loss: 0.6945Epoch 2/15: [============                  ] 20/47 batches, loss: 0.6940Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.6937Epoch 2/15: [==============                ] 22/47 batches, loss: 0.6936Epoch 2/15: [==============                ] 23/47 batches, loss: 0.6937Epoch 2/15: [===============               ] 24/47 batches, loss: 0.6937Epoch 2/15: [===============               ] 25/47 batches, loss: 0.6935Epoch 2/15: [================              ] 26/47 batches, loss: 0.6935Epoch 2/15: [=================             ] 27/47 batches, loss: 0.6939Epoch 2/15: [=================             ] 28/47 batches, loss: 0.6938Epoch 2/15: [==================            ] 29/47 batches, loss: 0.6937Epoch 2/15: [===================           ] 30/47 batches, loss: 0.6937Epoch 2/15: [===================           ] 31/47 batches, loss: 0.6937Epoch 2/15: [====================          ] 32/47 batches, loss: 0.6935Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.6936Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.6936Epoch 2/15: [======================        ] 35/47 batches, loss: 0.6937Epoch 2/15: [======================        ] 36/47 batches, loss: 0.6936Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.6936Epoch 2/15: [========================      ] 38/47 batches, loss: 0.6935Epoch 2/15: [========================      ] 39/47 batches, loss: 0.6935Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.6935Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.6935Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.6933Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.6933Epoch 2/15: [============================  ] 44/47 batches, loss: 0.6933Epoch 2/15: [============================  ] 45/47 batches, loss: 0.6934Epoch 2/15: [============================= ] 46/47 batches, loss: 0.6935Epoch 2/15: [==============================] 47/47 batches, loss: 0.6935
[2025-05-04 10:51:42,185][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6935
[2025-05-04 10:51:42,441][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:42,442][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.6948Epoch 3/15: [=                             ] 2/47 batches, loss: 0.6936Epoch 3/15: [=                             ] 3/47 batches, loss: 0.6935Epoch 3/15: [==                            ] 4/47 batches, loss: 0.6932Epoch 3/15: [===                           ] 5/47 batches, loss: 0.6922Epoch 3/15: [===                           ] 6/47 batches, loss: 0.6918Epoch 3/15: [====                          ] 7/47 batches, loss: 0.6919Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.6926Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.6924Epoch 3/15: [======                        ] 10/47 batches, loss: 0.6929Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.6929Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.6919Epoch 3/15: [========                      ] 13/47 batches, loss: 0.6919Epoch 3/15: [========                      ] 14/47 batches, loss: 0.6920Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.6921Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.6923Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.6926Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.6931Epoch 3/15: [============                  ] 19/47 batches, loss: 0.6930Epoch 3/15: [============                  ] 20/47 batches, loss: 0.6930Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.6933Epoch 3/15: [==============                ] 22/47 batches, loss: 0.6933Epoch 3/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 3/15: [===============               ] 24/47 batches, loss: 0.6933Epoch 3/15: [===============               ] 25/47 batches, loss: 0.6933Epoch 3/15: [================              ] 26/47 batches, loss: 0.6932Epoch 3/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 3/15: [=================             ] 28/47 batches, loss: 0.6933Epoch 3/15: [==================            ] 29/47 batches, loss: 0.6933Epoch 3/15: [===================           ] 30/47 batches, loss: 0.6933Epoch 3/15: [===================           ] 31/47 batches, loss: 0.6933Epoch 3/15: [====================          ] 32/47 batches, loss: 0.6933Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.6933Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.6935Epoch 3/15: [======================        ] 35/47 batches, loss: 0.6935Epoch 3/15: [======================        ] 36/47 batches, loss: 0.6934Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.6935Epoch 3/15: [========================      ] 38/47 batches, loss: 0.6935Epoch 3/15: [========================      ] 39/47 batches, loss: 0.6934Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.6936Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.6936Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.6936Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.6936Epoch 3/15: [============================  ] 44/47 batches, loss: 0.6936Epoch 3/15: [============================  ] 45/47 batches, loss: 0.6936Epoch 3/15: [============================= ] 46/47 batches, loss: 0.6935Epoch 3/15: [==============================] 47/47 batches, loss: 0.6935
[2025-05-04 10:51:43,891][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6935
[2025-05-04 10:51:44,142][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:44,143][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.6930Epoch 4/15: [=                             ] 2/47 batches, loss: 0.6931Epoch 4/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 4/15: [==                            ] 4/47 batches, loss: 0.6936Epoch 4/15: [===                           ] 5/47 batches, loss: 0.6935Epoch 4/15: [===                           ] 6/47 batches, loss: 0.6935Epoch 4/15: [====                          ] 7/47 batches, loss: 0.6934Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.6933Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.6933Epoch 4/15: [======                        ] 10/47 batches, loss: 0.6933Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.6933Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.6933Epoch 4/15: [========                      ] 13/47 batches, loss: 0.6933Epoch 4/15: [========                      ] 14/47 batches, loss: 0.6933Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.6933Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.6931Epoch 4/15: [==============                ] 22/47 batches, loss: 0.6931Epoch 4/15: [==============                ] 23/47 batches, loss: 0.6931Epoch 4/15: [===============               ] 24/47 batches, loss: 0.6931Epoch 4/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 4/15: [================              ] 26/47 batches, loss: 0.6931Epoch 4/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 4/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 4/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 4/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 4/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 4/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 4/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 4/15: [======================        ] 36/47 batches, loss: 0.6933Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 4/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 4/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 4/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 4/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 4/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 4/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:51:45,597][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:51:45,850][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:45,851][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:51:45,851][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 4
[2025-05-04 10:51:45,851][src.training.lm_trainer][INFO] - Training completed in 8.10 seconds
[2025-05-04 10:51:45,851][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:51:47,968][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5385656292286874, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:47,968][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:47,968][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:51:49,664][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ko/ko/model.pt
[2025-05-04 10:51:49,665][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁
wandb:           best_val_f1 ▁
wandb:         best_val_loss ▁
wandb:    best_val_precision ▁
wandb:       best_val_recall ▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▃▃▆▆██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁
wandb:            train_loss █▂▂▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁
wandb:                val_f1 ▁▁▁▁
wandb:              val_loss ▁▅▇█
wandb:         val_precision ▁▁▁▁
wandb:            val_recall ▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.6931
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 4
wandb:                 epoch 4
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.53857
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69321
wandb:            train_time 8.10087
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69313
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105123-yd4t484x
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105123-yd4t484x/logs
Experiment probe_layer10_question_type_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_question_type_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:52:04,746][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ko
experiment_name: probe_layer10_question_type_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:52:04,746][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:52:04,746][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:52:04,746][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:52:04,752][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ko']
[2025-05-04 10:52:04,752][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:52:06,757][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:52:08,988][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:52:08,988][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:52:09,059][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:52:09,135][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:52:09,262][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:52:09,267][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:52:09,268][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:52:09,269][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:52:09,304][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:52:09,380][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:52:09,405][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:52:09,406][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:52:09,406][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:52:09,407][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:52:09,447][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:52:09,505][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:52:09,558][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:52:09,560][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:52:09,560][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:52:09,572][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:52:09,573][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:52:09,573][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:52:09,573][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:52:09,573][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:52:09,573][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-05-04 10:52:09,573][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-05-04 10:52:09,573][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:52:09,574][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:52:09,574][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:52:09,574][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:52:09,575][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:52:09,575][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:52:09,575][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:52:09,576][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:52:09,576][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:52:14,340][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:52:14,341][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:52:14,341][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:52:14,342][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:52:14,347][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:52:14,348][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:52:14,348][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:52:14,348][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:52:14,348][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:52:14,349][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:52:14,349][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:52:14,350][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.6898Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6923Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7019Epoch 1/15: [==                            ] 4/47 batches, loss: 0.7007Epoch 1/15: [===                           ] 5/47 batches, loss: 0.7023Epoch 1/15: [===                           ] 6/47 batches, loss: 0.7036Epoch 1/15: [====                          ] 7/47 batches, loss: 0.7032Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.7013Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.7009Epoch 1/15: [======                        ] 10/47 batches, loss: 0.6999Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.6997Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.6994Epoch 1/15: [========                      ] 13/47 batches, loss: 0.6994Epoch 1/15: [========                      ] 14/47 batches, loss: 0.6989Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.6987Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.6986Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.6983Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.6980Epoch 1/15: [============                  ] 19/47 batches, loss: 0.6978Epoch 1/15: [============                  ] 20/47 batches, loss: 0.6976Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.6974Epoch 1/15: [==============                ] 22/47 batches, loss: 0.6973Epoch 1/15: [==============                ] 23/47 batches, loss: 0.6972Epoch 1/15: [===============               ] 24/47 batches, loss: 0.6971Epoch 1/15: [===============               ] 25/47 batches, loss: 0.6969Epoch 1/15: [================              ] 26/47 batches, loss: 0.6968Epoch 1/15: [=================             ] 27/47 batches, loss: 0.6966Epoch 1/15: [=================             ] 28/47 batches, loss: 0.6965Epoch 1/15: [==================            ] 29/47 batches, loss: 0.6964Epoch 1/15: [===================           ] 30/47 batches, loss: 0.6964Epoch 1/15: [===================           ] 31/47 batches, loss: 0.6963Epoch 1/15: [====================          ] 32/47 batches, loss: 0.6962Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.6961Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.6960Epoch 1/15: [======================        ] 35/47 batches, loss: 0.6959Epoch 1/15: [======================        ] 36/47 batches, loss: 0.6959Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.6958Epoch 1/15: [========================      ] 38/47 batches, loss: 0.6957Epoch 1/15: [========================      ] 39/47 batches, loss: 0.6956Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.6956Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.6955Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.6955Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.6954Epoch 1/15: [============================  ] 44/47 batches, loss: 0.6954Epoch 1/15: [============================  ] 45/47 batches, loss: 0.6953Epoch 1/15: [============================= ] 46/47 batches, loss: 0.6953Epoch 1/15: [==============================] 47/47 batches, loss: 0.6952
[2025-05-04 10:52:18,964][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6952
[2025-05-04 10:52:19,219][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.6929Epoch 2/15: [=                             ] 2/47 batches, loss: 0.6931Epoch 2/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 2/15: [==                            ] 4/47 batches, loss: 0.6931Epoch 2/15: [===                           ] 5/47 batches, loss: 0.6931Epoch 2/15: [===                           ] 6/47 batches, loss: 0.6932Epoch 2/15: [====                          ] 7/47 batches, loss: 0.6932Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.6932Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.6932Epoch 2/15: [======                        ] 10/47 batches, loss: 0.6933Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.6932Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 2/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 2/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.6933Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.6933Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.6933Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.6933Epoch 2/15: [============                  ] 19/47 batches, loss: 0.6933Epoch 2/15: [============                  ] 20/47 batches, loss: 0.6933Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.6933Epoch 2/15: [==============                ] 22/47 batches, loss: 0.6933Epoch 2/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 2/15: [===============               ] 24/47 batches, loss: 0.6933Epoch 2/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 2/15: [================              ] 26/47 batches, loss: 0.6932Epoch 2/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 2/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 2/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 2/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 2/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 2/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 2/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 2/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 2/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 2/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 2/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 2/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 2/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 2/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:21,087][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:52:21,361][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.6929Epoch 3/15: [=                             ] 2/47 batches, loss: 0.6932Epoch 3/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 3/15: [==                            ] 4/47 batches, loss: 0.6932Epoch 3/15: [===                           ] 5/47 batches, loss: 0.6932Epoch 3/15: [===                           ] 6/47 batches, loss: 0.6931Epoch 3/15: [====                          ] 7/47 batches, loss: 0.6931Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.6931Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.6931Epoch 3/15: [======                        ] 10/47 batches, loss: 0.6931Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.6931Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 3/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 3/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.6931Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.6931Epoch 3/15: [============                  ] 19/47 batches, loss: 0.6931Epoch 3/15: [============                  ] 20/47 batches, loss: 0.6931Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.6931Epoch 3/15: [==============                ] 22/47 batches, loss: 0.6931Epoch 3/15: [==============                ] 23/47 batches, loss: 0.6931Epoch 3/15: [===============               ] 24/47 batches, loss: 0.6931Epoch 3/15: [===============               ] 25/47 batches, loss: 0.6931Epoch 3/15: [================              ] 26/47 batches, loss: 0.6931Epoch 3/15: [=================             ] 27/47 batches, loss: 0.6931Epoch 3/15: [=================             ] 28/47 batches, loss: 0.6931Epoch 3/15: [==================            ] 29/47 batches, loss: 0.6931Epoch 3/15: [===================           ] 30/47 batches, loss: 0.6931Epoch 3/15: [===================           ] 31/47 batches, loss: 0.6931Epoch 3/15: [====================          ] 32/47 batches, loss: 0.6931Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.6931Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.6931Epoch 3/15: [======================        ] 35/47 batches, loss: 0.6931Epoch 3/15: [======================        ] 36/47 batches, loss: 0.6931Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.6931Epoch 3/15: [========================      ] 38/47 batches, loss: 0.6931Epoch 3/15: [========================      ] 39/47 batches, loss: 0.6931Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.6931Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.6931Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 3/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 3/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 3/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 3/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:23,249][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-04 10:52:23,500][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.6932Epoch 4/15: [=                             ] 2/47 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/47 batches, loss: 0.6930Epoch 4/15: [==                            ] 4/47 batches, loss: 0.6931Epoch 4/15: [===                           ] 5/47 batches, loss: 0.6932Epoch 4/15: [===                           ] 6/47 batches, loss: 0.6931Epoch 4/15: [====                          ] 7/47 batches, loss: 0.6931Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.6932Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.6932Epoch 4/15: [======                        ] 10/47 batches, loss: 0.6932Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.6932Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 4/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 4/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 4/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 4/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 4/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 4/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 4/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 4/15: [================              ] 26/47 batches, loss: 0.6932Epoch 4/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 4/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 4/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 4/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 4/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 4/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 4/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 4/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 4/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 4/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 4/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 4/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 4/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 4/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:25,311][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6932
[2025-05-04 10:52:25,577][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.6930Epoch 5/15: [=                             ] 2/47 batches, loss: 0.6931Epoch 5/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 5/15: [==                            ] 4/47 batches, loss: 0.6932Epoch 5/15: [===                           ] 5/47 batches, loss: 0.6932Epoch 5/15: [===                           ] 6/47 batches, loss: 0.6932Epoch 5/15: [====                          ] 7/47 batches, loss: 0.6932Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.6932Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.6932Epoch 5/15: [======                        ] 10/47 batches, loss: 0.6932Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.6932Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 5/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 5/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 5/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 5/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 5/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 5/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 5/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 5/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 5/15: [================              ] 26/47 batches, loss: 0.6932Epoch 5/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 5/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 5/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 5/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 5/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 5/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 5/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 5/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 5/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 5/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 5/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 5/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 5/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 5/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:27,406][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6932
[2025-05-04 10:52:27,674][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.6931Epoch 6/15: [=                             ] 2/47 batches, loss: 0.6932Epoch 6/15: [=                             ] 3/47 batches, loss: 0.6931Epoch 6/15: [==                            ] 4/47 batches, loss: 0.6931Epoch 6/15: [===                           ] 5/47 batches, loss: 0.6931Epoch 6/15: [===                           ] 6/47 batches, loss: 0.6931Epoch 6/15: [====                          ] 7/47 batches, loss: 0.6931Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.6931Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.6931Epoch 6/15: [======                        ] 10/47 batches, loss: 0.6931Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.6931Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.6931Epoch 6/15: [========                      ] 13/47 batches, loss: 0.6931Epoch 6/15: [========                      ] 14/47 batches, loss: 0.6931Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 6/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 6/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 6/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 6/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 6/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 6/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 6/15: [================              ] 26/47 batches, loss: 0.6932Epoch 6/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 6/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 6/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 6/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 6/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 6/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 6/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 6/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 6/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 6/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 6/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 6/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 6/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 6/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:29,545][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6932
[2025-05-04 10:52:29,814][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:29,815][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/47 batches, loss: 0.6931Epoch 7/15: [=                             ] 2/47 batches, loss: 0.6932Epoch 7/15: [=                             ] 3/47 batches, loss: 0.6932Epoch 7/15: [==                            ] 4/47 batches, loss: 0.6932Epoch 7/15: [===                           ] 5/47 batches, loss: 0.6932Epoch 7/15: [===                           ] 6/47 batches, loss: 0.6932Epoch 7/15: [====                          ] 7/47 batches, loss: 0.6932Epoch 7/15: [=====                         ] 8/47 batches, loss: 0.6932Epoch 7/15: [=====                         ] 9/47 batches, loss: 0.6932Epoch 7/15: [======                        ] 10/47 batches, loss: 0.6932Epoch 7/15: [=======                       ] 11/47 batches, loss: 0.6932Epoch 7/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 7/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 7/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 7/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 7/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 7/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 7/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 7/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 7/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 7/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 7/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 7/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 7/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 7/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 7/15: [================              ] 26/47 batches, loss: 0.6932Epoch 7/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 7/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 7/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 7/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 7/15: [===================           ] 31/47 batches, loss: 0.6931Epoch 7/15: [====================          ] 32/47 batches, loss: 0.6931Epoch 7/15: [=====================         ] 33/47 batches, loss: 0.6931Epoch 7/15: [=====================         ] 34/47 batches, loss: 0.6931Epoch 7/15: [======================        ] 35/47 batches, loss: 0.6931Epoch 7/15: [======================        ] 36/47 batches, loss: 0.6931Epoch 7/15: [=======================       ] 37/47 batches, loss: 0.6931Epoch 7/15: [========================      ] 38/47 batches, loss: 0.6931Epoch 7/15: [========================      ] 39/47 batches, loss: 0.6931Epoch 7/15: [=========================     ] 40/47 batches, loss: 0.6931Epoch 7/15: [==========================    ] 41/47 batches, loss: 0.6931Epoch 7/15: [==========================    ] 42/47 batches, loss: 0.6931Epoch 7/15: [===========================   ] 43/47 batches, loss: 0.6931Epoch 7/15: [============================  ] 44/47 batches, loss: 0.6931Epoch 7/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 7/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 7/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:31,287][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6932
[2025-05-04 10:52:31,555][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:31,555][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/47 batches, loss: 0.6931Epoch 8/15: [=                             ] 2/47 batches, loss: 0.6931Epoch 8/15: [=                             ] 3/47 batches, loss: 0.6931Epoch 8/15: [==                            ] 4/47 batches, loss: 0.6931Epoch 8/15: [===                           ] 5/47 batches, loss: 0.6931Epoch 8/15: [===                           ] 6/47 batches, loss: 0.6931Epoch 8/15: [====                          ] 7/47 batches, loss: 0.6931Epoch 8/15: [=====                         ] 8/47 batches, loss: 0.6931Epoch 8/15: [=====                         ] 9/47 batches, loss: 0.6931Epoch 8/15: [======                        ] 10/47 batches, loss: 0.6932Epoch 8/15: [=======                       ] 11/47 batches, loss: 0.6932Epoch 8/15: [=======                       ] 12/47 batches, loss: 0.6932Epoch 8/15: [========                      ] 13/47 batches, loss: 0.6932Epoch 8/15: [========                      ] 14/47 batches, loss: 0.6932Epoch 8/15: [=========                     ] 15/47 batches, loss: 0.6932Epoch 8/15: [==========                    ] 16/47 batches, loss: 0.6932Epoch 8/15: [==========                    ] 17/47 batches, loss: 0.6932Epoch 8/15: [===========                   ] 18/47 batches, loss: 0.6932Epoch 8/15: [============                  ] 19/47 batches, loss: 0.6932Epoch 8/15: [============                  ] 20/47 batches, loss: 0.6932Epoch 8/15: [=============                 ] 21/47 batches, loss: 0.6932Epoch 8/15: [==============                ] 22/47 batches, loss: 0.6932Epoch 8/15: [==============                ] 23/47 batches, loss: 0.6932Epoch 8/15: [===============               ] 24/47 batches, loss: 0.6932Epoch 8/15: [===============               ] 25/47 batches, loss: 0.6932Epoch 8/15: [================              ] 26/47 batches, loss: 0.6932Epoch 8/15: [=================             ] 27/47 batches, loss: 0.6932Epoch 8/15: [=================             ] 28/47 batches, loss: 0.6932Epoch 8/15: [==================            ] 29/47 batches, loss: 0.6932Epoch 8/15: [===================           ] 30/47 batches, loss: 0.6932Epoch 8/15: [===================           ] 31/47 batches, loss: 0.6932Epoch 8/15: [====================          ] 32/47 batches, loss: 0.6932Epoch 8/15: [=====================         ] 33/47 batches, loss: 0.6932Epoch 8/15: [=====================         ] 34/47 batches, loss: 0.6932Epoch 8/15: [======================        ] 35/47 batches, loss: 0.6932Epoch 8/15: [======================        ] 36/47 batches, loss: 0.6932Epoch 8/15: [=======================       ] 37/47 batches, loss: 0.6932Epoch 8/15: [========================      ] 38/47 batches, loss: 0.6932Epoch 8/15: [========================      ] 39/47 batches, loss: 0.6932Epoch 8/15: [=========================     ] 40/47 batches, loss: 0.6932Epoch 8/15: [==========================    ] 41/47 batches, loss: 0.6932Epoch 8/15: [==========================    ] 42/47 batches, loss: 0.6932Epoch 8/15: [===========================   ] 43/47 batches, loss: 0.6932Epoch 8/15: [============================  ] 44/47 batches, loss: 0.6932Epoch 8/15: [============================  ] 45/47 batches, loss: 0.6932Epoch 8/15: [============================= ] 46/47 batches, loss: 0.6932Epoch 8/15: [==============================] 47/47 batches, loss: 0.6932
[2025-05-04 10:52:33,037][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6932
[2025-05-04 10:52:33,325][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:33,326][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:52:33,326][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-04 10:52:33,326][src.training.lm_trainer][INFO] - Training completed in 16.72 seconds
[2025-05-04 10:52:33,326][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:52:35,446][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5385656292286874, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:35,446][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:35,447][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:52:37,124][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ko/ko/model.pt
[2025-05-04 10:52:37,126][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁
wandb:         best_val_loss █▇▄▂▁
wandb:    best_val_precision ▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁
wandb:            train_loss █▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁
wandb:              val_loss █▇▄▂▁▁▁▁
wandb:         val_precision ▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69314
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.53857
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69318
wandb:            train_time 16.71739
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69314
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105204-8d1bfr4g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105204-8d1bfr4g/logs
Experiment probe_layer10_question_type_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_complexity_control1_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control1_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:52:56,091][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ko
experiment_name: probe_layer10_complexity_control1_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:52:56,092][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:52:56,092][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:52:56,092][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:52:56,098][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-04 10:52:56,098][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:52:59,253][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:53:01,526][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:53:01,527][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:01,725][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:53:01,796][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:53:02,071][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:53:02,077][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:02,078][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:53:02,079][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:02,145][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:02,175][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:02,188][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:53:02,189][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:02,189][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:53:02,190][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:02,211][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:02,248][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:02,269][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:53:02,271][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:02,271][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:53:02,272][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:53:02,272][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:02,272][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:02,272][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:02,273][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:02,273][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Sample label: 0.2160857915878296
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:02,273][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:02,274][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:02,274][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:02,274][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:02,274][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:02,275][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-04 10:53:02,275][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:53:02,275][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-04 10:53:02,275][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:53:02,275][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:53:02,275][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:53:02,275][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:53:02,276][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:53:08,223][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:53:08,223][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:53:08,224][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:53:08,224][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:53:08,227][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:53:08,227][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:53:08,227][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:53:08,227][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:53:08,227][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:53:08,228][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:53:08,228][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.4009Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6132Epoch 1/15: [=                             ] 3/47 batches, loss: 0.6490Epoch 1/15: [==                            ] 4/47 batches, loss: 0.6355Epoch 1/15: [===                           ] 5/47 batches, loss: 0.5888Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5440Epoch 1/15: [====                          ] 7/47 batches, loss: 0.5213Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.5109Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.5261Epoch 1/15: [======                        ] 10/47 batches, loss: 0.5341Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.5010Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.4885Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4880Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4794Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4692Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4698Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4647Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4681Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4508Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4411Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4512Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4472Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4451Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4383Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4338Epoch 1/15: [================              ] 26/47 batches, loss: 0.4320Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4268Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4219Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4199Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4182Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4129Epoch 1/15: [====================          ] 32/47 batches, loss: 0.4070Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.4009Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.4027Epoch 1/15: [======================        ] 35/47 batches, loss: 0.4044Epoch 1/15: [======================        ] 36/47 batches, loss: 0.4010Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.4074Epoch 1/15: [========================      ] 38/47 batches, loss: 0.4027Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3981Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3936Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3895Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3853Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3811Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3806Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3777Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3719Epoch 1/15: [==============================] 47/47 batches, loss: 0.3676
[2025-05-04 10:53:14,198][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3676
[2025-05-04 10:53:14,436][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0610, Metrics: {'mse': 0.06309114396572113, 'rmse': 0.2511795054651576, 'r2': -0.33920466899871826}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2385Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1779Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2190Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1965Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2231Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2008Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2104Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2019Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2008Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2082Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2087Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2136Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2164Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2191Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2173Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2080Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2030Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2059Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2012Epoch 2/15: [============                  ] 20/47 batches, loss: 0.1992Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1964Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1993Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2014Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1990Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1966Epoch 2/15: [================              ] 26/47 batches, loss: 0.1939Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1909Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1954Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1948Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1936Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1910Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1896Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1882Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1868Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1838Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1844Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1836Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1821Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1826Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1817Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1816Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1809Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1802Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1792Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1772Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1793Epoch 2/15: [==============================] 47/47 batches, loss: 0.1821
[2025-05-04 10:53:16,350][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1821
[2025-05-04 10:53:16,590][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0599, Metrics: {'mse': 0.06303499639034271, 'rmse': 0.25106771275961137, 'r2': -0.3380129337310791}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.0922Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1070Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1053Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1273Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1322Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1252Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1249Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1183Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1278Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1268Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1281Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1295Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1270Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1216Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1270Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1237Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1209Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1224Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1191Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1175Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1174Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1178Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1195Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1195Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1186Epoch 3/15: [================              ] 26/47 batches, loss: 0.1175Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1178Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1183Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1163Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1158Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1177Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1186Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1188Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1187Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1203Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1196Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1184Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1192Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1202Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1200Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1201Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1209Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1215Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1213Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1220Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1216Epoch 3/15: [==============================] 47/47 batches, loss: 0.1212
[2025-05-04 10:53:18,504][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1212
[2025-05-04 10:53:18,755][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0815, Metrics: {'mse': 0.08673608303070068, 'rmse': 0.29450990311142455, 'r2': -0.8411041498184204}
[2025-05-04 10:53:18,755][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1649Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1244Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1328Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1422Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1372Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1254Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1259Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1300Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1325Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1303Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1256Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1224Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1247Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1221Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1236Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1226Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1193Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1213Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1227Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1242Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1228Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1219Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1204Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1200Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1235Epoch 4/15: [================              ] 26/47 batches, loss: 0.1219Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1224Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1206Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1205Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1195Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1206Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1209Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1189Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1169Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1173Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1155Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1155Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1156Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1141Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1140Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1147Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1153Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1134Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1126Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1113Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1133Epoch 4/15: [==============================] 47/47 batches, loss: 0.1156
[2025-05-04 10:53:20,215][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1156
[2025-05-04 10:53:20,501][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0914, Metrics: {'mse': 0.09671042114496231, 'rmse': 0.31098299172939076, 'r2': -1.0528244972229004}
[2025-05-04 10:53:20,501][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.0758Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1144Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1019Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0949Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0798Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0858Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0832Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0828Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0848Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0824Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0799Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0784Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0783Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0770Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0767Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0771Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0771Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0789Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0804Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0840Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0831Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0861Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0856Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0896Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0896Epoch 5/15: [================              ] 26/47 batches, loss: 0.0882Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0901Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0906Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0897Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0920Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0930Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0920Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0924Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0916Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0925Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0931Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0928Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0926Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0922Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0924Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0923Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0925Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0944Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0934Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0931Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0918Epoch 5/15: [==============================] 47/47 batches, loss: 0.0912
[2025-05-04 10:53:21,964][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0912
[2025-05-04 10:53:22,225][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0986, Metrics: {'mse': 0.104246124625206, 'rmse': 0.32287168445871184, 'r2': -1.2127811908721924}
[2025-05-04 10:53:22,226][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1934Epoch 6/15: [=                             ] 2/47 batches, loss: 0.1325Epoch 6/15: [=                             ] 3/47 batches, loss: 0.1322Epoch 6/15: [==                            ] 4/47 batches, loss: 0.1135Epoch 6/15: [===                           ] 5/47 batches, loss: 0.1079Epoch 6/15: [===                           ] 6/47 batches, loss: 0.1009Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0985Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0975Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0972Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0946Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0939Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0987Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0953Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0910Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0920Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0889Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0869Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0849Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0823Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0804Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0803Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0803Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0816Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0812Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0804Epoch 6/15: [================              ] 26/47 batches, loss: 0.0812Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0798Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0821Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0833Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0840Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0855Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0864Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0862Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0866Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0853Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0847Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0847Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0839Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0833Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0831Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0821Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0816Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0806Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0801Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0792Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0788Epoch 6/15: [==============================] 47/47 batches, loss: 0.0775
[2025-05-04 10:53:23,716][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0775
[2025-05-04 10:53:23,983][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0778, Metrics: {'mse': 0.08241583406925201, 'rmse': 0.28708158086030533, 'r2': -0.74940025806427}
[2025-05-04 10:53:23,984][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:53:23,984][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-04 10:53:23,984][src.training.lm_trainer][INFO] - Training completed in 12.14 seconds
[2025-05-04 10:53:23,984][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:53:26,189][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02797011472284794, 'rmse': 0.16724268212046808, 'r2': -0.2564321756362915}
[2025-05-04 10:53:26,189][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06303499639034271, 'rmse': 0.25106771275961137, 'r2': -0.3380129337310791}
[2025-05-04 10:53:26,189][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.03951529413461685, 'rmse': 0.19878454199111373, 'r2': -0.22695708274841309}
[2025-05-04 10:53:27,886][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ko/ko/model.pt
[2025-05-04 10:53:27,887][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▅▃▂▁
wandb:       train_loss █▄▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▁▅▇█▄
wandb:          val_mse ▁▁▅▇█▄
wandb:           val_r2 ██▄▂▁▅
wandb:         val_rmse ▁▁▅▇█▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05993
wandb:     best_val_mse 0.06303
wandb:      best_val_r2 -0.33801
wandb:    best_val_rmse 0.25107
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.03952
wandb:    final_test_r2 -0.22696
wandb:  final_test_rmse 0.19878
wandb:  final_train_mse 0.02797
wandb:   final_train_r2 -0.25643
wandb: final_train_rmse 0.16724
wandb:    final_val_mse 0.06303
wandb:     final_val_r2 -0.33801
wandb:   final_val_rmse 0.25107
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07749
wandb:       train_time 12.14131
wandb:         val_loss 0.07776
wandb:          val_mse 0.08242
wandb:           val_r2 -0.7494
wandb:         val_rmse 0.28708
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105256-6m4y7fni
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105256-6m4y7fni/logs
Experiment probe_layer10_complexity_control1_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:53:45,581][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ko
experiment_name: probe_layer10_complexity_control2_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:53:45,581][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:53:45,581][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:53:45,581][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:53:45,586][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-04 10:53:45,587][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:53:48,022][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:53:50,262][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:53:50,263][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:50,362][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:53:50,468][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:53:50,605][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:53:50,611][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:50,611][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:53:50,613][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:50,659][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:50,733][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:50,748][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:53:50,749][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:50,749][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:53:50,750][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:53:50,785][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:50,869][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:53:50,896][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:53:50,897][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:53:50,897][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:53:50,899][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:53:50,899][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:50,899][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:50,899][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:50,899][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:50,899][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:50,900][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Sample label: 0.4206434190273285
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:50,900][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:50,900][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-04 10:53:50,900][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:53:50,901][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:53:50,901][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:53:50,901][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:53:50,902][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:53:50,902][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:53:50,902][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:53:55,816][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:53:55,817][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:53:55,817][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:53:55,817][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:53:55,820][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:53:55,820][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:53:55,820][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:53:55,821][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:53:55,821][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:53:55,821][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:53:55,822][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3653Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6212Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7483Epoch 1/15: [==                            ] 4/47 batches, loss: 0.7351Epoch 1/15: [===                           ] 5/47 batches, loss: 0.6524Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5889Epoch 1/15: [====                          ] 7/47 batches, loss: 0.5687Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.5463Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.5493Epoch 1/15: [======                        ] 10/47 batches, loss: 0.5576Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.5195Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.5015Epoch 1/15: [========                      ] 13/47 batches, loss: 0.4984Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4847Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4696Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4776Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4764Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4761Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4604Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4497Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4566Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4541Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4522Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4419Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4374Epoch 1/15: [================              ] 26/47 batches, loss: 0.4334Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4279Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4212Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4167Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4148Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4100Epoch 1/15: [====================          ] 32/47 batches, loss: 0.4019Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.3946Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.3980Epoch 1/15: [======================        ] 35/47 batches, loss: 0.3985Epoch 1/15: [======================        ] 36/47 batches, loss: 0.3962Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.4081Epoch 1/15: [========================      ] 38/47 batches, loss: 0.4010Epoch 1/15: [========================      ] 39/47 batches, loss: 0.3972Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.3933Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3880Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3817Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3779Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3759Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3728Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3694Epoch 1/15: [==============================] 47/47 batches, loss: 0.3661
[2025-05-04 10:54:00,675][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3661
[2025-05-04 10:54:00,940][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0602, Metrics: {'mse': 0.06271907687187195, 'rmse': 0.25043777045779647, 'r2': -0.331307053565979}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2337Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1647Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2216Epoch 2/15: [==                            ] 4/47 batches, loss: 0.1985Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2386Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2182Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2348Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2266Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2270Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2228Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2261Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2342Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2319Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2329Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2252Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2187Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2144Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2129Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2056Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2072Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.2034Epoch 2/15: [==============                ] 22/47 batches, loss: 0.2052Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2069Epoch 2/15: [===============               ] 24/47 batches, loss: 0.2055Epoch 2/15: [===============               ] 25/47 batches, loss: 0.2033Epoch 2/15: [================              ] 26/47 batches, loss: 0.2002Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1990Epoch 2/15: [=================             ] 28/47 batches, loss: 0.2021Epoch 2/15: [==================            ] 29/47 batches, loss: 0.2035Epoch 2/15: [===================           ] 30/47 batches, loss: 0.2015Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1982Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1953Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1936Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1923Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1899Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1910Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1902Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1872Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1878Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1857Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1852Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1843Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1841Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1833Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1823Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1827Epoch 2/15: [==============================] 47/47 batches, loss: 0.1867
[2025-05-04 10:54:02,824][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1867
[2025-05-04 10:54:03,065][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0618, Metrics: {'mse': 0.06540060043334961, 'rmse': 0.25573541098829, 'r2': -0.38822638988494873}
[2025-05-04 10:54:03,066][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1528Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1679Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1731Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1714Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1568Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1370Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1336Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1258Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1312Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1297Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1270Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1310Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1320Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1289Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1338Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1307Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1305Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1320Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1294Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1276Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1279Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1265Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1321Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1318Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1305Epoch 3/15: [================              ] 26/47 batches, loss: 0.1294Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1283Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1277Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1259Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1252Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1267Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1265Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1278Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1270Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1284Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1278Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1278Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1271Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1274Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1264Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1254Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1262Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1271Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1280Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1280Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1282Epoch 3/15: [==============================] 47/47 batches, loss: 0.1292
[2025-05-04 10:54:04,528][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1292
[2025-05-04 10:54:04,772][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0845, Metrics: {'mse': 0.09026148915290833, 'rmse': 0.30043549915565626, 'r2': -0.9159362316131592}
[2025-05-04 10:54:04,773][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1068Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1111Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1255Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1300Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1248Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1156Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1184Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1148Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1156Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1209Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1161Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1150Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1159Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1164Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1170Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1150Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1123Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1143Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1166Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1197Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1178Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1180Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1179Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1162Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1169Epoch 4/15: [================              ] 26/47 batches, loss: 0.1175Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1192Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1174Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1176Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1171Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1177Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1177Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1171Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1146Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1155Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1146Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1149Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1140Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1126Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1121Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1144Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1154Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1142Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1138Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1125Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1134Epoch 4/15: [==============================] 47/47 batches, loss: 0.1129
[2025-05-04 10:54:06,217][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1129
[2025-05-04 10:54:06,475][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0865, Metrics: {'mse': 0.09191038459539413, 'rmse': 0.3031672551503446, 'r2': -0.9509364366531372}
[2025-05-04 10:54:06,476][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1086Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1300Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1048Epoch 5/15: [==                            ] 4/47 batches, loss: 0.0933Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0863Epoch 5/15: [===                           ] 6/47 batches, loss: 0.0949Epoch 5/15: [====                          ] 7/47 batches, loss: 0.0965Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.0908Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.0884Epoch 5/15: [======                        ] 10/47 batches, loss: 0.0864Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0853Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0809Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0784Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0781Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0757Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0779Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0773Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0777Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0766Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0776Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0778Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0777Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0777Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0812Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0813Epoch 5/15: [================              ] 26/47 batches, loss: 0.0810Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0822Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0814Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0820Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0853Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0857Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0850Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0843Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0837Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0837Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0846Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0843Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0846Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0841Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0851Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0846Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0845Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0861Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0862Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0861Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0854Epoch 5/15: [==============================] 47/47 batches, loss: 0.0841
[2025-05-04 10:54:07,926][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0841
[2025-05-04 10:54:08,179][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1221, Metrics: {'mse': 0.12904685735702515, 'rmse': 0.3592309248339084, 'r2': -1.7392141819000244}
[2025-05-04 10:54:08,180][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:54:08,180][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:54:08,180][src.training.lm_trainer][INFO] - Training completed in 9.68 seconds
[2025-05-04 10:54:08,180][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:54:10,355][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04323307424783707, 'rmse': 0.2079256459598889, 'r2': -0.9420521259307861}
[2025-05-04 10:54:10,356][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06271907687187195, 'rmse': 0.25043777045779647, 'r2': -0.331307053565979}
[2025-05-04 10:54:10,356][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.041455402970314026, 'rmse': 0.20360599934754875, 'r2': -0.2871978282928467}
[2025-05-04 10:54:12,063][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ko/ko/model.pt
[2025-05-04 10:54:12,064][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▄▁▁
wandb:       train_loss █▄▂▂▁
wandb:       train_time ▁
wandb:         val_loss ▁▁▄▄█
wandb:          val_mse ▁▁▄▄█
wandb:           val_r2 ██▅▅▁
wandb:         val_rmse ▁▁▄▄█
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06023
wandb:     best_val_mse 0.06272
wandb:      best_val_r2 -0.33131
wandb:    best_val_rmse 0.25044
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.04146
wandb:    final_test_r2 -0.2872
wandb:  final_test_rmse 0.20361
wandb:  final_train_mse 0.04323
wandb:   final_train_r2 -0.94205
wandb: final_train_rmse 0.20793
wandb:    final_val_mse 0.06272
wandb:     final_val_r2 -0.33131
wandb:   final_val_rmse 0.25044
wandb:    learning_rate 0.0001
wandb:       train_loss 0.0841
wandb:       train_time 9.68287
wandb:         val_loss 0.12211
wandb:          val_mse 0.12905
wandb:           val_r2 -1.73921
wandb:         val_rmse 0.35923
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105345-by4lhw89
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105345-by4lhw89/logs
Experiment probe_layer10_complexity_control2_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_ko
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ko]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_ko"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ko"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:54:33,298][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ko
experiment_name: probe_layer10_complexity_control3_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ko
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:54:33,298][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:54:33,298][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:54:33,298][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:54:33,304][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ko']
[2025-05-04 10:54:33,305][__main__][INFO] - Processing language: ko
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:54:36,299][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:54:38,574][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:54:38,574][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:54:38,758][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:54:38,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 10:54:39,001][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-05-04 10:54:39,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:54:39,007][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-05-04 10:54:39,009][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:54:39,086][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:54:39,163][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:54:39,176][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-05-04 10:54:39,178][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:54:39,178][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-05-04 10:54:39,189][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:54:39,214][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:54:39,263][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:54:39,300][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-05-04 10:54:39,302][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:54:39,302][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-05-04 10:54:39,303][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-05-04 10:54:39,304][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:54:39,304][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:54:39,304][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:54:39,304][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:54:39,304][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:54:39,305][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Sample label: 0.2868632674217224
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:54:39,305][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:54:39,305][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:54:39,306][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:54:39,306][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:54:39,306][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-05-04 10:54:39,306][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-05-04 10:54:39,307][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-05-04 10:54:39,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:54:39,307][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:54:39,307][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:54:39,307][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:54:45,359][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:54:45,360][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:54:45,360][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:54:45,361][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:54:45,363][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:54:45,364][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:54:45,364][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:54:45,364][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:54:45,364][__main__][INFO] - Successfully created lm_probe model for ko
[2025-05-04 10:54:45,365][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:54:45,365][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/47 batches, loss: 0.3959Epoch 1/15: [=                             ] 2/47 batches, loss: 0.6875Epoch 1/15: [=                             ] 3/47 batches, loss: 0.7271Epoch 1/15: [==                            ] 4/47 batches, loss: 0.7187Epoch 1/15: [===                           ] 5/47 batches, loss: 0.6537Epoch 1/15: [===                           ] 6/47 batches, loss: 0.5829Epoch 1/15: [====                          ] 7/47 batches, loss: 0.5552Epoch 1/15: [=====                         ] 8/47 batches, loss: 0.5452Epoch 1/15: [=====                         ] 9/47 batches, loss: 0.5489Epoch 1/15: [======                        ] 10/47 batches, loss: 0.5602Epoch 1/15: [=======                       ] 11/47 batches, loss: 0.5230Epoch 1/15: [=======                       ] 12/47 batches, loss: 0.5022Epoch 1/15: [========                      ] 13/47 batches, loss: 0.5033Epoch 1/15: [========                      ] 14/47 batches, loss: 0.4984Epoch 1/15: [=========                     ] 15/47 batches, loss: 0.4857Epoch 1/15: [==========                    ] 16/47 batches, loss: 0.4866Epoch 1/15: [==========                    ] 17/47 batches, loss: 0.4826Epoch 1/15: [===========                   ] 18/47 batches, loss: 0.4854Epoch 1/15: [============                  ] 19/47 batches, loss: 0.4721Epoch 1/15: [============                  ] 20/47 batches, loss: 0.4593Epoch 1/15: [=============                 ] 21/47 batches, loss: 0.4661Epoch 1/15: [==============                ] 22/47 batches, loss: 0.4613Epoch 1/15: [==============                ] 23/47 batches, loss: 0.4628Epoch 1/15: [===============               ] 24/47 batches, loss: 0.4548Epoch 1/15: [===============               ] 25/47 batches, loss: 0.4479Epoch 1/15: [================              ] 26/47 batches, loss: 0.4442Epoch 1/15: [=================             ] 27/47 batches, loss: 0.4407Epoch 1/15: [=================             ] 28/47 batches, loss: 0.4340Epoch 1/15: [==================            ] 29/47 batches, loss: 0.4332Epoch 1/15: [===================           ] 30/47 batches, loss: 0.4285Epoch 1/15: [===================           ] 31/47 batches, loss: 0.4233Epoch 1/15: [====================          ] 32/47 batches, loss: 0.4174Epoch 1/15: [=====================         ] 33/47 batches, loss: 0.4115Epoch 1/15: [=====================         ] 34/47 batches, loss: 0.4145Epoch 1/15: [======================        ] 35/47 batches, loss: 0.4128Epoch 1/15: [======================        ] 36/47 batches, loss: 0.4131Epoch 1/15: [=======================       ] 37/47 batches, loss: 0.4197Epoch 1/15: [========================      ] 38/47 batches, loss: 0.4127Epoch 1/15: [========================      ] 39/47 batches, loss: 0.4057Epoch 1/15: [=========================     ] 40/47 batches, loss: 0.4014Epoch 1/15: [==========================    ] 41/47 batches, loss: 0.3973Epoch 1/15: [==========================    ] 42/47 batches, loss: 0.3914Epoch 1/15: [===========================   ] 43/47 batches, loss: 0.3884Epoch 1/15: [============================  ] 44/47 batches, loss: 0.3874Epoch 1/15: [============================  ] 45/47 batches, loss: 0.3851Epoch 1/15: [============================= ] 46/47 batches, loss: 0.3807Epoch 1/15: [==============================] 47/47 batches, loss: 0.3761
[2025-05-04 10:54:50,229][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3761
[2025-05-04 10:54:50,517][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0589, Metrics: {'mse': 0.06156112253665924, 'rmse': 0.2481151396764398, 'r2': -0.30672764778137207}
Epoch 2/15: [Epoch 2/15: [                              ] 1/47 batches, loss: 0.2446Epoch 2/15: [=                             ] 2/47 batches, loss: 0.1905Epoch 2/15: [=                             ] 3/47 batches, loss: 0.2290Epoch 2/15: [==                            ] 4/47 batches, loss: 0.2065Epoch 2/15: [===                           ] 5/47 batches, loss: 0.2215Epoch 2/15: [===                           ] 6/47 batches, loss: 0.2068Epoch 2/15: [====                          ] 7/47 batches, loss: 0.2056Epoch 2/15: [=====                         ] 8/47 batches, loss: 0.2073Epoch 2/15: [=====                         ] 9/47 batches, loss: 0.2168Epoch 2/15: [======                        ] 10/47 batches, loss: 0.2176Epoch 2/15: [=======                       ] 11/47 batches, loss: 0.2108Epoch 2/15: [=======                       ] 12/47 batches, loss: 0.2175Epoch 2/15: [========                      ] 13/47 batches, loss: 0.2176Epoch 2/15: [========                      ] 14/47 batches, loss: 0.2146Epoch 2/15: [=========                     ] 15/47 batches, loss: 0.2089Epoch 2/15: [==========                    ] 16/47 batches, loss: 0.2058Epoch 2/15: [==========                    ] 17/47 batches, loss: 0.2024Epoch 2/15: [===========                   ] 18/47 batches, loss: 0.2074Epoch 2/15: [============                  ] 19/47 batches, loss: 0.2024Epoch 2/15: [============                  ] 20/47 batches, loss: 0.2018Epoch 2/15: [=============                 ] 21/47 batches, loss: 0.1977Epoch 2/15: [==============                ] 22/47 batches, loss: 0.1983Epoch 2/15: [==============                ] 23/47 batches, loss: 0.2003Epoch 2/15: [===============               ] 24/47 batches, loss: 0.1966Epoch 2/15: [===============               ] 25/47 batches, loss: 0.1957Epoch 2/15: [================              ] 26/47 batches, loss: 0.1938Epoch 2/15: [=================             ] 27/47 batches, loss: 0.1931Epoch 2/15: [=================             ] 28/47 batches, loss: 0.1949Epoch 2/15: [==================            ] 29/47 batches, loss: 0.1942Epoch 2/15: [===================           ] 30/47 batches, loss: 0.1923Epoch 2/15: [===================           ] 31/47 batches, loss: 0.1900Epoch 2/15: [====================          ] 32/47 batches, loss: 0.1866Epoch 2/15: [=====================         ] 33/47 batches, loss: 0.1853Epoch 2/15: [=====================         ] 34/47 batches, loss: 0.1844Epoch 2/15: [======================        ] 35/47 batches, loss: 0.1818Epoch 2/15: [======================        ] 36/47 batches, loss: 0.1857Epoch 2/15: [=======================       ] 37/47 batches, loss: 0.1860Epoch 2/15: [========================      ] 38/47 batches, loss: 0.1834Epoch 2/15: [========================      ] 39/47 batches, loss: 0.1815Epoch 2/15: [=========================     ] 40/47 batches, loss: 0.1798Epoch 2/15: [==========================    ] 41/47 batches, loss: 0.1787Epoch 2/15: [==========================    ] 42/47 batches, loss: 0.1788Epoch 2/15: [===========================   ] 43/47 batches, loss: 0.1778Epoch 2/15: [============================  ] 44/47 batches, loss: 0.1760Epoch 2/15: [============================  ] 45/47 batches, loss: 0.1749Epoch 2/15: [============================= ] 46/47 batches, loss: 0.1768Epoch 2/15: [==============================] 47/47 batches, loss: 0.1818
[2025-05-04 10:54:52,377][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1818
[2025-05-04 10:54:52,622][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0573, Metrics: {'mse': 0.06018586456775665, 'rmse': 0.24532807537613108, 'r2': -0.27753567695617676}
Epoch 3/15: [Epoch 3/15: [                              ] 1/47 batches, loss: 0.1255Epoch 3/15: [=                             ] 2/47 batches, loss: 0.1467Epoch 3/15: [=                             ] 3/47 batches, loss: 0.1385Epoch 3/15: [==                            ] 4/47 batches, loss: 0.1600Epoch 3/15: [===                           ] 5/47 batches, loss: 0.1463Epoch 3/15: [===                           ] 6/47 batches, loss: 0.1302Epoch 3/15: [====                          ] 7/47 batches, loss: 0.1259Epoch 3/15: [=====                         ] 8/47 batches, loss: 0.1176Epoch 3/15: [=====                         ] 9/47 batches, loss: 0.1234Epoch 3/15: [======                        ] 10/47 batches, loss: 0.1211Epoch 3/15: [=======                       ] 11/47 batches, loss: 0.1177Epoch 3/15: [=======                       ] 12/47 batches, loss: 0.1199Epoch 3/15: [========                      ] 13/47 batches, loss: 0.1174Epoch 3/15: [========                      ] 14/47 batches, loss: 0.1148Epoch 3/15: [=========                     ] 15/47 batches, loss: 0.1191Epoch 3/15: [==========                    ] 16/47 batches, loss: 0.1186Epoch 3/15: [==========                    ] 17/47 batches, loss: 0.1181Epoch 3/15: [===========                   ] 18/47 batches, loss: 0.1170Epoch 3/15: [============                  ] 19/47 batches, loss: 0.1156Epoch 3/15: [============                  ] 20/47 batches, loss: 0.1177Epoch 3/15: [=============                 ] 21/47 batches, loss: 0.1174Epoch 3/15: [==============                ] 22/47 batches, loss: 0.1152Epoch 3/15: [==============                ] 23/47 batches, loss: 0.1153Epoch 3/15: [===============               ] 24/47 batches, loss: 0.1155Epoch 3/15: [===============               ] 25/47 batches, loss: 0.1135Epoch 3/15: [================              ] 26/47 batches, loss: 0.1126Epoch 3/15: [=================             ] 27/47 batches, loss: 0.1138Epoch 3/15: [=================             ] 28/47 batches, loss: 0.1143Epoch 3/15: [==================            ] 29/47 batches, loss: 0.1134Epoch 3/15: [===================           ] 30/47 batches, loss: 0.1126Epoch 3/15: [===================           ] 31/47 batches, loss: 0.1148Epoch 3/15: [====================          ] 32/47 batches, loss: 0.1146Epoch 3/15: [=====================         ] 33/47 batches, loss: 0.1152Epoch 3/15: [=====================         ] 34/47 batches, loss: 0.1147Epoch 3/15: [======================        ] 35/47 batches, loss: 0.1156Epoch 3/15: [======================        ] 36/47 batches, loss: 0.1158Epoch 3/15: [=======================       ] 37/47 batches, loss: 0.1153Epoch 3/15: [========================      ] 38/47 batches, loss: 0.1147Epoch 3/15: [========================      ] 39/47 batches, loss: 0.1145Epoch 3/15: [=========================     ] 40/47 batches, loss: 0.1146Epoch 3/15: [==========================    ] 41/47 batches, loss: 0.1152Epoch 3/15: [==========================    ] 42/47 batches, loss: 0.1167Epoch 3/15: [===========================   ] 43/47 batches, loss: 0.1162Epoch 3/15: [============================  ] 44/47 batches, loss: 0.1167Epoch 3/15: [============================  ] 45/47 batches, loss: 0.1170Epoch 3/15: [============================= ] 46/47 batches, loss: 0.1185Epoch 3/15: [==============================] 47/47 batches, loss: 0.1187
[2025-05-04 10:54:54,491][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1187
[2025-05-04 10:54:54,753][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0732, Metrics: {'mse': 0.07830478250980377, 'rmse': 0.27982991711002553, 'r2': -0.6621370315551758}
[2025-05-04 10:54:54,753][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/47 batches, loss: 0.1289Epoch 4/15: [=                             ] 2/47 batches, loss: 0.1027Epoch 4/15: [=                             ] 3/47 batches, loss: 0.1146Epoch 4/15: [==                            ] 4/47 batches, loss: 0.1222Epoch 4/15: [===                           ] 5/47 batches, loss: 0.1213Epoch 4/15: [===                           ] 6/47 batches, loss: 0.1156Epoch 4/15: [====                          ] 7/47 batches, loss: 0.1150Epoch 4/15: [=====                         ] 8/47 batches, loss: 0.1093Epoch 4/15: [=====                         ] 9/47 batches, loss: 0.1096Epoch 4/15: [======                        ] 10/47 batches, loss: 0.1138Epoch 4/15: [=======                       ] 11/47 batches, loss: 0.1109Epoch 4/15: [=======                       ] 12/47 batches, loss: 0.1091Epoch 4/15: [========                      ] 13/47 batches, loss: 0.1127Epoch 4/15: [========                      ] 14/47 batches, loss: 0.1124Epoch 4/15: [=========                     ] 15/47 batches, loss: 0.1132Epoch 4/15: [==========                    ] 16/47 batches, loss: 0.1137Epoch 4/15: [==========                    ] 17/47 batches, loss: 0.1108Epoch 4/15: [===========                   ] 18/47 batches, loss: 0.1126Epoch 4/15: [============                  ] 19/47 batches, loss: 0.1178Epoch 4/15: [============                  ] 20/47 batches, loss: 0.1210Epoch 4/15: [=============                 ] 21/47 batches, loss: 0.1233Epoch 4/15: [==============                ] 22/47 batches, loss: 0.1226Epoch 4/15: [==============                ] 23/47 batches, loss: 0.1242Epoch 4/15: [===============               ] 24/47 batches, loss: 0.1215Epoch 4/15: [===============               ] 25/47 batches, loss: 0.1231Epoch 4/15: [================              ] 26/47 batches, loss: 0.1214Epoch 4/15: [=================             ] 27/47 batches, loss: 0.1217Epoch 4/15: [=================             ] 28/47 batches, loss: 0.1209Epoch 4/15: [==================            ] 29/47 batches, loss: 0.1200Epoch 4/15: [===================           ] 30/47 batches, loss: 0.1198Epoch 4/15: [===================           ] 31/47 batches, loss: 0.1189Epoch 4/15: [====================          ] 32/47 batches, loss: 0.1183Epoch 4/15: [=====================         ] 33/47 batches, loss: 0.1168Epoch 4/15: [=====================         ] 34/47 batches, loss: 0.1150Epoch 4/15: [======================        ] 35/47 batches, loss: 0.1157Epoch 4/15: [======================        ] 36/47 batches, loss: 0.1136Epoch 4/15: [=======================       ] 37/47 batches, loss: 0.1147Epoch 4/15: [========================      ] 38/47 batches, loss: 0.1160Epoch 4/15: [========================      ] 39/47 batches, loss: 0.1149Epoch 4/15: [=========================     ] 40/47 batches, loss: 0.1135Epoch 4/15: [==========================    ] 41/47 batches, loss: 0.1144Epoch 4/15: [==========================    ] 42/47 batches, loss: 0.1143Epoch 4/15: [===========================   ] 43/47 batches, loss: 0.1130Epoch 4/15: [============================  ] 44/47 batches, loss: 0.1123Epoch 4/15: [============================  ] 45/47 batches, loss: 0.1103Epoch 4/15: [============================= ] 46/47 batches, loss: 0.1107Epoch 4/15: [==============================] 47/47 batches, loss: 0.1124
[2025-05-04 10:54:56,219][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1124
[2025-05-04 10:54:56,491][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0787, Metrics: {'mse': 0.0837571993470192, 'rmse': 0.28940836087960414, 'r2': -0.7778729200363159}
[2025-05-04 10:54:56,491][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/47 batches, loss: 0.1255Epoch 5/15: [=                             ] 2/47 batches, loss: 0.1267Epoch 5/15: [=                             ] 3/47 batches, loss: 0.1173Epoch 5/15: [==                            ] 4/47 batches, loss: 0.1062Epoch 5/15: [===                           ] 5/47 batches, loss: 0.0973Epoch 5/15: [===                           ] 6/47 batches, loss: 0.1037Epoch 5/15: [====                          ] 7/47 batches, loss: 0.1058Epoch 5/15: [=====                         ] 8/47 batches, loss: 0.1040Epoch 5/15: [=====                         ] 9/47 batches, loss: 0.1035Epoch 5/15: [======                        ] 10/47 batches, loss: 0.1021Epoch 5/15: [=======                       ] 11/47 batches, loss: 0.0992Epoch 5/15: [=======                       ] 12/47 batches, loss: 0.0955Epoch 5/15: [========                      ] 13/47 batches, loss: 0.0909Epoch 5/15: [========                      ] 14/47 batches, loss: 0.0879Epoch 5/15: [=========                     ] 15/47 batches, loss: 0.0874Epoch 5/15: [==========                    ] 16/47 batches, loss: 0.0872Epoch 5/15: [==========                    ] 17/47 batches, loss: 0.0858Epoch 5/15: [===========                   ] 18/47 batches, loss: 0.0875Epoch 5/15: [============                  ] 19/47 batches, loss: 0.0884Epoch 5/15: [============                  ] 20/47 batches, loss: 0.0898Epoch 5/15: [=============                 ] 21/47 batches, loss: 0.0900Epoch 5/15: [==============                ] 22/47 batches, loss: 0.0913Epoch 5/15: [==============                ] 23/47 batches, loss: 0.0921Epoch 5/15: [===============               ] 24/47 batches, loss: 0.0948Epoch 5/15: [===============               ] 25/47 batches, loss: 0.0940Epoch 5/15: [================              ] 26/47 batches, loss: 0.0931Epoch 5/15: [=================             ] 27/47 batches, loss: 0.0930Epoch 5/15: [=================             ] 28/47 batches, loss: 0.0924Epoch 5/15: [==================            ] 29/47 batches, loss: 0.0911Epoch 5/15: [===================           ] 30/47 batches, loss: 0.0931Epoch 5/15: [===================           ] 31/47 batches, loss: 0.0935Epoch 5/15: [====================          ] 32/47 batches, loss: 0.0923Epoch 5/15: [=====================         ] 33/47 batches, loss: 0.0919Epoch 5/15: [=====================         ] 34/47 batches, loss: 0.0913Epoch 5/15: [======================        ] 35/47 batches, loss: 0.0907Epoch 5/15: [======================        ] 36/47 batches, loss: 0.0914Epoch 5/15: [=======================       ] 37/47 batches, loss: 0.0906Epoch 5/15: [========================      ] 38/47 batches, loss: 0.0908Epoch 5/15: [========================      ] 39/47 batches, loss: 0.0903Epoch 5/15: [=========================     ] 40/47 batches, loss: 0.0895Epoch 5/15: [==========================    ] 41/47 batches, loss: 0.0887Epoch 5/15: [==========================    ] 42/47 batches, loss: 0.0883Epoch 5/15: [===========================   ] 43/47 batches, loss: 0.0902Epoch 5/15: [============================  ] 44/47 batches, loss: 0.0896Epoch 5/15: [============================  ] 45/47 batches, loss: 0.0900Epoch 5/15: [============================= ] 46/47 batches, loss: 0.0886Epoch 5/15: [==============================] 47/47 batches, loss: 0.0869
[2025-05-04 10:54:57,975][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0869
[2025-05-04 10:54:58,220][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0987, Metrics: {'mse': 0.10456420481204987, 'rmse': 0.3233638891590245, 'r2': -1.2195329666137695}
[2025-05-04 10:54:58,221][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/47 batches, loss: 0.1096Epoch 6/15: [=                             ] 2/47 batches, loss: 0.0963Epoch 6/15: [=                             ] 3/47 batches, loss: 0.0954Epoch 6/15: [==                            ] 4/47 batches, loss: 0.0852Epoch 6/15: [===                           ] 5/47 batches, loss: 0.0947Epoch 6/15: [===                           ] 6/47 batches, loss: 0.0928Epoch 6/15: [====                          ] 7/47 batches, loss: 0.0912Epoch 6/15: [=====                         ] 8/47 batches, loss: 0.0961Epoch 6/15: [=====                         ] 9/47 batches, loss: 0.0917Epoch 6/15: [======                        ] 10/47 batches, loss: 0.0920Epoch 6/15: [=======                       ] 11/47 batches, loss: 0.0904Epoch 6/15: [=======                       ] 12/47 batches, loss: 0.0947Epoch 6/15: [========                      ] 13/47 batches, loss: 0.0912Epoch 6/15: [========                      ] 14/47 batches, loss: 0.0883Epoch 6/15: [=========                     ] 15/47 batches, loss: 0.0862Epoch 6/15: [==========                    ] 16/47 batches, loss: 0.0845Epoch 6/15: [==========                    ] 17/47 batches, loss: 0.0835Epoch 6/15: [===========                   ] 18/47 batches, loss: 0.0818Epoch 6/15: [============                  ] 19/47 batches, loss: 0.0811Epoch 6/15: [============                  ] 20/47 batches, loss: 0.0795Epoch 6/15: [=============                 ] 21/47 batches, loss: 0.0783Epoch 6/15: [==============                ] 22/47 batches, loss: 0.0792Epoch 6/15: [==============                ] 23/47 batches, loss: 0.0789Epoch 6/15: [===============               ] 24/47 batches, loss: 0.0780Epoch 6/15: [===============               ] 25/47 batches, loss: 0.0777Epoch 6/15: [================              ] 26/47 batches, loss: 0.0785Epoch 6/15: [=================             ] 27/47 batches, loss: 0.0774Epoch 6/15: [=================             ] 28/47 batches, loss: 0.0789Epoch 6/15: [==================            ] 29/47 batches, loss: 0.0795Epoch 6/15: [===================           ] 30/47 batches, loss: 0.0805Epoch 6/15: [===================           ] 31/47 batches, loss: 0.0807Epoch 6/15: [====================          ] 32/47 batches, loss: 0.0813Epoch 6/15: [=====================         ] 33/47 batches, loss: 0.0812Epoch 6/15: [=====================         ] 34/47 batches, loss: 0.0830Epoch 6/15: [======================        ] 35/47 batches, loss: 0.0820Epoch 6/15: [======================        ] 36/47 batches, loss: 0.0804Epoch 6/15: [=======================       ] 37/47 batches, loss: 0.0801Epoch 6/15: [========================      ] 38/47 batches, loss: 0.0799Epoch 6/15: [========================      ] 39/47 batches, loss: 0.0796Epoch 6/15: [=========================     ] 40/47 batches, loss: 0.0795Epoch 6/15: [==========================    ] 41/47 batches, loss: 0.0798Epoch 6/15: [==========================    ] 42/47 batches, loss: 0.0801Epoch 6/15: [===========================   ] 43/47 batches, loss: 0.0795Epoch 6/15: [============================  ] 44/47 batches, loss: 0.0790Epoch 6/15: [============================  ] 45/47 batches, loss: 0.0780Epoch 6/15: [============================= ] 46/47 batches, loss: 0.0775Epoch 6/15: [==============================] 47/47 batches, loss: 0.0774
[2025-05-04 10:54:59,687][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0774
[2025-05-04 10:54:59,948][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0823, Metrics: {'mse': 0.08728455752134323, 'rmse': 0.2954396004623335, 'r2': -0.8527463674545288}
[2025-05-04 10:54:59,949][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:54:59,949][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-04 10:54:59,949][src.training.lm_trainer][INFO] - Training completed in 11.98 seconds
[2025-05-04 10:54:59,949][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:55:02,077][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.029407037422060966, 'rmse': 0.17148480230638796, 'r2': -0.32097959518432617}
[2025-05-04 10:55:02,077][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06018586456775665, 'rmse': 0.24532807537613108, 'r2': -0.27753567695617676}
[2025-05-04 10:55:02,077][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.037200070917606354, 'rmse': 0.19287319906510172, 'r2': -0.15506887435913086}
[2025-05-04 10:55:04,016][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ko/ko/model.pt
[2025-05-04 10:55:04,018][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▆▆▄▄▁
wandb:       train_loss █▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁▁▄▅█▅
wandb:          val_mse ▁▁▄▅█▅
wandb:           val_r2 ██▅▄▁▄
wandb:         val_rmse ▁▁▄▅█▅
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05727
wandb:     best_val_mse 0.06019
wandb:      best_val_r2 -0.27754
wandb:    best_val_rmse 0.24533
wandb: early_stop_epoch 6
wandb:            epoch 6
wandb:   final_test_mse 0.0372
wandb:    final_test_r2 -0.15507
wandb:  final_test_rmse 0.19287
wandb:  final_train_mse 0.02941
wandb:   final_train_r2 -0.32098
wandb: final_train_rmse 0.17148
wandb:    final_val_mse 0.06019
wandb:     final_val_r2 -0.27754
wandb:   final_val_rmse 0.24533
wandb:    learning_rate 0.0001
wandb:       train_loss 0.07743
wandb:       train_time 11.97605
wandb:         val_loss 0.08227
wandb:          val_mse 0.08728
wandb:           val_r2 -0.85275
wandb:         val_rmse 0.29544
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105433-bt2rt9zi
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105433-bt2rt9zi/logs
Experiment probe_layer10_complexity_control3_ko completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ko/ko/results.json for layer 10
Running experiment: probe_layer10_question_type_control1_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control1_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:55:20,596][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ru
experiment_name: probe_layer10_question_type_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:55:20,596][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:55:20,596][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:55:20,596][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:55:20,601][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-04 10:55:20,601][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:55:22,818][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:55:25,113][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:55:25,114][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:55:25,181][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:55:25,236][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 10:55:25,412][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 10:55:25,420][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:55:25,421][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 10:55:25,422][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:55:25,464][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:55:25,510][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:55:25,528][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 10:55:25,529][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:55:25,529][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 10:55:25,530][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:55:25,599][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:55:25,692][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:55:25,706][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 10:55:25,708][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:55:25,708][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 10:55:25,709][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 10:55:25,710][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:55:25,710][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:55:25,710][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:55:25,710][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:55:25,711][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-04 10:55:25,711][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:55:25,711][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:55:25,712][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:55:25,712][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:55:25,712][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:55:25,712][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 10:55:25,712][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:55:25,713][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 10:55:25,713][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:55:25,713][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:55:25,713][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:55:25,714][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:55:30,919][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:55:30,921][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:55:30,921][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:55:30,921][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:55:30,927][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:55:30,927][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:55:30,927][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:55:30,928][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:55:30,928][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 10:55:30,928][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:55:30,929][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:55:30,929][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6457Epoch 1/15: [                              ] 2/75 batches, loss: 0.6518Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6847Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6876Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6920Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6943Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6877Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6845Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6908Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6936Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6922Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.6915Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.6916Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6907Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6906Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6904Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6925Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6928Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6925Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6922Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6922Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6918Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6917Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6920Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6917Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6917Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6920Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6919Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6919Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6920Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6922Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6927Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6922Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6921Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6921Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6924Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6924Epoch 1/15: [================              ] 40/75 batches, loss: 0.6925Epoch 1/15: [================              ] 41/75 batches, loss: 0.6924Epoch 1/15: [================              ] 42/75 batches, loss: 0.6924Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6924Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6925Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6927Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6927Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6928Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6925Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6925Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6925Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6926Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6928Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6929Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6928Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6929Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6929Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6929Epoch 1/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:55:37,256][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6929
[2025-05-04 10:55:37,488][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6979Epoch 2/15: [                              ] 2/75 batches, loss: 0.6951Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6945Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6962Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6956Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6953Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6948Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6939Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6942Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6935Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6933Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6933Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6926Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6926Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6923Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6919Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6922Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6923Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6922Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6923Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6924Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6924Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6924Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6923Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6922Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6923Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6923Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6923Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6926Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6928Epoch 2/15: [================              ] 40/75 batches, loss: 0.6928Epoch 2/15: [================              ] 41/75 batches, loss: 0.6927Epoch 2/15: [================              ] 42/75 batches, loss: 0.6927Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6927Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6925Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6927Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6926Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6927Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6927Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6928Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6926Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6926Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6926Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6925Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6925Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6924Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6925Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6926Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6922Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6922Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6920Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6922Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6923Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6921Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6921Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6923Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6928Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6928Epoch 2/15: [==============================] 75/75 batches, loss: 0.6928
[2025-05-04 10:55:40,181][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6928
[2025-05-04 10:55:40,453][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6944, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:40,453][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6907Epoch 3/15: [                              ] 2/75 batches, loss: 0.6938Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6948Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6943Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6959Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6955Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6946Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6941Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6940Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6934Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6934Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6936Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6936Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6933Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6935Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6936Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6942Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6942Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6941Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6937Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6937Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6937Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6936Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6937Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6937Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6937Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6937Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6939Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6938Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6938Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6938Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6938Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6939Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6939Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6938Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6938Epoch 3/15: [================              ] 40/75 batches, loss: 0.6943Epoch 3/15: [================              ] 41/75 batches, loss: 0.6942Epoch 3/15: [================              ] 42/75 batches, loss: 0.6942Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6942Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6941Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6942Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6941Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6941Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6940Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6940Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6940Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6940Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6940Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6939Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6939Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6939Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6939Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6939Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6938Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6938Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6938Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6938Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6938Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6938Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6937Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6937Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6937Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6937Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6937Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6937Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6937Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6937Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6937Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6937Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6937Epoch 3/15: [==============================] 75/75 batches, loss: 0.6937
[2025-05-04 10:55:42,725][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6937
[2025-05-04 10:55:42,980][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6932Epoch 4/15: [                              ] 2/75 batches, loss: 0.6933Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6935Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6933Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6928Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6928Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6928Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6928Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6928Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6930Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6930Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 4/15: [================              ] 40/75 batches, loss: 0.6929Epoch 4/15: [================              ] 41/75 batches, loss: 0.6929Epoch 4/15: [================              ] 42/75 batches, loss: 0.6929Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6929Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6929Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6929Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6928Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6929Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6928Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6929Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6928Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6928Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6929Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6929Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6929Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6929Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6929Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6929Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6928Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6929Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6929Epoch 4/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:55:45,731][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6929
[2025-05-04 10:55:46,004][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6943, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:46,005][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6969Epoch 5/15: [                              ] 2/75 batches, loss: 0.6949Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6951Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6953Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6942Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6939Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6940Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6938Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6936Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6935Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6929Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6929Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6933Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6928Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 5/15: [================              ] 40/75 batches, loss: 0.6932Epoch 5/15: [================              ] 41/75 batches, loss: 0.6931Epoch 5/15: [================              ] 42/75 batches, loss: 0.6930Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6929Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6928Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6927Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6926Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6927Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6927Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6927Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6927Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6926Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6929Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6928Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6928Epoch 5/15: [==============================] 75/75 batches, loss: 0.6928
[2025-05-04 10:55:48,293][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6928
[2025-05-04 10:55:48,575][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6983, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:48,575][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6900Epoch 6/15: [                              ] 2/75 batches, loss: 0.6912Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6917Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6935Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6955Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6959Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6957Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6951Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6941Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6941Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6939Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6935Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6936Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6937Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6934Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6928Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6926Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6922Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6918Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6920Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6921Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6922Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6928Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6929Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6927Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6935Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6934Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6933Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6934Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6934Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6933Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 6/15: [================              ] 40/75 batches, loss: 0.6928Epoch 6/15: [================              ] 41/75 batches, loss: 0.6928Epoch 6/15: [================              ] 42/75 batches, loss: 0.6929Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6929Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6929Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6927Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6924Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6924Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6923Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6923Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6926Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6926Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6927Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6925Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6924Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6926Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6923Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6924Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6923Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6924Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6924Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6926Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6926Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6925Epoch 6/15: [==============================] 75/75 batches, loss: 0.6926
[2025-05-04 10:55:50,888][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6926
[2025-05-04 10:55:51,166][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.7017, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:51,167][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:55:51,167][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-04 10:55:51,167][src.training.lm_trainer][INFO] - Training completed in 17.00 seconds
[2025-05-04 10:55:51,167][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:55:54,083][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:54,084][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:54,084][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:55:55,916][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ru/ru/model.pt
[2025-05-04 10:55:55,917][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁
wandb:           best_val_f1 ▁▁
wandb:         best_val_loss █▁
wandb:    best_val_precision ▁▁
wandb:       best_val_recall ▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁
wandb:            train_loss ▃▂█▃▂▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁
wandb:              val_loss ▁▂▁▂▅█
wandb:         val_precision ▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69353
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 6
wandb:                 epoch 6
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69263
wandb:            train_time 16.99712
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.70169
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105520-elk1xc7j
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105520-elk1xc7j/logs
Experiment probe_layer10_question_type_control1_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer10/ru/ru/results.json for layer 10
Running experiment: probe_layer10_question_type_control2_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control2_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:56:15,491][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ru
experiment_name: probe_layer10_question_type_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:56:15,491][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:56:15,491][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:56:15,491][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:56:15,496][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-04 10:56:15,497][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:56:18,427][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:56:20,736][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:56:20,736][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:56:20,899][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:56:20,952][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 10:56:21,148][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 10:56:21,156][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:56:21,157][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 10:56:21,168][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:56:21,221][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:56:21,297][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:56:21,334][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 10:56:21,335][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:56:21,335][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 10:56:21,336][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:56:21,392][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:56:21,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:56:21,472][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 10:56:21,474][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:56:21,474][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 10:56:21,475][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 10:56:21,476][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:56:21,476][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:56:21,476][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:56:21,477][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-04 10:56:21,477][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:56:21,477][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:56:21,477][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:56:21,478][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:56:21,478][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:56:21,478][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 10:56:21,478][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:56:21,479][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 10:56:21,479][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:56:21,479][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:56:21,479][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:56:21,479][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:56:27,730][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:56:27,731][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:56:27,731][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:56:27,731][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:56:27,737][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:56:27,738][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:56:27,738][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:56:27,738][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:56:27,738][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 10:56:27,739][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:56:27,739][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:56:27,740][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7218Epoch 1/15: [                              ] 2/75 batches, loss: 0.7156Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7143Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7103Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7037Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7058Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7031Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7021Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7022Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7009Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6998Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7001Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7003Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7002Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6997Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6992Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6988Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6987Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6985Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6982Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6978Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6976Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6975Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6973Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6972Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6971Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6970Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6969Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6968Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6966Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6964Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6963Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6962Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6961Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6961Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6960Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6959Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6958Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6958Epoch 1/15: [================              ] 40/75 batches, loss: 0.6957Epoch 1/15: [================              ] 41/75 batches, loss: 0.6956Epoch 1/15: [================              ] 42/75 batches, loss: 0.6955Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6955Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6955Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6954Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6954Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6953Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6953Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6952Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6952Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6951Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6951Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6951Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6950Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6950Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6950Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6949Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6949Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6948Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6948Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6948Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6947Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6947Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6947Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6946Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6946Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6946Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6946Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6945Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6945Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6945Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6944Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6944Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6944Epoch 1/15: [==============================] 75/75 batches, loss: 0.6944
[2025-05-04 10:56:33,728][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6944
[2025-05-04 10:56:33,985][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6936Epoch 2/15: [                              ] 2/75 batches, loss: 0.6936Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6943Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6941Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6939Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6937Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6936Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6936Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6935Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6935Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6935Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6935Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6935Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6935Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6935Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6934Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6935Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6935Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6935Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6935Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6935Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6935Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6935Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6935Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6935Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6934Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6934Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6934Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6934Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6934Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6934Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6934Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6934Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6934Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6934Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6934Epoch 2/15: [================              ] 40/75 batches, loss: 0.6934Epoch 2/15: [================              ] 41/75 batches, loss: 0.6934Epoch 2/15: [================              ] 42/75 batches, loss: 0.6934Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6933Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6933Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6933Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6933Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6933Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6933Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6933Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6933Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6933Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 2/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:56:36,658][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 10:56:36,919][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6931Epoch 3/15: [                              ] 2/75 batches, loss: 0.6928Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6932Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6928Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6933Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6933Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6933Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 3/15: [================              ] 40/75 batches, loss: 0.6930Epoch 3/15: [================              ] 41/75 batches, loss: 0.6931Epoch 3/15: [================              ] 42/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:56:39,931][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-04 10:56:40,222][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6932Epoch 4/15: [                              ] 2/75 batches, loss: 0.6932Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6929Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6930Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6929Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6930Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6930Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6935Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6934Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6934Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6934Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6933Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6933Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 4/15: [================              ] 40/75 batches, loss: 0.6930Epoch 4/15: [================              ] 41/75 batches, loss: 0.6931Epoch 4/15: [================              ] 42/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6934Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6934Epoch 4/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-04 10:56:42,869][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6933
[2025-05-04 10:56:43,131][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6936Epoch 5/15: [                              ] 2/75 batches, loss: 0.6935Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6939Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6936Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6935Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6933Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6932Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 5/15: [================              ] 40/75 batches, loss: 0.6932Epoch 5/15: [================              ] 41/75 batches, loss: 0.6932Epoch 5/15: [================              ] 42/75 batches, loss: 0.6932Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6932Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 5/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:56:45,822][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6931
[2025-05-04 10:56:46,124][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:56:46,124][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6932Epoch 6/15: [                              ] 2/75 batches, loss: 0.6932Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6933Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6933Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6933Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6933Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6933Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 6/15: [================              ] 40/75 batches, loss: 0.6931Epoch 6/15: [================              ] 41/75 batches, loss: 0.6931Epoch 6/15: [================              ] 42/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6932Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6932Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6932Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6932Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6932Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6932Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6932Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6932Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 6/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:56:48,423][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6931
[2025-05-04 10:56:48,675][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:56:48,675][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.6935Epoch 7/15: [                              ] 2/75 batches, loss: 0.6935Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6934Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6934Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6933Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6932Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6931Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6930Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6930Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6930Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6930Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6930Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6930Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6930Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6930Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 7/15: [================              ] 40/75 batches, loss: 0.6931Epoch 7/15: [================              ] 41/75 batches, loss: 0.6930Epoch 7/15: [================              ] 42/75 batches, loss: 0.6930Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6929Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6930Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6930Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6930Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 7/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:56:50,969][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6932
[2025-05-04 10:56:51,260][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6950Epoch 8/15: [                              ] 2/75 batches, loss: 0.6939Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6932Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6932Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6930Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6930Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6930Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6930Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6930Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6931Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6931Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6931Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6931Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6931Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6931Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 8/15: [================              ] 40/75 batches, loss: 0.6931Epoch 8/15: [================              ] 41/75 batches, loss: 0.6931Epoch 8/15: [================              ] 42/75 batches, loss: 0.6931Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 8/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:56:54,010][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6931
[2025-05-04 10:56:54,275][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:56:54,275][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6936Epoch 9/15: [                              ] 2/75 batches, loss: 0.6932Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6931Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6931Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6932Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6931Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6932Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6932Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6932Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6932Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6932Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6932Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6932Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6932Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6932Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6932Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6932Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6931Epoch 9/15: [================              ] 40/75 batches, loss: 0.6931Epoch 9/15: [================              ] 41/75 batches, loss: 0.6931Epoch 9/15: [================              ] 42/75 batches, loss: 0.6931Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6931Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6931Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 9/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:56:56,594][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6931
[2025-05-04 10:56:56,854][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:56:56,855][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/15: [Epoch 10/15: [                              ] 1/75 batches, loss: 0.6935Epoch 10/15: [                              ] 2/75 batches, loss: 0.6928Epoch 10/15: [=                             ] 3/75 batches, loss: 0.6926Epoch 10/15: [=                             ] 4/75 batches, loss: 0.6924Epoch 10/15: [==                            ] 5/75 batches, loss: 0.6924Epoch 10/15: [==                            ] 6/75 batches, loss: 0.6924Epoch 10/15: [==                            ] 7/75 batches, loss: 0.6927Epoch 10/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 10/15: [===                           ] 9/75 batches, loss: 0.6928Epoch 10/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 10/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 10/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 10/15: [=====                         ] 13/75 batches, loss: 0.6928Epoch 10/15: [=====                         ] 14/75 batches, loss: 0.6928Epoch 10/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 10/15: [======                        ] 16/75 batches, loss: 0.6927Epoch 10/15: [======                        ] 17/75 batches, loss: 0.6928Epoch 10/15: [=======                       ] 18/75 batches, loss: 0.6927Epoch 10/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 10/15: [========                      ] 20/75 batches, loss: 0.6926Epoch 10/15: [========                      ] 21/75 batches, loss: 0.6925Epoch 10/15: [========                      ] 22/75 batches, loss: 0.6927Epoch 10/15: [=========                     ] 23/75 batches, loss: 0.6925Epoch 10/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 10/15: [==========                    ] 25/75 batches, loss: 0.6925Epoch 10/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 10/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 10/15: [===========                   ] 28/75 batches, loss: 0.6929Epoch 10/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 10/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 10/15: [============                  ] 31/75 batches, loss: 0.6926Epoch 10/15: [============                  ] 32/75 batches, loss: 0.6930Epoch 10/15: [=============                 ] 33/75 batches, loss: 0.6927Epoch 10/15: [=============                 ] 34/75 batches, loss: 0.6923Epoch 10/15: [==============                ] 35/75 batches, loss: 0.6922Epoch 10/15: [==============                ] 36/75 batches, loss: 0.6923Epoch 10/15: [==============                ] 37/75 batches, loss: 0.6920Epoch 10/15: [===============               ] 38/75 batches, loss: 0.6920Epoch 10/15: [===============               ] 39/75 batches, loss: 0.6916Epoch 10/15: [================              ] 40/75 batches, loss: 0.6938Epoch 10/15: [================              ] 41/75 batches, loss: 0.6935Epoch 10/15: [================              ] 42/75 batches, loss: 0.6936Epoch 10/15: [=================             ] 43/75 batches, loss: 0.6936Epoch 10/15: [=================             ] 44/75 batches, loss: 0.6936Epoch 10/15: [==================            ] 45/75 batches, loss: 0.6935Epoch 10/15: [==================            ] 46/75 batches, loss: 0.6934Epoch 10/15: [==================            ] 47/75 batches, loss: 0.6934Epoch 10/15: [===================           ] 48/75 batches, loss: 0.6934Epoch 10/15: [===================           ] 49/75 batches, loss: 0.6933Epoch 10/15: [====================          ] 50/75 batches, loss: 0.6933Epoch 10/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 10/15: [====================          ] 52/75 batches, loss: 0.6933Epoch 10/15: [=====================         ] 53/75 batches, loss: 0.6933Epoch 10/15: [=====================         ] 54/75 batches, loss: 0.6933Epoch 10/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 10/15: [======================        ] 56/75 batches, loss: 0.6933Epoch 10/15: [======================        ] 57/75 batches, loss: 0.6933Epoch 10/15: [=======================       ] 58/75 batches, loss: 0.6933Epoch 10/15: [=======================       ] 59/75 batches, loss: 0.6933Epoch 10/15: [========================      ] 60/75 batches, loss: 0.6933Epoch 10/15: [========================      ] 61/75 batches, loss: 0.6933Epoch 10/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 10/15: [=========================     ] 63/75 batches, loss: 0.6932Epoch 10/15: [=========================     ] 64/75 batches, loss: 0.6932Epoch 10/15: [==========================    ] 65/75 batches, loss: 0.6932Epoch 10/15: [==========================    ] 66/75 batches, loss: 0.6932Epoch 10/15: [==========================    ] 67/75 batches, loss: 0.6932Epoch 10/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 10/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 10/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 10/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 10/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 10/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 10/15: [============================= ] 74/75 batches, loss: 0.6932Epoch 10/15: [==============================] 75/75 batches, loss: 0.6932
[2025-05-04 10:56:59,172][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6932
[2025-05-04 10:56:59,470][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:56:59,471][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:56:59,471][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-04 10:56:59,471][src.training.lm_trainer][INFO] - Training completed in 28.91 seconds
[2025-05-04 10:56:59,471][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:57:02,465][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:02,465][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:02,465][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:04,380][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ru/ru/model.pt
[2025-05-04 10:57:04,382][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁
wandb:         best_val_loss ██▇▅▁
wandb:    best_val_precision ▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁
wandb:            train_loss █▁▁▂▁▁▂▁▁▂
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁▁
wandb:              val_loss ██▇▅▆▅▁▅▂▃
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69304
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 10
wandb:                 epoch 10
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69323
wandb:            train_time 28.91043
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69308
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105615-1xccwsv7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105615-1xccwsv7/logs
Experiment probe_layer10_question_type_control2_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer10/ru/ru/results.json for layer 10
Running experiment: probe_layer10_question_type_control3_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_question_type_control3_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:57:24,949][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ru
experiment_name: probe_layer10_question_type_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 10:57:24,949][__main__][INFO] - Normalized task: question_type
[2025-05-04 10:57:24,949][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 10:57:24,950][__main__][INFO] - Determined Task Type: classification
[2025-05-04 10:57:24,955][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ru']
[2025-05-04 10:57:24,955][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:57:27,835][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:57:30,149][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:57:30,150][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:57:30,366][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:57:30,426][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 10:57:30,653][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 10:57:30,662][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:57:30,662][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 10:57:30,664][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:57:30,695][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:57:30,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:57:30,775][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 10:57:30,776][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:57:30,776][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 10:57:30,777][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:57:30,823][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:57:30,876][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:57:30,914][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 10:57:30,915][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:57:30,916][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 10:57:30,917][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 10:57:30,918][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:57:30,919][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-05-04 10:57:30,919][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:57:30,919][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:57:30,920][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-05-04 10:57:30,920][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 10:57:30,920][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 10:57:30,921][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-05-04 10:57:30,921][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-05-04 10:57:30,921][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 10:57:30,921][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 10:57:30,921][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 10:57:30,921][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:57:30,921][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:57:30,922][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 10:57:30,922][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:57:37,727][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:57:37,728][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:57:37,728][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:57:37,729][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 10:57:37,735][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 10:57:37,735][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 10:57:37,735][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 10:57:37,735][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 10:57:37,735][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 10:57:37,736][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 10:57:37,736][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 10:57:37,737][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6536Epoch 1/15: [                              ] 2/75 batches, loss: 0.7012Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7056Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7242Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7222Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7072Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7029Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7059Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7019Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7012Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7029Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7005Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7016Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7016Epoch 1/15: [======                        ] 15/75 batches, loss: 0.6997Epoch 1/15: [======                        ] 16/75 batches, loss: 0.6986Epoch 1/15: [======                        ] 17/75 batches, loss: 0.6984Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.6981Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.6983Epoch 1/15: [========                      ] 20/75 batches, loss: 0.6976Epoch 1/15: [========                      ] 21/75 batches, loss: 0.6976Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6977Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6977Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6979Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6977Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6976Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6975Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6973Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6972Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6972Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6970Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6967Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6966Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6966Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6964Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6961Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6959Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6959Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6959Epoch 1/15: [================              ] 40/75 batches, loss: 0.6958Epoch 1/15: [================              ] 41/75 batches, loss: 0.6958Epoch 1/15: [================              ] 42/75 batches, loss: 0.6959Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6957Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6958Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6958Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6957Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6955Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6956Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6955Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6956Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6954Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6953Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6953Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6953Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6951Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6951Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6952Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6951Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6950Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6949Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6949Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6949Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6949Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6949Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6949Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6949Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6949Epoch 1/15: [==============================] 75/75 batches, loss: 0.6948
[2025-05-04 10:57:44,417][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6948
[2025-05-04 10:57:44,664][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6936, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6918Epoch 2/15: [                              ] 2/75 batches, loss: 0.6921Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6933Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6938Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6939Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6934Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6937Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6931Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6928Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6924Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6927Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6928Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6926Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6925Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6926Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6925Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6928Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6926Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6927Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6926Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6926Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6929Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6927Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6928Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6929Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6929Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 2/15: [================              ] 40/75 batches, loss: 0.6931Epoch 2/15: [================              ] 41/75 batches, loss: 0.6932Epoch 2/15: [================              ] 42/75 batches, loss: 0.6932Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6932Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6932Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6933Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6932Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6929Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6932Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6932Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 2/15: [==============================] 75/75 batches, loss: 0.6933
[2025-05-04 10:57:47,348][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6933
[2025-05-04 10:57:47,608][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6930Epoch 3/15: [                              ] 2/75 batches, loss: 0.6932Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6936Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6928Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6931Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6930Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6927Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6929Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6927Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6928Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6927Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6928Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6928Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6928Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6928Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6930Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6929Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6929Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6930Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 3/15: [================              ] 40/75 batches, loss: 0.6930Epoch 3/15: [================              ] 41/75 batches, loss: 0.6930Epoch 3/15: [================              ] 42/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6930Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6929Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6929Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6931Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6931Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6931Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 3/15: [==============================] 75/75 batches, loss: 0.6931
[2025-05-04 10:57:50,466][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-04 10:57:50,841][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6944Epoch 4/15: [                              ] 2/75 batches, loss: 0.6934Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6945Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6944Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6942Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6938Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6938Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6936Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6938Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6937Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6936Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6935Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6935Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6936Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6934Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6932Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6931Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6931Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6931Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6930Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6931Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6932Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6931Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 4/15: [================              ] 40/75 batches, loss: 0.6931Epoch 4/15: [================              ] 41/75 batches, loss: 0.6932Epoch 4/15: [================              ] 42/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6931Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6931Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6930Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6930Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6930Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6930Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6930Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6929Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6929Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6929Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6929Epoch 4/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:57:53,513][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6929
[2025-05-04 10:57:53,788][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6937, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:53,788][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6908Epoch 5/15: [                              ] 2/75 batches, loss: 0.6905Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6926Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6937Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6933Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6938Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6935Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6925Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6932Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6936Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6935Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6938Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6937Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6940Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6938Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6937Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6936Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6937Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6936Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6935Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6935Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6935Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6935Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6933Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6933Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6933Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6934Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6933Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6933Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6934Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6933Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6932Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6930Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 5/15: [================              ] 40/75 batches, loss: 0.6930Epoch 5/15: [================              ] 41/75 batches, loss: 0.6931Epoch 5/15: [================              ] 42/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6930Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6931Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6931Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6931Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6932Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6932Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6932Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6930Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6931Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6931Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6929Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6929Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6931Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6930Epoch 5/15: [==============================] 75/75 batches, loss: 0.6929
[2025-05-04 10:57:56,095][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6929
[2025-05-04 10:57:56,368][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6950, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:56,368][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6922Epoch 6/15: [                              ] 2/75 batches, loss: 0.6954Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6969Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6937Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6922Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6902Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6892Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6901Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6910Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6910Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6903Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6899Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6889Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6901Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6929Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6933Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6934Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6926Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6928Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6929Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6934Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6932Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6931Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6933Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6932Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6931Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6931Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6931Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6930Epoch 6/15: [================              ] 40/75 batches, loss: 0.6931Epoch 6/15: [================              ] 41/75 batches, loss: 0.6931Epoch 6/15: [================              ] 42/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6932Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6931Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6930Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6930Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6930Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6930Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6929Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6929Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6929Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6929Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6929Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6929Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6928Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6929Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6929Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6928Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6928Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6927Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6926Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6926Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6927Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6928Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6928Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6927Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6927Epoch 6/15: [==============================] 75/75 batches, loss: 0.6928
[2025-05-04 10:57:58,683][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6928
[2025-05-04 10:57:59,003][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6946, Metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:57:59,004][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 10:57:59,004][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-04 10:57:59,004][src.training.lm_trainer][INFO] - Training completed in 17.72 seconds
[2025-05-04 10:57:59,004][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:58:02,042][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:58:02,043][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:58:02,043][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 10:58:03,976][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ru/ru/model.pt
[2025-05-04 10:58:03,977][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁
wandb:           best_val_f1 ▁▁▁
wandb:         best_val_loss █▃▁
wandb:    best_val_precision ▁▁▁
wandb:       best_val_recall ▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁
wandb:            train_loss █▃▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁
wandb:              val_loss ▂▁▁▂█▆
wandb:         val_precision ▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.5
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69341
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 6
wandb:                 epoch 6
wandb:   final_test_accuracy 0.5
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.5
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69283
wandb:            train_time 17.72303
wandb:          val_accuracy 0.5
wandb:                val_f1 0
wandb:              val_loss 0.69457
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105724-nm911ncl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105724-nm911ncl/logs
Experiment probe_layer10_question_type_control3_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer10/ru/ru/results.json for layer 10
Running experiment: probe_layer10_complexity_control1_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control1_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:58:19,366][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ru
experiment_name: probe_layer10_complexity_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:58:19,366][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:58:19,367][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:58:19,367][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:58:19,372][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-04 10:58:19,372][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:58:21,182][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:58:23,441][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:58:23,441][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:58:23,513][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:58:23,588][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 10:58:23,759][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 10:58:23,769][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:58:23,770][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 10:58:23,771][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:58:23,814][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:58:23,888][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:58:23,902][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 10:58:23,904][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:58:23,904][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 10:58:23,915][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:58:23,989][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:58:24,074][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:58:24,097][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 10:58:24,099][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:58:24,099][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 10:58:24,100][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:58:24,101][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:58:24,101][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 10:58:24,101][src.data.datasets][INFO] - Sample label: 0.639226496219635
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:58:24,102][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:58:24,102][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-04 10:58:24,102][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:58:24,103][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:58:24,103][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 10:58:24,103][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:58:24,104][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:58:24,104][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:58:24,104][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:58:28,818][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:58:28,819][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:58:28,819][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:58:28,820][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:58:28,822][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:58:28,823][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:58:28,823][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:58:28,823][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:58:28,823][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 10:58:28,824][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:58:28,824][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6258Epoch 1/15: [                              ] 2/75 batches, loss: 0.7551Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8988Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8528Epoch 1/15: [==                            ] 5/75 batches, loss: 0.8169Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7100Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6650Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6439Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6343Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6260Epoch 1/15: [====                          ] 11/75 batches, loss: 0.5875Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5566Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5433Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5275Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5083Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5167Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5059Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4963Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4818Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4735Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4704Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4692Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4686Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4612Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4772Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4715Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4729Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4645Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4619Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4551Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4481Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4410Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4326Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4276Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4347Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4346Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4295Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4267Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4212Epoch 1/15: [================              ] 40/75 batches, loss: 0.4178Epoch 1/15: [================              ] 41/75 batches, loss: 0.4101Epoch 1/15: [================              ] 42/75 batches, loss: 0.4072Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4023Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4012Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3961Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3898Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3879Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3833Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3799Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3743Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3708Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3687Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3645Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3664Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3627Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3607Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3590Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3587Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3578Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3547Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3506Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3471Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3441Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3420Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3409Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3385Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3361Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3332Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3324Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3297Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3268Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3256Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3237Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3215Epoch 1/15: [==============================] 75/75 batches, loss: 0.3216
[2025-05-04 10:58:35,417][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3216
[2025-05-04 10:58:35,669][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0696, Metrics: {'mse': 0.06979522109031677, 'rmse': 0.2641878518976919, 'r2': -0.500354528427124}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1983Epoch 2/15: [                              ] 2/75 batches, loss: 0.1553Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1355Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1884Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1753Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1621Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1564Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1750Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1640Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1664Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1650Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1592Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1574Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1550Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1548Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1492Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1500Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1550Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1537Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1547Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1536Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1536Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1534Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1511Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1498Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1519Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1553Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1547Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1575Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1579Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1577Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1554Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1553Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1581Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1569Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1539Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1516Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1518Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1518Epoch 2/15: [================              ] 40/75 batches, loss: 0.1502Epoch 2/15: [================              ] 41/75 batches, loss: 0.1506Epoch 2/15: [================              ] 42/75 batches, loss: 0.1489Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1484Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1466Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1465Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1466Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1454Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1451Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1439Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1429Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1421Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1430Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1435Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1434Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1441Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1436Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1431Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1421Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1416Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1411Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1409Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1399Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1397Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1407Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1393Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1390Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1399Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1402Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1399Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1393Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1385Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1380Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1379Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1376Epoch 2/15: [==============================] 75/75 batches, loss: 0.1361
[2025-05-04 10:58:38,350][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1361
[2025-05-04 10:58:38,583][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1274, Metrics: {'mse': 0.13425734639167786, 'rmse': 0.36641144413306453, 'r2': -1.88606595993042}
[2025-05-04 10:58:38,583][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1646Epoch 3/15: [                              ] 2/75 batches, loss: 0.1784Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1655Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1498Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1334Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1269Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1289Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1225Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1333Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1262Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1231Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1168Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1174Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1156Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1165Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1143Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1137Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1103Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1083Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1102Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1094Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1099Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1114Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1105Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1087Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1099Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1083Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1075Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1089Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1071Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1068Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1087Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1095Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1096Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1088Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1074Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1071Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1072Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1059Epoch 3/15: [================              ] 40/75 batches, loss: 0.1042Epoch 3/15: [================              ] 41/75 batches, loss: 0.1065Epoch 3/15: [================              ] 42/75 batches, loss: 0.1063Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1057Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1050Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1038Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1030Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1034Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1036Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1052Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1049Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1049Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1045Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1041Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1036Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1024Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1023Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1014Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1008Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1009Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1002Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0991Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0988Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0984Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0982Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0979Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0982Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0975Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0990Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0985Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0980Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0973Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0969Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0965Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0962Epoch 3/15: [==============================] 75/75 batches, loss: 0.0972
[2025-05-04 10:58:40,839][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0972
[2025-05-04 10:58:41,091][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0797, Metrics: {'mse': 0.08122403174638748, 'rmse': 0.2849983013043893, 'r2': -0.7460341453552246}
[2025-05-04 10:58:41,092][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0967Epoch 4/15: [                              ] 2/75 batches, loss: 0.1023Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0905Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0879Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0990Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1010Epoch 4/15: [==                            ] 7/75 batches, loss: 0.1019Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0981Epoch 4/15: [===                           ] 9/75 batches, loss: 0.1069Epoch 4/15: [====                          ] 10/75 batches, loss: 0.1041Epoch 4/15: [====                          ] 11/75 batches, loss: 0.1000Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0993Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0970Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0942Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0947Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0953Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0932Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0913Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0928Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0933Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0938Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0925Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0920Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0912Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0898Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0896Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0888Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0882Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0871Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0853Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0849Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0833Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0826Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0828Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0815Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0816Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0809Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0816Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0812Epoch 4/15: [================              ] 40/75 batches, loss: 0.0832Epoch 4/15: [================              ] 41/75 batches, loss: 0.0822Epoch 4/15: [================              ] 42/75 batches, loss: 0.0826Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0830Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0824Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0821Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0812Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0810Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0799Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0797Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0795Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0792Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0796Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0787Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0785Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0779Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0779Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0778Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0777Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0779Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0782Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0787Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0785Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0782Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0778Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0781Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0780Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0778Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0783Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0782Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0779Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0771Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0766Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0763Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0762Epoch 4/15: [==============================] 75/75 batches, loss: 0.0762
[2025-05-04 10:58:43,369][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0762
[2025-05-04 10:58:43,613][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0705, Metrics: {'mse': 0.07087157666683197, 'rmse': 0.2662171607294165, 'r2': -0.5234924554824829}
[2025-05-04 10:58:43,613][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0782Epoch 5/15: [                              ] 2/75 batches, loss: 0.0665Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0804Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0881Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0836Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0795Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0914Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0877Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0832Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0796Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0766Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0756Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0731Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0712Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0696Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0712Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0712Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0722Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0719Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0719Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0705Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0706Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0701Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0703Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0694Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0690Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0706Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0693Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0689Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0686Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0681Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0676Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0683Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0686Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0685Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0688Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0692Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0697Epoch 5/15: [================              ] 40/75 batches, loss: 0.0696Epoch 5/15: [================              ] 41/75 batches, loss: 0.0702Epoch 5/15: [================              ] 42/75 batches, loss: 0.0707Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0714Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0717Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0716Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0720Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0725Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0720Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0724Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0717Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0714Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0713Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0707Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0709Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0701Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0703Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0697Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0696Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0688Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0686Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0684Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0679Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0684Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0683Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0681Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0676Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0678Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0674Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0668Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0670Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0669Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0674Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0672Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0669Epoch 5/15: [==============================] 75/75 batches, loss: 0.0664
[2025-05-04 10:58:45,910][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0664
[2025-05-04 10:58:46,161][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0825, Metrics: {'mse': 0.08485683053731918, 'rmse': 0.29130195766132294, 'r2': -0.8241266012191772}
[2025-05-04 10:58:46,162][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:58:46,162][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 5
[2025-05-04 10:58:46,162][src.training.lm_trainer][INFO] - Training completed in 13.98 seconds
[2025-05-04 10:58:46,162][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:58:49,053][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03400489315390587, 'rmse': 0.18440415709496863, 'r2': -0.705600380897522}
[2025-05-04 10:58:49,053][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06979522109031677, 'rmse': 0.2641878518976919, 'r2': -0.500354528427124}
[2025-05-04 10:58:49,053][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.08123266696929932, 'rmse': 0.28501345050593546, 'r2': -1.054703950881958}
[2025-05-04 10:58:51,224][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ru/ru/model.pt
[2025-05-04 10:58:51,226][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▆▁▅▆
wandb:       train_loss █▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▂▁▃
wandb:          val_mse ▁█▂▁▃
wandb:           val_r2 █▁▇█▆
wandb:         val_rmse ▁█▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06964
wandb:     best_val_mse 0.0698
wandb:      best_val_r2 -0.50035
wandb:    best_val_rmse 0.26419
wandb: early_stop_epoch 5
wandb:            epoch 5
wandb:   final_test_mse 0.08123
wandb:    final_test_r2 -1.0547
wandb:  final_test_rmse 0.28501
wandb:  final_train_mse 0.034
wandb:   final_train_r2 -0.7056
wandb: final_train_rmse 0.1844
wandb:    final_val_mse 0.0698
wandb:     final_val_r2 -0.50035
wandb:   final_val_rmse 0.26419
wandb:    learning_rate 0.0001
wandb:       train_loss 0.06642
wandb:       train_time 13.98464
wandb:         val_loss 0.08249
wandb:          val_mse 0.08486
wandb:           val_r2 -0.82413
wandb:         val_rmse 0.2913
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105819-nh10r8i9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105819-nh10r8i9/logs
Experiment probe_layer10_complexity_control1_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer10/ru/ru/results.json for layer 10
Running experiment: probe_layer10_complexity_control2_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control2_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 10:59:09,093][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ru
experiment_name: probe_layer10_complexity_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 10:59:09,093][__main__][INFO] - Normalized task: complexity
[2025-05-04 10:59:09,093][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 10:59:09,093][__main__][INFO] - Determined Task Type: regression
[2025-05-04 10:59:09,098][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-04 10:59:09,099][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 10:59:11,869][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 10:59:14,326][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 10:59:14,326][src.data.datasets][INFO] - Loading 'control_complexity_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:59:14,498][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:59:14,544][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:58:22 2025).
[2025-05-04 10:59:14,670][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 10:59:14,679][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:59:14,679][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 10:59:14,685][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:59:14,779][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:59:14,854][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:59:14,878][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 10:59:14,880][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:59:14,880][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 10:59:14,881][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 10:59:14,971][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:59:15,092][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 10:59:15,117][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 10:59:15,119][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 10:59:15,119][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 10:59:15,120][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 10:59:15,121][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:59:15,121][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:59:15,121][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:59:15,121][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:59:15,121][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:59:15,121][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Sample label: 0.28674033284187317
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:59:15,122][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:59:15,122][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-04 10:59:15,122][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 10:59:15,123][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 10:59:15,123][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 10:59:15,123][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 10:59:15,124][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 10:59:15,124][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 10:59:15,124][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 10:59:21,030][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 10:59:21,031][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 10:59:21,031][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 10:59:21,031][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 10:59:21,034][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 10:59:21,034][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 10:59:21,034][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 10:59:21,035][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 10:59:21,035][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 10:59:21,036][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 10:59:21,036][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.7164Epoch 1/15: [                              ] 2/75 batches, loss: 0.7476Epoch 1/15: [=                             ] 3/75 batches, loss: 0.9055Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8941Epoch 1/15: [==                            ] 5/75 batches, loss: 0.8430Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7323Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6880Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6655Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6590Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6531Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6087Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5756Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5550Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5401Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5188Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5187Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5076Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.5062Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4954Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4842Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4835Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4840Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4811Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4767Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4866Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4807Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4807Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4715Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4693Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4599Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4517Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4449Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4365Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4324Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4378Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4358Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4324Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4304Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4250Epoch 1/15: [================              ] 40/75 batches, loss: 0.4208Epoch 1/15: [================              ] 41/75 batches, loss: 0.4127Epoch 1/15: [================              ] 42/75 batches, loss: 0.4116Epoch 1/15: [=================             ] 43/75 batches, loss: 0.4049Epoch 1/15: [=================             ] 44/75 batches, loss: 0.4039Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3991Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3924Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3883Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3847Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3814Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3763Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3732Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3694Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3657Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3651Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3627Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3614Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3600Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3624Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3618Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3585Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3543Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3519Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3481Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3456Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3432Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3410Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3378Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3344Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3338Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3307Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3280Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3266Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3253Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3228Epoch 1/15: [==============================] 75/75 batches, loss: 0.3223
[2025-05-04 10:59:26,658][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3223
[2025-05-04 10:59:26,891][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0725, Metrics: {'mse': 0.0731370747089386, 'rmse': 0.2704386708829538, 'r2': -0.5721926689147949}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2072Epoch 2/15: [                              ] 2/75 batches, loss: 0.1668Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1402Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1653Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1591Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1492Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1511Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1572Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1547Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1635Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1709Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1663Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1661Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1632Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1613Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1553Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1541Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1581Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1578Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1599Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1604Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1611Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1623Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1601Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1592Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1604Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1612Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1595Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1638Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1636Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1612Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1589Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1583Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1572Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1567Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1547Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1533Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1542Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1531Epoch 2/15: [================              ] 40/75 batches, loss: 0.1517Epoch 2/15: [================              ] 41/75 batches, loss: 0.1507Epoch 2/15: [================              ] 42/75 batches, loss: 0.1488Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1492Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1494Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1485Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1479Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1473Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1467Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1452Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1444Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1431Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1434Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1443Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1433Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1424Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1436Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1424Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1412Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1407Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1396Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1395Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1384Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1387Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1393Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1382Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1387Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1396Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1397Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1392Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1381Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1374Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1368Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1367Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1363Epoch 2/15: [==============================] 75/75 batches, loss: 0.1348
[2025-05-04 10:59:29,602][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1348
[2025-05-04 10:59:29,850][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1280, Metrics: {'mse': 0.1352030634880066, 'rmse': 0.3676996919879137, 'r2': -1.906395673751831}
[2025-05-04 10:59:29,851][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1459Epoch 3/15: [                              ] 2/75 batches, loss: 0.1297Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1104Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1082Epoch 3/15: [==                            ] 5/75 batches, loss: 0.0976Epoch 3/15: [==                            ] 6/75 batches, loss: 0.0993Epoch 3/15: [==                            ] 7/75 batches, loss: 0.0992Epoch 3/15: [===                           ] 8/75 batches, loss: 0.0957Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1169Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1104Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1072Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1028Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1043Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1054Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1062Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1055Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1069Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1048Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1049Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1046Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1041Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1028Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1063Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1068Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1063Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1053Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1033Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1017Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1028Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1010Epoch 3/15: [============                  ] 31/75 batches, loss: 0.0998Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1006Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1005Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1000Epoch 3/15: [==============                ] 35/75 batches, loss: 0.0993Epoch 3/15: [==============                ] 36/75 batches, loss: 0.0984Epoch 3/15: [==============                ] 37/75 batches, loss: 0.0987Epoch 3/15: [===============               ] 38/75 batches, loss: 0.0983Epoch 3/15: [===============               ] 39/75 batches, loss: 0.0970Epoch 3/15: [================              ] 40/75 batches, loss: 0.0962Epoch 3/15: [================              ] 41/75 batches, loss: 0.0983Epoch 3/15: [================              ] 42/75 batches, loss: 0.0978Epoch 3/15: [=================             ] 43/75 batches, loss: 0.0979Epoch 3/15: [=================             ] 44/75 batches, loss: 0.0973Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0964Epoch 3/15: [==================            ] 46/75 batches, loss: 0.0955Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0954Epoch 3/15: [===================           ] 48/75 batches, loss: 0.0967Epoch 3/15: [===================           ] 49/75 batches, loss: 0.0975Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0971Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0976Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0972Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0967Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0969Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0968Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0963Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0957Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0959Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0962Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0972Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0967Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0965Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0963Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0964Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0963Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0965Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0962Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0967Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0962Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0956Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0949Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0948Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0942Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0943Epoch 3/15: [==============================] 75/75 batches, loss: 0.0948
[2025-05-04 10:59:32,146][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0948
[2025-05-04 10:59:32,412][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0822, Metrics: {'mse': 0.08430653810501099, 'rmse': 0.2903558818157658, 'r2': -0.8122973442077637}
[2025-05-04 10:59:32,413][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0519Epoch 4/15: [                              ] 2/75 batches, loss: 0.0656Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0668Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0683Epoch 4/15: [==                            ] 5/75 batches, loss: 0.0732Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0802Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0855Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0795Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0849Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0818Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0837Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0802Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0809Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0813Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0863Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0901Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0881Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0872Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0894Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0894Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0905Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0901Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0905Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0910Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0913Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0899Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0883Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0895Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0893Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0879Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0866Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0857Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0843Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0850Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0842Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0839Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0828Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0822Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0814Epoch 4/15: [================              ] 40/75 batches, loss: 0.0810Epoch 4/15: [================              ] 41/75 batches, loss: 0.0800Epoch 4/15: [================              ] 42/75 batches, loss: 0.0797Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0807Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0806Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0803Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0806Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0808Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0798Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0794Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0793Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0799Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0804Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0804Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0800Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0799Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0801Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0798Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0793Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0790Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0784Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0810Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0812Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0804Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0804Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0799Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0790Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0791Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0789Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0787Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0783Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0775Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0778Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0772Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0775Epoch 4/15: [==============================] 75/75 batches, loss: 0.0781
[2025-05-04 10:59:34,686][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0781
[2025-05-04 10:59:34,941][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0719, Metrics: {'mse': 0.07265865802764893, 'rmse': 0.2695526999079195, 'r2': -0.561908483505249}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0505Epoch 5/15: [                              ] 2/75 batches, loss: 0.0500Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0673Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0736Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0753Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0740Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0838Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0839Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0801Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0766Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0768Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0768Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0756Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0735Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0720Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0737Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0730Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0721Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0751Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0749Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0741Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0715Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0701Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0693Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0713Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0704Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0712Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0715Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0717Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0715Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0716Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0715Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0711Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0718Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0718Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0714Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0710Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0697Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0690Epoch 5/15: [================              ] 40/75 batches, loss: 0.0695Epoch 5/15: [================              ] 41/75 batches, loss: 0.0711Epoch 5/15: [================              ] 42/75 batches, loss: 0.0720Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0730Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0728Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0730Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0729Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0730Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0736Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0730Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0720Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0734Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0729Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0730Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0731Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0725Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0725Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0718Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0718Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0708Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0706Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0701Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0697Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0699Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0699Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0696Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0692Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0697Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0694Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0691Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0688Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0685Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0689Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0688Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0681Epoch 5/15: [==============================] 75/75 batches, loss: 0.0674
[2025-05-04 10:59:37,849][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0674
[2025-05-04 10:59:38,127][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0780, Metrics: {'mse': 0.07999168336391449, 'rmse': 0.28282801021807313, 'r2': -0.7195429801940918}
[2025-05-04 10:59:38,127][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0795Epoch 6/15: [                              ] 2/75 batches, loss: 0.0556Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0598Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0551Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0494Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0581Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0619Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0622Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0611Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0608Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0589Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0567Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0548Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0568Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0561Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0588Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0576Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0601Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0603Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0590Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0581Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0585Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0573Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0560Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0573Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0579Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0588Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0589Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0585Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0584Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0581Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0594Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0590Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0588Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0589Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0592Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0587Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0583Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0584Epoch 6/15: [================              ] 40/75 batches, loss: 0.0581Epoch 6/15: [================              ] 41/75 batches, loss: 0.0585Epoch 6/15: [================              ] 42/75 batches, loss: 0.0588Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0584Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0588Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0590Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0588Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0585Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0582Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0576Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0580Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0580Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0575Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0568Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0564Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0568Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0571Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0572Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0569Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0566Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0566Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0570Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0564Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0561Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0565Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0570Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0571Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0573Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0572Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0569Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0568Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0567Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0565Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0560Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0554Epoch 6/15: [==============================] 75/75 batches, loss: 0.0550
[2025-05-04 10:59:40,453][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0550
[2025-05-04 10:59:40,718][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0842, Metrics: {'mse': 0.08699847757816315, 'rmse': 0.2949550433170505, 'r2': -0.8701646327972412}
[2025-05-04 10:59:40,719][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0452Epoch 7/15: [                              ] 2/75 batches, loss: 0.0407Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0537Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0539Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0494Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0481Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0491Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0482Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0488Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0469Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0457Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0545Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0559Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0558Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0543Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0536Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0538Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0533Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0526Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0518Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0516Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0503Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0499Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0489Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0506Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0509Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0514Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0507Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0511Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0507Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0499Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0495Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0497Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0494Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0500Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0497Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0496Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0496Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0499Epoch 7/15: [================              ] 40/75 batches, loss: 0.0496Epoch 7/15: [================              ] 41/75 batches, loss: 0.0499Epoch 7/15: [================              ] 42/75 batches, loss: 0.0498Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0493Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0497Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0497Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0499Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0494Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0496Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0497Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0492Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0497Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0497Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0502Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0505Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0510Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0508Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0508Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0508Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0510Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0513Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0509Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0508Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0506Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0510Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0512Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0509Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0513Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0508Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0509Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0509Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0507Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0503Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0505Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0504Epoch 7/15: [==============================] 75/75 batches, loss: 0.0503
[2025-05-04 10:59:43,031][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0503
[2025-05-04 10:59:43,288][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0778, Metrics: {'mse': 0.07944324612617493, 'rmse': 0.28185678300543865, 'r2': -0.7077534198760986}
[2025-05-04 10:59:43,289][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0364Epoch 8/15: [                              ] 2/75 batches, loss: 0.0385Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0427Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0478Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0459Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0506Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0499Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0471Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0481Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0485Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0473Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0478Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0470Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0461Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0446Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0444Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0458Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0463Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0456Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0450Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0444Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0436Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0433Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0435Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0440Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0450Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0443Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0445Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0453Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0449Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0448Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0451Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0446Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0441Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0437Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0437Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0440Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0440Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0440Epoch 8/15: [================              ] 40/75 batches, loss: 0.0442Epoch 8/15: [================              ] 41/75 batches, loss: 0.0440Epoch 8/15: [================              ] 42/75 batches, loss: 0.0442Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0445Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0444Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0441Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0442Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0443Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0441Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0449Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0448Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0444Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0447Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0446Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0442Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0441Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0441Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0437Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0437Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0434Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0435Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0432Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0431Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0431Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0429Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0428Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0427Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0431Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0430Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0432Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0431Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0431Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0433Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0438Epoch 8/15: [==============================] 75/75 batches, loss: 0.0437
[2025-05-04 10:59:45,597][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0437
[2025-05-04 10:59:45,899][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0778, Metrics: {'mse': 0.07953139394521713, 'rmse': 0.28201310952722947, 'r2': -0.7096482515335083}
[2025-05-04 10:59:45,900][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 10:59:45,900][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-04 10:59:45,900][src.training.lm_trainer][INFO] - Training completed in 22.35 seconds
[2025-05-04 10:59:45,900][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 10:59:48,812][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.021119773387908936, 'rmse': 0.14532643733302256, 'r2': -0.05931496620178223}
[2025-05-04 10:59:48,812][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07265865802764893, 'rmse': 0.2695526999079195, 'r2': -0.561908483505249}
[2025-05-04 10:59:48,813][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07311351597309113, 'rmse': 0.2703951108527873, 'r2': -0.8493375778198242}
[2025-05-04 10:59:50,717][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ru/ru/model.pt
[2025-05-04 10:59:50,718][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▆▁▅▆▅▅▅
wandb:       train_loss █▃▂▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▂▁▂▃▂▂
wandb:          val_mse ▁█▂▁▂▃▂▂
wandb:           val_r2 █▁▇█▇▆▇▇
wandb:         val_rmse ▁█▂▁▂▃▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07192
wandb:     best_val_mse 0.07266
wandb:      best_val_r2 -0.56191
wandb:    best_val_rmse 0.26955
wandb: early_stop_epoch 8
wandb:            epoch 8
wandb:   final_test_mse 0.07311
wandb:    final_test_r2 -0.84934
wandb:  final_test_rmse 0.2704
wandb:  final_train_mse 0.02112
wandb:   final_train_r2 -0.05931
wandb: final_train_rmse 0.14533
wandb:    final_val_mse 0.07266
wandb:     final_val_r2 -0.56191
wandb:   final_val_rmse 0.26955
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04372
wandb:       train_time 22.34948
wandb:         val_loss 0.07778
wandb:          val_mse 0.07953
wandb:           val_r2 -0.70965
wandb:         val_rmse 0.28201
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105909-vfiywev1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_105909-vfiywev1/logs
Experiment probe_layer10_complexity_control2_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control2/layer10/ru/ru/results.json for layer 10
Running experiment: probe_layer10_complexity_control3_ru
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=10"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ru]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer10_complexity_control3_ru"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ru"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 11:00:15,772][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ru
experiment_name: probe_layer10_complexity_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 11:00:15,772][__main__][INFO] - Normalized task: complexity
[2025-05-04 11:00:15,772][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 11:00:15,772][__main__][INFO] - Determined Task Type: regression
[2025-05-04 11:00:15,777][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ru']
[2025-05-04 11:00:15,778][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 11:00:19,630][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 11:00:22,001][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 11:00:22,002][src.data.datasets][INFO] - Loading 'control_complexity_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:00:22,316][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 11:00:22,428][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:54:59 2025).
[2025-05-04 11:00:22,737][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-05-04 11:00:22,751][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:00:22,751][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-05-04 11:00:22,754][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:00:22,833][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:00:22,946][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:00:22,983][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-05-04 11:00:22,985][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:00:22,985][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-05-04 11:00:22,988][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:00:23,092][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:00:23,193][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:00:23,227][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-05-04 11:00:23,229][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:00:23,229][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-05-04 11:00:23,232][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-05-04 11:00:23,233][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:00:23,233][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:00:23,233][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:00:23,233][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:00:23,233][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:00:23,233][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Sample label: 0.4091160297393799
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:00:23,234][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:00:23,234][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-05-04 11:00:23,234][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:00:23,235][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:00:23,235][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-05-04 11:00:23,235][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 11:00:23,236][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 11:00:23,236][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 11:00:23,236][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 11:00:30,836][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 11:00:30,837][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 11:00:30,837][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=10, freeze_model=True
[2025-05-04 11:00:30,837][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 11:00:30,840][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 11:00:30,841][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 11:00:30,841][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 11:00:30,841][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 11:00:30,841][__main__][INFO] - Successfully created lm_probe model for ru
[2025-05-04 11:00:30,842][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 11:00:30,842][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.5513Epoch 1/15: [                              ] 2/75 batches, loss: 0.6698Epoch 1/15: [=                             ] 3/75 batches, loss: 0.8491Epoch 1/15: [=                             ] 4/75 batches, loss: 0.8364Epoch 1/15: [==                            ] 5/75 batches, loss: 0.8280Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7161Epoch 1/15: [==                            ] 7/75 batches, loss: 0.6650Epoch 1/15: [===                           ] 8/75 batches, loss: 0.6497Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6533Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6391Epoch 1/15: [====                          ] 11/75 batches, loss: 0.6040Epoch 1/15: [====                          ] 12/75 batches, loss: 0.5691Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.5561Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.5377Epoch 1/15: [======                        ] 15/75 batches, loss: 0.5168Epoch 1/15: [======                        ] 16/75 batches, loss: 0.5209Epoch 1/15: [======                        ] 17/75 batches, loss: 0.5105Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.4986Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.4861Epoch 1/15: [========                      ] 20/75 batches, loss: 0.4750Epoch 1/15: [========                      ] 21/75 batches, loss: 0.4702Epoch 1/15: [========                      ] 22/75 batches, loss: 0.4680Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.4662Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.4598Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.4682Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.4626Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.4611Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.4545Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.4508Epoch 1/15: [============                  ] 30/75 batches, loss: 0.4467Epoch 1/15: [============                  ] 31/75 batches, loss: 0.4410Epoch 1/15: [============                  ] 32/75 batches, loss: 0.4354Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.4267Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.4220Epoch 1/15: [==============                ] 35/75 batches, loss: 0.4315Epoch 1/15: [==============                ] 36/75 batches, loss: 0.4307Epoch 1/15: [==============                ] 37/75 batches, loss: 0.4270Epoch 1/15: [===============               ] 38/75 batches, loss: 0.4245Epoch 1/15: [===============               ] 39/75 batches, loss: 0.4200Epoch 1/15: [================              ] 40/75 batches, loss: 0.4162Epoch 1/15: [================              ] 41/75 batches, loss: 0.4100Epoch 1/15: [================              ] 42/75 batches, loss: 0.4059Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3996Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3997Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3947Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3889Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3859Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3822Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3785Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3728Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3709Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3675Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3638Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3643Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3620Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3603Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3596Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3596Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3580Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3552Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3514Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3488Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3460Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.3438Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.3418Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.3399Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.3364Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.3331Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.3324Epoch 1/15: [============================  ] 70/75 batches, loss: 0.3301Epoch 1/15: [============================  ] 71/75 batches, loss: 0.3275Epoch 1/15: [============================  ] 72/75 batches, loss: 0.3255Epoch 1/15: [============================= ] 73/75 batches, loss: 0.3244Epoch 1/15: [============================= ] 74/75 batches, loss: 0.3230Epoch 1/15: [==============================] 75/75 batches, loss: 0.3212
[2025-05-04 11:00:37,249][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3212
[2025-05-04 11:00:37,526][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0717, Metrics: {'mse': 0.07165828347206116, 'rmse': 0.2676906488319328, 'r2': -0.5404037237167358}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.1240Epoch 2/15: [                              ] 2/75 batches, loss: 0.1164Epoch 2/15: [=                             ] 3/75 batches, loss: 0.0960Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1326Epoch 2/15: [==                            ] 5/75 batches, loss: 0.1352Epoch 2/15: [==                            ] 6/75 batches, loss: 0.1293Epoch 2/15: [==                            ] 7/75 batches, loss: 0.1306Epoch 2/15: [===                           ] 8/75 batches, loss: 0.1468Epoch 2/15: [===                           ] 9/75 batches, loss: 0.1418Epoch 2/15: [====                          ] 10/75 batches, loss: 0.1465Epoch 2/15: [====                          ] 11/75 batches, loss: 0.1552Epoch 2/15: [====                          ] 12/75 batches, loss: 0.1527Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.1532Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.1544Epoch 2/15: [======                        ] 15/75 batches, loss: 0.1548Epoch 2/15: [======                        ] 16/75 batches, loss: 0.1494Epoch 2/15: [======                        ] 17/75 batches, loss: 0.1484Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.1542Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.1567Epoch 2/15: [========                      ] 20/75 batches, loss: 0.1558Epoch 2/15: [========                      ] 21/75 batches, loss: 0.1557Epoch 2/15: [========                      ] 22/75 batches, loss: 0.1582Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.1585Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.1552Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.1545Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.1551Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.1554Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.1545Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.1559Epoch 2/15: [============                  ] 30/75 batches, loss: 0.1573Epoch 2/15: [============                  ] 31/75 batches, loss: 0.1555Epoch 2/15: [============                  ] 32/75 batches, loss: 0.1526Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.1515Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.1526Epoch 2/15: [==============                ] 35/75 batches, loss: 0.1514Epoch 2/15: [==============                ] 36/75 batches, loss: 0.1495Epoch 2/15: [==============                ] 37/75 batches, loss: 0.1484Epoch 2/15: [===============               ] 38/75 batches, loss: 0.1491Epoch 2/15: [===============               ] 39/75 batches, loss: 0.1486Epoch 2/15: [================              ] 40/75 batches, loss: 0.1462Epoch 2/15: [================              ] 41/75 batches, loss: 0.1459Epoch 2/15: [================              ] 42/75 batches, loss: 0.1440Epoch 2/15: [=================             ] 43/75 batches, loss: 0.1430Epoch 2/15: [=================             ] 44/75 batches, loss: 0.1422Epoch 2/15: [==================            ] 45/75 batches, loss: 0.1414Epoch 2/15: [==================            ] 46/75 batches, loss: 0.1417Epoch 2/15: [==================            ] 47/75 batches, loss: 0.1399Epoch 2/15: [===================           ] 48/75 batches, loss: 0.1392Epoch 2/15: [===================           ] 49/75 batches, loss: 0.1381Epoch 2/15: [====================          ] 50/75 batches, loss: 0.1370Epoch 2/15: [====================          ] 51/75 batches, loss: 0.1353Epoch 2/15: [====================          ] 52/75 batches, loss: 0.1359Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1368Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1366Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1365Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1369Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1365Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1350Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1343Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1331Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1327Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1318Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1325Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1339Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1327Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1323Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1333Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1334Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1331Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1323Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1318Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1312Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1319Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1316Epoch 2/15: [==============================] 75/75 batches, loss: 0.1301
[2025-05-04 11:00:40,259][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1301
[2025-05-04 11:00:40,524][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1103, Metrics: {'mse': 0.11576840281486511, 'rmse': 0.3402475610711488, 'r2': -1.4886181354522705}
[2025-05-04 11:00:40,524][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1458Epoch 3/15: [                              ] 2/75 batches, loss: 0.1210Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1218Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1143Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1071Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1088Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1090Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1039Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1099Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1061Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1045Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1020Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1071Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1059Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1086Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1065Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1057Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1060Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1061Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1091Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1082Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1078Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1100Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1091Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1100Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1097Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1068Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1045Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1046Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1029Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1029Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1053Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1058Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1055Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1041Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1025Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1024Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1027Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1010Epoch 3/15: [================              ] 40/75 batches, loss: 0.1004Epoch 3/15: [================              ] 41/75 batches, loss: 0.1029Epoch 3/15: [================              ] 42/75 batches, loss: 0.1017Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1017Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1009Epoch 3/15: [==================            ] 45/75 batches, loss: 0.0998Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1000Epoch 3/15: [==================            ] 47/75 batches, loss: 0.0998Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1003Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1001Epoch 3/15: [====================          ] 50/75 batches, loss: 0.0998Epoch 3/15: [====================          ] 51/75 batches, loss: 0.0992Epoch 3/15: [====================          ] 52/75 batches, loss: 0.0987Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.0982Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.0987Epoch 3/15: [======================        ] 55/75 batches, loss: 0.0995Epoch 3/15: [======================        ] 56/75 batches, loss: 0.0987Epoch 3/15: [======================        ] 57/75 batches, loss: 0.0983Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.0991Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.0990Epoch 3/15: [========================      ] 60/75 batches, loss: 0.0989Epoch 3/15: [========================      ] 61/75 batches, loss: 0.0982Epoch 3/15: [========================      ] 62/75 batches, loss: 0.0980Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.0975Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.0981Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.0983Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.0980Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.0975Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.0991Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.0985Epoch 3/15: [============================  ] 70/75 batches, loss: 0.0980Epoch 3/15: [============================  ] 71/75 batches, loss: 0.0978Epoch 3/15: [============================  ] 72/75 batches, loss: 0.0974Epoch 3/15: [============================= ] 73/75 batches, loss: 0.0968Epoch 3/15: [============================= ] 74/75 batches, loss: 0.0967Epoch 3/15: [==============================] 75/75 batches, loss: 0.0976
[2025-05-04 11:00:42,799][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.0976
[2025-05-04 11:00:43,114][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0795, Metrics: {'mse': 0.08130818605422974, 'rmse': 0.28514590309914983, 'r2': -0.7478431463241577}
[2025-05-04 11:00:43,115][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0678Epoch 4/15: [                              ] 2/75 batches, loss: 0.0876Epoch 4/15: [=                             ] 3/75 batches, loss: 0.0916Epoch 4/15: [=                             ] 4/75 batches, loss: 0.0938Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1010Epoch 4/15: [==                            ] 6/75 batches, loss: 0.1021Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0967Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0927Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0944Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0931Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0912Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0866Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0865Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0840Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0874Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0870Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0869Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0873Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0887Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0896Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0886Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0888Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0886Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0874Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0880Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0874Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0858Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0852Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0842Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0831Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0822Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0810Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0814Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0813Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0808Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0807Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0800Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0810Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0801Epoch 4/15: [================              ] 40/75 batches, loss: 0.0793Epoch 4/15: [================              ] 41/75 batches, loss: 0.0781Epoch 4/15: [================              ] 42/75 batches, loss: 0.0778Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0782Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0782Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0789Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0786Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0794Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0792Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0789Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0785Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0787Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0786Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0790Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0789Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0781Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0782Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0779Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0780Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0776Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0775Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0783Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0780Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0776Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0769Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0770Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0766Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0764Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0764Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0764Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0763Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0756Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0754Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0752Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0748Epoch 4/15: [==============================] 75/75 batches, loss: 0.0747
[2025-05-04 11:00:45,397][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0747
[2025-05-04 11:00:45,705][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0829, Metrics: {'mse': 0.08519214391708374, 'rmse': 0.291876932828005, 'r2': -0.8313348293304443}
[2025-05-04 11:00:45,706][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0784Epoch 5/15: [                              ] 2/75 batches, loss: 0.0610Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0725Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0691Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0746Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0717Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0822Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0801Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0778Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0729Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0702Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0721Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0726Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0709Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0723Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0728Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0730Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0762Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0779Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0771Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0755Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0741Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0738Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0732Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0752Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0744Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0749Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0748Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0752Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0743Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0728Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0730Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0728Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0722Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0725Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0720Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0716Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0719Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0709Epoch 5/15: [================              ] 40/75 batches, loss: 0.0707Epoch 5/15: [================              ] 41/75 batches, loss: 0.0708Epoch 5/15: [================              ] 42/75 batches, loss: 0.0723Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0720Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0721Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0716Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0714Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0709Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0704Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0707Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0705Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0707Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0701Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0696Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0692Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0686Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0685Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0681Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0686Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0678Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0675Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0672Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0667Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0667Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0671Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0671Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0672Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0674Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0675Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0671Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0667Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0666Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0665Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0663Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0660Epoch 5/15: [==============================] 75/75 batches, loss: 0.0659
[2025-05-04 11:00:47,993][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0659
[2025-05-04 11:00:48,277][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0707, Metrics: {'mse': 0.07107201963663101, 'rmse': 0.26659336007603607, 'r2': -0.5278012752532959}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0604Epoch 6/15: [                              ] 2/75 batches, loss: 0.0586Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0555Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0595Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0582Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0618Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0636Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0608Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0581Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0577Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0567Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0550Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0543Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0554Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0575Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0634Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0618Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0632Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0632Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0638Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0626Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0629Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0619Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0604Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0608Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0601Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0597Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0589Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0584Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0577Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0593Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0584Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0572Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0568Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0565Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0565Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0555Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0560Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0559Epoch 6/15: [================              ] 40/75 batches, loss: 0.0556Epoch 6/15: [================              ] 41/75 batches, loss: 0.0563Epoch 6/15: [================              ] 42/75 batches, loss: 0.0571Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0571Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0572Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0570Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0566Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0566Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0564Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0562Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0570Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0568Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0565Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0562Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0559Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0557Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0561Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0562Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0561Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0558Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0553Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0553Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0551Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0547Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0546Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0543Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0546Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0544Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0545Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0541Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0542Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0540Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0539Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0536Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0538Epoch 6/15: [==============================] 75/75 batches, loss: 0.0539
[2025-05-04 11:00:51,222][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0539
[2025-05-04 11:00:51,550][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0819, Metrics: {'mse': 0.08391024172306061, 'rmse': 0.28967264579704555, 'r2': -0.8037782907485962}
[2025-05-04 11:00:51,550][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0342Epoch 7/15: [                              ] 2/75 batches, loss: 0.0451Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0521Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0573Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0612Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0579Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0559Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0530Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0557Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0538Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0532Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0545Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0541Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0543Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0526Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0535Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0527Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0519Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0510Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0533Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0534Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0534Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0530Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0533Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0532Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0535Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0537Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0533Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0533Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0523Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0519Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0518Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0519Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0516Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0519Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0516Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0516Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0516Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0520Epoch 7/15: [================              ] 40/75 batches, loss: 0.0515Epoch 7/15: [================              ] 41/75 batches, loss: 0.0516Epoch 7/15: [================              ] 42/75 batches, loss: 0.0522Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0528Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0529Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0531Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0543Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0539Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0542Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0542Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0538Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0532Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0531Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0526Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0525Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0524Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0521Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0520Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0518Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0516Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0520Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0516Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0514Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0513Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0511Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0510Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0509Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0507Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0505Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0505Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0503Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0500Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0497Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0501Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0498Epoch 7/15: [==============================] 75/75 batches, loss: 0.0496
[2025-05-04 11:00:53,876][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0496
[2025-05-04 11:00:54,138][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0818, Metrics: {'mse': 0.08378907293081284, 'rmse': 0.2894634224402331, 'r2': -0.8011735677719116}
[2025-05-04 11:00:54,139][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0623Epoch 8/15: [                              ] 2/75 batches, loss: 0.0423Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0456Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0521Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0485Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0501Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0519Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0504Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0479Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0501Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0471Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0468Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0468Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0465Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0447Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0439Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0468Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0469Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0460Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0451Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0448Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0441Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0437Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0447Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0461Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0464Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0459Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0456Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0456Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0450Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0443Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0449Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0451Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0449Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0444Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0440Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0443Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0440Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0440Epoch 8/15: [================              ] 40/75 batches, loss: 0.0438Epoch 8/15: [================              ] 41/75 batches, loss: 0.0442Epoch 8/15: [================              ] 42/75 batches, loss: 0.0445Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0444Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0442Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0439Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0442Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0442Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0445Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0449Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0449Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0449Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0453Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0451Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0449Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0448Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0447Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0448Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0445Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0443Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0438Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0437Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0436Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0437Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0440Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0439Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0438Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0438Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0439Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0439Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0440Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0447Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0449Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0451Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0449Epoch 8/15: [==============================] 75/75 batches, loss: 0.0450
[2025-05-04 11:00:56,446][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0450
[2025-05-04 11:00:56,693][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0828, Metrics: {'mse': 0.08486412465572357, 'rmse': 0.29131447725048537, 'r2': -0.8242835998535156}
[2025-05-04 11:00:56,694][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0163Epoch 9/15: [                              ] 2/75 batches, loss: 0.0262Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0419Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0439Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0425Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0447Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0455Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0453Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0436Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0430Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0421Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0425Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0418Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0409Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0428Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0427Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0425Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0408Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0416Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0413Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0402Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0399Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0395Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0401Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0408Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0398Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0399Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0402Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0406Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0402Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0403Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0399Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0404Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0407Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0404Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0404Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0412Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0410Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0423Epoch 9/15: [================              ] 40/75 batches, loss: 0.0428Epoch 9/15: [================              ] 41/75 batches, loss: 0.0425Epoch 9/15: [================              ] 42/75 batches, loss: 0.0427Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0427Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0428Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0427Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0424Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0425Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0422Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0422Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0421Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0421Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0421Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0430Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0428Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0427Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0427Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0426Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0423Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0424Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0424Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0427Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0427Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0433Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0432Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0431Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0434Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0433Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0430Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0430Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0432Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0429Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0432Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0432Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0429Epoch 9/15: [==============================] 75/75 batches, loss: 0.0430
[2025-05-04 11:00:59,027][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0430
[2025-05-04 11:00:59,301][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0779, Metrics: {'mse': 0.07919356971979141, 'rmse': 0.281413520854616, 'r2': -0.7023863792419434}
[2025-05-04 11:00:59,301][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 4/4
[2025-05-04 11:00:59,302][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 9
[2025-05-04 11:00:59,302][src.training.lm_trainer][INFO] - Training completed in 25.17 seconds
[2025-05-04 11:00:59,302][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 11:01:02,324][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.02094894088804722, 'rmse': 0.1447374895735283, 'r2': -0.05074656009674072}
[2025-05-04 11:01:02,325][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07107201963663101, 'rmse': 0.26659336007603607, 'r2': -0.5278012752532959}
[2025-05-04 11:01:02,325][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.07048693299293518, 'rmse': 0.2654937532088753, 'r2': -0.7829005718231201}
[2025-05-04 11:01:04,201][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ru/ru/model.pt
[2025-05-04 11:01:04,202][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb: early_stop_epoch ▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▅▁▄▄▅▄▄▄
wandb:       train_loss █▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▁█▃▃▁▃▃▃▂
wandb:          val_mse ▁█▃▃▁▃▃▃▂
wandb:           val_r2 █▁▆▆█▆▆▆▇
wandb:         val_rmse ▁█▃▃▁▃▃▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07066
wandb:     best_val_mse 0.07107
wandb:      best_val_r2 -0.5278
wandb:    best_val_rmse 0.26659
wandb: early_stop_epoch 9
wandb:            epoch 9
wandb:   final_test_mse 0.07049
wandb:    final_test_r2 -0.7829
wandb:  final_test_rmse 0.26549
wandb:  final_train_mse 0.02095
wandb:   final_train_r2 -0.05075
wandb: final_train_rmse 0.14474
wandb:    final_val_mse 0.07107
wandb:     final_val_r2 -0.5278
wandb:   final_val_rmse 0.26659
wandb:    learning_rate 0.0001
wandb:       train_loss 0.04296
wandb:       train_time 25.16817
wandb:         val_loss 0.07788
wandb:          val_mse 0.07919
wandb:           val_r2 -0.70239
wandb:         val_rmse 0.28141
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110015-22wfbzuf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110015-22wfbzuf/logs
Experiment probe_layer10_complexity_control3_ru completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/control3/layer10/ru/ru/results.json for layer 10
=======================
PROBING LAYER 11 (CONTROL EXPERIMENTS)
=======================
Running experiment: probe_layer11_question_type_control1_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_question_type_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer11/ar"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 11:01:33,322][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer11/ar
experiment_name: probe_layer11_question_type_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 11:01:33,322][__main__][INFO] - Normalized task: question_type
[2025-05-04 11:01:33,322][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 11:01:33,322][__main__][INFO] - Determined Task Type: classification
[2025-05-04 11:01:33,328][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-04 11:01:33,328][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 11:01:37,243][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 11:01:39,575][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 11:01:39,575][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:01:39,748][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 11:01:39,787][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-05-04 11:01:40,059][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-04 11:01:40,066][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:01:40,067][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-04 11:01:40,078][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:01:40,160][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:01:40,285][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:01:40,311][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-04 11:01:40,313][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:01:40,314][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-04 11:01:40,315][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:01:40,374][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:01:40,476][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:01:40,489][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-04 11:01:40,491][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:01:40,491][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-04 11:01:40,492][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-04 11:01:40,493][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:01:40,493][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:01:40,493][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:01:40,493][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:01:40,493][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-04 11:01:40,493][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-04 11:01:40,493][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:01:40,494][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-04 11:01:40,494][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:01:40,494][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:01:40,495][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-04 11:01:40,495][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-04 11:01:40,495][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 11:01:40,496][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 11:01:40,496][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 11:01:40,496][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 11:01:47,175][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 11:01:47,176][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 11:01:47,176][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-04 11:01:47,176][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 11:01:47,182][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 11:01:47,182][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 11:01:47,182][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 11:01:47,182][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 11:01:47,183][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-04 11:01:47,183][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 11:01:47,184][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 11:01:47,184][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.6690Epoch 1/15: [                              ] 2/63 batches, loss: 0.6580Epoch 1/15: [=                             ] 3/63 batches, loss: 0.6574Epoch 1/15: [=                             ] 4/63 batches, loss: 0.6816Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6781Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6809Epoch 1/15: [===                           ] 7/63 batches, loss: 0.6851Epoch 1/15: [===                           ] 8/63 batches, loss: 0.6844Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6834Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6873Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.6887Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.6877Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6878Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6899Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6908Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6905Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6906Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6915Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6917Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6917Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6900Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6925Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6920Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6921Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6915Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6922Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6930Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6929Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6929Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6935Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6940Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6940Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6942Epoch 1/15: [================              ] 34/63 batches, loss: 0.6942Epoch 1/15: [================              ] 35/63 batches, loss: 0.6941Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6937Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6937Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6939Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6942Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6942Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6938Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6938Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6938Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6938Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6938Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6938Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6939Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6939Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6939Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6938Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6938Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6939Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6939Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6939Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6938Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6938Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6938Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6938Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6938Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6937Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6937Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6937Epoch 1/15: [==============================] 63/63 batches, loss: 0.6936
[2025-05-04 11:01:53,343][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6936
[2025-05-04 11:01:53,583][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6926Epoch 2/15: [                              ] 2/63 batches, loss: 0.6928Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6929Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6918Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6920Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6922Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6924Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6925Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6918Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6920Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6931Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6928Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6928Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6929Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6935Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6936Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6947Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6946Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6941Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6939Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6939Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6938Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6937Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6935Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6938Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6941Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6941Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6941Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6940Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6940Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6939Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6939Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6939Epoch 2/15: [================              ] 34/63 batches, loss: 0.6939Epoch 2/15: [================              ] 35/63 batches, loss: 0.6939Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6938Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6938Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6937Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6938Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6937Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6937Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6937Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6937Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6937Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6938Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6938Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6938Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6937Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6938Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6938Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6938Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6938Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6938Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6937Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6937Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6937Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6937Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6937Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6936Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6936Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6936Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6936Epoch 2/15: [==============================] 63/63 batches, loss: 0.6936
[2025-05-04 11:01:55,956][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6936
[2025-05-04 11:01:56,170][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6946Epoch 3/15: [                              ] 2/63 batches, loss: 0.6906Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6914Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6919Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6916Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6919Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6923Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6915Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6916Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6922Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6923Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6923Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6923Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6926Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6926Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6928Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6929Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6928Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6926Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6925Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6925Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6925Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6925Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6926Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6926Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6926Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6927Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6927Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6927Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6927Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6927Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6925Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6924Epoch 3/15: [================              ] 34/63 batches, loss: 0.6925Epoch 3/15: [================              ] 35/63 batches, loss: 0.6924Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6924Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6924Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6924Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6924Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6925Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6925Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6926Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6926Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6926Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6926Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6924Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6925Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6921Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6923Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6924Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6925Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6925Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6927Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6927Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6927Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6927Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6926Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6926Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6932Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6932Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6932Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6932Epoch 3/15: [==============================] 63/63 batches, loss: 0.6932
[2025-05-04 11:01:58,749][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6932
[2025-05-04 11:01:59,007][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:01:59,007][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.7080Epoch 4/15: [                              ] 2/63 batches, loss: 0.6977Epoch 4/15: [=                             ] 3/63 batches, loss: 0.7011Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6991Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6977Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6968Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6963Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6955Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6956Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6954Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6952Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6948Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6952Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6952Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6951Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6949Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6948Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6947Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6946Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6945Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6944Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6944Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6943Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6943Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6942Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6942Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6941Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6940Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6940Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6939Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6939Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6939Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6938Epoch 4/15: [================              ] 34/63 batches, loss: 0.6938Epoch 4/15: [================              ] 35/63 batches, loss: 0.6938Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6938Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6938Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6938Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6938Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6937Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6937Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6937Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6937Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6937Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6937Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6937Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6936Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6936Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6936Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6936Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6936Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6936Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6936Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6936Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6935Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6935Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6935Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6935Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6935Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6935Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6935Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6935Epoch 4/15: [==============================] 63/63 batches, loss: 0.6935
[2025-05-04 11:02:01,087][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6935
[2025-05-04 11:02:01,340][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6931Epoch 5/15: [                              ] 2/63 batches, loss: 0.6931Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6931Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6934Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6932Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6933Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6927Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6932Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6932Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6931Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6931Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6931Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6931Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6931Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6931Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6931Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6931Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6931Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6931Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6931Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6931Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6932Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6932Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6933Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6933Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6933Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6933Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6932Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6932Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6932Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6932Epoch 5/15: [================              ] 34/63 batches, loss: 0.6932Epoch 5/15: [================              ] 35/63 batches, loss: 0.6932Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6932Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6932Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6932Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6932Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6932Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6932Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6932Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6932Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6932Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6932Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6933Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6933Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6933Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6933Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6933Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6933Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6933Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6932Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6932Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6932Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6932Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6932Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6932Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6932Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6932Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6932Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6932Epoch 5/15: [==============================] 63/63 batches, loss: 0.6932
[2025-05-04 11:02:03,716][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6932
[2025-05-04 11:02:04,084][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.6945Epoch 6/15: [                              ] 2/63 batches, loss: 0.6942Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6937Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6936Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6935Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6934Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6934Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6934Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6932Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6932Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6932Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6931Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6932Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6932Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6932Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6932Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6932Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6932Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6932Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6932Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6932Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6931Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6932Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 6/15: [================              ] 34/63 batches, loss: 0.6931Epoch 6/15: [================              ] 35/63 batches, loss: 0.6931Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6931Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6930Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6930Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6930Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6930Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6930Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6930Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6930Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6930Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6930Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6930Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6930Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6930Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6930Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6930Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6930Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6930Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6930Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 6/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:02:06,516][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6931
[2025-05-04 11:02:06,733][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:06,733][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.6931Epoch 7/15: [                              ] 2/63 batches, loss: 0.6929Epoch 7/15: [=                             ] 3/63 batches, loss: 0.6929Epoch 7/15: [=                             ] 4/63 batches, loss: 0.6930Epoch 7/15: [==                            ] 5/63 batches, loss: 0.6930Epoch 7/15: [==                            ] 6/63 batches, loss: 0.6939Epoch 7/15: [===                           ] 7/63 batches, loss: 0.6935Epoch 7/15: [===                           ] 8/63 batches, loss: 0.6934Epoch 7/15: [====                          ] 9/63 batches, loss: 0.6933Epoch 7/15: [====                          ] 10/63 batches, loss: 0.6933Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.6932Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.6932Epoch 7/15: [======                        ] 13/63 batches, loss: 0.6932Epoch 7/15: [======                        ] 14/63 batches, loss: 0.6933Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.6932Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.6933Epoch 7/15: [========                      ] 17/63 batches, loss: 0.6933Epoch 7/15: [========                      ] 18/63 batches, loss: 0.6932Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.6932Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.6932Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.6932Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.6932Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.6932Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.6930Epoch 7/15: [============                  ] 26/63 batches, loss: 0.6930Epoch 7/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 7/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 7/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 7/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 7/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 7/15: [================              ] 34/63 batches, loss: 0.6931Epoch 7/15: [================              ] 35/63 batches, loss: 0.6931Epoch 7/15: [=================             ] 36/63 batches, loss: 0.6929Epoch 7/15: [=================             ] 37/63 batches, loss: 0.6929Epoch 7/15: [==================            ] 38/63 batches, loss: 0.6929Epoch 7/15: [==================            ] 39/63 batches, loss: 0.6929Epoch 7/15: [===================           ] 40/63 batches, loss: 0.6929Epoch 7/15: [===================           ] 41/63 batches, loss: 0.6929Epoch 7/15: [====================          ] 42/63 batches, loss: 0.6929Epoch 7/15: [====================          ] 43/63 batches, loss: 0.6929Epoch 7/15: [====================          ] 44/63 batches, loss: 0.6929Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.6929Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.6929Epoch 7/15: [======================        ] 47/63 batches, loss: 0.6929Epoch 7/15: [======================        ] 48/63 batches, loss: 0.6928Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.6928Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.6928Epoch 7/15: [========================      ] 51/63 batches, loss: 0.6928Epoch 7/15: [========================      ] 52/63 batches, loss: 0.6928Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.6929Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.6929Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.6926Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.6925Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.6925Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.6925Epoch 7/15: [============================  ] 59/63 batches, loss: 0.6924Epoch 7/15: [============================  ] 60/63 batches, loss: 0.6924Epoch 7/15: [============================= ] 61/63 batches, loss: 0.6924Epoch 7/15: [============================= ] 62/63 batches, loss: 0.6924Epoch 7/15: [==============================] 63/63 batches, loss: 0.6924
[2025-05-04 11:02:08,779][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6924
[2025-05-04 11:02:09,073][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6937, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:09,073][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6932Epoch 8/15: [                              ] 2/63 batches, loss: 0.6927Epoch 8/15: [=                             ] 3/63 batches, loss: 0.6914Epoch 8/15: [=                             ] 4/63 batches, loss: 0.6920Epoch 8/15: [==                            ] 5/63 batches, loss: 0.6921Epoch 8/15: [==                            ] 6/63 batches, loss: 0.6923Epoch 8/15: [===                           ] 7/63 batches, loss: 0.6915Epoch 8/15: [===                           ] 8/63 batches, loss: 0.6918Epoch 8/15: [====                          ] 9/63 batches, loss: 0.6905Epoch 8/15: [====                          ] 10/63 batches, loss: 0.6899Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.6901Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.6890Epoch 8/15: [======                        ] 13/63 batches, loss: 0.6904Epoch 8/15: [======                        ] 14/63 batches, loss: 0.6927Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.6933Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.6932Epoch 8/15: [========                      ] 17/63 batches, loss: 0.6932Epoch 8/15: [========                      ] 18/63 batches, loss: 0.6933Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.6933Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.6941Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.6949Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.6948Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.6947Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.6947Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.6946Epoch 8/15: [============                  ] 26/63 batches, loss: 0.6943Epoch 8/15: [============                  ] 27/63 batches, loss: 0.6946Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.6944Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.6944Epoch 8/15: [==============                ] 30/63 batches, loss: 0.6943Epoch 8/15: [==============                ] 31/63 batches, loss: 0.6943Epoch 8/15: [===============               ] 32/63 batches, loss: 0.6942Epoch 8/15: [===============               ] 33/63 batches, loss: 0.6944Epoch 8/15: [================              ] 34/63 batches, loss: 0.6942Epoch 8/15: [================              ] 35/63 batches, loss: 0.6940Epoch 8/15: [=================             ] 36/63 batches, loss: 0.6940Epoch 8/15: [=================             ] 37/63 batches, loss: 0.6938Epoch 8/15: [==================            ] 38/63 batches, loss: 0.6937Epoch 8/15: [==================            ] 39/63 batches, loss: 0.6937Epoch 8/15: [===================           ] 40/63 batches, loss: 0.6938Epoch 8/15: [===================           ] 41/63 batches, loss: 0.6937Epoch 8/15: [====================          ] 42/63 batches, loss: 0.6937Epoch 8/15: [====================          ] 43/63 batches, loss: 0.6937Epoch 8/15: [====================          ] 44/63 batches, loss: 0.6936Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.6936Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.6934Epoch 8/15: [======================        ] 47/63 batches, loss: 0.6934Epoch 8/15: [======================        ] 48/63 batches, loss: 0.6934Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.6934Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.6933Epoch 8/15: [========================      ] 51/63 batches, loss: 0.6933Epoch 8/15: [========================      ] 52/63 batches, loss: 0.6934Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.6934Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.6934Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.6934Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.6934Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.6934Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.6934Epoch 8/15: [============================  ] 59/63 batches, loss: 0.6934Epoch 8/15: [============================  ] 60/63 batches, loss: 0.6934Epoch 8/15: [============================= ] 61/63 batches, loss: 0.6934Epoch 8/15: [============================= ] 62/63 batches, loss: 0.6935Epoch 8/15: [==============================] 63/63 batches, loss: 0.6935
[2025-05-04 11:02:11,028][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6935
[2025-05-04 11:02:11,273][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:11,273][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 11:02:11,273][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 8
[2025-05-04 11:02:11,274][src.training.lm_trainer][INFO] - Training completed in 20.65 seconds
[2025-05-04 11:02:11,274][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 11:02:13,918][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:13,918][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:13,918][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:02:15,782][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer11/ar/ar/model.pt
[2025-05-04 11:02:15,784][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁
wandb:         best_val_loss █▅▂▁
wandb:    best_val_precision ▁▁▁▁
wandb:       best_val_recall ▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁
wandb:            train_loss ██▅▇▅▅▁▇
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁
wandb:              val_loss ▃▂▄▁▁▂█▅
wandb:         val_precision ▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69331
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 8
wandb:                 epoch 8
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69352
wandb:            train_time 20.65308
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69351
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110133-qjbx441q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110133-qjbx441q/logs
Experiment probe_layer11_question_type_control1_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control1/layer11/ar/ar/results.json for layer 11
Running experiment: probe_layer11_question_type_control2_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_question_type_control2_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer11/ar"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=2"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 11:02:46,357][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer11/ar
experiment_name: probe_layer11_question_type_control2_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 11:02:46,357][__main__][INFO] - Normalized task: question_type
[2025-05-04 11:02:46,357][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 11:02:46,357][__main__][INFO] - Determined Task Type: classification
[2025-05-04 11:02:46,363][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-04 11:02:46,363][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 11:02:50,197][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 11:02:52,578][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 11:02:52,578][src.data.datasets][INFO] - Loading 'control_question_type_seed2' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:02:52,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 11:02:53,003][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:48:35 2025).
[2025-05-04 11:02:53,232][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-04 11:02:53,240][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:02:53,241][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-04 11:02:53,243][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:02:53,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:02:53,450][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:02:53,480][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-04 11:02:53,485][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:02:53,487][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-04 11:02:53,491][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:02:53,578][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:02:53,674][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:02:53,733][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-04 11:02:53,735][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:02:53,735][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-04 11:02:53,737][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:02:53,738][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-04 11:02:53,738][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-04 11:02:53,738][src.data.datasets][INFO] - Sample label: 1
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:02:53,739][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-04 11:02:53,739][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:02:53,739][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:02:53,740][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-04 11:02:53,740][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-04 11:02:53,740][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 11:02:53,741][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 11:02:53,741][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 11:02:53,741][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 11:03:01,186][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 11:03:01,187][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 11:03:01,187][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-04 11:03:01,187][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 11:03:01,193][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 11:03:01,193][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 11:03:01,194][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 11:03:01,194][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 11:03:01,194][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-04 11:03:01,195][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 11:03:01,195][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 11:03:01,196][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7231Epoch 1/15: [                              ] 2/63 batches, loss: 0.7129Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7175Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7160Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7108Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7107Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7099Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7092Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7096Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7081Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7069Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7057Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7049Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7040Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7033Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7028Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7023Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7017Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7011Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7007Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7004Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7000Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6997Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6995Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6992Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6990Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6987Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6985Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6983Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6982Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6980Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6979Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6977Epoch 1/15: [================              ] 34/63 batches, loss: 0.6976Epoch 1/15: [================              ] 35/63 batches, loss: 0.6974Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6973Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6972Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6971Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6970Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6969Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6968Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6967Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6966Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6966Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6965Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6964Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6963Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6963Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6962Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6961Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6961Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6960Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6960Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6959Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6959Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6958Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6958Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6958Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6957Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6957Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6956Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6956Epoch 1/15: [==============================] 63/63 batches, loss: 0.6956
[2025-05-04 11:03:07,498][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6956
[2025-05-04 11:03:07,697][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6938Epoch 2/15: [                              ] 2/63 batches, loss: 0.6936Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6933Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6931Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6933Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6933Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6933Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6933Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6934Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6934Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6935Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6935Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6934Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6934Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6933Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6933Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6933Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6933Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6933Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6933Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6933Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6933Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6933Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6933Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6933Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6933Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6933Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6933Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6933Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6933Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6933Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6932Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6932Epoch 2/15: [================              ] 34/63 batches, loss: 0.6933Epoch 2/15: [================              ] 35/63 batches, loss: 0.6932Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6932Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6932Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6932Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6932Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6932Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6932Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6932Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6932Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6932Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6932Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6932Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6932Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6932Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6932Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6932Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6932Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6932Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6932Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6932Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6932Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6932Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6932Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6932Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6932Epoch 2/15: [==============================] 63/63 batches, loss: 0.6932
[2025-05-04 11:03:10,132][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6932
[2025-05-04 11:03:10,492][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6927Epoch 3/15: [                              ] 2/63 batches, loss: 0.6937Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6935Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6934Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6933Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6932Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6931Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6931Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6931Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6930Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6930Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6930Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6931Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6930Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6929Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6929Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6930Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6930Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6930Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6931Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6931Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6932Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6932Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6932Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 3/15: [================              ] 34/63 batches, loss: 0.6931Epoch 3/15: [================              ] 35/63 batches, loss: 0.6931Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6931Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6931Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6931Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6931Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6932Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6932Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6932Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6932Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6932Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6932Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6932Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6932Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6932Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6932Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6932Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6932Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6932Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6932Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6932Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6932Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6932Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6932Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 3/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:03:13,196][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6931
[2025-05-04 11:03:13,463][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.6932Epoch 4/15: [                              ] 2/63 batches, loss: 0.6930Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6926Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6928Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6929Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6930Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6930Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6930Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6930Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6930Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6930Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6931Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6930Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6931Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6931Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6931Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6931Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6931Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6931Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6931Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6931Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6931Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 4/15: [================              ] 34/63 batches, loss: 0.6931Epoch 4/15: [================              ] 35/63 batches, loss: 0.6931Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6931Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6931Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6931Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6931Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6931Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6931Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6931Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6931Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6931Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6931Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6931Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6931Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6931Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6931Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6931Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6931Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6931Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 4/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:03:15,910][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6931
[2025-05-04 11:03:16,274][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6934, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:16,274][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6933Epoch 5/15: [                              ] 2/63 batches, loss: 0.6932Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6934Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6930Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6931Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6931Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6929Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6928Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6929Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6928Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6929Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6929Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6929Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6928Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6929Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6929Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6928Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6928Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6928Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6928Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6928Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6928Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6928Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6929Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6928Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6928Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6928Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6928Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6928Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6929Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6929Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6929Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6929Epoch 5/15: [================              ] 34/63 batches, loss: 0.6930Epoch 5/15: [================              ] 35/63 batches, loss: 0.6930Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6930Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6930Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6931Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6930Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6930Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6930Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6930Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6930Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6930Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6930Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6930Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6930Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6930Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6930Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6930Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6930Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6930Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6931Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6932Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6932Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6932Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6932Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 5/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:03:18,243][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6931
[2025-05-04 11:03:18,478][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:18,479][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.6924Epoch 6/15: [                              ] 2/63 batches, loss: 0.6932Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6937Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6935Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6933Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6933Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6934Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6933Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6933Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6933Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6933Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6933Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6933Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6933Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6932Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6932Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6932Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6933Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6932Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6932Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6932Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6932Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6932Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6932Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6932Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 6/15: [================              ] 34/63 batches, loss: 0.6931Epoch 6/15: [================              ] 35/63 batches, loss: 0.6931Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6932Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6932Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6932Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6932Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6932Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6932Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6932Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6932Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6932Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6932Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6932Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6932Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6931Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6932Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6932Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6931Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6931Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6931Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 6/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:03:20,437][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6931
[2025-05-04 11:03:20,667][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:20,667][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 11:03:20,667][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 6
[2025-05-04 11:03:20,681][src.training.lm_trainer][INFO] - Training completed in 15.96 seconds
[2025-05-04 11:03:20,681][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 11:03:23,382][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:23,382][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:23,382][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:03:25,304][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer11/ar/ar/model.pt
[2025-05-04 11:03:25,306][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁
wandb:           best_val_f1 ▁▁▁
wandb:         best_val_loss █▄▁
wandb:    best_val_precision ▁▁▁
wandb:       best_val_recall ▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁
wandb:            train_loss █▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁
wandb:              val_loss ▂▂▁▃▇█
wandb:         val_precision ▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.6933
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 6
wandb:                 epoch 6
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.6931
wandb:            train_time 15.96401
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69354
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110246-yr8631od
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110246-yr8631od/logs
Experiment probe_layer11_question_type_control2_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control2/layer11/ar/ar/results.json for layer 11
Running experiment: probe_layer11_question_type_control3_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=385" "model.probe_depth=2" "model.dropout=0.05" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-3" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_question_type_control3_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer11/ar"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=3"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 11:03:56,978][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer11/ar
experiment_name: probe_layer11_question_type_control3_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.05
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 385
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-04 11:03:56,978][__main__][INFO] - Normalized task: question_type
[2025-05-04 11:03:56,978][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-04 11:03:56,978][__main__][INFO] - Determined Task Type: classification
[2025-05-04 11:03:56,983][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-04 11:03:56,983][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 11:04:01,197][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 11:04:03,512][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 11:04:03,513][src.data.datasets][INFO] - Loading 'control_question_type_seed3' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:04:03,863][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 11:04:04,032][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:49:56 2025).
[2025-05-04 11:04:04,312][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-04 11:04:04,319][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:04:04,320][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-04 11:04:04,321][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:04:04,383][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:04:04,492][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:04:04,507][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-04 11:04:04,508][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:04:04,508][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-04 11:04:04,509][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:04:04,617][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:04:04,788][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:04:04,823][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-04 11:04:04,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:04:04,824][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-04 11:04:04,825][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:04:04,826][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-04 11:04:04,826][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-04 11:04:04,826][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:04:04,827][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-04 11:04:04,827][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-04 11:04:04,827][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-04 11:04:04,828][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-04 11:04:04,828][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Sample label: 0
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 11:04:04,828][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 11:04:04,829][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-04 11:04:04,829][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 11:04:12,714][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 11:04:12,715][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 11:04:12,715][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-04 11:04:12,715][src.models.model_factory][INFO] - Using provided probe_hidden_size: 385
[2025-05-04 11:04:12,721][src.models.model_factory][INFO] - Model has 447,367 trainable parameters out of 394,568,839 total parameters
[2025-05-04 11:04:12,722][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 447,367 trainable parameters
[2025-05-04 11:04:12,722][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=385, depth=2, activation=gelu, normalization=layer
[2025-05-04 11:04:12,722][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 385 hidden size
[2025-05-04 11:04:12,722][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-04 11:04:12,723][__main__][INFO] - Total parameters: 394,568,839
[2025-05-04 11:04:12,723][__main__][INFO] - Trainable parameters: 447,367 (0.11%)
[2025-05-04 11:04:12,723][__main__][INFO] - Adjusting learning rate for probe from 0.001 to 1e-4
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7762Epoch 1/15: [                              ] 2/63 batches, loss: 0.7075Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7277Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7022Epoch 1/15: [==                            ] 5/63 batches, loss: 0.6961Epoch 1/15: [==                            ] 6/63 batches, loss: 0.6919Epoch 1/15: [===                           ] 7/63 batches, loss: 0.6957Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7010Epoch 1/15: [====                          ] 9/63 batches, loss: 0.6996Epoch 1/15: [====                          ] 10/63 batches, loss: 0.6999Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7013Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7003Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7001Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6983Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6977Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6964Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6964Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6968Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6966Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6960Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6955Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6964Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6962Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6962Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6961Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6957Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6957Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6956Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6956Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6953Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6953Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6952Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6950Epoch 1/15: [================              ] 34/63 batches, loss: 0.6949Epoch 1/15: [================              ] 35/63 batches, loss: 0.6949Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6951Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6951Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6951Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6953Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6953Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6956Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6956Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6955Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6954Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6954Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6953Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6953Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6952Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6953Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6951Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6950Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6950Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6950Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6950Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6949Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6949Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6949Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6948Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6948Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6947Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6947Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6947Epoch 1/15: [==============================] 63/63 batches, loss: 0.6949
[2025-05-04 11:04:19,225][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6949
[2025-05-04 11:04:19,581][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6943, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6904Epoch 2/15: [                              ] 2/63 batches, loss: 0.6908Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6912Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6929Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6932Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6932Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6932Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6931Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6929Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6930Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6921Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6924Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6927Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6926Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6925Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6925Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6931Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6932Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6928Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6928Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6928Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6928Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6928Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6928Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6925Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6925Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6925Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6924Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6923Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6924Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6923Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6923Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6923Epoch 2/15: [================              ] 34/63 batches, loss: 0.6923Epoch 2/15: [================              ] 35/63 batches, loss: 0.6923Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6923Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6923Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6924Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6925Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6924Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6925Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6924Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6925Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6924Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6926Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6925Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6925Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6925Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6924Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6925Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6925Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6926Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6926Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6926Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6926Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6926Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6926Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6926Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6925Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6926Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6926Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6926Epoch 2/15: [==============================] 63/63 batches, loss: 0.6927
[2025-05-04 11:04:21,942][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6927
[2025-05-04 11:04:22,239][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6943, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:22,239][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6834Epoch 3/15: [                              ] 2/63 batches, loss: 0.6735Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6791Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6819Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6866Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6880Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6917Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6896Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6890Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6860Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6865Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6873Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6877Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6867Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6874Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6864Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6868Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6869Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6868Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6871Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6875Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6877Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6885Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6892Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6894Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6895Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6906Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6909Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6909Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6909Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6910Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6922Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 3/15: [================              ] 34/63 batches, loss: 0.6931Epoch 3/15: [================              ] 35/63 batches, loss: 0.6934Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6929Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6930Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6930Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6930Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6930Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6930Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6930Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6931Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6930Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6928Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6928Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6924Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6925Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6925Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6925Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6922Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6923Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6923Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6923Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6923Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6924Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6925Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6923Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6922Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6922Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6922Epoch 3/15: [==============================] 63/63 batches, loss: 0.6922
[2025-05-04 11:04:24,202][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6922
[2025-05-04 11:04:24,545][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6939, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.7125Epoch 4/15: [                              ] 2/63 batches, loss: 0.7060Epoch 4/15: [=                             ] 3/63 batches, loss: 0.6919Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6924Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6948Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6943Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6945Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6934Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6941Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6938Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6939Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6944Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6938Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6935Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6934Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6930Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6930Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6930Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6930Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6930Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6929Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6928Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6927Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6927Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6928Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6927Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6927Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6935Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6935Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6935Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6935Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6935Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6940Epoch 4/15: [================              ] 34/63 batches, loss: 0.6940Epoch 4/15: [================              ] 35/63 batches, loss: 0.6939Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6939Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6939Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6940Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6939Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6939Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6938Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6938Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6938Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6938Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6937Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6938Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6937Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6937Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6937Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6937Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6937Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6936Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6937Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6936Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6936Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6936Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6936Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6936Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6936Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6936Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6936Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6936Epoch 4/15: [==============================] 63/63 batches, loss: 0.6935
[2025-05-04 11:04:27,187][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6935
[2025-05-04 11:04:27,413][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6940, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:27,414][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.6916Epoch 5/15: [                              ] 2/63 batches, loss: 0.6929Epoch 5/15: [=                             ] 3/63 batches, loss: 0.6932Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6931Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6936Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6932Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6936Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6940Epoch 5/15: [====                          ] 9/63 batches, loss: 0.6937Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6938Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6937Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6935Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6937Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6935Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6935Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6935Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6929Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6930Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6930Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6929Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6929Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6929Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6929Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6930Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6929Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6930Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6930Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6930Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6930Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6930Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6930Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6930Epoch 5/15: [================              ] 34/63 batches, loss: 0.6930Epoch 5/15: [================              ] 35/63 batches, loss: 0.6930Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6931Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6930Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6929Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6929Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6929Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6927Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6927Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6928Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6928Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6927Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6927Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6927Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6928Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6928Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6929Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6928Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6928Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6928Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6929Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6929Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6930Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6929Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6929Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6928Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6928Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6927Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6927Epoch 5/15: [==============================] 63/63 batches, loss: 0.6928
[2025-05-04 11:04:30,349][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6928
[2025-05-04 11:04:30,660][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6963, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:30,661][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.6964Epoch 6/15: [                              ] 2/63 batches, loss: 0.6914Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6889Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6875Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6887Epoch 6/15: [==                            ] 6/63 batches, loss: 0.6899Epoch 6/15: [===                           ] 7/63 batches, loss: 0.6881Epoch 6/15: [===                           ] 8/63 batches, loss: 0.6883Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6884Epoch 6/15: [====                          ] 10/63 batches, loss: 0.6882Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.6884Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.6853Epoch 6/15: [======                        ] 13/63 batches, loss: 0.6838Epoch 6/15: [======                        ] 14/63 batches, loss: 0.6910Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.6956Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.6969Epoch 6/15: [========                      ] 17/63 batches, loss: 0.6974Epoch 6/15: [========                      ] 18/63 batches, loss: 0.6961Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.6963Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.6960Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.6958Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.6955Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.6955Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.6953Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.6951Epoch 6/15: [============                  ] 26/63 batches, loss: 0.6948Epoch 6/15: [============                  ] 27/63 batches, loss: 0.6949Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.6948Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.6948Epoch 6/15: [==============                ] 30/63 batches, loss: 0.6944Epoch 6/15: [==============                ] 31/63 batches, loss: 0.6943Epoch 6/15: [===============               ] 32/63 batches, loss: 0.6943Epoch 6/15: [===============               ] 33/63 batches, loss: 0.6943Epoch 6/15: [================              ] 34/63 batches, loss: 0.6945Epoch 6/15: [================              ] 35/63 batches, loss: 0.6945Epoch 6/15: [=================             ] 36/63 batches, loss: 0.6945Epoch 6/15: [=================             ] 37/63 batches, loss: 0.6945Epoch 6/15: [==================            ] 38/63 batches, loss: 0.6945Epoch 6/15: [==================            ] 39/63 batches, loss: 0.6945Epoch 6/15: [===================           ] 40/63 batches, loss: 0.6944Epoch 6/15: [===================           ] 41/63 batches, loss: 0.6944Epoch 6/15: [====================          ] 42/63 batches, loss: 0.6944Epoch 6/15: [====================          ] 43/63 batches, loss: 0.6943Epoch 6/15: [====================          ] 44/63 batches, loss: 0.6942Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.6942Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.6941Epoch 6/15: [======================        ] 47/63 batches, loss: 0.6941Epoch 6/15: [======================        ] 48/63 batches, loss: 0.6941Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.6941Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.6941Epoch 6/15: [========================      ] 51/63 batches, loss: 0.6941Epoch 6/15: [========================      ] 52/63 batches, loss: 0.6940Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.6940Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.6940Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.6939Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.6939Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.6939Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.6939Epoch 6/15: [============================  ] 59/63 batches, loss: 0.6939Epoch 6/15: [============================  ] 60/63 batches, loss: 0.6939Epoch 6/15: [============================= ] 61/63 batches, loss: 0.6939Epoch 6/15: [============================= ] 62/63 batches, loss: 0.6939Epoch 6/15: [==============================] 63/63 batches, loss: 0.6939
[2025-05-04 11:04:32,629][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6939
[2025-05-04 11:04:32,939][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6936, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.6931Epoch 7/15: [                              ] 2/63 batches, loss: 0.6927Epoch 7/15: [=                             ] 3/63 batches, loss: 0.6917Epoch 7/15: [=                             ] 4/63 batches, loss: 0.6917Epoch 7/15: [==                            ] 5/63 batches, loss: 0.6917Epoch 7/15: [==                            ] 6/63 batches, loss: 0.6917Epoch 7/15: [===                           ] 7/63 batches, loss: 0.6922Epoch 7/15: [===                           ] 8/63 batches, loss: 0.6924Epoch 7/15: [====                          ] 9/63 batches, loss: 0.6924Epoch 7/15: [====                          ] 10/63 batches, loss: 0.6924Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.6925Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.6926Epoch 7/15: [======                        ] 13/63 batches, loss: 0.6926Epoch 7/15: [======                        ] 14/63 batches, loss: 0.6926Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.6925Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.6924Epoch 7/15: [========                      ] 17/63 batches, loss: 0.6924Epoch 7/15: [========                      ] 18/63 batches, loss: 0.6925Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.6926Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.6927Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.6927Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.6927Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.6926Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.6926Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.6926Epoch 7/15: [============                  ] 26/63 batches, loss: 0.6925Epoch 7/15: [============                  ] 27/63 batches, loss: 0.6927Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.6927Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.6927Epoch 7/15: [==============                ] 30/63 batches, loss: 0.6927Epoch 7/15: [==============                ] 31/63 batches, loss: 0.6926Epoch 7/15: [===============               ] 32/63 batches, loss: 0.6925Epoch 7/15: [===============               ] 33/63 batches, loss: 0.6925Epoch 7/15: [================              ] 34/63 batches, loss: 0.6925Epoch 7/15: [================              ] 35/63 batches, loss: 0.6925Epoch 7/15: [=================             ] 36/63 batches, loss: 0.6926Epoch 7/15: [=================             ] 37/63 batches, loss: 0.6925Epoch 7/15: [==================            ] 38/63 batches, loss: 0.6925Epoch 7/15: [==================            ] 39/63 batches, loss: 0.6925Epoch 7/15: [===================           ] 40/63 batches, loss: 0.6924Epoch 7/15: [===================           ] 41/63 batches, loss: 0.6925Epoch 7/15: [====================          ] 42/63 batches, loss: 0.6924Epoch 7/15: [====================          ] 43/63 batches, loss: 0.6924Epoch 7/15: [====================          ] 44/63 batches, loss: 0.6923Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.6924Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.6924Epoch 7/15: [======================        ] 47/63 batches, loss: 0.6924Epoch 7/15: [======================        ] 48/63 batches, loss: 0.6924Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.6925Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.6925Epoch 7/15: [========================      ] 51/63 batches, loss: 0.6923Epoch 7/15: [========================      ] 52/63 batches, loss: 0.6923Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.6923Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.6924Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.6923Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.6923Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.6923Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.6924Epoch 7/15: [============================  ] 59/63 batches, loss: 0.6927Epoch 7/15: [============================  ] 60/63 batches, loss: 0.6926Epoch 7/15: [============================= ] 61/63 batches, loss: 0.6925Epoch 7/15: [============================= ] 62/63 batches, loss: 0.6923Epoch 7/15: [==============================] 63/63 batches, loss: 0.6924
[2025-05-04 11:04:35,318][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6924
[2025-05-04 11:04:35,599][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6951, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:35,600][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6980Epoch 8/15: [                              ] 2/63 batches, loss: 0.6952Epoch 8/15: [=                             ] 3/63 batches, loss: 0.6961Epoch 8/15: [=                             ] 4/63 batches, loss: 0.6966Epoch 8/15: [==                            ] 5/63 batches, loss: 0.6958Epoch 8/15: [==                            ] 6/63 batches, loss: 0.6948Epoch 8/15: [===                           ] 7/63 batches, loss: 0.6929Epoch 8/15: [===                           ] 8/63 batches, loss: 0.6936Epoch 8/15: [====                          ] 9/63 batches, loss: 0.6922Epoch 8/15: [====                          ] 10/63 batches, loss: 0.6923Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.6930Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.6923Epoch 8/15: [======                        ] 13/63 batches, loss: 0.6923Epoch 8/15: [======                        ] 14/63 batches, loss: 0.6929Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.6918Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.6917Epoch 8/15: [========                      ] 17/63 batches, loss: 0.6914Epoch 8/15: [========                      ] 18/63 batches, loss: 0.6915Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.6919Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.6909Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.6905Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.6903Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.6900Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.6920Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.6917Epoch 8/15: [============                  ] 26/63 batches, loss: 0.6915Epoch 8/15: [============                  ] 27/63 batches, loss: 0.6917Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.6918Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.6912Epoch 8/15: [==============                ] 30/63 batches, loss: 0.6910Epoch 8/15: [==============                ] 31/63 batches, loss: 0.6915Epoch 8/15: [===============               ] 32/63 batches, loss: 0.6912Epoch 8/15: [===============               ] 33/63 batches, loss: 0.6906Epoch 8/15: [================              ] 34/63 batches, loss: 0.6898Epoch 8/15: [================              ] 35/63 batches, loss: 0.6900Epoch 8/15: [=================             ] 36/63 batches, loss: 0.6897Epoch 8/15: [=================             ] 37/63 batches, loss: 0.6891Epoch 8/15: [==================            ] 38/63 batches, loss: 0.6884Epoch 8/15: [==================            ] 39/63 batches, loss: 0.6901Epoch 8/15: [===================           ] 40/63 batches, loss: 0.6912Epoch 8/15: [===================           ] 41/63 batches, loss: 0.6924Epoch 8/15: [====================          ] 42/63 batches, loss: 0.6923Epoch 8/15: [====================          ] 43/63 batches, loss: 0.6936Epoch 8/15: [====================          ] 44/63 batches, loss: 0.6936Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.6937Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.6938Epoch 8/15: [======================        ] 47/63 batches, loss: 0.6938Epoch 8/15: [======================        ] 48/63 batches, loss: 0.6937Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.6938Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.6936Epoch 8/15: [========================      ] 51/63 batches, loss: 0.6936Epoch 8/15: [========================      ] 52/63 batches, loss: 0.6936Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.6936Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.6936Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.6936Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.6935Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.6935Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.6935Epoch 8/15: [============================  ] 59/63 batches, loss: 0.6935Epoch 8/15: [============================  ] 60/63 batches, loss: 0.6935Epoch 8/15: [============================= ] 61/63 batches, loss: 0.6935Epoch 8/15: [============================= ] 62/63 batches, loss: 0.6934Epoch 8/15: [==============================] 63/63 batches, loss: 0.6935
[2025-05-04 11:04:37,659][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6935
[2025-05-04 11:04:37,962][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.6936Epoch 9/15: [                              ] 2/63 batches, loss: 0.6935Epoch 9/15: [=                             ] 3/63 batches, loss: 0.6933Epoch 9/15: [=                             ] 4/63 batches, loss: 0.6933Epoch 9/15: [==                            ] 5/63 batches, loss: 0.6931Epoch 9/15: [==                            ] 6/63 batches, loss: 0.6932Epoch 9/15: [===                           ] 7/63 batches, loss: 0.6938Epoch 9/15: [===                           ] 8/63 batches, loss: 0.6937Epoch 9/15: [====                          ] 9/63 batches, loss: 0.6935Epoch 9/15: [====                          ] 10/63 batches, loss: 0.6935Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.6934Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.6934Epoch 9/15: [======                        ] 13/63 batches, loss: 0.6934Epoch 9/15: [======                        ] 14/63 batches, loss: 0.6934Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.6933Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.6935Epoch 9/15: [========                      ] 17/63 batches, loss: 0.6935Epoch 9/15: [========                      ] 18/63 batches, loss: 0.6934Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.6934Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.6934Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.6934Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.6934Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.6934Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.6933Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.6933Epoch 9/15: [============                  ] 26/63 batches, loss: 0.6934Epoch 9/15: [============                  ] 27/63 batches, loss: 0.6933Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.6934Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.6933Epoch 9/15: [==============                ] 30/63 batches, loss: 0.6933Epoch 9/15: [==============                ] 31/63 batches, loss: 0.6933Epoch 9/15: [===============               ] 32/63 batches, loss: 0.6933Epoch 9/15: [===============               ] 33/63 batches, loss: 0.6933Epoch 9/15: [================              ] 34/63 batches, loss: 0.6933Epoch 9/15: [================              ] 35/63 batches, loss: 0.6933Epoch 9/15: [=================             ] 36/63 batches, loss: 0.6933Epoch 9/15: [=================             ] 37/63 batches, loss: 0.6933Epoch 9/15: [==================            ] 38/63 batches, loss: 0.6933Epoch 9/15: [==================            ] 39/63 batches, loss: 0.6934Epoch 9/15: [===================           ] 40/63 batches, loss: 0.6934Epoch 9/15: [===================           ] 41/63 batches, loss: 0.6934Epoch 9/15: [====================          ] 42/63 batches, loss: 0.6934Epoch 9/15: [====================          ] 43/63 batches, loss: 0.6934Epoch 9/15: [====================          ] 44/63 batches, loss: 0.6934Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.6934Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.6934Epoch 9/15: [======================        ] 47/63 batches, loss: 0.6934Epoch 9/15: [======================        ] 48/63 batches, loss: 0.6934Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.6933Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.6933Epoch 9/15: [========================      ] 51/63 batches, loss: 0.6933Epoch 9/15: [========================      ] 52/63 batches, loss: 0.6933Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.6933Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.6933Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.6933Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.6933Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.6933Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.6933Epoch 9/15: [============================  ] 59/63 batches, loss: 0.6933Epoch 9/15: [============================  ] 60/63 batches, loss: 0.6933Epoch 9/15: [============================= ] 61/63 batches, loss: 0.6933Epoch 9/15: [============================= ] 62/63 batches, loss: 0.6933Epoch 9/15: [==============================] 63/63 batches, loss: 0.6933
[2025-05-04 11:04:40,380][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6933
[2025-05-04 11:04:40,665][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.6933Epoch 10/15: [                              ] 2/63 batches, loss: 0.6933Epoch 10/15: [=                             ] 3/63 batches, loss: 0.6932Epoch 10/15: [=                             ] 4/63 batches, loss: 0.6932Epoch 10/15: [==                            ] 5/63 batches, loss: 0.6932Epoch 10/15: [==                            ] 6/63 batches, loss: 0.6932Epoch 10/15: [===                           ] 7/63 batches, loss: 0.6932Epoch 10/15: [===                           ] 8/63 batches, loss: 0.6932Epoch 10/15: [====                          ] 9/63 batches, loss: 0.6932Epoch 10/15: [====                          ] 10/63 batches, loss: 0.6932Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.6932Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.6932Epoch 10/15: [======                        ] 13/63 batches, loss: 0.6932Epoch 10/15: [======                        ] 14/63 batches, loss: 0.6932Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.6932Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.6932Epoch 10/15: [========                      ] 17/63 batches, loss: 0.6932Epoch 10/15: [========                      ] 18/63 batches, loss: 0.6932Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.6932Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.6932Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.6932Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.6932Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.6931Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.6931Epoch 10/15: [============                  ] 26/63 batches, loss: 0.6931Epoch 10/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 10/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 10/15: [==============                ] 31/63 batches, loss: 0.6931Epoch 10/15: [===============               ] 32/63 batches, loss: 0.6931Epoch 10/15: [===============               ] 33/63 batches, loss: 0.6931Epoch 10/15: [================              ] 34/63 batches, loss: 0.6931Epoch 10/15: [================              ] 35/63 batches, loss: 0.6931Epoch 10/15: [=================             ] 36/63 batches, loss: 0.6931Epoch 10/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 10/15: [==================            ] 38/63 batches, loss: 0.6931Epoch 10/15: [==================            ] 39/63 batches, loss: 0.6931Epoch 10/15: [===================           ] 40/63 batches, loss: 0.6931Epoch 10/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 10/15: [====================          ] 42/63 batches, loss: 0.6931Epoch 10/15: [====================          ] 43/63 batches, loss: 0.6931Epoch 10/15: [====================          ] 44/63 batches, loss: 0.6931Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.6931Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.6931Epoch 10/15: [======================        ] 47/63 batches, loss: 0.6931Epoch 10/15: [======================        ] 48/63 batches, loss: 0.6931Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.6931Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.6931Epoch 10/15: [========================      ] 51/63 batches, loss: 0.6931Epoch 10/15: [========================      ] 52/63 batches, loss: 0.6931Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.6931Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.6931Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 10/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 10/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 10/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 10/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 10/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:04:43,045][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.6931
[2025-05-04 11:04:43,325][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.6932, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:43,325][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.6933Epoch 11/15: [                              ] 2/63 batches, loss: 0.6930Epoch 11/15: [=                             ] 3/63 batches, loss: 0.6929Epoch 11/15: [=                             ] 4/63 batches, loss: 0.6930Epoch 11/15: [==                            ] 5/63 batches, loss: 0.6931Epoch 11/15: [==                            ] 6/63 batches, loss: 0.6931Epoch 11/15: [===                           ] 7/63 batches, loss: 0.6931Epoch 11/15: [===                           ] 8/63 batches, loss: 0.6931Epoch 11/15: [====                          ] 9/63 batches, loss: 0.6931Epoch 11/15: [====                          ] 10/63 batches, loss: 0.6931Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.6931Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.6931Epoch 11/15: [======                        ] 13/63 batches, loss: 0.6931Epoch 11/15: [======                        ] 14/63 batches, loss: 0.6931Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.6931Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.6931Epoch 11/15: [========                      ] 17/63 batches, loss: 0.6931Epoch 11/15: [========                      ] 18/63 batches, loss: 0.6931Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.6931Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.6931Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.6931Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.6931Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.6931Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.6930Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.6930Epoch 11/15: [============                  ] 26/63 batches, loss: 0.6930Epoch 11/15: [============                  ] 27/63 batches, loss: 0.6931Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.6931Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.6931Epoch 11/15: [==============                ] 30/63 batches, loss: 0.6931Epoch 11/15: [==============                ] 31/63 batches, loss: 0.6930Epoch 11/15: [===============               ] 32/63 batches, loss: 0.6930Epoch 11/15: [===============               ] 33/63 batches, loss: 0.6930Epoch 11/15: [================              ] 34/63 batches, loss: 0.6930Epoch 11/15: [================              ] 35/63 batches, loss: 0.6930Epoch 11/15: [=================             ] 36/63 batches, loss: 0.6930Epoch 11/15: [=================             ] 37/63 batches, loss: 0.6931Epoch 11/15: [==================            ] 38/63 batches, loss: 0.6931Epoch 11/15: [==================            ] 39/63 batches, loss: 0.6931Epoch 11/15: [===================           ] 40/63 batches, loss: 0.6931Epoch 11/15: [===================           ] 41/63 batches, loss: 0.6931Epoch 11/15: [====================          ] 42/63 batches, loss: 0.6931Epoch 11/15: [====================          ] 43/63 batches, loss: 0.6931Epoch 11/15: [====================          ] 44/63 batches, loss: 0.6931Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.6931Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.6931Epoch 11/15: [======================        ] 47/63 batches, loss: 0.6931Epoch 11/15: [======================        ] 48/63 batches, loss: 0.6931Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.6931Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.6931Epoch 11/15: [========================      ] 51/63 batches, loss: 0.6931Epoch 11/15: [========================      ] 52/63 batches, loss: 0.6931Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.6931Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.6931Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.6931Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.6931Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.6931Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.6931Epoch 11/15: [============================  ] 59/63 batches, loss: 0.6931Epoch 11/15: [============================  ] 60/63 batches, loss: 0.6931Epoch 11/15: [============================= ] 61/63 batches, loss: 0.6931Epoch 11/15: [============================= ] 62/63 batches, loss: 0.6931Epoch 11/15: [==============================] 63/63 batches, loss: 0.6931
[2025-05-04 11:04:45,341][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.6931
[2025-05-04 11:04:45,597][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:45,598][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.6930Epoch 12/15: [                              ] 2/63 batches, loss: 0.6928Epoch 12/15: [=                             ] 3/63 batches, loss: 0.6928Epoch 12/15: [=                             ] 4/63 batches, loss: 0.6929Epoch 12/15: [==                            ] 5/63 batches, loss: 0.6929Epoch 12/15: [==                            ] 6/63 batches, loss: 0.6928Epoch 12/15: [===                           ] 7/63 batches, loss: 0.6924Epoch 12/15: [===                           ] 8/63 batches, loss: 0.6925Epoch 12/15: [====                          ] 9/63 batches, loss: 0.6925Epoch 12/15: [====                          ] 10/63 batches, loss: 0.6926Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.6926Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.6926Epoch 12/15: [======                        ] 13/63 batches, loss: 0.6926Epoch 12/15: [======                        ] 14/63 batches, loss: 0.6926Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.6926Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.6926Epoch 12/15: [========                      ] 17/63 batches, loss: 0.6926Epoch 12/15: [========                      ] 18/63 batches, loss: 0.6926Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.6926Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.6927Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.6927Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.6927Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.6927Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.6928Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.6928Epoch 12/15: [============                  ] 26/63 batches, loss: 0.6928Epoch 12/15: [============                  ] 27/63 batches, loss: 0.6928Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.6928Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.6928Epoch 12/15: [==============                ] 30/63 batches, loss: 0.6928Epoch 12/15: [==============                ] 31/63 batches, loss: 0.6928Epoch 12/15: [===============               ] 32/63 batches, loss: 0.6928Epoch 12/15: [===============               ] 33/63 batches, loss: 0.6928Epoch 12/15: [================              ] 34/63 batches, loss: 0.6928Epoch 12/15: [================              ] 35/63 batches, loss: 0.6928Epoch 12/15: [=================             ] 36/63 batches, loss: 0.6928Epoch 12/15: [=================             ] 37/63 batches, loss: 0.6927Epoch 12/15: [==================            ] 38/63 batches, loss: 0.6928Epoch 12/15: [==================            ] 39/63 batches, loss: 0.6927Epoch 12/15: [===================           ] 40/63 batches, loss: 0.6927Epoch 12/15: [===================           ] 41/63 batches, loss: 0.6928Epoch 12/15: [====================          ] 42/63 batches, loss: 0.6928Epoch 12/15: [====================          ] 43/63 batches, loss: 0.6928Epoch 12/15: [====================          ] 44/63 batches, loss: 0.6928Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.6928Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.6928Epoch 12/15: [======================        ] 47/63 batches, loss: 0.6928Epoch 12/15: [======================        ] 48/63 batches, loss: 0.6928Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.6928Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.6928Epoch 12/15: [========================      ] 51/63 batches, loss: 0.6928Epoch 12/15: [========================      ] 52/63 batches, loss: 0.6928Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.6928Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.6928Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.6928Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.6928Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.6928Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.6928Epoch 12/15: [============================  ] 59/63 batches, loss: 0.6928Epoch 12/15: [============================  ] 60/63 batches, loss: 0.6928Epoch 12/15: [============================= ] 61/63 batches, loss: 0.6928Epoch 12/15: [============================= ] 62/63 batches, loss: 0.6928Epoch 12/15: [==============================] 63/63 batches, loss: 0.6928
[2025-05-04 11:04:47,548][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.6928
[2025-05-04 11:04:47,762][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.6935, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:47,763][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-04 11:04:47,763][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 12
[2025-05-04 11:04:47,763][src.training.lm_trainer][INFO] - Training completed in 31.28 seconds
[2025-05-04 11:04:47,763][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-04 11:04:50,305][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:50,305][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:50,305][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
[2025-05-04 11:04:52,261][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer11/ar/ar/model.pt
[2025-05-04 11:04:52,262][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▁▁▁
wandb:           best_val_f1 ▁▁▁▁▁
wandb:         best_val_loss █▅▄▁▁
wandb:    best_val_precision ▁▁▁▁▁
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_loss █▂▁▄▃▅▁▄▄▃▃▃
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:              val_loss ▃▄▃▃█▂▅▁▁▁▁▂
wandb:         val_precision ▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.54545
wandb:           best_val_f1 0
wandb:         best_val_loss 0.69322
wandb:    best_val_precision 0
wandb:       best_val_recall 0
wandb:      early_stop_epoch 12
wandb:                 epoch 12
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0
wandb:  final_test_precision 0
wandb:     final_test_recall 0
wandb:  final_train_accuracy 0.5005
wandb:        final_train_f1 0
wandb: final_train_precision 0
wandb:    final_train_recall 0
wandb:    final_val_accuracy 0.54545
wandb:          final_val_f1 0
wandb:   final_val_precision 0
wandb:      final_val_recall 0
wandb:         learning_rate 0.0001
wandb:            train_loss 0.69283
wandb:            train_time 31.27765
wandb:          val_accuracy 0.54545
wandb:                val_f1 0
wandb:              val_loss 0.69349
wandb:         val_precision 0
wandb:            val_recall 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110357-s1tw8g6a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250504_110357-s1tw8g6a/logs
Experiment probe_layer11_question_type_control3_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/control3/layer11/ar/ar/results.json for layer 11
Running experiment: probe_layer11_complexity_control1_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=128" "model.probe_depth=3" "model.dropout=0.2" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_complexity_control1_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer11/ar"         "wandb.mode=offline"             "experiment.use_controls=true"             "experiment.control_index=1"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-04 11:05:25,452][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/control1/layer11/ar
experiment_name: probe_layer11_complexity_control1_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 3
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-04 11:05:25,453][__main__][INFO] - Normalized task: complexity
[2025-05-04 11:05:25,453][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-04 11:05:25,453][__main__][INFO] - Determined Task Type: regression
[2025-05-04 11:05:25,458][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-04 11:05:25,459][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-04 11:05:29,662][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-04 11:05:31,982][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-04 11:05:31,983][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:05:32,257][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 11:05:32,366][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-05-04 11:05:32,692][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-04 11:05:32,699][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:05:32,700][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-04 11:05:32,703][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:05:32,824][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:05:32,952][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:05:33,009][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-04 11:05:33,011][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:05:33,011][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-04 11:05:33,013][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-04 11:05:33,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:05:33,234][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-04 11:05:33,284][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-04 11:05:33,286][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-04 11:05:33,286][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-04 11:05:33,299][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:05:33,300][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:05:33,300][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-04 11:05:33,300][src.data.datasets][INFO] - Sample label: 0.20462249219417572
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:05:33,301][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:05:33,301][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-04 11:05:33,301][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-04 11:05:33,302][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-04 11:05:33,302][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-04 11:05:33,302][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-04 11:05:33,303][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-04 11:05:33,303][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-04 11:05:33,303][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-04 11:05:41,274][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-04 11:05:41,275][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-04 11:05:41,276][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-04 11:05:41,276][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-04 11:05:41,279][src.models.model_factory][INFO] - Model has 133,633 trainable parameters out of 394,255,105 total parameters
[2025-05-04 11:05:41,279][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 133,633 trainable parameters
[2025-05-04 11:05:41,279][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=3, activation=silu, normalization=layer
[2025-05-04 11:05:41,279][src.models.model_factory][INFO] - Created specialized regression probe with 3 layers, 128 hidden size
[2025-05-04 11:05:41,279][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-04 11:05:41,280][__main__][INFO] - Total parameters: 394,255,105
[2025-05-04 11:05:41,280][__main__][INFO] - Trainable parameters: 133,633 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.3970Epoch 1/15: [                              ] 2/63 batches, loss: 0.5789Epoch 1/15: [=                             ] 3/63 batches, loss: 0.5260Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4982Epoch 1/15: [==                            ] 5/63 batches, loss: 0.5068Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4502Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4456Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4634Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4859Epoch 1/15: [====                          ] 10/63 batches, loss: 0.4681Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.4436Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.4514Epoch 1/15: [======                        ] 13/63 batches, loss: 0.4435Epoch 1/15: [======                        ] 14/63 batches, loss: 0.4496Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.4373Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.4666Epoch 1/15: [========                      ] 17/63 batches, loss: 0.4559Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4615Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.4499Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.4442Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.4397Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.4324Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.4209Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.4180Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.4243Epoch 1/15: [============                  ] 26/63 batches, loss: 0.4250Epoch 1/15: [============                  ] 27/63 batches, loss: 0.4281Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.4327Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.4336Epoch 1/15: [==============                ] 30/63 batches, loss: 0.4333Epoch 1/15: [==============                ] 31/63 batches, loss: 0.4287Epoch 1/15: [===============               ] 32/63 batches, loss: 0.4221Epoch 1/15: [===============               ] 33/63 batches, loss: 0.4198Epoch 1/15: [================              ] 34/63 batches, loss: 0.4150Epoch 1/15: [================              ] 35/63 batches, loss: 0.4204Epoch 1/15: [=================             ] 36/63 batches, loss: 0.4268Epoch 1/15: [=================             ] 37/63 batches, loss: 0.4243Epoch 1/15: [==================            ] 38/63 batches, loss: 0.4232Epoch 1/15: [==================            ] 39/63 batches, loss: 0.4173Epoch 1/15: [===================           ] 40/63 batches, loss: 0.4124Epoch 1/15: [===================           ] 41/63 batches, loss: 0.4062Epoch 1/15: [====================          ] 42/63 batches, loss: 0.4042Epoch 1/15: [====================          ] 43/63 batches, loss: 0.3979Epoch 1/15: [====================          ] 44/63 batches, loss: 0.3971Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.3941Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.3903Epoch 1/15: [======================        ] 47/63 batches, loss: 0.3868Epoch 1/15: [======================        ] 48/63 batches, loss: 0.3811Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.3778Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.3739Epoch 1/15: [========================      ] 51/63 batches, loss: 0.3702Epoch 1/15: [========================      ] 52/63 batches, loss: 0.3658Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.3647Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3662Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3645Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.3606Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.3605Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.3579Epoch 1/15: [============================  ] 59/63 batches, loss: 0.3540Epoch 1/15: [============================  ] 60/63 batches, loss: 0.3507Epoch 1/15: [============================= ] 61/63 batches, loss: 0.3471Epoch 1/15: [============================= ] 62/63 batches, loss: 0.3443Epoch 1/15: [==============================] 63/63 batches, loss: 0.3414
[2025-05-04 11:05:48,037][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3414
[2025-05-04 11:05:48,306][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.0842, Metrics: {'mse': 0.0837390273809433, 'rmse': 0.28937696415047154, 'r2': -0.2906968593597412}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.1289Epoch 2/15: [                              ] 2/63 batches, loss: 0.2273Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2137Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2203Epoch 2/15: [==                            ] 5/63 batches, loss: 0.2006Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2074Epoch 2/15: [===                           ] 7/63 batches, loss: 0.2077Epoch 2/15: [===                           ] 8/63 batches, loss: 0.2084Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2215Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2210Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.2133Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.2247Epoch 2/15: [======                        ] 13/63 batches, loss: 0.2264Epoch 2/15: [======                        ] 14/63 batches, loss: 0.2200Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2163Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.2122Epoch 2/15: [========                      ] 17/63 batches, loss: 0.2103Epoch 2/15: [========                      ] 18/63 batches, loss: 0.2068Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.2029Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.1989Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.1967Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.1931Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.1928Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.1963Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.1940Epoch 2/15: [============                  ] 26/63 batches, loss: 0.1971Epoch 2/15: [============                  ] 27/63 batches, loss: 0.1941Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.1917Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1898Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1866Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1855Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1878Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1868Epoch 2/15: [================              ] 34/63 batches, loss: 0.1890Epoch 2/15: [================              ] 35/63 batches, loss: 0.1889Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1895Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1884Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1877Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1859Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1855Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1845Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1827Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1810Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1801Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1787Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1812Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1825Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1815Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1793Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1777Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1778Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1754Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1746Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1751Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1727Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1733Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1731Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1713Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1706Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1718Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1710Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1705Epoch 2/15: [==============================] 63/63 batches, loss: 0.1682
[2025-05-04 11:05:50,674][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1682
[2025-05-04 11:05:50,927][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.0712, Metrics: {'mse': 0.0712149441242218, 'rmse': 0.26686128254998287, 'r2': -0.09765923023223877}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.2449Epoch 3/15: [                              ] 2/63 batches, loss: 0.1570Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1509Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1363Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1476Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1485Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1379Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1426Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1454Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1527Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1462Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1462Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1411Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1430Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1396Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1393Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1370Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1348Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1332Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1311Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1327Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1325Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1313Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1301Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1295Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1270Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1274Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1264Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1243Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1244Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1251Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1262Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1264Epoch 3/15: [================              ] 34/63 batches, loss: 0.1259Epoch 3/15: [================              ] 35/63 batches, loss: 0.1280Epoch 3/15: [=================             ] 36/63 batches, loss: 0.1258Epoch 3/15: [=================             ] 37/63 batches, loss: 0.1248Epoch 3/15: [==================            ] 38/63 batches, loss: 0.1240Epoch 3/15: [==================            ] 39/63 batches, loss: 0.1221Epoch 3/15: [===================           ] 40/63 batches, loss: 0.1222Epoch 3/15: [===================           ] 41/63 batches, loss: 0.1230Epoch 3/15: [====================          ] 42/63 batches, loss: 0.1233Epoch 3/15: [====================          ] 43/63 batches, loss: 0.1237Epoch 3/15: [====================          ] 44/63 batches, loss: 0.1235Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.1227Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.1214Epoch 3/15: [======================        ] 47/63 batches, loss: 0.1214Epoch 3/15: [======================        ] 48/63 batches, loss: 0.1215Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.1210Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.1233Epoch 3/15: [========================      ] 51/63 batches, loss: 0.1224Epoch 3/15: [========================      ] 52/63 batches, loss: 0.1230Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.1227Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.1229Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.1218Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.1228Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.1232Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.1238Epoch 3/15: [============================  ] 59/63 batches, loss: 0.1230Epoch 3/15: [============================  ] 60/63 batches, loss: 0.1226Epoch 3/15: [============================= ] 61/63 batches, loss: 0.1220Epoch 3/15: [============================= ] 62/63 batches, loss: 0.1209Epoch 3/15: [==============================] 63/63 batches, loss: 0.1232
[2025-05-04 11:05:53,476][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1232
[2025-05-04 11:05:53,707][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0719, Metrics: {'mse': 0.0722387507557869, 'rmse': 0.2687726748681623, 'r2': -0.11343932151794434}
[2025-05-04 11:05:53,708][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.0701Epoch 4/15: [                              ] 2/63 batches, loss: 0.0711Epoch 4/15: [=                             ] 3/63 batches, loss: 0.0650Epoch 4/15: [=                             ] 4/63 batches, loss: 0.0645Epoch 4/15: [==                            ] 5/63 batches, loss: 0.0840Epoch 4/15: [==                            ] 6/63 batches, loss: 0.0780Epoch 4/15: [===                           ] 7/63 batches, loss: 0.0785Epoch 4/15: [===                           ] 8/63 batches, loss: 0.0904Epoch 4/15: [====                          ] 9/63 batches, loss: 0.0950Epoch 4/15: [====                          ] 10/63 batches, loss: 0.0930Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.1013Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.1019Epoch 4/15: [======                        ] 13/63 batches, loss: 0.1105Epoch 4/15: [======                        ] 14/63 batches, loss: 0.1137Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.1104Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.1117Epoch 4/15: [========                      ] 17/63 batches, loss: 0.1164Epoch 4/15: [========                      ] 18/63 batches, loss: 0.1119Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.1106Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.1138Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.1120Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.1099Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.1080Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.1081Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.1079Epoch 4/15: [============                  ] 26/63 batches, loss: 0.1069Epoch 4/15: [============                  ] 27/63 batches, loss: 0.1053Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.1064Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.1047Epoch 4/15: [==============                ] 30/63 batches, loss: 0.1057Epoch 4/15: [==============                ] 31/63 batches, loss: 0.1049Epoch 4/15: [===============               ] 32/63 batches, loss: 0.1057Epoch 4/15: [===============               ] 33/63 batches, loss: 0.1066Epoch 4/15: [================              ] 34/63 batches, loss: 0.1048Epoch 4/15: [================              ] 35/63 batches, loss: 0.1032Epoch 4/15: [=================             ] 36/63 batches, loss: 0.1038Epoch 4/15: [=================             ] 37/63 batches, loss: 0.1017Epoch 4/15: [==================            ] 38/63 batches, loss: 0.1021Epoch 4/15: [==================            ] 39/63 batches, loss: 0.1019Epoch 4/15: [===================           ] 40/63 batches, loss: 0.1009Epoch 4/15: [===================           ] 41/63 batches, loss: 0.1003Epoch 4/15: [====================          ] 42/63 batches, loss: 0.1019Epoch 4/15: [====================          ] 43/63 batches, loss: 0.1018Epoch 4/15: [====================          ] 44/63 batches, loss: 0.1030Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.1033Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.1032Epoch 4/15: [======================        ] 47/63 batches, loss: 0.1032Epoch 4/15: [======================        ] 48/63 batches, loss: 0.1027Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.1024Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.1025Epoch 4/15: [========================      ] 51/63 batches, loss: 0.1025Epoch 4/15: [========================      ] 52/63 batches, loss: 0.1014Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.1018Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.1012Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.1003Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.1004Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.1000Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.1002Epoch 4/15: [============================  ] 59/63 batches, loss: 0.1000Epoch 4/15: [============================  ] 60/63 batches, loss: 0.0992Epoch 4/15: [============================= ] 61/63 batches, loss: 0.1005Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1002Epoch 4/15: [==============================] 63/63 batches, loss: 0.0988
[2025-05-04 11:05:55,715][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0988
[2025-05-04 11:05:55,997][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0724, Metrics: {'mse': 0.07288256287574768, 'rmse': 0.2699677070979929, 'r2': -0.12336277961730957}
[2025-05-04 11:05:55,998][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1978Epoch 5/15: [                              ] 2/63 batches, loss: 0.1462Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1203Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1105Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1070Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1114Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1089Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1100Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1051Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1013Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1011Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.0995Epoch 5/15: [======                        ] 13/63 batches, loss: 0.0983Epoch 5/15: [======                        ] 14/63 batches, loss: 0.0955Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.0940Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.0911Epoch 5/15: [========                      ] 17/63 batches, loss: 0.0895Epoch 5/15: [========                      ] 18/63 batches, loss: 0.0883Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.0866Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.0866Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.0857Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.0833Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.0834Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.0829Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.0832Epoch 5/15: [============                  ] 26/63 batches, loss: 0.0839Epoch 5/15: [============                  ] 27/63 batches, loss: 0.0827Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.0849Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.0860Epoch 5/15: [==============                ] 30/63 batches, loss: 0.0855Epoch 5/15: [==============                ] 31/63 batches, loss: 0.0849Epoch 5/15: [===============               ] 32/63 batches, loss: 0.0836Epoch 5/15: [===============               ] 33/63 batches, loss: 0.0831Epoch 5/15: [================              ] 34/63 batches, loss: 0.0859Epoch 5/15: [================              ] 35/63 batches, loss: 0.0857Epoch 5/15: [=================             ] 36/63 batches, loss: 0.0848Epoch 5/15: [=================             ] 37/63 batches, loss: 0.0840Epoch 5/15: [==================            ] 38/63 batches, loss: 0.0831Epoch 5/15: [==================            ] 39/63 batches, loss: 0.0828Epoch 5/15: [===================           ] 40/63 batches, loss: 0.0825Epoch 5/15: [===================           ] 41/63 batches, loss: 0.0828Epoch 5/15: [====================          ] 42/63 batches, loss: 0.0827Epoch 5/15: [====================          ] 43/63 batches, loss: 0.0834Epoch 5/15: [====================          ] 44/63 batches, loss: 0.0832Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.0833Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.0833Epoch 5/15: [======================        ] 47/63 batches, loss: 0.0841Epoch 5/15: [======================        ] 48/63 batches, loss: 0.0837Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.0832Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.0837Epoch 5/15: [========================      ] 51/63 batches, loss: 0.0834Epoch 5/15: [========================      ] 52/63 batches, loss: 0.0836Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.0840Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.0838Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.0850Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.0848Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.0843Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.0848Epoch 5/15: [============================  ] 59/63 batches, loss: 0.0847Epoch 5/15: [============================  ] 60/63 batches, loss: 0.0850Epoch 5/15: [============================= ] 61/63 batches, loss: 0.0843Epoch 5/15: [============================= ] 62/63 batches, loss: 0.0843Epoch 5/15: [==============================] 63/63 batches, loss: 0.0834
[2025-05-04 11:05:57,980][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0834
[2025-05-04 11:05:58,257][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0674, Metrics: {'mse': 0.06776653230190277, 'rmse': 0.2603200574329661, 'r2': -0.044507622718811035}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.0440Epoch 6/15: [                              ] 2/63 batches, loss: 0.0513Epoch 6/15: [=                             ] 3/63 batches, loss: 0.0605Epoch 6/15: [=                             ] 4/63 batches, loss: 0.0625Epoch 6/15: [==                            ] 5/63 batches, loss: 0.0736Epoch 6/15: [==                            ] 6/63 batches, loss: 0.0700Epoch 6/15: [===                           ] 7/63 batches, loss: 0.0729Epoch 6/15: [===                           ] 8/63 batches, loss: 0.0732Epoch 6/15: [====                          ] 9/63 batches, loss: 0.0750Epoch 6/15: [====                          ] 10/63 batches, loss: 0.0731Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.0759Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.0758Epoch 6/15: [======                        ] 13/63 batches, loss: 0.0760Epoch 6/15: [======                        ] 14/63 batches, loss: 0.0785Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.0779Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.0755Epoch 6/15: [========                      ] 17/63 batches, loss: 0.0763Epoch 6/15: [========                      ] 18/63 batches, loss: 0.0778Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.0775Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.0806Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.0810Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.0812Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.0817Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.0814Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.0815Epoch 6/15: [============                  ] 26/63 batches, loss: 0.0825Epoch 6/15: [============                  ] 27/63 batches, loss: 0.0818Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.0821Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.0846Epoch 6/15: [==============                ] 30/63 batches, loss: 0.0837Epoch 6/15: [==============                ] 31/63 batches, loss: 0.0833Epoch 6/15: [===============               ] 32/63 batches, loss: 0.0829Epoch 6/15: [===============               ] 33/63 batches, loss: 0.0820Epoch 6/15: [================              ] 34/63 batches, loss: 0.0815Epoch 6/15: [================              ] 35/63 batches, loss: 0.0822Epoch 6/15: [=================             ] 36/63 batches, loss: 0.0831Epoch 6/15: [=================             ] 37/63 batches, loss: 0.0819Epoch 6/15: [==================            ] 38/63 batches, loss: 0.0818Epoch 6/15: [==================            ] 39/63 batches, loss: 0.0818Epoch 6/15: [===================           ] 40/63 batches, loss: 0.0814Epoch 6/15: [===================           ] 41/63 batches, loss: 0.0810Epoch 6/15: [====================          ] 42/63 batches, loss: 0.0810Epoch 6/15: [====================          ] 43/63 batches, loss: 0.0820Epoch 6/15: [====================          ] 44/63 batches, loss: 0.0831Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.0831Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.0821Epoch 6/15: [======================        ] 47/63 batches, loss: 0.0826Epoch 6/15: [======================        ] 48/63 batches, loss: 0.0824Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.0817Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.0808Epoch 6/15: [========================      ] 51/63 batches, loss: 0.0819Epoch 6/15: [========================      ] 52/63 batches, loss: 0.0817Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.0814Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.0805Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.0808Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.0806Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.0808Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.0805Epoch 6/15: [============================  ] 59/63 batches, loss: 0.0805Epoch 6/15: [============================  ] 60/63 batches, loss: 0.0804Epoch 6/15: [============================= ] 61/63 batches, loss: 0.0804Epoch 6/15: [============================= ] 62/63 batches, loss: 0.0803Epoch 6/15: [==============================] 63/63 batches, loss: 0.0799
[2025-05-04 11:06:00,682][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0799
[2025-05-04 11:06:00,946][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0679, Metrics: {'mse': 0.0682767778635025, 'rmse': 0.2612982546124304, 'r2': -0.05237221717834473}
[2025-05-04 11:06:00,947][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.0595Epoch 7/15: [                              ] 2/63 batches, loss: 0.0713Epoch 7/15: [=                             ] 3/63 batches, loss: 0.0907Epoch 7/15: [=                             ] 4/63 batches, loss: 0.0865Epoch 7/15: [==                            ] 5/63 batches, loss: 0.0856Epoch 7/15: [==                            ] 6/63 batches, loss: 0.0841Epoch 7/15: [===                           ] 7/63 batches, loss: 0.0819Epoch 7/15: [===                           ] 8/63 batches, loss: 0.0821Epoch 7/15: [====                          ] 9/63 batches, loss: 0.0883Epoch 7/15: [====                          ] 10/63 batches, loss: 0.0838Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.0789Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.0832Epoch 7/15: [======                        ] 13/63 batches, loss: 0.0801Epoch 7/15: [======                        ] 14/63 batches, loss: 0.0807Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.0795Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.0769Epoch 7/15: [========                      ] 17/63 batches, loss: 0.0763Epoch 7/15: [========                      ] 18/63 batches, loss: 0.0743Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.0749Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.0754Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.0743Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.0753Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.0745Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.0742Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.0743Epoch 7/15: [============                  ] 26/63 batches, loss: 0.0728Epoch 7/15: [============                  ] 27/63 batches, loss: 0.0723Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.0719Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.0723Epoch 7/15: [==============                ] 30/63 batches, loss: 0.0718Epoch 7/15: [==============                ] 31/63 batches, loss: 0.0712Epoch 7/15: [===============               ] 32/63 batches, loss: 0.0702Epoch 7/15: [===============               ] 33/63 batches, loss: 0.0697Epoch 7/15: [================              ] 34/63 batches, loss: 0.0693Epoch 7/15: [================              ] 35/63 batches, loss: 0.0687Epoch 7/15: [=================             ] 36/63 batches, loss: 0.0692Epoch 7/15: [=================             ] 37/63 batches, loss: 0.0689Epoch 7/15: [==================            ] 38/63 batches, loss: 0.0700Epoch 7/15: [==================            ] 39/63 batches, loss: 0.0698Epoch 7/15: [===================           ] 40/63 batches, loss: 0.0714Epoch 7/15: [===================           ] 41/63 batches, loss: 0.0717Epoch 7/15: [====================          ] 42/63 batches, loss: 0.0710Epoch 7/15: [====================          ] 43/63 batches, loss: 0.0703Epoch 7/15: [====================          ] 44/63 batches, loss: 0.0700Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.0697Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.0687Epoch 7/15: [======================        ] 47/63 batches, loss: 0.0689Epoch 7/15: [======================        ] 48/63 batches, loss: 0.0684Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.0681Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.0676Epoch 7/15: [========================      ] 51/63 batches, loss: 0.0670Epoch 7/15: [========================      ] 52/63 batches, loss: 0.0668Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.0664Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.0676Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.0677Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.0675Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.0672Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.0684Epoch 7/15: [============================  ] 59/63 batches, loss: 0.0681Epoch 7/15: [============================  ] 60/63 batches, loss: 0.0696Epoch 7/15: [============================= ] 61/63 batches, loss: 0.0695Epoch 7/15: [============================= ] 62/63 batches, loss: 0.0690Epoch 7/15: [==============================] 63/63 batches, loss: 0.0689
[2025-05-04 11:06:02,963][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0689
[2025-05-04 11:06:03,233][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0675, Metrics: {'mse': 0.06766227632761002, 'rmse': 0.26011973459853066, 'r2': -0.042900681495666504}
[2025-05-04 11:06:03,234][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0412Epoch 8/15: [                              ] 2/63 batches, loss: 0.0435Epoch 8/15: [=                             ] 3/63 batches, loss: 0.0552Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0517Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0511Epoch 8/15: [==                            ] 6/63 batches, loss: 0.0489Epoch 8/15: [===                           ] 7/63 batches, loss: 0.0520Epoch 8/15: [===                           ] 8/63 batches, loss: 0.0541Epoch 8/15: [====                          ] 9/63 batches, loss: 0.0602Epoch 8/15: [====                          ] 10/63 batches, loss: 0.0632Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.0665Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.0644Epoch 8/15: [======                        ] 13/63 batches, loss: 0.0639Epoch 8/15: [======                        ] 14/63 batches, loss: 0.0659Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.0653Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.0644Epoch 8/15: [========                      ] 17/63 batches, loss: 0.0647Epoch 8/15: [========                      ] 18/63 batches, loss: 0.0650Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.0653Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.0635slurmstepd: error: *** JOB 64444234 ON k28i22 CANCELLED AT 2025-05-04T11:06:03 ***
